{
    "0": "一级标题：API文档\n二级标题：无\n内容：",
    "1": "一级标题：API文档\n二级标题：Develop\n内容：\n- [开放接口](/api/py-openapi.md)\n- [环境变量](/api/environment-variable.md)",
    "2": "一级标题：API文档\n二级标题：命令行\n内容：\n- [swanlab watch](/api/cli-swanlab-watch.md): 启动离线实验看板\n- [swanlab login](/api/cli-swanlab-login.md): 登录SwanLab\n- [swanlab logout](/api/cli-swanlab-logout.md): 登出SwanLab\n- [swanlab convert](/api/cli-swanlab-convert.md): 将其他产品的日志转换为SwanLab项目\n- [swanlab sync](/api/cli-swanlab-sync.md): 将本地日志同步到SwanLab云端/私有化部署端",
    "3": "一级标题：API文档\n二级标题：Python SDK\n内容：\n- [init](/api/py-init.md)\n- [login](/api/py-login.md)\n- [Image](/api/py-Image.md)\n- [Audio](/api/py-Audio.md)\n- [Text](/api/py-Text.md)\n- [Video](/api/py-video.md)\n- [ECharts](/api/py-echarts.md)\n- [Object3D](/api/py-object3d.md)\n- [Molecule](/api/py-molecule.md)\n- [pr_curve](/api/py-pr_curve.md)\n- [roc_curve](/api/py-roc_curve.md)\n- [confusion_matrix](/api/py-confusion_matrix.md)\n- [run](/api/py-run.md)\n- [convert](/api/py-converter.md)\n- [sync_wandb](/api/py-sync-wandb.md)\n- [sync_tensorboard](/api/py-sync-tensorboard.md)\n- [sync_mlflow](/api/py-sync-mlflow.md)\n- [register_callback](/api/py-register-callback.md)",
    "4": "一级标题：swanlab convert\n二级标题：无\n内容：\n```bash\nswanlab convert [OPTIONS]\n```\n\n| 选项 | 描述 |\n| --- | --- |\n| `-t`, `--type` | 选择转换类型，可选`tensorboard`、`wandb`、`mlflow`，默认为`tensorboard`。 |\n| `-p`, `--project` | 设置转换创建的SwanLab项目名，默认为None。 |\n| `-w`, `--workspace` | 设置SwanLab项目所在空间，默认为None。 |\n| `-l`, `--logdir` | 设置SwanLab项目的日志文件保存路径，默认为None。 |\n| `--cloud` | 设置SwanLab项目是否将日志上传到云端，默认为True。 |\n| `--tb-logdir` | 需要转换的Tensorboard日志文件路径(tfevent) |\n| `--wb-project` | 需要转换的Wandb项目名 |\n| `--wb-entity` | 需要转换的Wandb项目所在实体 |\n| `--wb-runid` | 需要转换的Wandb Run的id |\n| `--mlflow-uri` | 需要转换的MLFlow项目URI |\n| `--mlflow-exp` | 需要转换的MLFlow实验ID |",
    "5": "一级标题：swanlab convert\n二级标题：介绍\n内容：\n将其他日志工具的内容转换为SwanLab项目。\n支持转换的工具包括：`Tensorboard`、`Weights & Biases`、`MLFlow`。",
    "6": "一级标题：swanlab convert\n二级标题：使用案例\n内容：\n### Tensorboard\n\n[集成-Tensorboard](/guide_cloud/integration/integration-tensorboard.md)\n\n### Weights & Biases\n\n[集成-Weights & Biases](/guide_cloud/integration/integration-wandb.md)\n\n### MLFlow\n\n[集成-MLFlow](/guide_cloud/integration/integration-mlflow.md)",
    "7": "一级标题：swanlab login\n二级标题：无\n内容：\n``` bash\nswanlab login [OPTIONS]\n```\n\n| 选项 | 描述 |\n| --- | --- |\n| `-r`, `--relogin` | 重新登录。|\n| `-h`, `--host` | 指定SwanLab服务所在的主机。比如`http://localhost:8000`。|\n| `-k`, `--api-key` | 指定API Key。如果您不喜欢使用命令行来输入 API 密钥，这将允许自动登录。|\n| `-w`, `--web-host` | 指定SwanLab前端所在的Web主机。|",
    "8": "一级标题：swanlab login\n二级标题：介绍\n内容：\n登录SwanLab账号，以同步实验到云端。\n\n执行下面的命令后，如果第一次登录，会让你填写[API_KEY](https://swanlab.cn/settings)：\n\n```bash\nswanlab login\n```\n\n登录过一次后，凭证会保存到本地，并覆盖之前登录过的凭证，无需再次通过`swanlab.login`或`swanlab login`登录。\n\n> 如果你不希望凭证保存在本地，请在python脚本中使用[swanlab.login()](./py-login.md)进行登录。\n\n如果你的电脑不太适合命令行粘贴API Key（比如一些Windows CMD）的方式登录，可以使用：\n\n```bash\nswanlab login -k <api-key>\n```",
    "9": "一级标题：swanlab login\n二级标题：重新登录\n内容：\n如果需要登录一个别的账号，则用下面的命令：\n\n```bash\nswanlab login --relogin\n```\n\n这会让你输入一个新的API Key以重新登录。",
    "10": "一级标题：swanlab login\n二级标题：退出登录\n内容：\n```bash\nswanlab logout\n```",
    "11": "一级标题：swanlab login\n二级标题：登录到私有化服务\n内容：\n```bash\nswanlab login --host <host>\n```",
    "12": "一级标题：swanlab logout\n二级标题：无\n内容：\n```bash\nswanlab logout\n```\n\n在编程环境上退出账号。",
    "13": "一级标题：其他CLI命令\n二级标题：无\n内容：\n- `swanlab -v`：查看SwanLab库版本\n- `swanlab --help`：API帮助",
    "14": "一级标题：swanlab sync\n二级标题：无\n内容：\n```bash\nswanlab sync [options] [logdir]\n```\n\n| 选项 | 描述 |\n| --- | --- |\n| `-k`, `--api-key` | 用于身份验证的API密钥。如果未指定，将使用环境中的默认API密钥。如果指定，将使用此API密钥登录但不会保存密钥。|\n| `-h`, `--host` | 同步日志的主机地址。如果未指定，将使用默认主机(`https://swanlab.cn`)。|\n| `-w`, `--workspace` | 同步日志的工作空间。如果未指定，将使用默认工作空间。|\n| `-p`, `--project` | 同步日志的项目。如果未指定，将使用默认项目。|\n| `-i`, `--id` | 同步日志的实验ID。仅当路径为单个目录时可用。|",
    "15": "一级标题：swanlab sync\n二级标题：介绍\n内容：\n将本地日志，同步上传到SwanLab云端/私有化部署端。",
    "16": "一级标题：swanlab sync\n二级标题：版本对照\n内容：\n> 版本对照仅适用于`swanlab sync`命令\n\n| swanlab库版本 | 特性 | 支持的日志文件 |\n| --- | --- | --- |\n| >=0.6.8 | 支持同步训练异常终端的日志文件；支持`id`参数 | 由`>=0.6.8`版本的swanlab库产生 |\n| <0.6.8 | - | 由`<0.6.8`版本的swanlab库产生 |",
    "17": "一级标题：swanlab sync\n二级标题：命令行示例\n内容：\n找到你需要上传到云端的日志文件目录（默认是`swanlog`下的以`run-`开头的目录），然后执行命令：\n\n```bash\nswanlab sync ./swanlog/run-xxx\n```\n\n::: info\n默认同步到的项目的是日志文件中记录的`project`，即跑该实验时设置的`project`。\n如果想要同步到其他项目，可以使用`-p`选项指定项目。\n:::\n\n看到下面的打印信息，则表示同步成功：\n\n![swanlab sync](./cli-swanlab-sync/console.png)\n\n完成sync操作后，项目上会多出一个新的实验。",
    "18": "一级标题：swanlab sync\n二级标题：Python代码示例\n内容：\n```python\nimport swanlab\n\nswanlab.login(api_key=\"你的API Key\")\n\nswanlab.sync(\n    dir_path=\"./swanlog/run-xxx\",\n    workspace=\"swanlab\",\n    project_name=\"sync_test\",\n)\n```",
    "19": "一级标题：swanlab sync\n二级标题：批量上传\n内容：\n```bash\nswanlab sync ./swanlog/run-*\n```",
    "20": "一级标题：swanlab sync\n二级标题：resume式同步\n内容：\n如果你不希望创建1个新实验，而是在原本的实验上同步（会自行比对数据，增加差异的部分），可以使用`--id`参数：\n\n```bash\nswanlab sync ./swanlog/run-xxx --id <实验ID>\n```\n\n实验ID获取方式见：[恢复实验/断点续训](/guide_cloud/experiment_track/resume-experiment.md)",
    "21": "一级标题：swanlab watch\n二级标题：无\n内容：\n``` bash\nswanlab watch [OPTIONS]\n```\n\n| 选项 | 描述 | 例子 |\n| --- | --- | --- |\n| `-p`, `--port` | 设置实验看板Web服务运行的端口，默认为**5092**。 | `swanlab watch -p 8080`：将实验看板Web服务设置为8080端口 |\n| `-h`, `--host` | 设置实验看板Web服务运行的IP地址，默认为**127.0.0.1**。 | `swanlab watch -h 0.0.0.0`：将实验看板Web服务的IP地址设置为0.0.0.0 |\n| `-l`, `--logdir` | 设置实验看板Web服务读取的日志文件路径，默认为`swanlog`。 | `swanlab watch --logdir ./logs`：将当前目录下的logs文件夹设置为日志文件读取路径 |\n| `--help` | 查看终端帮助信息。 | `swanlab watch --help` |",
    "22": "一级标题：swanlab watch\n二级标题：介绍\n内容：\n本地启动SwanLab[离线看板](/zh/guide_cloud/self_host/offline-board.md)。\n在创建SwanLab实验时（并设置mode=\"local\"），会在本地目录下创建一个日志文件夹（默认名称为`swanlog`），使用`swanlab watch`可以本地离线打开实验看板，查看指标图表和配置。",
    "23": "一级标题：swanlab watch\n二级标题：使用案例\n内容：\n### 打开SwanLab离线看板\n\n首先，我们找到日志文件夹（默认名称为`swanlog`），然后在命令行执行下面的命令：\n\n```bash\nswanlab watch -l [logfile_path]\n```\n\n其中`logfile_path`是日志文件夹的路径，可以是绝对路径或相对路径。如果你的日志文件夹名称是默认的`swanlog`，那么也可以直接用`swanlab watch`启动而无需`-l`选项。\n\n执行命令后，会看到下面的输出：\n```bash{6}\nswanlab watch -l [logfile_path]\n\n*swanlab: Try to explore the swanlab experiment logs in: [logfile_path]\n*swanlab: SwanLab Experiment Dashboard ready in 465ms\n\n        ➜  Local:   http://127.0.0.1:5092\n```\n\n访问提供的URL，即可访问SwanLab离线看板。\n\n### 设置IP和端口号\n\n我们可以通过`-h`参数设置IP，`-p`参数设置端口号。\n比如我们希望能够在本地访问云服务器上的离线看板，那么需要在云服务器上开启实验看板时，设置IP为0.0.0.0：\n\n```bash\nswanlab watch -h 0.0.0.0\n```\n\n如果需要设置端口的话：\n```bash\nswanlab watch -h 0.0.0.0 -p 8080\n```",
    "24": "一级标题：环境变量\n二级标题：无\n内容：\n[⚙️完整环境变量1 -> Github](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/env.py)、[⚙️完整环境变量2 -> Github](https://github.com/SwanHubX/SwanLab-Toolkit/blob/main/swankit/env.py)",
    "25": "一级标题：环境变量\n二级标题：全局配置\n内容：\n| 环境变量 | 描述 | 默认值 |\n| --- | --- | --- |\n| `SWANLAB_SAVE_DIR` | SwanLab 全局文件夹保存的路径 | 用户主目录下的 `.swanlab` 文件夹 |\n| `SWANLAB_LOG_DIR` | SwanLab 解析日志文件保存的路径 | 当前运行目录的 `swanlog` 文件夹 |\n| `SWANLAB_MODE` | SwanLab 的解析模式，涉及操作员注册的回调。目前有三种模式：`local`、`cloud`、`disabled`。**注意：大小写敏感** | `cloud` |",
    "26": "一级标题：环境变量\n二级标题：服务配置\n内容：\n| 环境变量 | 描述 |\n| --- | --- |\n| `SWANLAB_BOARD_PORT` | CLI 离线看板 `swanboard` 服务的端口 |\n| `SWANLAB_BOARD_HOST` | CLI 离线看板 `swanboard` 服务的地址 |\n| `SWANLAB_WEB_HOST` | SwanLab 云端环境的 Web 地址，私有化部署仅需设置此环境变量而无需设置 `SWANLAB_API_HOST` |\n| `SWANLAB_API_HOST` | SwanLab 云端环境的 API 地址 |",
    "27": "一级标题：环境变量\n二级标题：实验配置\n内容：\n| 环境变量 | 描述 |\n| --- | --- |\n| `SWANLAB_PROJ_NAME` | 项目名称，效果等价于 `swanlab.init(project=\"...\")` |\n| `SWANLAB_WORKSPACE` | 工作空间名称，效果等价于 `swanlab.init(workspace=\"...\")` |\n| `SWANLAB_EXP_NAME` | 实验名称，效果等价于 `swanlab.init(experiment_name=\"...\")` |\n| `SWANLAB_RUN_ID` | 实验运行ID，效果等价于 `swanlab.init(id=\"...\")` |\n| `SWANLAB_RESUME` | 是否断点续训，效果等价于 `swanlab.init(resume=...)`，可选值为 `must`、`allow`、`never` |",
    "28": "一级标题：环境变量\n二级标题：登录认证\n内容：\n| 环境变量 | 描述 |\n| --- | --- |\n| `SWANLAB_API_KEY` | 云端 API Key。登录时会首先查找此环境变量，如果不存在，判断用户是否已登录，未登录则进入登录流程。<br>- 如果 `login` 接口传入字符串，此环境变量无效<br>- 如果用户已登录，此环境变量的优先级高于本地存储的登录信息 |",
    "29": "一级标题：环境变量\n二级标题：其他\n内容：\n| 环境变量 | 描述 |\n| --- | --- |\n| `SWANLAB_WEBHOOK` | Webhook 地址。<br> SwanLab 初始化完毕时，如果此环境变量存在，会调用此地址发送消息 |",
    "30": "一级标题：swanlab.Audio\n二级标题：无\n内容：\n[Github源代码](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/modules/audio.py)\n\n```python\nAudio(\n    data_or_path: Union[str, np.ndarray],\n    sample_rate: int = 44100,\n    caption: str = None,\n) -> None\n```\n\n| 参数          | 描述                                                                                                     |\n|-------------|--------------------------------------------------------------------------------------------------------|\n| data_or_path | (Union[str, np.ndarray]) 接收音频文件路径、numpy数组。Audio类将判断接收的数据类型做相应的转换。 |\n| sample_rate | (int) 音频的采样率，默认为44100。                                             |\n| caption     | (str) 音频的标签。用于在实验看板中展示音频时进行标记。                                                      |",
    "31": "一级标题：swanlab.Audio\n二级标题：介绍\n内容：\n对各种类型的音频数据做转换，以被`swanlab.log()`记录。\n\n![](/assets/media-audio-1.jpg)\n\n### 从numpy array创建\n\n记录单个音频：\n\n```python\nimport numpy as np\nimport swanlab\n\nrun = swanlab.init()\n\n# 创建一个numpy array类型的音频\nwhite_noise = np.random.randn(2, 100000)\n# 传入swanlab.Audio，设置采样率\naudio = swanlab.Audio(white_noise, caption=\"white_noise\")\n\nrun.log({\"examples\": audio})\n```\n\n记录多个音频：\n\n```python\nimport numpy as np\nimport swanlab\n\nrun = swanlab.init()\n\n# 创建一个列表\nexamples = []\nfor i in range(3):\n    white_noise = np.random.randn(100000)\n    audio = swanlab.Audio(white_noise, caption=\"audio_{i}\")\n    # 列表中添加swanlab.Audio类型对象\n    examples.append(audio)\n\nrun.log({\"examples\": examples})\n```\n\n### 从文件路径创建\n\n```python\nimport swanlab\n\nrun = swanlab.init()\naudio = swanlab.Audio(\"path/to/file\")\n\nrun.log({\"examples\": audio})\n```",
    "32": "一级标题：swanlab.Image\n二级标题：无\n内容：\n[Github源代码](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/modules/image.py)\n\n```python\nImage(\n    data_or_path: Union[str, np.ndarray, PILImage.Image],\n    mode: str = \"RGB\",\n    caption: str = None,\n    file_type: str = None,\n    size: Union[int, list, tuple] = None,\n) -> None\n```\n\n| 参数        | 描述                                                                                                                                                                   |\n|-----------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| data_or_path | (Union[str, np.ndarray, PILImage.Image]) 接收图像文件路径、numpy数组、或者PIL图像。Image类将判断接收的数据类型做相应的转换。                                      |\n| mode      | (str) 图像的 PIL 模式。最常见的是 \"L\"、\"RGB\"、\"RGBA\"。完整解释请参阅：[Pillow mode](https://pillow.readthedocs.io/en/stable/handbook/concepts.html#modes)                         |\n| caption   | (str) 图像的标签。用于在实验看板中展示图像时进行标记。                                                                                                                 |\n| file_type | (str) 设置图片的格式，可选['png', 'jpg', 'jpeg', 'bmp']，默认为'png'                                                                                                   |\n| size      | (Union[int, list, tuple]) 设置图像的尺寸，默认保持原图尺寸。如果size设置为int类型，如512，将根据最长边不超过512的标准做图像缩放, [size更多用法](#对传入图像做resize)|",
    "33": "一级标题：swanlab.Image\n二级标题：介绍\n内容：\n对各种类型的图像数据做转换，以被`swanlab.log()`记录。\n\n![](/assets/media-image-1.jpg)\n\n### 从numpy array创建\n\n记录单张图像：\n\n```python\nimport numpy as np\nimport swanlab\n\nrun = swanlab.init()\n\n# 1. 创建一个numpy array\nrandom_image = np.random.randint(low=0, high=256, size=(100, 100, 3), dtype=np.uint8)\n# 2. 传入swanlab.Image\nimage = swanlab.Image(random_image, caption=\"random image\")\n\nrun.log({\"examples\": image})\n```\n\n记录多张图像：\n\n```python\nimport numpy as np\nimport swanlab\n\nrun = swanlab.init()\n\n# 创建一个列表\nexamples = []\nfor i in range(3):\n    random_image = np.random.randint(low=0, high=256, size=(100, 100, 3), dtype=np.uint8)\n    image = swanlab.Image(random_image, caption=\"random image\")\n    # 列表中添加swanlab.Image类型对象\n    examples.append(image)\n\n# 记录图列\nrun.log({\"examples\": examples})\n```\n\n### 从PyTorch Tensor创建\n\n`swanlab.Image`支持传入尺寸为[B, C, H, W]与[C, H, W]的Tensor。\n\n```python\nimport torch\nimport swanlab\n\nrun = swanlab.init()\n···\nfor batch, ground_truth in train_dataloader():\n    # 假设batch是尺寸为[16, 3, 256, 256]的tensor\n    tensors = swanlab.Image(batch)\n    run.log({\"examples\": tensors})\n```\n\n\n### 从PIL Image创建\n\n```python\nimport numpy as np\nfrom PIL import Image\nimport swanlab\n\nrun = swanlab.init()\n\n# 创建一个列表\nexamples = []\nfor i in range(3):\n    random_image = np.random.randint(low=0, high=256, size=(100, 100, 3), dtype=np.uint8)\n    pil_image = Image.fromarray(random_image)\n    image = swanlab.Image(pil_image, caption=\"random image\")\n    examples.append(image)\n\nrun.log({\"examples\": examples})\n```\n\n### 从文件路径创建\n\n```python\nimport swanlab\n\nrun = swanlab.init()\nimage = swanlab.Image(\"path/to/file\", caption=\"random image\")\n\nrun.log({\"examples\": image})\n```\n\n`swanlab.Image`在默认情况下，是以`png`的格式做图像转换与存储。\n\n如果想要用`jpg`格式：\n\n```python{3}\nimage = swanlab.Image(\"path/to/file\",\n                      caption=\"random image\",\n                      file_type=\"jpg\")\n```\n\n### 对传入图像做Resize\n\n在默认情况，`swanlab.Image`不对图像做任何尺寸缩放。\n\n如果需要放缩图像，我们可以通过设置`size`参数，来调节图像尺寸。\n\n放缩规则为：\n\n1. 默认: 不对图像做任何缩放\n\n2. `size`为int类型: 如果最长边超过`size`, 则将最长边设为`size`, 另一边等比例缩放; 否则不缩放\n\n3. `size`为list/tuple类型:\n\n    - (int, int): 将图像缩放到宽为size[0], 高为size[1]\n    - (int, None): 将图像缩放到宽为size[0], 高等比例缩放\n    - (None, int): 将缩放缩放到高为size[1], 宽等比例缩放\n\n```python\nprint(im_array.shape)\n# [1024, 512, 3]\n\nim1 = swanlab.Image(im_array, size=512)\n# [512, 256, 3]\n\nim2 = swanlab.Image(im_array, size=(512, 512))\n# [512, 512, 3]\n\nim3 = swanlab.Image(im_array, size=(None, 1024))\n# [2048, 1024, 3]\n\nim4 = swanlab.Image(im_array, size=(256, None))\n# [256, 128, 3]\n```\n\n### 记录Matplotlib图表\n\n```python\nimport swanlab\nimport matplotlib.pyplot as plt\n\n# 定义横纵坐标的数据\nx = [1, 2, 3, 4, 5]\ny = [2, 3, 5, 7, 11]\n\n# plt创建折线图\nplt.plot(x, y)\n\n# 添加标题和标签\nplt.title(\"Examples\")\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")\n\nswanlab.init()\n\n# 记录plt\nswanlab.log({\"example\": swanlab.Image(plt)})\n```",
    "34": "一级标题：swanlab.Text\n二级标题：无\n内容：\n[Github源代码](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/modules/text.py)\n\n```python\nText(\n    data: Union[str],\n    caption: str = None,\n) -> None\n```\n\n| 参数    | 描述                                                              |\n|-------|-----------------------------------------------------------------|\n| data  | (Union[str]) 接收字符串。                                      |\n| caption | (str) 文本的标签。用于在实验看板中对data进行标记。                     |",
    "35": "一级标题：swanlab.Text\n二级标题：介绍\n内容：\n对文本数据做转换，以被`swanlab.log()`记录。\n\n![](./py-text/show.png)\n\n### 记录字符串文本\n\n记录单个字符串文本：\n\n```python{4}\nimport swanlab\n\nswanlab.init()\ntext = swanlab.Text(\"an awesome text.\")\nswanlab.log({\"examples\": text})\n```\n\n记录多个字符串文本：\n\n```python\nimport swanlab\n\nswanlab.init()\n\nexamples = []\nfor i in range(3):\n    text = swanlab.Text(\"an awesome text.\")\n    examples.append(text)\n\nswanlab.log({\"examples\": examples})\n```",
    "36": "一级标题：swanlab.confusion_matrix\n二级标题：无\n内容：\n[Github源代码](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/modules/custom_charts/metrics.py)\n\n```python\nconfusion_matrix(\n    y_true: Union[List, np.ndarray],\n    y_pred: Union[List, np.ndarray],\n    class_names: List[str] = None,\n) -> None\n```\n\n| 参数          | 描述                                                                                                                           |\n|-------------|------------------------------------------------------------------------------------------------------------------------------|\n| y_true      | (Union[List, np.ndarray]) 真实标签，分类问题中的真实类别标签                                                                                    |\n| y_pred      | (Union[List, np.ndarray]) 预测标签，模型预测的类别标签                                                                                        |\n| class_names | (List[str]) 类别名称列表，用于在混淆矩阵中显示类别标签。如果为None，将使用数字索引作为标签                                                           |",
    "37": "一级标题：swanlab.confusion_matrix\n二级标题：介绍\n内容：\n绘制混淆矩阵（Confusion Matrix），用于评估分类模型的性能。混淆矩阵展示了模型预测结果与真实标签之间的对应关系，能够直观地显示各类别的预测准确性和错误类型。\n\n混淆矩阵是评估分类模型性能的基础工具，特别适用于多分类问题。\n\n### 基本用法\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nimport swanlab\n\n# 加载鸢尾花数据集\niris_data = load_iris()\nX = iris_data.data\ny = iris_data.target\nclass_names = iris_data.target_names.tolist()\n\n# 划分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 训练模型\nmodel = xgb.XGBClassifier(objective='multi:softmax', num_class=len(class_names))\nmodel.fit(X_train, y_train)\n\n# 获取预测结果\ny_pred = model.predict(X_test)\n\n# 初始化SwanLab\nswanlab.init(project=\"Confusion-Matrix-Demo\", experiment_name=\"Confusion-Matrix-Example\")\n\n# 记录混淆矩阵\nswanlab.log({\n    \"confusion_matrix\": swanlab.confusion_matrix(y_test, y_pred, class_names)\n})\n\nswanlab.finish()\n```\n\n![](./py-confusion_martix/demo.png)\n\n### 使用自定义类别名称\n\n```python\n# 定义自定义类别名称\ncustom_class_names = [\"类别A\", \"类别B\", \"类别C\"]\n\n# 记录混淆矩阵\nconfusion_matrix = swanlab.confusion_matrix(y_test, y_pred, custom_class_names)\nswanlab.log({\"confusion_matrix_custom\": confusion_matrix})\n```\n\n### 不使用类别名称\n\n```python\n# 不指定类别名称，将使用数字索引\nconfusion_matrix = swanlab.confusion_matrix(y_test, y_pred)\nswanlab.log({\"confusion_matrix_default\": confusion_matrix})\n```\n\n\n### 二分类示例\n\n```python\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nimport swanlab\n\n# 生成二分类数据\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# 训练模型\nmodel = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\nmodel.fit(X_train, y_train)\n\n# 获取预测结果\ny_pred = model.predict(X_test)\n\n# 记录混淆矩阵\nswanlab.log({\n    \"confusion_matrix\": swanlab.confusion_matrix(y_test, y_pred, [\"负类\", \"正类\"])\n})\n```\n\n### 注意事项\n\n1. **数据格式**: `y_true`和`y_pred`可以是列表或numpy数组\n2. **多分类支持**: 此函数支持二分类和多分类问题\n3. **类别名称**: `class_names`的长度应该与类别数量一致\n4. **依赖包**: 需要安装`scikit-learn`和`pyecharts`包\n5. **坐标轴**: sklearn的confusion_matrix左上角为(0,0)，在pyecharts的heatmap中是左下角，函数会自动处理坐标转换\n6. **矩阵解读**: 混淆矩阵中，行表示真实标签，列表示预测标签",
    "38": "一级标题：swanlab.converter\n二级标题：无\n内容：\n将其他日志工具的内容转换为SwanLab项目的API。\n\n- [swanlab.converter.TFBConverter](/guide_cloud/integration/integration-tensorboard)\n- [swanlab.converter.WandbConverter](/guide_cloud/integration/integration-wandb)\n- [swanlab.converter.MLFlowConverter](/guide_cloud/integration/integration-mlflow)",
    "39": "一级标题：swanlab.echarts\n二级标题：无\n内容：\n<!--@include: @zh/shared/custom-charts.md-->\n\n<!--@include: @zh/shared/custom-charts-3d.md-->",
    "40": "一级标题：swanlab.init\n二级标题：无\n内容：\n```python\ninit(\n    project: str = None,\n    workspace: str = None,\n    experiment_name: str = None,\n    description: str = None,\n    tags: List[str] = None,\n    config: Union[dict, str] = None,\n    logdir: str = None,\n    mode: str = \"cloud\",\n    load: str = None,\n    public: bool = None,\n    callbacks: list = None,\n    settings: Settings = None,\n    id: str = None,\n    resume: Union[Literal['must', 'allow', 'never'], bool] = None,\n    reinit: bool = None,\n    **kwargs,\n)\n```\n\n| 参数         | 描述 |\n|-------------|------|\n| project |(str)项目名，如果不指定则取运行目录的名称。|\n| workspace |(str)工作空间，默认将实验同步到你的个人空间下，如果要上传到组织，则填写组织的username。|\n| experiment_name | (str) 实验名称, 如果不指定则取\"swan-1\"这样的`动物名+序号`作为实验名。 |\n| tags       | (list) 实验标签。可以传入多个字符串组成的列表，标签会显示在实验顶部的标签栏。|\n| description   | (str) 实验描述, 如果不指定默认为None。                                   |\n| config       | (dict, str) 实验配置，在此处可以记录一些实验的超参数等信息。支持传入配置文件路径，支持yaml和json文件。                   |\n| logdir       | (str) 离线看板日志文件存储路径，默认为`swanlog `。                                 |\n| mode       | (str) 设置swanlab实验创建的模式，可选\"cloud\"、\"local\"、\"offline\"、\"disabled\"，默认设置为\"cloud\"。<br>`cloud`：将实验上传到云端。（公有云和私有化部署）<br>`offline`：仅将实验数据保存到本地。<br>`local`：不上传到云端，但会记录实验数据和一些可被`swanlab watch`打开的数据到本地。<br>`disabled`：不上传也不记录。|\n| load       | (str) 加载的配置文件路径，支持yaml和json文件。|\n| public       | (bool) 设置使用代码直接创建SwanLab项目的可见性，默认为False即私有。|\n| callbacks       | (list) 设置实验回调函数，支持`swankit.callback.SwanKitCallback`的子类。|\n| name       | (str) 与experiment_name效果一致，优先级低于experiment_name。|\n| notes       | (str) 与description效果一致，优先级低于description。|\n| settings       | (dict) 实验配置。支持传入1个`swanlab.Settings`对象。|\n| id       | (str) 上次实验的运行ID，用于恢复上次实验。ID必须为21位字符串。|\n| resume       | (str) 断点续训模式，可选True、False、\"must\"、\"allow\"、\"never\"，默认取None。<br>`True`： 效果同`resume=\"allow\"`。<br>`False`：效果同`resume=\"never\"`。<br>`must`：你必须传递 `id` 参数，并且实验必须存在。<br>`allow`：如果存在实验，则会resume该实验，否则将创建新的实验。<br>`never`：你不能传递 `id` 参数，将会创建一个新的实验。(即不开启resume的效果)|\n| reinit       | (bool) 是否重新创建实验，如果为True，则每次调用`swanlab.init()`时，会把上一次实验`finish`掉；默认取None。|",
    "41": "一级标题：swanlab.init\n二级标题：介绍\n内容：\n- 在机器学习训练流程中，我们可以将`swandb.init()`添加到训练脚本和测试脚本的开头，SwanLab将跟踪机器学习流程的每个环节。\n\n- `swanlab.init()`会生成一个新的后台进程来将数据记录到实验中，默认情况下，它还会将数据同步到swanlab.cn，以便你可以在线实时看到可视化结果。\n\n- 在使用`swanlab.log()`记录数据之前，需要先调用`swanlab.init()`：\n\n```python\nimport swanlab\n\nswanlab.init()\nswanlab.log({\"loss\": 0.1846})\n```\n\n- 调用`swanlab.init()`会返回一个`SwanLabRun`类型的对象，同样可以执行`log`操作：\n\n```python\nimport swanlab\n\nrun = swanlab.init()\nrun.log({\"loss\": 0.1846})\n```\n\n- 在脚本运行结束时，我们将自动调用`swanlab.finish`来结束SwanLab实验。但是，如果从子进程调用`swanlab.init()`，如在jupyter notebook中，则必须在子进程结束时显式调用`swanlab.finish`。\n\n```python\nimport swanlab\n\nswanlab.init()\nswanlab.finish()\n```",
    "42": "一级标题：swanlab.init\n二级标题：更多用法\n内容：\n### 设置项目、实验名、描述\n\n```python\nswanlab.init(\n    project=\"cats-detection\",\n    experiment_name=\"YoloX-baseline\",\n    description=\"YoloX检测模型的基线实验，主要用于后续对比。\",\n)\n```\n\n### 设置标签\n\n```python\nswanlab.init(\n    tags=[\"yolo\", \"detection\", \"baseline\"]\n)\n```\n\n### 设置日志文件保存位置\n\n下面的代码展示了如何将日志文件保存到自定义的目录下：\n\n```python\nswanlab.init(\n    logdir=\"path/to/my_custom_dir\",\n)\n```\n\n### 将实验相关的元数据添加到实验配置中\n\n```python\nswanlab.init(\n    config={\n        \"learning-rate\": 1e-4,\n        \"model\": \"CNN\",\n    }\n)\n\n```\n\n### 上传到组织\n\n```python\nswanlab.init(\n    workspace=\"[组织的username]\"\n)\n```\n\n### 插件\n\n关于插件的更多信息，请参考[插件](/plugin/plugin-index.md)。\n\n```python\nfrom swanlab.plugin.notification import EmailCallback\n\nemail_callback = EmailCallback(...)\n\nswanlab.init(\n    callbacks=[email_callback]\n)\n```\n\n### 断点续训\n\n断点续训的意思是，如果你之前有一个状态为`完成`或`中断`的实验，需要补一些实验数据，那么你可以通过`resume`和`id`参数来恢复这个实验。\n\n```python\nswanlab.init(\n    resume=True,\n    id=\"14pk4qbyav4toobziszli\",  # id必须为21位字符串\n)\n```\n\n实验id可以在实验的「环境」选项卡或URL中找到，必须为1个21位字符串。\n\n\n:::tip resume使用场景\n\n1. 之前的训练进程断了，基于checkpoint继续训练时，希望实验图表能和之前的swanlab实验续上，而非创建1个新swanlab实验\n2. 训练和评估分为了两个进程，但希望评估和训练记录在同一个swanlab实验中\n3. config中有一些参数填写有误，希望更新config参数\n\n:::\n\n:::warning ⚠️注意\n\n1. 由项目克隆产生的实验，不能被resume\n\n:::\n\n\n断点续训可以选择三种模式：\n\n1. `allow`：如果项目下存在`id`对应的实验，则会resume该实验，否则将创建新的实验。\n2. `must`：如果项目下存在`id`对应的实验，则会resume该实验，否则将报错\n3. `never`：不能传递 `id` 参数，将会创建一个新的实验。(即不开启resume的效果)\n\n::: info\n`resume=True` 效果同 `resume=\"allow\"`。<br>\n`resume=False` 效果同 `resume=\"never\"`。\n:::\n\n测试代码：\n\n```python\nimport swanlab\n\nrun = swanlab.init()\nswanlab.log({\"loss\": 2, \"acc\":0.4})\nrun.finish()\n\nrun = swanlab.init(resume=True, id=run.id)\nswanlab.log({\"loss\": 0.2, \"acc\": 0.9})\n```",
    "43": "一级标题：swanlab.init\n二级标题：过期参数\n内容：\n- `cloud`：在v0.3.4被`mode`参数取代。参数仍然可用，且会覆盖掉`mode`的设置。",
    "44": "一级标题：swanlab.integration\n二级标题：无\n内容：\n[源代码](https://github.com/SwanHubX/SwanLab/tree/main/swanlab/integration)\n\nSwanLab与外部项目的集成API。\n\n- [swanlab.integration.accelerate](/guide_cloud/integration/integration-huggingface-accelerate.md)\n- [swanlab.integration.fastai](/guide_cloud/integration/integration-fastai.md)\n- [swanlab.integration.keras](/guide_cloud/integration/integration-keras.md)\n- [swanlab.integration.lightgbm](/guide_cloud/integration/integration-lightgbm.md)\n- [swanlab.integration.mmengine](/guide_cloud/integration/integration-mmengine.md)\n- [swanlab.integration.pytorch_lightning](/guide_cloud/integration/integration-pytorch-lightning.md)\n- [swanlab.integration.sb3](/guide_cloud/integration/integration-sb3.md)\n- [swanlab.integration.torchtune](/guide_cloud/integration/integration-pytorch-torchtune.md)\n- [swanlab.integration.transformers](/guide_cloud/integration/integration-huggingface-transformers.md)\n- [swanlab.integration.ultralytics](/guide_cloud/integration/integration-ultralytics.md)\n- [swanlab.integration.xgboost](/guide_cloud/integration/integration-xgboost.md)",
    "45": "一级标题：log\n二级标题：无\n内容：\n[Github源代码](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/sdk.py)\n\n```python\nlog(\n    data: Dict[str, DataType],\n    step: int = None,\n    print_to_console: bool = False,\n)\n```\n\n| 参数   | 描述                                       |\n|--------|------------------------------------------|\n| data   | (Dict[str, DataType]) 必须。传入一个键值对字典，key为指标名，value为指标值。value支持int、float、可被float()转换的类型、或任何`BaseType`类型。 |\n| step   | (int) 可选，该参数设置了data的步数。如不设置step，则将以0开始，后续每1次step累加1。 |\n| print_to_console | (bool) 可选，默认值为False。当设置为True时，会将data的key和value以字典的形式打印到终端。 |",
    "46": "一级标题：log\n二级标题：介绍\n内容：\n`swanlab.log`是指标记录的核心API，使用它记录实验中的数据，例如标量、图像、音频和文本。\n\n最基本的用法是如下面代码所示，这将会将准确率与损失值记录到实验中，生成可视化图表并更新这些指标的汇总值（summary）。：\n\n```python\nswanlab.log({\"acc\": 0.9, \"loss\":0.1462})\n```\n\n除了标量以外，`swanlab.log`支持记录多媒体数据，包括图像、音频、文本等，并在UI上有很好的显示效果。",
    "47": "一级标题：log\n二级标题：打印传入的字典\n内容：\n`swanlab.log`支持打印传入的`data`的`key`和`value`到终端，默认情况下不打印。要开启打印的话，需要设置`print_to_console=True`。\n\n```python\nswanlab.log({\"acc\": 0.9, \"loss\":0.1462}, print_to_console=True)\n```\n\n当然，你也可以用这种方式打印：\n\n```python\nprint(swanlab.log({\"acc\": 0.9, \"loss\":0.1462}))\n```",
    "48": "一级标题：log\n二级标题：更多用法\n内容：\n- 记录[图像](/api/py-Image.md)\n- 记录[音频](/api/py-Audio.md)\n- 记录[文本](/api/py-Text.md)",
    "49": "一级标题：swanlab.login\n二级标题：无\n内容：\n``` bash\nlogin(\n    api_key: str = None,\n    host: str = None,\n    web_host: str = None,\n    save: bool = False\n):\n```\n\n| 参数 | 描述 |\n| --- | --- |\n| `api_key` | (str) 身份验证密钥，如果未提供，密钥将从密钥文件中读取。|\n| `host` | (str) SwanLab服务所在的API主机，如果未提供，将使用默认主机（即云端版）|\n| `web_host` | (str) SwanLab服务所在的Web主机，如果未提供，将使用默认主机（即云端版）|\n| `save` | (bool) 是否将API密钥保存到密钥文件中，默认值为False。|",
    "50": "一级标题：swanlab.login\n二级标题：介绍\n内容：\n在Python代码中登录SwanLab账号，以将实验上传到指定的云端服务器。API Key从你的SwanLab「设置」-「常规」页面中获取。",
    "51": "一级标题：swanlab.login\n二级标题：登录到公有云\n内容：\n```python\nimport swanlab\n\nswanlab.login(api_key='your-api-key', save=True)\n```\n\n默认将登录到`swanlab.cn`，即SwanLab公有云服务。\n\n如果需要登录到其他主机，可以指定`host`参数，如`http://localhost:8000`。\n\n将`save`参数设置为`True`，会将登录凭证保存到本地（会覆盖之前保存的凭证），无需再次通过`swanlab.login`或`swanlab login`登录。\n\n**如果你在公共机器上使用，请将`save`参数设置为`False`**，这样不会泄露你的API Key，也避免其他人不小心上传数据到你的空间。",
    "52": "一级标题：swanlab.login\n二级标题：登录到私有化服务\n内容：\n```python\nswanlab.login(api_key='your-api-key', host='your-private-host')\n```",
    "53": "一级标题：swanlab.Molecule\n二级标题：无\n内容：\n[源代码](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/modules/object3d/molecule.py)\n\n| 参数        | 描述       |\n|-----------|------------------------------------------------------------------------------------------------|\n| pdb_data | (str) 接收的PDB数据（字符串形式）                                 |\n| caption   | (str) 分子对象的标签。用于在实验看板中展示分子对象时进行标记。                |",
    "54": "一级标题：swanlab.Molecule\n二级标题：简介\n内容：\n对各种类型的生物化学分子做转换，以被`swanlab.log()`记录。\n\n![molecule gif](/assets/molecule.gif)",
    "55": "一级标题：swanlab.Molecule\n二级标题：从RDKit Mol对象创建\n内容：\n```python\nfrom rdkit import Chem\nimport swanlab\n\nmol = Chem.MolFromSmiles(\"CCO\")\nmolecule = swanlab.Molecule.from_mol(mol, caption=\"Ethanol\")\n\nswanlab.init(project=\"molecule_demo\")\nswanlab.log({\"molecule\": molecule})\n```",
    "56": "一级标题：swanlab.Molecule\n二级标题：从PDB文件创建\n内容：\n```python\nimport swanlab\n\nmolecule = swanlab.Molecule.from_pdb(\"path/to/your/pdb/file.pdb\")\n\nswanlab.init(project=\"molecule_demo\")\nswanlab.log({\"molecule\": molecule})\n```",
    "57": "一级标题：swanlab.Molecule\n二级标题：从SDF文件创建\n内容：\n```python\nimport swanlab\n\nmolecule = swanlab.Molecule.from_sdf(\"path/to/your/sdf/file.sdf\")\n\nswanlab.init(project=\"molecule_demo\")\nswanlab.log({\"molecule\": molecule})\n```",
    "58": "一级标题：swanlab.Molecule\n二级标题：从SMILES字符串创建\n内容：\n```python\nimport swanlab\n\nmolecule = swanlab.Molecule.from_smiles(\"CCO\")\n\nswanlab.init(project=\"molecule_demo\")\nswanlab.log({\"molecule\": molecule})\n```",
    "59": "一级标题：swanlab.Molecule\n二级标题：从MOL文件创建\n内容：\n```python\nimport swanlab\n\nmolecule = swanlab.Molecule.from_mol(\"path/to/your/mol/file.mol\")\n\nswanlab.init(project=\"molecule_demo\")\nswanlab.log({\"molecule\": molecule})\n```",
    "60": "一级标题：swanlab.Object3D\n二级标题：无\n内容：\n[Github源代码](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/modules/object3d/object3d.py)\n\n| 参数        | 描述   |\n|-----------|---------------|\n| data | (Union[np.ndarray, str, Path]) 接收点云文件路径、numpy数组。Object3D类将判断接收的数据类型做相应的转换。                                      |              |\n| caption   | (str) 3D对象的标签。用于在实验看板中展示3D对象时进行标记。                                                                                                                 |",
    "61": "一级标题：swanlab.Object3D\n二级标题：介绍\n内容：\n对各种类型的点云数据做转换，以被`swanlab.log()`记录。\n\n![](./py-object3d/demo.png)\n\n### 从文件/字典创建\n\n::: warning 示例文件\ndata.swanlab.pts.json：[Google Drive下载](https://drive.google.com/file/d/1mFill-BXw3cirPHwIHndb1wNX4pWvSXb/view)\n:::\n\n文件的格式为`json`，内容格式如下：\n\n```json\n{\n    \"points\": [\n        [x1, y1, z1, r1, g1, b1],\n        [x2, y2, z2, r2, g2, b2],\n        ...\n    ],\n    // （可选）检测框，用于点云检测等任务，会框住对应位置\n    \"boxes\": [\n        {\n            \"color\": [r, g, b],\n            \"corners\": [[x1,y1,z1], ..., [x8,y8,z8]],\n            // （可选）检测框的标签文本，会在视图中显示\n            \"label\": \"class_name\",\n            // （可选）置信度，会在视图中显示\n            \"score\": 0.95,\n        },\n        ...\n    ]\n}\n```\n\n**json文件参数详细解释：**\n\n* **`points`**：\n    * 这是一个数组，用于存储3D点云数据。\n    * 每个元素都是一个包含6个数值的数组 `[x, y, z, r, g, b]`，分别代表：\n        * `x`, `y`, `z`：点的三维坐标。\n        * `r`, `g`, `b`：点的颜色，分别代表红、绿、蓝三个通道的数值，通常取值范围为0-255。\n\n* **`boxes`**（可选）：\n    * 这是一个数组，用于存储3D检测框数据。\n    * 每个元素都是一个对象，代表一个检测框，包含以下字段：\n        * **`color`**：检测框的颜色，`[r, g, b]` 数组，代表红、绿、蓝三个通道的数值。\n        * **`corners`**：检测框的八个顶点坐标，`[[x1, y1, z1], ..., [x8, y8, z8]]` 数组，每个元素是一个三维坐标 `[x, y, z]`。\n        * **`label`**（可选）：检测框的标签文本，字符串类型，用于在视图中显示检测框的类别。\n        * **`score`**（可选）：检测框的置信度，数值类型，通常取值范围为0-1，用于表示检测框的可靠程度。\n\n---\n\n使用SwanLab从`json`文件中记录3D点云数据：\n\n::: code-group\n\n```python [Object3D]\nimport swanlab\n\nswanlab.init()\n\nobj = swanlab.Object3D(\"data.swanlab.pts.json\", caption=\"3d_point_cloud\")\nswanlab.log({\"examples\": obj})\n```\n\n```python [Object3D.from_point_data]\nimport swanlab\n\nswanlab.init()\n\nwith open(\"data.swanlab.pts.json\", \"r\") as f:\n    cloud_point = json.load(f)\n\nobj = swanlab.Object3D.from_point_data(\n    points=cloud_point[\"points\"],\n    boxes=cloud_point[\"boxes\"],\n    caption=\"3d_point_cloud\"\n)\n\nswanlab.log({\"examples\": obj})\n```\n:::\n\n\n<video controls src=\"./py-object3d/video.mp4\"></video>\n\n<br>\n\n### 从numpy数组创建\n\n::: code-group\n\n```python [从坐标创建]\nimport numpy as np\n\n# Example 1: Create point cloud from coordinates\npoints_xyz = np.array([\n    [0, 0, 0],  # Point1: x=0, y=0, z=0\n    [1, 1, 1],  # Point2: x=1, y=1, z=1\n    [2, 0, 1]   # Point3: x=2, y=0, z=1\n])\n\ncloud_xyz = swanlab.Object3D(points_xyz, caption=\"Basic XYZ Points\")\nswanlab.log({\"examples\": cloud_xyz})\n```\n\n```python [从坐标和类别创建]\nimport numpy as np\n\n# Example 2: Create point cloud with categories\npoints_xyzc = np.array([\n    [0, 0, 0, 0],  # Point1: xyz + category 0\n    [1, 1, 1, 1],  # Point2: xyz + category 1\n    [2, 0, 1, 2]   # Point3: xyz + category 2\n])\n\ncloud_xyzc = swanlab.Object3D(points_xyzc, caption=\"Points with Categories\")\nswanlab.log({\"examples\": cloud_xyzc})\n```\n\n```python [从坐标和RGB创建]\nimport numpy as np\n\n# Example 3: Create point cloud with RGB colors\npoints_xyzrgb = np.array([\n    [0, 0, 0, 255, 0, 0],    # Point1: xyz + red\n    [1, 1, 1, 0, 255, 0],    # Point2: xyz + green\n    [2, 0, 1, 0, 0, 255]     # Point3: xyz + blue\n])\n\ncloud_xyzrgb = swanlab.Object3D(points_xyzrgb, caption=\"Colored Points\")\nswanlab.log({\"examples\": cloud_xyzrgb})\n```\n:::\n\n### 单步记录多个点云\n\n```python\nimport swanlab\n\n...\n\ncloud1 = swanlab.Object3D(points1, caption=\"cloud1\")\ncloud2 = swanlab.Object3D(points2, caption=\"cloud2\")\ncloud3 = swanlab.Object3D(points3, caption=\"cloud3\")\n\n...\n\nswanlab.log({\"examples\": [cloud1, cloud2, cloud3, ...]})\n```",
    "62": "一级标题：swanlab.OpenApi\n二级标题：无\n内容：\n基于 SwanLab 云端功能, 在 SDK 端提供访问 **开放 API（OpenAPI）** 的能力, 允许用户通过编程方式在本地环境中操作云端 **实验/项目/工作空间** 资源。\n\n![](./py-openapi/logo.jpg)\n\n通过开放 API 的形式, 用户可以在本地编程环境中:\n\n- 获取实验数据、个人信息、工作空间信息、项目列表等\n- 进行实验的自动管理（如查询、组织、元数据编辑等）\n- 更方便地与其他工具集成（如 CI/CD、实验调度等）\n\n利用好此特性可极大提升 SDK 的灵活性和可扩展性, 方便构建高级用法或扩展体系",
    "63": "一级标题：swanlab.OpenApi\n二级标题：支持的API列表\n内容：\n下表列出了SwanLab OpenAPI支持的所有方法，点击API名称可跳转到详细说明：\n\n| API名称 | 分类 | 功能描述 | Ready |\n|---------|------|----------|------|\n| [`list_workspaces`](#list-workspaces) | WorkSpace | 获取当前用户的所有工作空间(组织)列表 | ✅ |\n| [`list_projects`](#list-projects) | Project | 获取指定工作空间下的所有项目列表 | ✅ |\n| [`delete_project`](#delete-project) | Project | 删除一个项目 | ✅ |\n| [`list_experiments`](#list-experiments) | Experiment | 获取指定项目下的所有实验列表 | ✅ |\n| [`get_experiment`](#get-experiment) | Experiment | 获取一个实验的详细信息（实验名、配置、环境等） | ✅ |\n| [`get_summary`](#get-summary) | Experiment | 获取一个实验的Summary信息，包含实验跟踪指标的最终值和最大最小值 | ✅ |\n| [`get_metrics`](#get-metrics) | Experiment | 获取一个实验指标的值 |  ✅ |\n| [`delete_experiment`](#delete-experiment) | Experiment | 删除一个实验 | ✅ |",
    "64": "一级标题：swanlab.OpenApi\n二级标题：介绍\n内容：\n> 前置条件：需要在编程环境下登录过SwanLab账号。\n\n要使用 SwanLab 的开放 API, 只需实例化一个 `OpenApi` 对象。\n\n```python\nfrom swanlab import OpenApi\n\nmy_api = OpenApi() # 使用本地登录信息\nprint(my_api.list_workspaces().data) # 获取当前用户的工作空间列表\n```\n\n如果你需要获取其他用户的数据：\n```python\nfrom swanlab import OpenApi\n\nother_api = OpenApi(api_key='other_api_key') # 使用另一个账户的api_key\nprint(other_api.list_workspaces().data)\n```\n\n\n具体来说, **OpenApi**的认证逻辑如下：\n\n1. 如果显式提供了`api_key`参数, 则优先使用该`api_key`进行身份认证, 可以在[这里](https://swanlab.cn/space/~/settings)查看自己的 API 密钥；\n2. 否则,使用本地的认证信息。",
    "65": "一级标题：swanlab.OpenApi\n二级标题：常用参数\n内容：\n### 实验ID `exp_id`\n\n实验的唯一标识符**CUID**, 即`exp_id`, 可通过`list_experiments`方法获取对应的`cuid`字段\n\n要查看某一个实验的CUID, 可在云端版网页的\"环境\"标签页查看\"实验ID\"一行, 点击即可复制此实验的CUID\n\n![](./py-openapi/exp_id.png)\n\n### 工作空间名 `username`\n\n工作空间名即`username`, 用于标识用户所在的工作空间:\n\n- 若为个人空间, `username`即为用户的用户名\n- 若为组织空间, `username`为该组织的组织ID\n\n`username`可以通过`list_workspaces`方法获取, 返回的工作空间列表中每个元素的`username`字段即为工作空间名\n\n一般的, 若在开放API调用中不指定`username`, 则**默认**为当前用户的个人空间",
    "66": "一级标题：swanlab.OpenApi\n二级标题：模型定义\n内容：\n在使用开放 API 时, 获取到的部分云端资源组成较为复杂, 如实验、项目等, 难以用简单的Python数据类型表示\n\n因此, 这些资源在开放API的返回值中被定义为了对象, 支持 IDE 的自动补全与类型检查, 从而方便用户进行操作\n\n例如, 要获取一个实验对象的开始时间, 可以用:\n\n```python\napi_response: ApiResponse = my_api.get_experiment(project=\"project1\", exp_cuid=\"cuid1\")\nmy_exp: Experiment = api_response.data\ncreated_time: str = my_exp.createdAt\n```\n\n或者, 要获取一个项目对象所属工作空间的名字, 可以用:\n\n```python\napi_response: ApiResponse = my_api.list_projects()\nmy_project: Project = api_response.data[0]\nworkspace_name: str = my_project.group[\"name\"]\n```\n\n对于一个模型, 其属性可通过以下三种方式访问:\n\n- `my_exp.createdAt`\n- `my_exp[\"createdAt\"]`\n- `my_exp.get(\"createdAt\")`\n\n> Note: 模型可以通过字典风格访问, 但不是真正的字典, 可以通过`my_exp_dict: Dict = my_exp.model_dump()`获取此时模型对应的字典\n\n### API 响应 `ApiResponse`\n\n开放 API 方法返回`swanlab.api.openapi.types.ApiResponse`对象, 包含以下字段:\n\n| 字段 | 类型 |描述 |\n| --- | --- | --- |\n| `code` | `int` | HTTP 状态码 |\n| `errmsg` | `str` | 错误信息, 如果状态码不为`2XX`则非空 |\n| `data` | `Any` | 返回的具体数据, 下面API文档中提到的返回值即为该字段 |\n\n### 实验模型 `Experiment`\n\n实验对象的类型为`swanlab.api.openapi.types.Experiment`, 包含以下字段:\n\n| 字段 | 类型 | 描述 |\n| --- | --- | --- |\n| `cuid` | `str` | 实验CUID, 唯一标识符 |\n| `name` | `str` | 实验名 |\n| `description` | `str` | 实验描述 |\n| `state` | `str` | 实验状态, `FINISHED` 或 `RUNNING` |\n| `show` | `bool` | 显示状态 |\n| `createdAt` | `str` | 创建时间, 格式如 `2024-11-23T12:28:04.286Z` |\n| `finishedAt` | `str` | 完成时间, 格式如 `2024-11-23T12:28:04.286Z`, 若不存在则为 None |\n| `user` | `Dict[str, str]` | 实验创建者, 包含 `username` 与 `name` |\n| `profile` | `dict` | 详细包含了实验的所有配置信息, 如用户自定义配置与Python运行环境等 |\n\n### 项目模型 `Project`\n\n项目对象的类型为`swanlab.api.openapi.types.Project`, 包含以下字段:\n\n| 字段 | 类型 | 描述 |\n| --- | --- | --- |\n| `cuid` | `str` | 项目CUID, 唯一标识符 |\n| `name` | `str` | 项目名 |\n| `description` | `str` | 项目描述 |\n| `visibility` | `str` | 可见性, `PUBLIC` 或 `PRIVATE` |\n| `createdAt` | `str` | 创建时间, 格式如 `2024-11-23T12:28:04.286Z` |\n| `updatedAt` | `str` | 更新时间, 格式如 `2024-11-23T12:28:04.286Z` |\n| `group` | `Dict[str, str]` | 工作空间信息, 包含 `type`, `username`, `name` |\n| `count` | `Dict[str, int]` | 项目的统计信息, 如实验个数, 协作者数量等 |",
    "67": "一级标题：swanlab.OpenApi\n二级标题：OpenAPIs\n内容：\n每个开放 API 都是`OpenApi`对象的一个方法\n\n下面是所有可用的SwanLab 开放 API\n\n### WorkSpace\n\n#### `list_workspaces`\n\n获取当前用户的所有工作空间(组织)列表。\n\n**返回值**\n\n`data` `(List[Dict])`: 用户加入的工作空间列表, 每个元素是一个字典, 包含工作空间的基础信息:\n\n| 字段 | 类型 | 描述 |\n| --- | --- | --- |\n| `name` | `str` | 工作空间名称 |\n| `username` | `str` | 工作空间唯一标识(用于组织相关的 URL) |\n| `role` | `str` | 用户在该工作空间中的角色, 为 `OWNER` 或 `MEMBER` |\n\n**示例**\n\n::: code-group\n\n```python [获取工作区列表]\nfrom swanlab import OpenApi\nmy_api = OpenApi()\n\nmy_api.list_workspaces().data\n\"\"\"\n[\n    {\n        \"name\": \"workspace1\",\n        \"username\": \"kites-test3\",\n        \"role\": \"OWNER\"\n    },\n    {\n        \"name\": \"hello-openapi\",\n        \"username\": \"kites-test2\",\n        \"role\": \"MEMBER\"\n    }\n]\n\"\"\"\n```\n\n```python [获取第一个工作区名称]\nfrom swanlab import OpenApi\nmy_api = OpenApi()\n\nmy_api.list_workspaces().data[0][\"name\"]\n\"\"\"\n\"workspace1\"\n\"\"\"\n```\n\n```python [获取响应状态码]\nfrom swanlab import OpenApi\nmy_api = OpenApi()\n\nmy_api.list_workspaces().code\n\"\"\"\n200\n\"\"\"\n```\n\n:::\n\n<br>\n\n---\n\n### Experiment\n\n#### `list_experiments`\n\n获取指定项目下的所有实验列表\n\n**方法参数**\n\n| 参数  | 类型 | 描述 |\n| --- | --- | --- |\n| `project` | `str` | 项目名 |\n| `username` | `str` | 工作空间名, 默认为用户个人空间 |\n\n**返回值**\n\n`data` `(List[Experiment])`: 包含实验[(Experiment)](#实验模型-experiment)对象的列表\n\n**示例**\n\n::: code-group\n\n```python [获取实验列表]\nmy_api.list_experiments(project=\"project1\").data\n\"\"\"\n[\n    {\n        \"cuid\": \"cuid1\",\n        \"name\": \"experiment1\",\n        \"description\": \"Description 1\",\n        \"state\": \"RUNNING\",\n        \"show\": true,\n        \"createdAt\": \"2024-11-23T12:28:04.286Z\",\n        \"finishedAt\": null,\n        \"user\": {\n            \"username\": \"kites-test3\",\n            \"name\": \"Kites Test\"\n        },\n        \"profile\": {\n            \"config\": {\n                \"lr\": 0.001,\n                \"epochs\": 10\n            }\n        }\n    },\n    ...\n]\n\"\"\"\n```\n\n```python [获取第一个实验的CUID]\nmy_api.list_experiments(project=\"project1\").data[0].cuid\n\"\"\"\n\"cuid1\"\n\"\"\"\n```\n\n```python [获取第一个实验的名称]\nmy_api.list_experiments(project=\"project1\").data[0].name\n\"\"\"\n\"experiment1\"\n\"\"\"\n```\n\n:::\n\n<br>\n\n#### `get_experiment`\n\n获取一个实验的详细信息\n\n**方法参数**\n\n| 参数 | 类型 | 描述 |\n| --- | --- | --- |\n| `project` | `str` | 项目名 |\n| `exp_id` | `str` | 实验CUID, 唯一标识符, 可通过`list_experiments`获取, 也可在云端版实验\"环境\"标签页查看 |\n| `username` | `str` | 工作空间名, 默认为用户个人空间 |\n\n**返回值**\n\n`data` `(Experiment)`: 返回一个实验[(Experiment)](#实验模型-experiment)类型的对象, 包含实验的详细信息\n\n**示例**\n\n::: code-group\n\n```python [获取实验信息]\nmy_api.get_experiment(project=\"project1\", exp_id=\"cuid1\").data\n\"\"\"\n{\n    \"cuid\": \"cuid1\",\n    \"name\": \"experiment1\",\n    \"description\": \"This is a test experiment\",\n    \"state\": \"FINISHED\",\n    \"show\": true,\n    \"createdAt\": \"2024-11-23T12:28:04.286Z\",\n    \"finishedAt\": \"2024-11-25T15:56:48.123Z\",\n    \"user\": {\n        \"username\": \"kites-test3\",\n        \"name\": \"Kites Test\"\n    },\n    \"profile\": {\n        \"conda\": \"...\",\n        \"requirements\": \"...\",\n        ...\n    }\n}\n\"\"\"\n```\n\n```python [获取实验的状态]\nmy_api.get_experiment(project=\"project1\", exp_id=\"cuid1\").data.state\n\"\"\"\n\"FINISHED\"\n\"\"\"\n```\n\n```python [获取实验的创建者用户名]\nmy_api.get_experiment(project=\"project1\", exp_id=\"cuid1\").data.user[\"username\"]\n\"\"\"\n\"kites-test3\"\n\"\"\"\n```\n\n:::\n\n<br>\n\n#### `delete_experiment`\n\n删除一个实验\n\n**方法参数**\n| 参数 | 类型 | 描述 |\n| --- | --- | --- |\n| `project` | `str` | 项目名 |\n| `exp_id` | `str` | 实验CUID, 唯一标识符, 可通过`list_experiments`获取, 也可在云端版实验\"环境\"标签页查看 |\n| `username` | `str` | 工作空间名, 默认为用户个人空间 |\n\n**返回值**\n`data` `(dict)`: 空字典, 仅表示删除操作成功\n\n**示例**\n\n::: code-group\n\n```python [删除实验]\nmy_api.delete_experiment(project=\"project1\", exp_id=\"cuid1\")\n```\n\n:::\n\n<br>\n\n#### `get_summary`\n\n获取一个实验的概要信息, 包含实验跟踪指标的最终值和最大最小值, 以及其对应的步数\n\n**方法参数**\n\n| 参数 | 类型 | 描述 |\n| --- | --- | --- |\n| `project` | `str` | 项目名 |\n| `exp_id` | `str` | 实验CUID, 唯一标识符, 可通过`list_experiments`获取, 也可在云端版实验\"环境\"标签页查看 |\n| `username` | `str` | 工作空间名, 默认为用户个人空间 |\n\n**返回值**\n\n`data` `(Dict[str, Dict])`: 返回一个字典, 包含实验的概要信息\n\n字典中的每个键是一个指标名称, 值是一个结构如下的字典:\n\n| 字段 | 类型 | 描述 |\n| --- | --- | --- |\n| `step` | `int` | 最后一个步数 |\n| `value` | `float` | 最后一个步数的指标值 |\n| `min` | `Dict[str, float]` | 最小值对应的步数和指标值 |\n| `max` | `Dict[str, float]` | 最大值对应的步数和指标值 |\n\n\n**示例**\n\n::: code-group\n\n```python [获取实验概要信息]\nmy_api.get_summary(project=\"project1\", exp_id=\"cuid1\").data\n\"\"\"\n{\n    \"loss\": {\n        \"step\": 47,\n        \"value\": 0.1907215012216071,\n        \"min\": {\n            \"step\": 33,\n            \"value\": 0.1745886406861026\n        },\n        \"max\": {\n            \"step\": 0,\n            \"value\": 0.7108771095136294\n        }\n    },\n    ...\n}\n\"\"\"\n```\n\n\n```python [获取指标的最大值]\nmy_api.get_summary(project=\"project1\", exp_id=\"cuid1\").data[\"loss\"][\"max\"][\"value\"]\n\"\"\"\n0.7108771095136294\n\"\"\"\n```\n\n```python [获取指标最小值所在步]\nmy_api.get_summary(project=\"project1\", exp_id=\"cuid1\").data[\"loss\"][\"min\"][\"step\"]\n\"\"\"\n33\n\"\"\"\n```\n:::\n\n<br>\n\n#### get_metrics\n\n获取一个实验的指标值\n\n**方法参数**\n\n| 参数 | 类型 | 描述 |\n| --- | --- | --- |\n| `exp_id` | `str` | 实验CUID, 唯一标识符, 可通过`list_experiments`获取, 也可在云端版实验\"环境\"标签页查看 |\n| `keys` | `Union[str, List[str]]` | 指标名列表, 即swanlab.log({key: value})中的key, 可在网站查看, 也可通过`get_summary`获取 |\n\n**返回值**\n\n`data` `(DataFrame)`: 返回一个DataFrame, 包含实验的指标值\n\n**示例**\n\n::: code-group\n\n```python [获取实验指标]\nmy_api.get_metrics(exp_id=\"cuid1\", keys=[\"loss\", \"acc\"]).data\n\"\"\"\n          loss  loss_timestamp       acc  acc_timestamp\nstep\n1     0.336772   1751712864853  0.670422  1751712864852\n2     0.338035   1751712864858  0.830018  1751712864857\n3     0.282654   1751712864862  0.794594  1751712864862\n4     0.258216   1751712864866  0.832750  1751712864866\n5     0.097542   1751712864871  0.901684  1751712864871\n6     0.092955   1751712864875  0.907544  1751712864875\n7     0.149327   1751712864879  0.942524  1751712864879\n8     0.131631   1751712864884  0.921309  1751712864883\n\"\"\"\n```\n\n:::\n\n\n<br>\n\n---\n\n\n### Project\n\n#### `list_projects`\n\n获取指定工作空间下的所有项目列表\n\n**方法参数**\n\n| 参数  | 类型 | 描述 |\n| --- | --- | --- |\n| `username` | `str` | 工作空间名, 默认为用户个人空间 |\n| `detail` | `bool` | 是否项目统计信息, 默认为 True |\n\n**返回值**\n\n`data` `(List[Project])`: 包含项目[(Project)](#项目模型-project)对象的列表\n\n**示例**\n\n::: code-group\n\n```python [获取项目列表]\nmy_api.list_projects().data\n\"\"\"\n[\n    {\n        \"cuid\": \"project1\",\n        \"name\": \"Project 1\",\n        \"description\": \"Description 1\",\n        \"visibility\": \"PUBLIC\",\n        \"createdAt\": \"2024-11-23T12:28:04.286Z\",\n        \"updatedAt\": null,\n        \"group\": {\n            \"type\": \"PERSON\",\n            \"username\": \"kites-test3\",\n            \"name\": \"Kites Test\"\n        },\n        \"count\": {\n            \"experiments\": 4,\n            \"contributors\": 1,\n            \"children\": 0,\n            \"runningExps\": 0\n        }\n    },\n    ...\n]\n\"\"\"\n```\n\n:::\n\n#### `delete_project`\n\n删除一个项目\n\n**方法参数**\n\n| 参数 | 类型 | 描述 |\n| --- | --- | --- |\n| `project` | `str` | 项目名 |\n| `username` | `str` | 工作空间名, 默认为用户个人空间 |\n\n**返回值**\n\n`data` `(dict)`: 空字典, 仅表示删除操作成功\n\n**示例**\n\n::: code-group\n\n```python [删除项目]\nmy_api.delete_project(project=\"project1\")\n```\n\n:::\n\n<br>",
    "68": "一级标题：其他Python API\n二级标题：无\n内容：",
    "69": "一级标题：其他Python API\n二级标题：get_run\n内容：\n获取当前运行的实验对象（`SwanLabRun`）。\n\n```python\nrun = swanlab.init(...)\n\n...\n\nrun = swanlab.get_run()\n```",
    "70": "一级标题：其他Python API\n二级标题：get_url\n内容：\n获取实验的URL（cloud模式，否则为None）。\n\n```python\nprint(swanlab.get_url())\n```",
    "71": "一级标题：其他Python API\n二级标题：get_project_url\n内容：\n获取项目的URL（cloud模式，否则为None）。\n\n```python\nprint(swanlab.get_project_url())\n```",
    "72": "一级标题：swanlab.pr_curve\n二级标题：无\n内容：\n[Github源代码](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/modules/custom_charts/metrics.py)\n\n```python\npr_curve(\n    y_true: Union[List, np.ndarray],\n    y_pred_proba: Union[List, np.ndarray],\n    title: Optional[str, bool] = None,\n) -> None\n```\n\n| 参数          | 描述                                                                                                                           |\n|-------------|------------------------------------------------------------------------------------------------------------------------------|\n| y_true      | (Union[List, np.ndarray]) 真实标签，二分类问题中的真实类别标签（0或1）                                                                        |\n| y_pred_proba | (Union[List, np.ndarray]) 预测概率，模型对正类的预测概率值（范围0-1）                                                                        |\n| title       | (Optional[str, bool]) 是否显示图表标题，默认为None                                                                                                           |",
    "73": "一级标题：swanlab.pr_curve\n二级标题：介绍\n内容：\n绘制PR（Precision-Recall）曲线，用于评估二分类模型的性能。PR曲线展示了在不同阈值下精确率（Precision）和召回率（Recall）的关系。\n\nPR曲线特别适用于处理不平衡数据集，能够更好地评估模型在少数类上的表现。\n\n### 基本用法\n\n```python\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nimport swanlab\n\n# 生成示例数据\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# 训练模型\nmodel = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\nmodel.fit(X_train, y_train)\n\n# 获取预测概率\ny_pred_proba = model.predict_proba(X_test)[:, 1]\n\n# 初始化SwanLab\nswanlab.init(project=\"PR-Curve-Demo\", experiment_name=\"PR-Curve-Example\")\n\n# 记录PR曲线\nswanlab.log({\n    \"pr_curve\": swanlab.pr_curve(y_test, y_pred_proba, title=True)\n})\n\nswanlab.finish()\n```\n\n![](./py-pr_curve/demo.png)\n\n### 自定义标题\n\n```python\n# 不显示标题(默认)\npr_curve = swanlab.pr_curve(y_test, y_pred_proba, title=False)\nswanlab.log({\"pr_curve_no_title\": pr_curve})\n\n# 显示标题\npr_curve = swanlab.pr_curve(y_test, y_pred_proba, title=True)\nswanlab.log({\"pr_curve_with_title\": pr_curve})\n\n# 自定义标题\npr_curve = swanlab.pr_curve(y_test, y_pred_proba, title=\"demo\")\nswanlab.log({\"pr_curve_with_custom_title\": pr_curve})\n```\n\n### 注意事项\n\n1. **数据格式**: `y_true`和`y_pred_proba`可以是列表或numpy数组\n2. **二分类**: 此函数专用于二分类问题\n3. **概率值**: `y_pred_proba`应该是模型对正类的预测概率，范围在0-1之间\n4. **依赖包**: 需要安装`scikit-learn`和`pyecharts`包\n5. **AUC计算**: 函数会自动计算PR曲线下的面积（AUC），但不会默认在标题中显示",
    "74": "一级标题：swanlab.register_callback\n二级标题：无\n内容：\n```python\n@should_call_before_init(\"After calling swanlab.init(), you can't call it again.\")\ndef register_callbacks(\n    self,\n    callbacks: List[SwanKitCallback]\n) -> None:\n```\n\n| 参数 | 类型 | 描述 |\n| --- | --- | --- |\n| `callbacks` | `List[SwanKitCallback]` | 回调函数列表 |",
    "75": "一级标题：swanlab.register_callback\n二级标题：介绍\n内容：\n使用`swanlab.register_callbacks()`注册回调函数，以在SwanLab的执行生命周期中调用。\n\n```python {3}\nfrom swanlab.plugin.writer import EmailCallback\nemail_callback = EmailCallback(...)\nswanlab.register_callbacks([email_callback])\n\nswanlab.init(...)\n```\n\n效果等价于：\n\n```python\nfrom swanlab.plugin.writer import EmailCallback\nemail_callback = EmailCallback(...)\n\nswanlab.init(\n    ...\n    callbacks=[email_callback]\n)\n```\n\n**场景**：比如你使用时的是SwanLab与Transformers的集成，那么你要找到`swanlab.init()`是不容易的。那么，你可以在`trainer.train()`调用前，用`swanlab.register_callbacks()`注册回调函数，实现插件的注入。",
    "76": "一级标题：swanlab.roc_curve\n二级标题：无\n内容：\n[Github源代码](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/modules/custom_charts/metrics.py)\n\n```python\nroc_curve(\n    y_true: Union[List, np.ndarray],\n    y_pred_proba: Union[List, np.ndarray],\n    title: Optional[str, bool] = None,\n) -> None\n```\n\n| 参数          | 描述                                                                                                                           |\n|-------------|------------------------------------------------------------------------------------------------------------------------------|\n| y_true      | (Union[List, np.ndarray]) 真实标签，二分类问题中的真实类别标签（0或1）                                                                        |\n| y_pred_proba | (Union[List, np.ndarray]) 预测概率，模型对正类的预测概率值（范围0-1）                                                                        |\n| title       | (Optional[str, bool]) 是否显示图表标题，默认为None                                                                                                           |",
    "77": "一级标题：swanlab.roc_curve\n二级标题：介绍\n内容：\n绘制ROC（Receiver Operating Characteristic）曲线，用于评估二分类模型的性能。ROC曲线展示了在不同阈值下真正率（True Positive Rate）和假正率（False Positive Rate）的关系。\n\nROC曲线是评估分类模型性能的重要工具，能够直观地展示模型在不同决策阈值下的表现。\n\n### 基本用法\n\n```python\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nimport swanlab\n\n# 生成示例数据\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# 训练模型\nmodel = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\nmodel.fit(X_train, y_train)\n\n# 获取预测概率\ny_pred_proba = model.predict_proba(X_test)[:, 1]\n\n# 初始化SwanLab\nswanlab.init(project=\"ROC-Curve-Demo\", experiment_name=\"ROC-Curve-Example\")\n\n# 记录ROC曲线\nswanlab.log({\n    \"roc_curve\": swanlab.roc_curve(y_test, y_pred_proba, title=True)\n})\n\nswanlab.finish()\n```\n\n![](./pr-roc_curve/demo.png)\n\n### 自定义标题\n\n```python\n# 不显示标题(默认)\nroc_curve = swanlab.roc_curve(y_test, y_pred_proba, title=False)\nswanlab.log({\"roc_curve_no_title\": roc_curve})\n\n# 显示标题\nroc_curve = swanlab.roc_curve(y_test, y_pred_proba, title=True)\nswanlab.log({\"roc_curve_with_title\": roc_curve})\n\n# 自定义标题\nroc_curve = swanlab.roc_curve(y_test, y_pred_proba, title=\"demo\")\nswanlab.log({\"roc_curve_with_custom_title\": roc_curve})\n```\n\n### 与其他指标一起使用\n\n```python\nimport swanlab\n\n# 记录多个ML指标\nswanlab.log({\n    \"roc_curve\": swanlab.roc_curve(y_test, y_pred_proba),\n    \"pr_curve\": swanlab.pr_curve(y_test, y_pred_proba),\n    \"accuracy\": accuracy_score(y_test, y_pred),\n    \"f1_score\": f1_score(y_test, y_pred)\n})\n```\n\n### 注意事项\n\n1. **数据格式**: `y_true`和`y_pred_proba`可以是列表或numpy数组\n2. **二分类**: 此函数专用于二分类问题\n3. **概率值**: `y_pred_proba`应该是模型对正类的预测概率，范围在0-1之间\n4. **依赖包**: 需要安装`scikit-learn`和`pyecharts`包\n5. **AUC计算**: 函数会自动计算ROC曲线下的面积（AUC），但不会在标题中显示\n6. **曲线特征**: ROC曲线从(0,0)开始，到(1,1)结束，对角线表示随机分类器的性能",
    "78": "一级标题：run\n二级标题：无\n内容：\nrun 指的是 `swanlab.init()` 返回的 `SwanLabRun` 对象，这里介绍run具有的一些方法。\n(逐步更新中...)",
    "79": "一级标题：run\n二级标题：public\n内容：\npublic存储了SwanLabRun的一些公共信息，包括：\n- `project_name`: 项目名称\n- `version`: 版本\n- `run_id`: 实验ID\n- `swanlog_dir`: swanlog日志目录的路径\n- `run_dir`: 运行目录的路径\n- `cloud`: 云端信息\n    - `project_name`: 项目名称（仅在cloud模式时有效）\n    - `project_url`: 项目在云端的URL（仅在cloud模式时有效）\n    - `experiment_name`: 实验名称（仅在cloud模式时有效）\n    - `experiment_url`: 实验在云端的URL（仅在cloud模式时有效）\n\n以字典形式获取public信息：\n\n```python\nimport swanlab\nrun = swanlab.init()\nprint(run.public.json())\n```\n\n比如，你想要获取实验的URL，可以这样：\n\n```python\nprint(run.public.cloud.experiment_url)\n```",
    "80": "一级标题：swanlab.Settings\n二级标题：无\n内容：\n```python\nSettings(\n    model_config = ConfigDict(frozen=True),\n    metadata_collect: StrictBool = True,\n    collect_hardware: StrictBool = True,\n    collect_runtime: StrictBool = True,\n    security_mask: StrictBool = True,\n    requirements_collect: StrictBool = True,\n    conda_collect: StrictBool = False,\n    hardware_monitor: StrictBool = True,\n    disk_io_dir: DirectoryPath = Field(...),\n    upload_interval: PositiveInt = 1,\n    max_log_length: int = Field(ge=500, le=4096, default=1024),\n    log_proxy_type: Literal[\"all\", \"stdout\", \"stderr\", \"none\"] = \"all\",\n)\n```\n\n| 参数                     | 类型            | 描述                                                                              |\n|:-----------------------|:--------------|:--------------------------------------------------------------------------------|\n| `metadata_collect`     | StrictBool    | 是否开启元数据采集。默认值为 `True`。                                                          |\n| `collect_hardware`     | StrictBool    | 是否采集当前系统环境的硬件信息。默认值为 `True`。                                                    |\n| `collect_runtime`      | StrictBool    | 是否采集运行时信息。默认值为 `True`。                                                          |\n| `security_mask`        | StrictBool    | 是否自动隐藏隐私信息，如 api_key 等。开启后将在检测到隐私信息时，自动将其替换为加密字符（****）。默认值为 `True`。             |\n| `requirements_collect` | StrictBool    | 是否采集 Python 环境信息 (`pip list`)。默认值为 `True`。                                      |\n| `conda_collect`        | StrictBool    | 是否采集 Conda 环境信息。默认值为 `False`。                                                   |\n| `hardware_monitor`     | StrictBool    | 是否开启硬件监控。如果 `metadata_collect` 关闭，则此项无效。默认值为 `True`。                            |\n| `disk_io_dir`          | DirectoryPath | 磁盘 IO 监控的路径。默认值为系统根目录 (`/` 或 `C:\\`)。                                            |\n| `hardware_interval`    | PositiveInt   | 硬件监控采集间隔，以秒为单位，最小值为5秒。                                                          |\n| `backup`               | PositiveInt   | 日志备份开启功能，默认值为 `True`。开启后，日志将被备份到本地（默认为`swanlog`目录）。      |\n| `upload_interval`      | PositiveInt   | 日志上传间隔（单位：秒）。默认值为 `1`。                                                          |\n| `max_log_length`       | int           | 终端日志上传单行最大字符数（范围：500-4096）。默认值为 `1024`。                                         |\n| `log_proxy_type`       | Literal       | 日志代理类型，会影响实验的日志选项卡记录的内容。默认值为 `\"all\"`。\"stdout\" 表示只代理标准输出流，\"stderr\" 表示只代理标准错误流，\"all\" 表示代理标准输出流和标准错误流，\"none\" 表示不代理日志。|",
    "81": "一级标题：swanlab.Settings\n二级标题：介绍\n内容：\n- `swanlab.Settings`类用于和管理 SwanLab 的全局功能开关和设置。\n\n- 在`import swanlab`时，会创建一个默认的全局设置，各个设置及其默认值详见上表。\n\n- 如果我们要对某些设置进行调整，需要通过新建一个`Settings`实例如`new_settings`，在实例化时传入想要修改的配置参数，然后要通过运行`swanlab.merge_settings(new_settings)`来对全局设置进行更新。\n\n- 值得注意的是，`merge_settings()`方法只在`swanlab.init()`被调用之前可用，这意味着，在使用`swanlab`的过程中，一旦`swanlab.init()`被调用，全局设置将不再能被更改。",
    "82": "一级标题：swanlab.Settings\n二级标题：更多用法\n内容：\n### 更新全局设置\n\n::: code-group\n\n```python [方式一]\nimport swanlab\n\n# 创建新的设置对象\nnew_settings = swanlab.Settings(\n    metadata_collect=False,\n    hardware_monitor=False,\n    upload_interval=5\n)\n\nswanlab.init(settings=new_settings)\n...\n```\n\n```python [方式二]\nimport swanlab\n\n# 创建新的设置对象\nnew_settings = swanlab.Settings(\n    metadata_collect=False,\n    hardware_monitor=False,\n    upload_interval=5\n)\n\n# 更新全局设置\nswanlab.merge_settings(new_settings)\n\nswanlab.init()\n...\n```\n\n:::\n\n### 记录 conda 环境信息\n\n```python\nimport swanlab\nfrom swanlab import Settings\n\n# 创建新的设置对象\nnew_settings = Settings(\n    conda_collect=True  # 默认不开启\n)\n\n# 更新全局设置\nswanlab.merge_settings(new_settings)\n\nswanlab.init()\n...\n```",
    "83": "一级标题：swanlab.sync_mlflow\n二级标题：无\n内容：\n将MLFlow项目同步到SwanLab，[文档](/guide_cloud/integration/integration-mlflow.md)",
    "84": "一级标题：swanlab.sync_tensorboard\n二级标题：无\n内容：\n将tensorboard/tensorboardX的指标同步到SwanLab, [文档](/guide_cloud/integration/integration-tensorboard.md)",
    "85": "一级标题：swanlab.sync_wandb\n二级标题：无\n内容：\n将wandb的指标同步到SwanLab, [文档](/guide_cloud/integration/integration-wandb.md)",
    "86": "一级标题：swanlab.Video\n二级标题：无\n内容：\n[Github源代码](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/modules/video/__init__.py)\n\n```python\nVideo(\n    data_or_path: str,\n    caption: str = None,\n) -> None\n```\n\n| 参数          | 描述                                                                                                     |\n|-------------|--------------------------------------------------------------------------------------------------------|\n| data_or_path | (str) 接收视频文件路径。目前仅支持GIF格式文件。 |\n| caption     | (str) 视频的标签。用于在实验看板中展示视频时进行标记。                                                      |",
    "87": "一级标题：swanlab.Video\n二级标题：介绍\n内容：\n对视频数据做转换，以被`swanlab.log()`记录。目前仅支持GIF格式的视频文件。\n\n::: warning 格式限制\n目前 `swanlab.Video` 仅支持 GIF 格式的文件路径。其他视频格式暂不支持。\n:::\n\n### 记录GIF视频文件\n\n记录单个GIF视频：\n\n```python\nimport swanlab\n\nswanlab.init()\nvideo = swanlab.Video(\"path/to/video.gif\", caption=\"训练过程演示\")\nswanlab.log({\"video\": video})\n```\n\n记录多个GIF视频：\n\n```python\nimport swanlab\n\nswanlab.init()\n\nexamples = []\nfor i in range(3):\n    video = swanlab.Video(f\"video_{i}.gif\", caption=f\"视频示例 {i}\")\n    examples.append(video)\n\nswanlab.log({\"examples\": examples})\n```",
    "88": "一级标题：swanlab.Video\n二级标题：创建GIF视频示例\n内容：\n以下是一个创建GIF动画并记录的完整示例：\n\n```python\nimport os.path\nimport random\nfrom PIL import Image as PILImage\nfrom PIL import ImageDraw\nimport swanlab\n\nswanlab.init()\n\n# 创建一个GIF动画\ndef create_mock_gif(output_path, width=200, height=200, frames=10, duration=100):\n    \"\"\"\n    创建一个简单的mock GIF动画\n\n    参数:\n        output_path: 输出GIF文件路径\n        width: 图像宽度(像素)\n        height: 图像高度(像素)\n        frames: 动画帧数\n        duration: 每帧显示时间(毫秒)\n    \"\"\"\n    images = []\n\n    for i in range(frames):\n        # 创建一个新的RGB图像\n        img = PILImage.new('RGB', (width, height), color=(255, 255, 255))\n        draw = ImageDraw.Draw(img)\n\n        # 随机生成颜色\n        r = random.randint(0, 255)\n        g = random.randint(0, 255)\n        b = random.randint(0, 255)\n\n        # 在图像上绘制一个随机的圆形\n        x = random.randint(0, width)\n        y = random.randint(0, height)\n        radius = random.randint(10, min(width, height) // 2)\n        draw.ellipse([x - radius, y - radius, x + radius, y + radius], fill=(r, g, b))\n\n        # 将当前帧添加到列表中\n        images.append(img)\n\n    # 保存为GIF动画\n    images[0].save(output_path, save_all=True, append_images=images[1:], duration=duration, loop=0)\n\n# 创建GIF文件\ngif_path = \"test.gif\"\ncreate_mock_gif(gif_path, width=300, height=300, frames=15, duration=200)\n\n# 记录到SwanLab\nswanlab.log({\"video\": swanlab.Video(gif_path, caption=\"这是一个测试视频\")})\n```",
    "89": "一级标题：swanlab.Video\n二级标题：使用场景\n内容：\n`swanlab.Video` 适用于以下场景：\n\n1. **训练过程可视化**：记录模型训练过程中的动态变化\n2. **实验结果展示**：展示实验结果的动态演示\n3. **数据可视化**：将时间序列数据转换为动画展示\n4. **模型行为分析**：记录模型在不同输入下的动态响应",
    "90": "一级标题：swanlab.Video\n二级标题：注意事项\n内容：\n1. **文件格式**：目前仅支持 GIF 格式，不支持 MP4、AVI 等其他视频格式\n2. **文件大小**：建议控制 GIF 文件大小，过大的文件可能影响加载速度\n3. **帧率控制**：创建 GIF 时可以通过 `duration` 参数控制播放速度\n4. **文件路径**：确保提供的文件路径正确且文件存在",
    "91": "一级标题：前言\n二级标题：无\n内容：",
    "92": "一级标题：前言\n二级标题：为什么需要提示词工程\n内容：\n在学习提示词工程之前，我们提出一个疑问，为什么需要提示词工程？对于这个问题，我们需要从提示词的发展说起。\n\n以 BERT 为代表的预训练模型掀起了一场技术变革，**彼时主流的技术范式是 “预训练 - 微调”**：研究者们先在大规模通用语料上训练出基础模型，再针对具体任务（如文本分类、情感分析）在小规模标注数据上进行微调，通过调整模型参数来适配特定场景。这种模式虽然在众多任务中取得了优异效果，但也存在明显局限 —— 对标注数据的强依赖使得它在低资源场景中难以施展，且不同任务需要单独微调，模型的通用性受到制约。\n\n提示词能很好的解决这个问题。预训练后的大模型本身是具备零样本回答能力的[1]，在没有举任何例子的情况下只要多一句`Let's think step by step`，模型就能自主思考，无需微调，如下图所示。\n\n<div style=\"display:flex;justify-content:center;\">\n  <figure style=\"text-align:center;margin:0;\">\n    <img src=\"./picture/prompt3.png\">\n    <figcaption>CoT举例</figcaption>\n  </figure>\n</div>\n\n不过需要注意的是，大模型在高参数量的时候效果会很好，当参数量较少的时候，即便再怎么调整提示词，模型都不一定能完美表达正确的意思，**这源于大模型的涌现能力[2]**，简单点说，如果一个大模型本身比较“笨”，那你再怎么问、再怎么修改提示词，人家也回答不出来。\n\n而随着大语言模型技术的不断成熟，**“预训练 - 提示 - 预测” [3]这一范式逐渐成为主流**，如下图所示，这也让提示词的价值被彻底释放。就像我们前面说的，大模型在预训练阶段已经吸收了海量知识，提示词就像是一把钥匙，能把这些知识激活并引导到具体任务中。比如面对一道复杂的数学题，直接问答案可能会出错，但加上 “请分步骤推导” 这样的提示，模型就能顺着逻辑一步步拆解问题，这正是提示词对模型思维的引导作用。\n\n<div style=\"display:flex;justify-content:center;\">\n  <figure style=\"text-align:center;margin:0;\">\n    <img src=\"./picture/prompt2.png\">\n    <figcaption>“预训练-提示-预测”这一范式逐渐成为主流</figcaption>\n  </figure>\n</div>\n\n这种无需微调就能解决问题的特性，让提示词在实际应用中变得越来越重要。要知道，不是所有场景都有足够的标注数据来做微调，也不是所有团队都有能力承担微调的成本。这时候，一句精准的提示词就能让大模型在零样本或少样本的情况下完成任务，大大降低了使用门槛。但我们也要清楚，提示词的效果和模型本身的能力紧密相关。只有当模型参数量达到一定规模，涌现出足够强的理解和推理能力时，提示词才能发挥最大作用。要是模型本身 “底子薄”，哪怕提示词设计得再精巧，也很难得到理想的结果。\n\n\n<div style=\"display:flex;justify-content:center;\">\n  <figure style=\"text-align:center;margin:0;\">\n    <img src=\"./picture/prompt.png\">\n    <figcaption>提示词工程原理[4]</figcaption>\n  </figure>\n</div>\n\n提示词工程应运而生。它不是简单的文字组合，而是要结合模型特性、任务需求来设计提示策略。怎么让提示词更清晰地传达任务目标？怎么通过提示规避模型的常见错误？怎么在不同模型上调整提示方式以获得最佳效果？这些都是提示词工程要解决的问题。也正因如此，学习提示词工程，其实就是学会如何更好地和大模型 “沟通”，让这一强大的技术工具真正为我们所用，这也是我们深入研究它的意义所在。\n\n<div style=\"background:#d4edda;color:#000;padding:12px 16px;border-left:4px solid #28a745;\">\n✅ <strong>我们的所有代码均可在 <a href=\"https://github.com/828Tina/PromptEngineeringCourse\" target=\"_blank\" rel=\"noopener\">GitHub</a> 查看。</strong>\n</div>",
    "93": "一级标题：前言\n二级标题：与上下文工程的关系\n内容：\n最近上下文工程这个概念比较火，作为新出的概念，和之前爆火的提示词工程的关系和区别简单来说是包含的关系，也就是**提示词工程&sub;上下文工程**。\n\nContext Engineering（上下文工程 ），聚焦于大语言模型（[LLM](https://zhida.zhihu.com/search?content_id=260080027&content_type=Article&match_order=1&q=LLM&zhida_source=entity)）场景下，对“上下文（Context）”进行专业处理与优化的角色，核心是让LLM更高效、精准利用信息完成任务 。简单说，就是搭建、管理LLM“信息输入 - 处理”通道，让模型“聪明干活”。也包括了Prompt,RAG,FunctionCalling等内容。\n\n这里的处理不仅包括对提示词内容的处理，让输出更加精准、符合预期，还可以从资源利用角度，比如显存的占用，更加高效运用现有资源。\n\n提示词工程只处理提示词，上下文工程因为要处理上下文，因此历史记录、RAG、工具调用都归上下文工程管理，概念更偏向于Agent。\n\n我们可以从下图中了解二者的关系：\n\n<div style=\"display:flex;justify-content:center;\">\n  <figure style=\"text-align:center;margin:0;\">\n    <img src=\"./picture/context_engineering.png\">\n    <figcaption>提示词工程与上下文工程包含关系</figcaption>\n  </figure>\n</div>\n\n\n- Prompt Engineering（提示词工程）主要是激发LLM做好单一件事情，适合处理流程简单的工作。\n- Context Engineering（上下文工程）是利用LLM处理更加复杂，更加系统的任务，更加符合这个智能Agent的时代。\n\n在最新的Context Engineering综述[5]中详细解释了Context Engineering的概念，文章搜集了1400多篇论文资料，对上下文工程的每一个模块做了详细的概述，作者将上下文工程分为三个部分，分别是**上下文检索与生成（Context Retrieval and Generation）、上下文处理（Context Processing）、上下文管理（Context Management）**。简单来说：\n- **`上下文检索与生成（Context Retrieval and Generation）`**：为模型搜集和构建合适的上下文内容\n- **`上下文处理（Context Processing）`**：优化模型对已获取的上下文信息进行加工、变换和优化\n- **`上下文管理（Context Management）`**：对上下文进行高效的存储、记忆和优化\n\n<div style=\"display:flex;justify-content:center;\">\n  <figure style=\"text-align:center;margin:0;\">\n    <img src=\"./picture/fundation_components.png\">\n    <figcaption>上下文工程组件</figcaption>\n  </figure>\n</div>\n\n\n\n我们来详细的说明每一部分运用的知识组件。\n\n1、**`上下文检索与生成（Context Retrieval and Generation）`**\n\n> “上下文检索与生成”侧重于为模型搜集和构建合适的上下文内容，文中指出上下文检索的基础是提示词工程与上下文生成。\n\n- `提示词工程`通过精准指令引导模型思考，比如加入 “让我们一步步思考” 的引导语，或提供示例问答，这本质上是通过语言设计推动模型走向正确推理方向。\n- 更关键的是`外部知识检索`。由于大模型的预训练知识固定，必须通过外部获取新信息增强上下文。检索增强生成（RAG）技术就是典型应用：在回答前从文档库或数据库中调取相关资料插入上下文，让回答有据可依，大幅降低编造概率。关键词搜索、向量匹配等技术能将最新知识 “喂给” 模型。\n- 复杂场景还需要`动态上下文拼装`。智能助手需整合用户档案、对话记录、知识库文章等分散信息，通过程序或代理自动构建 “上下文工厂管道”，将原始信息加工成模型可直接使用的输入内容。\n\n2、**`上下文处理（Context Processing）`**\n\n> “上下文处理”关注对已获取的上下文信息进行加工、变换和优化。当上下文很长或信息结构复杂时，直接塞给模型可能效率低下甚至无法处理，此时需要在输入模型前进行预处理。\n\n- `长上下文处理`是核心挑战。传统 Transformer 对长文本敏感，研究者提出高效注意力机制（如 FlashAttention）、分块滑动窗口、递归总结等技术，可在窗口限制内处理百万级内容。例如将一本书逐级摘要压缩，既适配模型长度又保留关键信息。\n- `自我优化与适应`则利用模型自身能力改进上下文，比如插入分步解析或中间结论，让模型先 “反思” 再作答；通过自我纠错逐步逼近正确结果，提升复杂推理准确性。\n- `多模态上下文处理`需将图像、音频等非文本信息编码为模型可理解的形式。如图像问答中，用视觉模型提取图片要点并转化为文字描述，再融入文本上下文。多模态大模型（如 GPT-4V）正推动这一领域发展。\n- `结构化信息`整合强调利用知识图谱、表格等结构化数据，将杂乱文本转化为清单式要点，或通过代码转换格式，让模型更易吸收逻辑化数据。\n\n3、**`上下文管理（Context Management）`**\n\n> “上下文管理”关注的是对上下文进行高效的存储、记忆和优化 。随着交互进行，模型累积的上下文会越来越庞大，如何在有限窗口内保留关键信息、丢弃无用信息，并在需要时快速提取，这是上下文管理要解决的问题。\n>\n> 上下文管理是构建复杂 **Memory System 和 Agent System**的基础，其效果直接决定了模型交互的连贯性和智能水平 。\n\n- `硬约束`下的取舍需通过上下文压缩技术平衡信息量：多轮对话中总结早期内容为要点，或即时压缩用户大段文本，让后续调用仅参考精华，避免窗口浪费和噪音干扰。\n- `层次化记忆体`系类似计算机缓存设计：近期对话存于短期工作记忆，久远重要内容归档至长期记忆（向量数据库、文件等），需用时通过检索提取。外部存储扩展了模型 “记忆容量”，实现跨会话持续记忆。\n- `上下文优化`通过重排序（重要内容后置，契合模型注意力偏好）、格式优化（改为问答形式）、噪音过滤等技术提升有效性。高级技巧还包括调整信息排布适配模型注意力模式，或生成辅助信息作为隐藏上下文。\n\n\n\n在实际工程中，上述基础组件通常会集成到完整的系统架构中，从而打造出功能强大的智能应用。当前比较典型的上下文工程系统实现主要有以下几类 ：检索增强生成（RAG）、记忆系统、工具增强推理和多智能体系统。论文详细整理了每个类别都有哪些已经发表的工作。\n\n<div style=\"display:flex;justify-content:center;\">\n  <figure style=\"text-align:center;margin:0;\">\n    <img src=\"./picture/context_en_work.png\">\n    <figcaption>上下文工程相关工作</figcaption>\n  </figure>\n</div>\n\n\n它们各自侧重于不同的应用场景，但都利用了上下文工程的理念，将相关信息/工具动态注入模型上下文来提升性能。图中每个分支我们详细解释👇。\n\n1. **`检索增强生成（RAG）`**：\n我们在前文提到，RAG为了解决大模型幻觉问题，将外部知识检索融入到模型生成过程中，简单说，RAG 模型在回答用户请求时，会先根据请求内容从知识库中`检索`相关资料，将这些资料和原始提问一起提供给 LLM，然后由 LLM 生成结合了这些资料的回答。这样模型的知识不再局限于训练参数中固有的内容，还可以`动态访问`最新的、领域专门的信息。\n\n2. **`记忆系统（Memory Systems）`**：\n`记忆系统`致力于让模型拥有`持续的、跨会话的记忆能力`。它通过引入外部或内部的记忆模块，使模型可以“记住”过去的信息，并在需要时将其作为上下文提供给模型。传统的 LLM 在完成一次回答后状态即清空，无法自行记忆先前对话或交互。而一个上下文工程良好的记忆系统，则会**持久存储与用户交互或任务相关的内容，并在后续交互中检索召回，形成一种长短期记忆结合的机制**。\n\n3. **`工具增强推理（Tool-Integrated Reasoning）`**：\n`工具增强`推理是指让模型能够调用外部工具或API来协助完成任务，并将这些`工具使用过程的结果纳入上下文`。**这一机制使 LLM 从被动的“文本生成者”升级为可以与外界交互的“主动智能体”**。通过上下文工程，我们可以在提示中嵌入工具的使用说明和接口文档，然后当模型决定使用某个工具时，由外部程序实际执行该工具，将返回结果再注入模型上下文供其后续处理。\n\n4. **`多智能体系统（Multi-Agent Systems）`**：\n当一个任务过于复杂或需要不同技能时，往往会引入`多个智能体（Agents）协同工作`。这就产生了多智能体系统，其核心在于通过上下文工程来`协调多个模型间的交流与分工`。在这种架构下，不再是单一LLM面对一切，而是多个LLM（或混合了工具的Agent）各司其职、互相通信，共同完成目标。\n**多智能体系统的一个关键是通信协议和语言的设计**。Agent 间交流的消息本质上也是上下文的一部分。上下文工程需要制定统一的消息格式、角色定义和交互规则，以确保 Agent 彼此理解。其次，多Agent系统需要`编排与调度机制`。即在一个复杂任务中，如何决定下一个该由哪个Agent发言，或者某个Agent何时该暂停等待他人结果。这可以由固定脚本实现，也可以交给一个“控制Agent”通过上下文信息（比如任务完成度）来动态判断。",
    "94": "一级标题：前言\n二级标题：参考文献\n内容：\n[1].[Large Language Models are Zero-Shot Reasoners](https://arxiv.org/pdf/2205.11916)\n\n[2].[Emergent Abilities of Large Language Models](https://arxiv.org/pdf/2206.07682)\n\n[3].[Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing](https://dl.acm.org/doi/pdf/10.1145/3560815)\n\n[4].[一文了解提示词、提示词工程和上下文工程](https://juejin.cn/post/7531846123184406538)\n\n[5].[A Survey of Context Engineering for Large Language Models](https://arxiv.org/abs/2507.13334)",
    "95": "一级标题：提示词工程指南\n二级标题：无\n内容：\n提示词工程（Prompt Engingering），也被称为上下文提示（In-Context Prompting），指的是通过结构化文本的方法来完善提示词（Prompt），从而引导大模型生成更符合预期的输出结果的技术。简单点说就是“用更聪明的提问方式，让AI更好地理解并完成任务”。通过提示词工程可以在不通过训练更新模型权重的情况下，让大模型完成不同类型的任务。提示词工程的效果在不同的模型中可能会有很大的差异，因此需要大量的实验和探索。掌握了提示工程相关技能将有助于用户更好地了解大型语言模型的能力和局限性。\n\n提示工程不仅仅是关于设计和研发提示词。它包含了与大语言模型交互和研发的各种技能和技术。提示工程在实现和大语言模型交互、对接，以及理解大语言模型能力方面都起着重要作用。用户可以通过提示工程来提高大语言模型的安全性，也可以赋能大语言模型，比如借助专业领域知识和外部工具来增强大语言模型能力，同时，赋能大语言模型也可以用于生成训练模型的高质量数据集，能够大大提高原有人工构建数据集的效率。\n\n提示词工程可以帮助改善大语言模型的性能，使其更好地满足用户需求，这是在预模型互动时常用的策略，特别是在自然语言处理任务和生成任务中，比如文本生成、医疗问答、文章写作等。基于对大语言模型的浓厚兴趣，我们编写了这份全新的提示工程指南，介绍了大语言模型清关的学习指南、模型选择、提示词编写技巧等内容。\n\n\n---\n\n\n我们的教程包含前言、环境设置、常见推理任务、合成数据、RAG、Agent等，带大家全方位熟悉`提示词`的使用，下面我们给出了教程的内容导航👇：\n\n| 章节                                                                          | 关键内容                              | 状态 |\n|-----------------------------------------------------------------------------|-----------------------------------| --- |\n| [第一章 环境安装](../03-environmental_installation_platform_preparation/README.md) | 获取教程代码、本地调用大模型环境安装、调用API使用大模型     | ✅ |\n| [第二章 模型选择](../04-model_types/README.md)                                     | 什么时候分别使用Base、Instruct、Reasoning模型 | ✅ |\n| [第三章 提示词撰写技巧](../05-tips_for_prompt/README.md)                              | 模型参数设置、提示词基础结构、提示词要素、CoT等         | ✅ |\n| [第四章 常见任务示例](../06-common_task_examples/README.md)                          | 文本生成、信息提取、指令问答、代码生成、数学计算等         | ✅ |\n| [第五章 多模态大模型提示词](../07-multimodal_prompt/README.md)                          | 图像分析、视频分析提示词、音频大模型介绍              | ✅ |\n| [第六章 合成数据](../08-synthetic_data/README.md)                                  | 预训练、微调、推理数据合成原理&代码实践              | ✅ |\n| [第七章 RAG检索](../09-RAG/README.md)                                            | 构建向量数据库、文档处理、检索向量、提示词增强、模型生成      | ✅ |\n| [第八章 Agent实践](../10-Agent/README.md)                                        | 函数调用、MCP实践、多Agents介绍              | ✅ |\n| [第九章 RAG实战](../11-swanlab_rag/README.md)                               | 数据准备、RAG实站                        | ✅ |",
    "96": "一级标题：第一章 环境安装\n二级标题：无\n内容：",
    "97": "一级标题：第一章 环境安装\n二级标题：如何获取教程代码？\n内容：\n要在本地运行教程代码，你需要先安装某个版本的Python。\n\n然后，克隆仓库：\n\n```bash\ngit clone https://github.com/828Tina/PromptEngineeringCourse\ncd PromptEngineeringCourse\n```",
    "98": "一级标题：第一章 环境安装\n二级标题：如何使用大模型？\n内容：\n> 代码中需要我们使用*大模型*来完成提示词工程教学任务。\n\n大模型通常包含大量参数，推理时对显卡要求较高，本教程中，我们使用的大部分模型参数量为3B左右，对显存要求&le;20GB。\n\n<div style=\"background:#e7f5ff;color:#000;padding:12px 16px;border-left:4px solid #74c0fc;\">\n<strong>我们提供两种方式，<em>一种是本地调用</em>，<em>另一种是调用API</em>。本地调用需要自己配置至少一块3090，API调用对配置没有要求。下面我们分别从两种方式来讲如何安装环境，可以根据自身需求选择合适的方案👇。</strong>\n</div>\n\n\n### 本地调用大模型的环境安装指南\n\n**安装Miniconda**\n\n[Miniconda](https://www.anaconda.com/docs/getting-started/miniconda/main) 是一个轻量级安装器，用于安装 Conda、Python 以及一些包。Conda 是一个包管理器，方便你设置和切换不同的 Python 虚拟环境和包。它也适合安装那些无法通过 pip 获取的包。\n\n1. 请根据自己的设备安装对应的版本。\n\n2. 安装好Miniconda后需要激活环境，激活后才能使用conda命令\n\n```bash\nsource ~/miniconda/bin/activate\n```\n\n**硬件条件**\n\n如果将模型保存到本地运行，经过我们多次实验，我们发现Qwen系列模型规模较小的一些模型（集中于3B以下），回答质量都不算上乘，因此教程里大多使用的是3B、4B等规模的模型，当然，如果使用API调用的方法，可以使用更大规模的模型，比如7B，甚至32B等。\n\n不过，如果在本地运行，由于都没有使用量化技术，对于GPU的要求在20GB左右，一块3090足矣，如果是更小的模型，那么要求也可以降低。\n\n>  本教程基于英伟达芯片进行各个实验。\n\n\n**环境搭建**\n\n1. python>=3.9\n2. pytorch：在[官网](https://pytorch.org/)安装最新版本即可，需要注意的是2.5版本bug较多，不建议使用\n3. 其他的python库：transformers、modelscope、vllm等\n\n安装命令：\n\n```python\n# 创建环境\nconda create -n test python=3.10\n\n# pytorch\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# 其他\npip install -U transformers modelscope\n```\n\n**框架选择**\n\n本次教程我们使用两种框架，分别是HuggingFace和vLLM，两种框架都可以实现大模型的推理，HuggingFace框架的代码是我们教程中使用次数较多的框架，不过会稍微比较麻烦，速度较慢；vLLM可以让模型推理更加高效快速，显存占用会提高，可以根据自身条件需求选择合适框架。\n\n1. HuggingFace\n\n如果将模型保存在***本地***，可以使用Huggingface提供的推理代码，在后续的举例中，如果没有特殊说明，我们都会使用Qwen系列模型作为基线模型，关于推理代码，我们使用Qwen官网提供的[推理代码](https://www.modelscope.cn/models/Qwen/Qwen2.5-7B-Instruct)\n\n```python\n### 加载模型\nfrom modelscope import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name_or_path = 'Qwen/Qwen2.5-3B'  # 替换为你下载的模型路径\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path,device_map='auto', torch_dtype='auto')\n\n### 提示词\nprompt = \"Hello, Who are you?\"\n\n### 推理代码\nmessages = [\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512,\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(response)\n```\n\n我们所使用的所有的对话模板均和标准对话模板一致，也就是prompt中包含三个不同的角色：`system`、`user` 和 `assistant`。其中 `system` 不是必需的，但有助于设定 `assistant` 的整体行为，帮助模型了解用户的需求，并根据这些需求提供相应的响应。上面的示例仅包含一条 `user` 消息，你可以使用 `user` 消息直接作为 prompt。为简单起见，本指南所有示例（除非明确提及）将仅使用 `user` 消息来作为 `Qwen` 模型的 prompt。上面示例中 `assistant` 的消息是模型的响应。你还可以定义 `assistant` 消息来传递模型所需行为的示例。\n\n不过，在后面的示例中，会有推理型模型，我们使用Qwen3系列模型，其推理代码可以在[官网](https://huggingface.co/Qwen/Qwen3-4B)找到，其实唯一的区别仅在是否开启思考模块，也就是将`enable_thinking`设置为True或者False，其他基本一致。\n\n2. vLLM\n\n<div style=\"background:#e7f5ff;color:#000;padding:12px 16px;border-left:4px solid #74c0fc;\">\n  可以参考更详细的\n  <a href=\"https://docs.vllm.ai/en/latest/\" target=\"_blank\" style=\"color:#1971c2;text-decoration:none;\">\n    vLLM 文档\n  </a>，\n  这里我们仅介绍如何<strong>安装与操作</strong>\n</div>\n\n`vLLM`是伯克利大学LMSYS组织开源的大语言模型高速推理框架，旨在极大地提升实时场景下的语言模型服务的吞吐与内存使用效率。`vLLM`是一个快速且易于使用的库，用于 LLM 推理和服务，可以和HuggingFace 无缝集成。vLLM利用了全新的注意力算法「PagedAttention」，有效地管理注意力键和值。\n\n\nvllm在吞吐量方面，vLLM的性能比[HuggingFace Transformers](https://zhida.zhihu.com/search?content_id=238989790&content_type=Article&match_order=1&q=HuggingFace+Transformers&zhida_source=entity)(HF)高出 24 倍，文本生成推理（TGI）高出3.5倍。\n\n简单点说就是vllm框架的推理速度很快，但是显存占用较高，同样的3B模型，本地推理可能只需要15GB左右，而vllm框架则需要37GB。\n\n首先，我们需要在环境里安装vllm库:\n\n```bash\npip install -U vllm\n```\n\n然后我们使用Python代码运行，这对离线批量推理非常方便。\n\n我们在使用的时候主要参考的是推理时兼容OpenAI接口的情况，详细可以参考[中文文档](https://vllm.hyper.ai/docs/inference-and-serving/openai_compatible_server)。原理其实很简单，如下图所示，我们本来使用openai的各项服务，通过预设openai的API接口连接openai的服务器，然后使用相应的模型，但是我们如果希望使用本地模型，不希望去使用openai 的服务器准备的模型，因为很有可能因为网络连接的问题导致无法使用模型，那么我们可以使用vllm框架，输入仍是openai的API接口，因为vllm相当于模仿OpenAI搞了另外的路径，然后创建一个独立的后端服务，进而我们就可以使用本地的模型啦。\n\n简单来说vllm相当于使用的仍然是OpenAI的API接口，但是会适配本地模型的调用，本地模型在推理的时候相当于发送一个API请求，绕过了OpenAI的接口。\n\n<div style=\"display:flex;justify-content:center;\">\n  <figure style=\"text-align:center;margin:0;\">\n    <img src=\"./picture/openai_api_vllm.png\" style=\"width:600px;\" alt=\"vllm本地推理模型原理图\">\n    <figcaption>vllm本地推理模型原理图</figcaption>\n  </figure>\n</div>\n\n\n具体用法可以参考[Qwen模型的vllm用法](https://qwen.readthedocs.io/zh-cn/latest/deployment/vllm.html)。另外，也可以从[官网](https://docs.vllm.ai/en/latest/index.html)获得更加详细的信息。我们简单讲述下我们教程里的使用步骤。\n\n1. 在本地默认端口跑本地模型\n\n开启一个终端，运行下面的代码\n\n```Bash\nvllm serve /your/path/of/model\n```\n\n当出现下面的提示的时候，表示已经开启对应的服务。\n\n<div style=\"display:flex;justify-content:center;\">\n  <figure style=\"text-align:center;margin:0;\">\n    <img src=\"./picture/vllm_serve.png\" style=\"width:600px;\" alt=\"vllm成功使用\">\n    <figcaption>vllm成功使用</figcaption>\n  </figure>\n</div>\n\n\n\n然后我们需要再开启一个终端页面。\n\n2. 开启新的终端页面运行各个代码\n\n在新的终端页面，我们就可以跑我们对应的服务了，需要注意的是，我们使用大模型来进行推理的时候可以使用[openai的prompt的API的接口](https://openai.apifox.cn/api-55352401)或者[messages对应的API接口](https://openai.apifox.cn/api-67883981)等，输入接口参考给出的文档即可，讲起来比较抽象，我们看下代码例子：\n\n*模型生成回答的代码* ：\n\n```python\nresults = utils.openai_completion(\n            prompts=batch_inputs,\n            model_name=model_name,\n            batch_size=request_batch_size,\n            decoding_args=decoding_args,\n            # logit_bias={\"50256\": -100},  # prevent the <|endoftext|> token from being generated\n        )\n```\n\n*对应的工具utils的* *openai_completion函数*：\n\n```Python\ndef openai_completion(\n    prompts: Union[str, Sequence[str], Sequence[dict[str, str]], dict[str, str]],\n    decoding_args: OpenAIDecodingArguments,\n    model_name=\"text-davinci-003\",\n    sleep_time=2,\n    batch_size=1,\n    max_instances=sys.maxsize,\n    max_batches=sys.maxsize,\n    return_text=False,\n    **decoding_kwargs,\n):\n……\n……\n    completion_batch = client.completions.create(\n                prompt=prompt_batch, **shared_kwargs\n            )\n    choices = completion_batch.choices\n\n……\n……\n```\n\n`completion_batch = client.completions.create`的内容其实使用的就是openai库中的推理问答的函数。\n\n经过这些步骤后，我们在运行推理服务的时候就能充分利用GPU资源，高效完成各项推理任务。\n\n如果不太理解具体如何做，在下面的教程中按照步骤运行相应的文件，不明白原理的可以参考我们的代码。\n\n> 我们还有基于昇腾NPU的vllm[教程](./1.huawei.md)\n\n\n\n### 调用API使用大模型的环境安装指南\n\n上面的选择或多或少都对你的硬件条件要求较高，当然，经验丰富的你可能会选择大模型量化，在更低的精度、更低的显存占用、更快的推理速度等来实现推理，但是或许我们会有更好的选择，比如调用API。\n\n大模型的 API 调用，简单来说，是指开发者或用户通过应用程序编程接口（API） 与大模型进行交互，从而利用大模型的能力完成特定任务的进程，这些大模型无需部署到本地，你使用的资源其实是这些厂商提供的服务集群，不仅推理速度快，而且并不占用显存，不过每个模型会有费用的消耗，不一定能免费使用。\n\n1. **阿里百炼**\n\n或者我们也可以使用阿里百炼云平台，其中Qwen系列模型都是最新的，而且性价比也还可以，具体如何使用可以参考[API文档](https://bailian.console.aliyun.com/?spm=5176.12818093_47.resourceCenter.1.223c2cc96V9eQn&tab=api#/api/?type=model&url=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F2712576.html)，文档内容非常详细，无需多余的函数库安装，按照文档指示使用即可。\n\n<img src=\"./picture/bailian_api.png\" alt=\"硅基流动API文档\" style=\"zoom:50%;\" />\n\n2. **硅基流动**\n\n那我们简单找一个网站，比如[硅基流动](https://cloud.siliconflow.cn/sft-d1n0sv33jrms738gmgpg/models)，很多10B以下的小模型可以免费使用，而更大的模型的费用也不是很高，那我们找到Qwen3-8B的模型来实验下：\n\n<img src=\"./picture/siliconflow_model.png\" alt=\"硅基流动模型广场\" style=\"zoom:50%;\" />\n\n我们选择[API文档](https://docs.siliconflow.cn/cn/api-reference/chat-completions/chat-completions)，里面就有详细的创建文本对话请求的教程，我们使用Chat Completions的Python版本，\n\n<img src=\"./picture/api_chat_completions.png\" alt=\"硅基流动API文档\" style=\"zoom:50%;\" />\n\n将代码复制，其中的`model`更改为我们需要的模型，要记住，只能是硅基流动的模型广场里有的模型，没有上架的模型会报错。然后其中的`Authorization`需要我们的api_key，`messages`可以进行多轮对话，具体的推理代码可以在我们的代码中找到。",
    "99": "一级标题：第二章 模型选择\n二级标题：无\n内容：\n提示词工程应用于开发和优化提示词（Prompt），提高大语言模型的处理复杂任务场景的能力，比如特定任务领域的问答或者算术推理的能力。而不同的模型由于能力不同以及适用领域的不同，构建提示词需要参考模型类型，从而提高模型处理复杂任务的能力，本节介绍了不同类型的大语言模型能力，包括Base模型、Instruct模型以及Reasoning模型，并补充每个类型的模型适用任务领域的差异，帮助用户了解构建提示词的时候需要使用哪种类型的模型并提供指导建议。",
    "100": "一级标题：第二章 模型选择\n二级标题：Base模型\n内容：\n大模型通常包含预训练、微调、强化学习等训练阶段，其中Base模型是预训练后，没有经过特定领域微调的大语言模型。它通过海量通用数据，比如网页文本、书籍、论文、百科知识、代码等进行无监督或者半监督训练，学会了大量语言结构、逻辑规则以及世界知识，具备基础的语言理解和生成能力。\n\nBase模型的核心特点在于其 “通用性” 与 “可塑性”。一方面，它能处理多种自然语言任务，如文本生成、问答、翻译；另一方面，由于未被特定任务 “固化”，可灵活适配不同领域需求。其出色的上下文理解能力，能精准捕捉文本中的语义关联与逻辑线索，即使面对长段落或复杂语境，也能输出连贯、合理的内容。同时，base 模型具备较强的泛化能力，可基于训练中积累的知识，快速适应新场景或任务，降低 “过拟合” 风险。",
    "101": "一级标题：第二章 模型选择\n二级标题：Instruct模型\n内容：\nInstruct 模型，即指令微调模型（Instruction-Tuned Model），是在基础预训练模型（Base Model）之上，通过特定优化技术使其更擅长理解和执行人类指令的大型语言模型。其核心在于**指令微调（Instruction Tuning** 这一关键步骤：使用包含 “指令 - 响应” 对的数据集对 Base 模型进行有监督微调，让模型学习如何根据明确的用户指令生成高质量回复。例如，在训练数据中会有类似这样的样本：\n\n```Plain\n指令：“用简洁的语言解释区块链技术。”\n目标响应：“区块链是一种去中心化的分布式账本技术……”\n```\n\n这种训练方式使 Instruct 模型具备了更强的任务泛化能力，能够将在训练中学习到的指令理解模式应用到未见过的指令类型上，而非仅仅依赖预训练阶段学到的语言模式。\n\n相比于Base 模型主要擅长文本续写等无明确目标的生成任务，输出容易偏离用户需求，以重复文本、其他特殊字符等形式生成回答，Instruct模型会严格聚焦于遵循用户指令回答问题；同时通过指令微调数据集中的高质量示例，Instruct 模型学会了在不确定时保持谨慎，减少生成无根据信息的概率。例如，在回答历史问题时，Base 模型可能编造不存在的事件细节，而 Instruct 模型更倾向于承认不确定性或提供查证建议。Instruct 模型经过优化，回复更符合人类对话习惯，包括适当的礼貌表达、分步解释复杂问题等。\n\n简单来说，Instruct模型是基于Base基础上将指令部分的新特征添加到模型的认知中，通用问答领域内，不需要Base模型添加过多的约束，就能够回答大多数问题，也就是**擅长直接执行指令**，而经过专有领域数据集微调后的模型甚至可以直接询问，比如医疗领域微调模型。但是Instruct模型通常对训练数据的质量要求较高，即便少量的数据也可以达到很好的微调效果，但往往成本要求较高；同时Base 模型的输出更加自由，对于一些开放式的文本生成任务，如创意写作、故事续写等，可能更能发挥其优势，因为它不会受到指令的过多限制，能够产生更具多样性的内容。\n\nInstruct 模型经指令微调，擅长理解并执行明确指令，适用于问答、邮件生成、翻译等指令驱动任务，以及智能客服、教育辅助等需贴合用户意图的场景，能精准输出实用结果。Base 模型是未微调的基础模型，语言通用性强、生成灵活，适用于二次开发（如领域微调底座）、开放式创作（如小说续写）及语言规律研究等场景，是创新和定制化的起点，二者分工互补，共同支撑大模型应用生态。\n\n基于上述优势，我们可以用Instruct做下面的一些任务：\n\n1. 指令问答\n2. 情感分类\n3. 聊天对话\n4. 代码生成\n5. 数学计算\n6. 微调模型评估\n\n举这些例子并不代表模型只能做这些任务，只不过这些任务具有代表性，可以让用户更好的掌握提示词的用法。",
    "102": "一级标题：第二章 模型选择\n二级标题：Reasoning模型\n内容：\n在前文我们讲述，大模型通常包含预训练、微调等阶段，当然，所谓的强化学习更多指的是人类偏好对齐，并不是推理型模型的训练原理，除了这些训练方式，我们还见证了诸如 RAG（生成增强检索）和代码助手等专用应用的兴起。在2025年，大模型的发展更趋近于注重针对特定领域和应用的优化，推理型模型的开发正是这些专业化的集大成者。\n\n🤔那么什么是Reasoning（推理型）模型呢？\n\n如果你从事人工智能或机器学习工作，可能会对那些模糊且争议不断的定义有所了解。「推理型模型」这一术语也不例外。最终，某人会在论文中正式定义它，但很快就会在下一篇文章中重新定义，如此循环。\n\n在本文中，我们将`推理`定义为回答那些**需要复杂、多步骤生成并包含中间步骤的问题的过程**。例如，像`法国的首都是哪里？`这样的事实性问题不涉及推理。相比之下，像`如果一列火车以 60 英里每小时的速度行驶，行驶 3 小时，它能走多远？`这样的问题则需要一些简单的推理。比如，它需要认识到距离、速度和时间之间的关系，然后得出答案。\n\n在我们定义了推理型模型后，接下来可以进入更有趣的部分：如何构建和改进大语言模型以应对推理任务。然而，在深入技术细节之前，重要的是要考虑在什么情况下实际上需要推理型模型。\n\n🧐 我们什么时候需要推理型模型？\n\n推理型模型的设计目的是擅长解决一些复杂任务，如解谜、进阶数学问题和具有挑战性的编程任务。然而，对于像总结、翻译或基于知识的问题回答这些简单任务，它们并不是必需的。实际上，使用推理型模型处理所有任务可能会导致低效且成本较高。例如，推理型模型通常使用成本较高，生成的回答更冗长，并且有时由于“过度思考”而更容易出错。在这里，有一个简单的原则：根据任务选择合适的工具（或大语言模型类型）。\n\n事实上，在Instruct模型对应的数学计算示例中，我们举了一个思维链的例子，其实更应该归于`推理`一类，因为它其实是在鼓励模型生成中间推理步骤，不直接跳到最终答案，这类方法我们称为**测试时计算扩展[1]**。\n\n推理时扩展并不需要模型经过训练，但是如果要实现推理效果，直接对模型进行训练也是一种方法，往往有多种方法，比如RL（强化学习）、RL+SFT（强化学习+微调）、蒸馏等等，其中蒸馏是利用微调技术将知识从教师模型中学习到，而教师模型通常是具有高质量推理过程的模型。\n\n总而言之，Reasoning模型更多实现的是更具有挑战性、推理思考的任务，模型采用Qwen3系列的模型，因为它本身就具备思考过程。\n\n[1].[Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/abs/2408.03314)",
    "103": "一级标题：3.1 模型参数设置\n二级标题：无\n内容：\n无论是本地模型推理，还是API调用大模型进行交互，你都需要通过配置一些参数以获得不同的提示结果。\n\n<div style=\"background:#e7f5ff;color:#000;padding:12px 16px;border-left:4px solid #74c0fc;\">\n<strong>实际上大部分的API调用平台中模型的参数设置都采用默认参数，也就是最合适大模型生成的模型参数设置，很少需要我们手动调整(默认参数我们采用<a href=\"https://bailian.console.aliyun.com/?spm=5176.12818093_47.resourceCenter.1.223c2cc96V9eQn&tab=api#/api/?type=model&url=2712576\" target=\"_blank\" rel=\"noopener\">阿里云百炼模型广场中参数设置默认值</a>)</strong><br/>不过如果对其中某些参数有特殊需求，比如调低`temperature`限制生成的多样性，使得最终输出的结果更固定些，会需要我们对参数做一个简单的调整</br>下面我们展示了比较常见的<strong>参数设置以及核心原理👇</strong>：\n\n</div>\n\n| 参数名称 | 默认值 | 核心原理 |\n|:---:|:---|:---|\n| max_tokens | 4096 | 本次请求返回的最大Token数，如需提高该参数值（4097~8192范围）。**有时发现这个范围可能比开源模型给出的最大生成长度要小是因为资源受限，显存不够** |\n| temperature | 0.7 | 采样温度，控制模型生成文本的多样性。取值范围：[0, 2)。temperature越高，生成的文本更多样，反之，生成的文本更确定。**一般不需要修改，已是最佳值** |\n| top_p | 0.8 | 核采样的概率阈值，控制模型生成文本的多样性。取值范围：（0, 1.0]。top_p越高，生成的文本更多样。反之，生成的文本更确定。**由于temperature与top_p均可以控制生成文本的多样性，因此建议只设置其中一个值。** |\n| enable_thinking | True | 是否开启思考模式，**仅适用于Qwen3模型，其他模型（阿里云百炼）没有这个选项** |\n| thinking_budget | 4000 | 思考过程的最大长度，**只在enable_thinking为true时生效。** |\n\n\n\n> 其他的参数基本不用修改，因此不做参考，其实我们在实际使用的时候大多也就考虑修改生成文本的最大长度或者文本多样性的参数，因此仅展示上述参数了解即可。",
    "104": "一级标题：3.2 提示词结构\n二级标题：无\n内容：\n通用的提示词里至少包含下面这些部分：\n\n- 指令：想要模型执行的特定任务或指令。\n- 上下文：包含外部信息或额外的上下文信息，引导语言模型更好地响应。\n- 例子：当用户的问题不明确的时候，可以借助举例让大模型生成类似的结果。\n- 输入数据：用户输入的内容或问题。\n- 输出指示：指定输出的类型或格式。\n\n我们举一个简单的例子展示效果，比如让大模型写一首诗：\n\n*提示词* ：\n\n```Plain\n指令：你是一个富有创造力的诗人，擅长写诗歌，请你根据以下主题创作一首诗歌。\n主题：春天的花园\n输出格式：诗歌\n```\n\n*回答* ：\n\n```Plain\n桃红柳绿春水碧，\n鸟语花香醉游人。\n人间四月芳菲尽，\n此景只应天上逢。\n```\n\n在这次的例子中，我们令指令是“创造诗歌”，并且让模型完成了一次简单的角色扮演。主题，也就是输入数据是诗歌的中心主旨，并且要求模型回答的输出格式是“诗歌”，模型很好的完成了任务。\n\n而上下文主要考察模型的上下文理解能力，一般用于文本分析、信息提取或者文本摘要任务，这在后面的章节会提及，不过我们也可以简单举一个信息提取任务的例子：\n\n*提示词* ：\n\n```Plain\n指令：从以下新闻文本中提取关键信息，包括事件类型、时间、地点、人物、组织和关键数据。\n上下文：2025年7月8日，国家航天局宣布，嫦娥七号探测器计划于2026年发射，将开展月球南极环境与资源勘查。据悉，嫦娥七号将搭载4台国际合作载荷，包括与欧洲空间局合作的月表负离子分析仪。\n输入问题：仅告诉嫦娥七号的发射时间，不包含其他信息\n```\n\n*回答* ：\n\n```Plain\n2026年\n```\n\n这个信息提取的例子中，我们对新闻中的“嫦娥七号”的发射时间进行提取，模型准确的完成了我们交代的任务，新闻内容我们放在了上下文里，帮助模型更好地理解任务并引导预期的输出格式。\n\n注意，提示词所需的格式取决于您想要语言模型完成的任务类型，并非所有以上要素都是必须的。我们会在后续的指南中提供更多更具体的示例。",
    "105": "一级标题：3.3 提示词要素\n二级标题：无\n内容：\n提示词（Prompt）是向大语言模型传递任务指令的文本，其设计质量直接影响模型输出的效果，提示词，尤其是指令的构建需要注意一些核心要素，掌握了这些核心要素，大模型就能够不经过微调训练等得到符合预期的回复。",
    "106": "一级标题：3.3 提示词要素\n二级标题：明确任务类型\n内容：\n要非常具体地说明你希望模型执行的指令和任务。提示越具描述性和详细，结果越好。特别是当你对生成的结果或风格有要求时，这一点尤为重要。不存在什么特定的词元（tokens）或关键词（tokens）能确定带来更好的结果。更重要的是要有一个具有良好格式和描述性的提示词。事实上，在提示中提供示例对于获得特定格式的期望输出非常有效。\n\n- 使用动词（如 “总结”“生成”“解释”“翻译”）明确任务类型。 示例：\n\n```Plain\n❌ \"北京的天气\"\n✅ \"请生成一篇关于北京天气的简短报告\"\n```\n\n- 限定输出格式：指定所需的输出形式（如 JSON、列表、诗歌）。 示例：\n\n```Plain\n请以JSON格式列出三个编程语言及其特点：\n```",
    "107": "一级标题：3.3 提示词要素\n二级标题：明确任务目标\n内容：\n在明确了任务类型，比如是问答类型的话，你可能需要对任务目标加以约束，比如让大模型做什么、不做什么。下面我们举个例子。\n\n大模型由于知识广泛，回答问题的时候发散性思维比较强，因此有时候的回答会带有他的分析，比如我们想要计算一个简单的数学问题，并且只想要得到结果的数字：\n\n*提示词* ：\n\n```Plain\n请你计算出3*13的结果\n```\n\n*回答* ：\n\n```Plain\n3乘以13的结果是39。\n```\n\n可以看到结果往往是一句话，但是我们希望只有数字，那么我们可以在原有以示词的基础上进行修改：\n\n*提示词* ：\n\n```Plain\n请你计算出3*13的结果，仅输出结果，不要输出任何其他内容。\n```\n\n*回答* ：\n\n```Plain\n39\n```\n\n这里设计提示词的技巧是，让大模型知道要做什么，不要做什么，同时把目标明确，这样（说要做什么）更加的具体，并且聚焦于（有利于模型生成良好回复的）细节上。",
    "108": "一级标题：3.3 提示词要素\n二级标题：提供上下文信息\n内容：\n- 提供充足的文本信息或者背景知识，这点在文本摘要、信息提取任务中尤为重要\n\n示例：\n\n```Plain\n上下文：2025年7月8日，国家航天局宣布，嫦娥七号探测器计划于2026年发射，将开展月球南极环境与资源勘查。据悉，嫦娥七号将搭载4台国际合作载荷，包括与欧洲空间局合作的月表负离子分析仪。\n输入数据：仅告诉嫦娥七号的发射时间\n```\n\n- 当使用少样本提示的时候，需要补充些例子作为前文信息\n\n示例：\n\n```Plain\nQ: 苹果公司成立于哪一年？\nA: 1976\n\nQ: 爱因斯坦获得诺贝尔奖是在哪一年？\nA:\n```",
    "109": "一级标题：3.3 提示词要素\n二级标题：结构化提示词\n内容：\n我们在使用提示词的时候往往会直接把文本、问题等一股脑全部输入到大模型的输入中，但是实际上我们可以通过严格控制提示词的格式来提升模型输出的质量，因为结构化的提示词汇更具可控性，尤其是在需要精确格式或复杂任务的场景中。\n\n想象一下，如果输入的新闻数据是以下格式：\n\n```Plain\n2025年7月8日，国家航天局宣布，嫦娥七号探测器计划于2026年发射，将开展月球南极环境与资源勘查。据悉，嫦娥七号将搭载4台国际合作载荷，包括与欧洲空间局合作的月表负离子分析仪。\n```\n\n如果询问嫦娥七号的降落地点，模型需要回顾上下文，然后给出“降落地点是月球南极”这样的回答，这并不标准，但是如果输入数据是这样的json格式：\n\n```JSON\n{\"事件类型\": \"科技新闻\",\"时间\": [\"2025年7月8日\", \"2026年\"],\"地点\": [\"月球南极\"],\"组织\": [\"国家航天局\", \"欧洲空间局\"],\"关键数据\": [\"4台\"]}\n```\n\n那么想要得到“月球南极”这样的答案无疑是容易的，这是输入的上下文的结构化，而如果对于提示词本身，也可以做结构化改造。\n\n不过json格式的改造对于提示词来说还是有点难度，我们通常使用markdown格式来结构化。Markdown 是一种轻量级标记语言，常用于格式化文本（如表格、列表、代码块）。结构化提示词可以使用 Markdown 语法来表达，但核心是 “结构化” 的思想，而非特定语言。\n\n我们举一个例子：\n\n*提示词* ：\n\n```Plain\n# 任务：提取电影信息\n请从以下文本中提取关键信息，按要求填写：",
    "110": "一级标题：3.3 提示词要素\n二级标题：待分析文本\n内容：\n\"电影《流浪地球3》将于2027年春节上映，导演是郭帆，主演包括吴京和刘德华，票房预期50亿。\"",
    "111": "一级标题：3.3 提示词要素\n二级标题：提取要求\n内容：\n- 类型：______\n- 上映时间：______\n- 导演：________\n- 主演：______\n- 票房预期：______",
    "112": "一级标题：3.3 提示词要素\n二级标题：输出格式\n内容：\n直接填写上述空白，保持列表结构。\n```\n\n*回答* ：\n\n```Plain\n- 类型：电影\n- 上映时间：2027年春节\n- 导演：郭帆\n- 主演：吴京, 刘德华\n- 票房预期：50亿\n```\n\n这个例子用 `#` 标题区分任务模块，用 `-` 列表明确提取项，用 `##` 分隔不同内容块，是典型的 Markdown 结构化提示词，简洁且格式清晰。\n\n好了，我们已经学会了提示词的结构和要素，在接下来的章节中，我们将尝试更高级的提示词使用技巧。",
    "113": "一级标题：3.3 提示词要素\n二级标题：复杂任务拆解\n内容：\n大模型和人类一样，哪怕在现实生活中，如果Leader给你分配了一个比较复杂、内容较多的任务，经验丰富的你大概率会先列出任务细节，也就是大纲，然后对着大纲的每一步进行细化，比如实验步骤、完成时间、分配员工等，细化完后就开始每一步的完成细节了。大模型也是同样，比如做一个统计公司研发投入计划、资源配置等要素的PPT，如果你仅仅是把财务报表数据、整体PPT风格、要完成的目标交给大模型，大模型估计会给你一个乱七八糟的PPT出来，但是如果你先用大模型根据你的要求生成PPT的内容模板，然后再让大模型根据你的模板生成对应风格的PPT，那么显然最终的效果和你预计的目标就会更加接近。\n\n*提示词* ：\n\n```Plain",
    "114": "一级标题：3.3 提示词要素\n二级标题：文本内容：\n内容：\n《流浪地球 2》是由郭帆执导的科幻灾难电影，于 2023 年上映，故事围绕《流浪地球》前作展开，以计划建造 1 万座行星发动机的时代为背景。\n\n在不远的未来，太阳急速衰老膨胀，即将吞噬太阳系，地球面临灭顶之灾。为应对危机，地球各国成立联合政府，提出数百个自救计划，其中 “移山计划”“方舟计划”“逐月计划” 和 “数字生命计划” 进入论证阶段。“移山计划” 由中国提出，旨在建造 1 万座行星发动机推动地球前往新家园；“方舟计划” 是美国提议的在地球同步轨道建立空间站以带领人类逃离；“逐月计划” 由俄罗斯提出，想改造月球为逃生舱，后因月球结构等问题并入 “移山计划”；“数字生命计划” 则是将人类意识数字化，实现永生，但最终被伦理委员会禁止。经过考量，“移山计划” 被选定，人类开始着手建造行星发动机，同时准备建造卫星发动机以放逐月球，摆脱月球引力。\n\n然而，计划推进过程中危机四伏。2044 年，太空电梯基地遭遇危机，处在 9 万公里高度的方舟空间站爆炸坠落，引发连锁反应，导致太空电梯基地被摧毁，流浪地球计划面临重大挑战。影片中，满腔赤诚的刘培强（吴京饰）历经层层考验成为航天员大队的一员，他与韩朵朵（王智饰）在此过程中相知相恋，而刘培强也在后续故事中为了地球和家人，不断经历着艰难抉择与挑战。\n\n另一边，量子科学家图恒宇（刘德华饰）则与 “数字生命计划” 紧密相关。他致力于将女儿图丫丫的意识数字化，即使该计划被禁止，他也未放弃。在一系列意外后，图恒宇自己也意外进入数字世界，以一种特殊的方式继续参与到拯救地球的行动中，他在数字空间中的经历，也为影片增添了一层神秘的 “元宇宙” 色彩。\n\n联合政府中国代表周喆直（李雪健饰）则在国际舞台上，为 “移山计划” 的推进四处奔走，面对各方压力和危机，他始终坚定信念，相信人类能够团结起来拯救地球，展现出了强大的责任感与使命感。\n\n随着故事发展，月球发动机建造完成，但却因未知原因被启动，月球开始向地球逼近，引发了全球性的灾难，地震、海啸等灾害频发，人类再次面临生死考验。为了阻止月球撞击地球，人类决定启动行星发动机，利用其推力将月球推离。在这一过程中，无数普通人与主角们一起，克服了重重困难，最终成功放逐月球，为地球开启流浪之旅奠定了基础。\n\n《流浪地球 2》通过展现刘培强、图恒宇、周喆直等众多角色的经历，以及全球人类在末日危机下的挣扎与抗争，呈现了一个宏大而震撼的科幻世界，探讨了人类面对绝境时的生存选择、亲情、责任与勇气等主题，传达出 “人类命运共同体” 理念和 “没有人的文明，毫无意义” 的深刻内涵，以其壮观的视效和动人的情节，成为中国科幻电影的重要代表作。",
    "115": "一级标题：3.3 提示词要素\n二级标题：任务：\n内容：\n请从上述文本中提取出主要信息，然后写出PPT模板，按照markdown格式输出\n```\n\n*回答* ：\n\n~~~Plain\n以下是一个基于《流浪地球 2》的主要信息创建的PPT模板，采用Markdown格式展示：\n\n```markdown\n# 《流浪地球 2》概览",
    "116": "一级标题：3.3 提示词要素\n二级标题：基本信息\n内容：\n- 导演：郭帆\n- 上映时间：2023年\n- 类型：科幻灾难片",
    "117": "一级标题：3.3 提示词要素\n二级标题：故事背景\n内容：\n- 太阳急速衰老膨胀，即将吞噬太阳系，地球面临灭顶之灾。\n- 各国联合政府提出多种自救计划，包括：\n  - 移山计划：中国提出的建造1万座行星发动机推动地球前往新家园。\n  - 方舟计划：美国提出的在地球同步轨道建立空间站以带领人类逃离。\n  - 逐月计划：俄罗斯提出的改造月球为逃生舱，后并入移山计划。\n  - 数字生命计划：将人类意识数字化，实现永生，但最终被伦理委员会禁止。",
    "118": "一级标题：3.3 提示词要素\n二级标题：主要计划及进展\n内容：\n- **移山计划**：中国提出，中国开始建造1万座行星发动机。\n- **卫星发动机**：同时建造用于放逐月球，摆脱月球引力。\n- **方舟计划**：美国提出，建立在地球同步轨道上的空间站。\n- **逐月计划**：俄罗斯提出，改造月球为逃生舱。\n- **数字生命计划**：将人类意识数字化，实现永生。",
    "119": "一级标题：3.3 提示词要素\n二级标题：挑战与危机\n内容：\n- **太空电梯基地危机**：2044年，太空电梯基地遭遇危机，方舟空间站爆炸坠落，引发连锁反应，导致基地被摧毁。\n- **月球发动机启动**：月球发动机建造完成，因未知原因被启动，月球开始向地球逼近，引发全球性灾难。\n- **行星发动机启动**：为阻止月球撞击地球，人类决定启动行星发动机，将月球推离。",
    "120": "一级标题：3.3 提示词要素\n二级标题：角色介绍\n内容：\n- **刘培强**：吴京饰，航天员大队成员，与韩朵朵相知相恋，面对抉择和挑战。\n- **图恒宇**：刘德华饰，量子科学家，致力于将女儿的意识数字化，最终进入数字世界。\n- **周喆直**：李雪健饰，联合政府中国代表，为“移山计划”推进四处奔走。",
    "121": "一级标题：3.3 提示词要素\n二级标题：主题探讨\n内容：\n- **人类命运共同体**：展现人类团结一致面对绝境。\n- **生存选择、亲情、责任与勇气**：探讨人类在末日危机下的挣扎与抗争。\n- **没有人的文明，毫无意义**：传达深刻内涵。",
    "122": "一级标题：3.3 提示词要素\n二级标题：结论\n内容：\n《流浪地球 2》以其壮观的视效和动人的情节，成为中国科幻电影的重要代表作。\n```\n\n这个PPT模板简洁明了地展示了《流浪地球 2》的主要信息，帮助观众快速了解剧情背景、角色和主题等核心内容。\n~~~\n\n这样我们就简单生成了一个模板，接下来我们把它丢给Kimi的PPT一键生成，我们可以在Kimi的工具栏里找到\n\n<img src=\"./picture/kimi.png\" alt=\"Kimi的PPT助手\" style=\"zoom:50%;\" />\n\n选择合适的模板风格，我们就能快速获得PPT了，\n\n<img src=\"./picture/kimi_ppt.png\" alt=\"Kimi的PPT助手\" style=\"zoom:50%;\" />\n\n不过，很显然如果直接让大模型生成模板也可以，但是由于没有拆解成小任务依次执行，一旦其中某一步需要修改，重新让模型生成是一个很耗时耗力的过程，因此学会拆解复杂任务能够帮助模型更好的理解任务目标以及实现。",
    "123": "一级标题：3.4 其他提示词技巧\n二级标题：无\n内容：",
    "124": "一级标题：3.4 其他提示词技巧\n二级标题：零样本提示\n内容：\n在实际使用大模型的时候，尤其是开源的大模型时，我们通常倾向于使用经过许多高质量指令数据微调后的模型，比如Instruct模型进行指令问答，而经过大量数据训练并调整指令的LLM能够执行零样本提示任务。\n\n零样本提示的定义是不提供任何示例，直接让模型完成任务，依赖模型自身的知识和推理能力。一般用于简单的问答，而不对生成的结果有什么格式、内容等要求的情况，我们举一个例子：\n\n*提示词* ：\n\n```Plain\n把‘我今天很开心’翻译成英语。仅输出结果\n```\n\n*回答* ：\n\n```Plain\nI am very happy today.\n```\n\n或者：\n\n*提示词* ：\n\n```Plain\n请列出红色的三种水果，仅输出结果\n```\n\n*回答* ：\n\n```Plain\n苹果、草莓、樱桃\n```\n\n请注意，在上面的提示中，我们没有向模型提供任何示例——这就是零样本能力的作用。",
    "125": "一级标题：3.4 其他提示词技巧\n二级标题：少样本提示\n内容：\n虽然大型语言模型展示了惊人的零样本能力，但在使用零样本设置时，它们在更复杂的任务上仍然表现不佳。少样本提示可以作为一种技术，以启用上下文学习，我们在提示中提供演示以引导模型实现更好的性能。\n\n少样本提示（Few-shot Prompting）是指在提示词中提供少量示例（通常 1-5 个），让模型通过模仿示例的模式来完成任务。它比零样本提示更 “具象”，通过示例明确任务规则或格式，帮助模型更精准地理解需求，尤其适合需要特定格式、逻辑或风格的任务。我们举一个简单的例子：\n\n*提示词* ：\n\n```Plain",
    "126": "一级标题：3.4 其他提示词技巧\n二级标题：问题\n内容：\n请你根据输入的内容，判断其情绪属于“积极”、“消极”、“中立”的哪一个标签？仅输出标签内容",
    "127": "一级标题：3.4 其他提示词技巧\n二级标题：示例\n内容：\n输入：我今天心情很好，工作也很顺利。\n输出：积极\n\n输入：我对这部电影的评价很差，觉得浪费时间。\n输出：消极",
    "128": "一级标题：3.4 其他提示词技巧\n二级标题：用户输入\n内容：\n输入：我对这次旅行的体验还可以，但没有特别惊喜。\n输出：\n```\n\n*回答* ：\n\n```Plain\n中立\n```\n\n通过多样本提示，即使任务涉及细微的判断差异（比如区分 “中性事实” 和 “带情绪的描述”），模型也能更稳定地遵循规则，减少偏差。",
    "129": "一级标题：3.4 其他提示词技巧\n二级标题：思维链(CoT)提示\n内容：\n思维链提示是通过引导模型 `一步步说出推理过程`来解决问题的方式。它不直接要求答案，而是先让模型像人一样 “思考”—— 拆解问题、分析逻辑、逐步推导，最终得出结论。这种方式能帮助模型处理需要多步推理的任务（如数学题、逻辑题），减少因跳跃性思维导致的错误。\n\n我们举一个简单的例子，对比一下普通的零样本提示和思维链（CoT）提示的区别：\n\n*提示词* ：\n\n```Plain\n我今年6岁，是妹妹的两倍。那么在我过80岁生日的时候，我的妹妹多少岁？\n```\n\n*回答* ：\n\n```Plain\n您今年6岁，是您妹妹年龄的两倍。这意味着您妹妹今年3岁（因为6除以2等于3）。\n当您过80岁生日时，您妹妹的年龄将是80除以2，即40岁。\n所以，当您过80岁生日时，您的妹妹将会是40岁。\n```\n\n显而易见，当模型没有自己的思考过程的时候，回答的结果千奇百怪，当我过80岁生日的时候，我妹妹比我小3岁，明显应该是77，但是如果我们在原有的提示词基础上添加了思维链呢？\n\n*提示词* ：\n\n```Plain\n我今年6岁，是妹妹的两倍。那么在我过80岁生日的时候，我的妹妹多少岁？让我们一步一步思考\n```\n\n*回答* ：\n\n```Plain\n好的，我们来一步一步分析这个问题。\n\n1. **确定现在的年龄关系**：你现在6岁，你妹妹的年龄是你的一半，也就是说你妹妹现在3岁。\n\n2. **计算年龄差**：你们之间的年龄差是一样的。所以你和你妹妹的年龄差是6 - 3 = 3岁。\n\n3. **预测未来的情况**：无论时间如何流逝，你们之间的年龄差始终不变。当你80岁时，你妹妹的年龄就是你年龄减去你们之间的年龄差。\n\n4. **具体计算**：当你是80岁的时候，你的妹妹将会是80 - 3 = 77岁。\n\n因此，在你80岁生日的时候，你的妹妹将会是77岁。\n```\n\n当模型学会将解答按照步骤拆解，一步一步思考，最终给出的答案的准确度要比零样本提示要高。\n\n不过要注意，并不是所有的问题都适合用该方法，很显然，当你遇到难以解决的数学问题的时候，CoT会有奇效。可如果你仅仅只是问一个很简单的问题，比如“中国的首都在哪？”，让模型一步一步思考显然是一个很浪费时间的行为。\n\n因此可以得出结论，当需要保证结果的准确度、需要解决较为复杂的数学、推理问题的时候，思维链提示是一个好的方法，其他情况可以酌情考虑。",
    "130": "一级标题：第三章 提示词撰写技巧\n二级标题：无\n内容：\n提示词并不是简单地堆叠命令来实现目标，背后往往涉及模型参数设置、提示词要素、还有些提示词设计的通用技巧等等，下面的教程我们将详细讲述这些技巧的原理。\n\n\n📖下面是我们本节的课程目录：\n\n| 教程章节   | 核心内容 |\n|:--------|:------|\n| [3.1.模型参数设置](./1.model_parameter_settings.md)   | 模型推理参数设置   |\n| [3.2.提示词结构](./2.prompt_structure.md)   | 提示词基础结构  |\n| [3.3.提示词要素](./3.prompt_elements.md)   | 提示词需要注意的一些要素   |\n| [3.4.其他提示词技巧](./4.other_prompt_techniques.md)   | 零样本提示、少样本提示、CoT   |",
    "131": "一级标题：第四章 常见任务示例\n二级标题：无\n内容：",
    "132": "一级标题：第四章 常见任务示例\n二级标题：文本生成\n内容：\n无论是哪个模型，文本生成任务他们都能完成的不错，但是如果仅输入你要续写的内容而没有相应的指令，比如“这棵树”，这句话没有加任何约束限制，`base`模型会添加很多的字符填满到最长回答长度，因为模型无法理解你的想法，把训练数据中重复结构的文本当作最终结果输出。`instruct`模型会自己“夹带私货”自己设置指令，然后根据指令回答。所以如果想要续写生成，还是要多加一些自己的任务目标的描述。\n\n我们看下例子：\n\n*提示词* ：\n\n```JSON\n这棵树\n```\n\n*回答* ：\n\n```Plain\n# 3B-Base模型\n那棵树 🐉\\n 🐉\\n 🐉\\n 🐉\\n 🐉\\n 🐉\\n 🐉\n\n# 3B-instruct模型\n您提到的“这棵树”非常具体，但我需要更多的信息来为您提供准确的回答。例如，您是在某个特定地点看到的一棵具体的树，还是一般意义上的提问？如果您能提供更多细节，我会尽力帮助您获取相关信息或解答您的疑问。\n\n# 7B-Base模型\n这棵树在你的生活中扮演了什么角色？\\n\\n作为AI助手，我没有自己的生活或个人经验，所以我无法提供关于“这棵树”在现实生活中的具体角色。不过，我可以告诉你，树木在人类生活中扮演着多种重要角色：它们提供氧气、净化空气、防止水土流失、为野生动物提供栖息地，以及作为木材和其他资源的来源。此外，树木还具有文化和象征意义，在许多文化中被视为生命之源和自然界的重要组成部分。\n```\n\n<div style=\"background:#fff3cd;color:#000;padding:12px 16px;border-left:4px solid #ffeaa7;\">\n⚠️ <strong>从上面的提示示例中可以看出，语言模型能够基于我们给出的上下文内容 \"这棵树\" 完成续写。</strong><br/>\n1.对于参数量较低的Base模型，输出可能是出乎意料的，比如全是表情包，或者与你想要完成的任务相去甚远。这不仅和你没有指出任务类型有关，而且可能与<strong>Base模型能力不够强有关</strong>，如果出现了我们例子这种情况，大概率是预训练阶段数据集质量不高，并且过拟合了，大参数量的预训练模型这种情况会尽量少点。<br/>\n2.如果是instruct模型，以现在较新的模型的能力，通常会反问你指令，这是因为它们在训练过程中被显式地灌输了“澄清不确定指令”的行为模式。比如微调数据集中有这样的数据，同时会进行人类偏好对齐训练。<br/>\n其实这就是我们在提示词要素中提到的，要<strong>明确任务类型</strong>。\n</div>\n\n从上面的提示示例中可以看出，语言模型能够基于我们给出的上下文内容 `\"这棵树\"` 完成续写。\n\n- 对于参数量较低的Base模型，输出可能是出乎意料的，比如全是表情包，或者与你想要完成的任务相去甚远。这不仅和你没有指出任务类型有关，而且可能与**Base模型能力不够强有关**，如果出现了我们例子这种情况，大概率是预训练阶段数据集质量不高，并且过拟合了，大参数量的预训练模型这种情况会尽量少点。\n- 如果是instruct模型，以现在较新的模型的能力，通常会反问你指令，这是因为它们在训练过程中被显式地灌输了“澄清不确定指令”的行为模式。比如微调数据集中有这样的数据，同时会进行人类偏好对齐训练。\n\n其实这就是我们在提示词要素中提到的，要**明确任务类型**。\n\n让我们改进一下：\n\n*提示词* ：\n\n```Plain\n请续写这句话：这棵树\n```\n\n*回答* ：\n\n```Plain\n# 3B-Base模型\n这棵树茂盛，枝繁叶茂。它的叶子呈深绿色，形状如手掌般展开，每一片都郁郁葱葱。树冠高大挺拔，随着季节的变化，叶片颜色也会随之变换，春暖花开时为绿色，秋凉来临则变为金黄色、橙红色，美丽异常。枝干粗壮，承载着整个树木的生命，生长的节奏稳健而规律。树的根系深入地下，支撑着树的成长和稳定，同时也为周围的土壤提供了一定的营养和水分。这棵树以其生命力和美丽吸引着周围的人们，成为这片土地上的一道亮丽风景线。\n\n# 3B-instruct模型\n这棵树看起来非常古老，它的树干上布满了岁月的痕迹，枝叶繁茂，似乎在向天空诉说着不为人知的故事。\n\n# 7B-Base模型\n请提供更多信息或问题，以便我可以更好地帮助您。您想了解关于“这棵树”的哪些方面？\n```\n\n经过改正，模型的回答果然能符合我们的想法，续写这个句子，但是如果我们希望更多的要求，比如写一段漂亮的天空风景呢？正确的做法是在当前提示词条件下添加进一步描述：\n\n*提示词* ：\n\n```Plain\n请续写这句话，重点是写一段优美的文字，描述漂亮的风景：这棵树\n```\n\n*回答* ：\n\n```Plain\n# 3B-Base模型\n这棵树已经有上百年的历史了，它的枝干粗壮而坚韧，树皮上覆盖着一层厚厚的年轮，见证了无数春夏秋冬的更迭。\n\n# 3B-instruct模型\n这棵树，如同大自然精心绘制的一幅画卷，静静地伫立在翠绿的山谷之中。它的枝干遒劲有力，仿佛历史的见证者，历经岁月的洗礼而愈发坚韧不摧。树冠如一把巨大的绿色伞盖，遮蔽了阳光，为下方的小径铺上了一层柔软的绿荫。微风拂过，树叶沙沙作响，似是大自然最温柔的低语，轻抚着每一位路过的行者的脸颊。在这片静谧中，每一缕光线都显得格外珍贵，它们穿透层层叠叠的叶隙，在地面上洒下斑驳陆离的光影，宛如星辰落入凡尘，为这幅生动的山水画卷点缀上了灵动的色彩。\n\n# 7B-Base模型\n的周围是一片绿色的草坪\\n和五彩斑斓的花儿，阳光从枝叶的缝隙中漏下来，像金色的雨滴洒落一地。微风轻拂，绿草如茵，花香四溢，让人仿佛置身于一个绿色的梦幻世界。远处，山峦起伏，云雾缭绕，显得格外神秘和宁静。这棵树下，几只小野兔在欢快地跳跃，它们身上的毛茸茸的耳朵轻轻颤动，仿佛在诉说着这美好的时刻。天空湛蓝，几朵白云悠闲地飘过，与这美丽的风景相得益彰。这是一个让人心旷神怡的地方，仿佛所有的烦恼和忧愁都在这片美景中烟消云散，只留下心底的宁静和安详。\n```\n\n续写的结果要比想象中更美好，本例中，我们了解到，即便模型本身就具备文本续写的能力，我们仍然要对提示词加更多的约束，才能让模型实现符合我们预期的文本生成任务。\n\n而在下一个例子中，我们将进一步学习如何形成文本摘要任务。",
    "133": "一级标题：第四章 常见任务示例\n二级标题：文本摘要\n内容：\n在上述文本生成的例子中，我们了解到，若要完成一个制定的任务，需要在提示词中加以约束，明确任务类型，本例中要完成文本摘要任务，需要做到“文本”和“提示摘要任务”同时在提示词中，比如：\n\n*提示词* ：\n\n```Plain\nLLM是大型语言模型（Large Language Model）的缩写，是一个训练大量文本数据的人工智能系统。使用具有数十亿个参数的转换器架构，它学习语言模式以生成连贯的文本、回答问题、翻译等等。它通过统计模式预测下一个符号，但缺乏真正的理解，有时会产生不准确的结果。例子包括GPT和Llama，为聊天机器人和内容工具提供动力。\n\n用一个简短的句子描述上述文本：\n```\n\n*回答* ：\n\n```Plain\nLLM是一种大型语言模型，通过学习大量文本数据来生成连贯的文本、回答问题和翻译等，但缺乏真正的理解，有时会产生不准确的结果。\n```\n\n在不过多关注上面输出的准确性的情况下，模型尝试将段落总结成一句话。当然，你也可以对提示词进行更加细致的修改，看看最终是否能够达到更好的效果。",
    "134": "一级标题：第四章 常见任务示例\n二级标题：信息提取\n内容：\n因为优秀的上下文理解能力，Base模型可以轻松的从大段数据中提取重要信息，在提示词的约束下得到对应的内容。\n\n*提示词* ：\n\n```Plain\nEcoFusion Pro是一款便携式搅拌机，将于2023年发布，售价89.99美元。它有一个400W的电机，500ml的容量，由不含bpa的塑料制成。它有2年的保修期，有三种颜色可供选择：蓝色、白色和绿色。该产品专为户外使用而设计，可以在10秒内混合冰。\n\n请你从上述文本中总结EcoFusion Pro需要多少钱？\n```\n\n*回答* ：\n\n```Plain\nEcoFusion Pro的售价为89.99美元。\n```",
    "135": "一级标题：第四章 常见任务示例\n二级标题：合成数据\n内容：\n基于上述各种例子，数据的生成往往包含格式的约束，我们在构建数据集的时候当然更希望生成标准格式的数据，那样我们在数据清洗和整理的时候能更加轻松，大模型包含广泛的知识数据，生成数据的时候并不会偏向某一方，比如医疗、金融等特定领域任务，而且由于有良好的上下文理解能力，给够足够的指令约束，数据生成的效果会比预想中的更加准确，我们举一些例子：\n\n*提示词* ：\n\n```Plain\n请按下列要求，按规定格式生成单轮问答的数据：\n询问并解释什么是LLM\n格式: {\"input\":\"...\",\"output\":\"...\"}\n```\n\n*答案* ：\n\n```Plain\n{\"input\":\"什么是LLM？\",\"output\":\"LLM代表Large Language Model，即大型语言模型。它是一种人工智能模型，能够理解和生成人类自然语言，广泛应用于对话系统、文本生成、机器翻译等领域。这些模型通过大量语料训练来学习语言的结构和模式，并能够根据输入的信息生成连贯且符合语境的回复或内容。\"}\n```\n\n我们看到结果是符合我们要求的，而且也从侧面看到，提示词的影响力对于结果生成的提升效果。\n\n当然这里仅简单举例，下面我们分了单独的章节详细讲解预训练、微调、推理等合成数据如何生成。",
    "136": "一级标题：第四章 常见任务示例\n二级标题：指令问答\n内容：\n让模型给出具体回答的最佳方法之一是改善提示的格式。如前所述，一个提示可以结合指令、上下文、输入和输出指示（output indicators）来获得更好的结果。虽然这些组成部分不是必需的，但随着你给出的指令越具体，你将获得越好的结果，这成为了一种良好的实践。下面是一个例子，展示了按照更结构化的提示词来会是什么样子的。\n\n*提示词* ：\n\n```Plain\n根据下面的上下文回答问题。回答要简明扼要。如果不确定答案，回答“不确定答案”。\n\nQuestion: 为什么中国古代的 “丝绸之路” 被称为东西方文明交流的重要纽带？\nAnswer:\n```\n\n*回答* ：\n\n```Plain\n因为“丝绸之路”促进了中国与中亚、西亚乃至欧洲之间的政治、经济和文化交往，是古代东西方文明交流的重要通道。\n```",
    "137": "一级标题：第四章 常见任务示例\n二级标题：情感分类\n内容：\n情感分类任务是需要大模型根据描述的句子给出对应的情感标签，这种指向性很强的目标，就需要进一步对提示词进行约束，在提示词中不仅要包含输入的句子，也要包含标签的种类，否则大模型是不知道具体如何分类的。而在进行大量的实验后，我们发现如果要输出仅输出标签，更加准确，那么可能需要举例表述才能实现。\n\n让我们通过一个情感分类的示例来演示这一点。\n\n*提示词* ：\n\n```Plain\nClassify the text into neutral, negative or positive.\nText: I think the food was okay.\nSentiment:\n```\n\n*回答* ：\n\n```Plain\nNeutral\n\nThe text \"I think the food was okay\" expresses a mild approval but does not express strong enthusiasm or dissatisfaction, hence it falls into the neutral sentiment category.\n```\n\n你希望回答的是`neutral`这样没有其他任何补充的内容，同时你希望它返回的是`neutral`而不是`Neutral`，很明显，现在模型的回答一个都不符合。那么如何实现呢？有几种不同的方法可以做到这一点。在这里，你关心的是具体性，所以你给提示词提供的信息越多，结果就越好。你可以尝试提供示例来指定正确的行为。并且对于回答的约束我们需要跟进一步，让我们再试一次：\n\n*提示词* ：\n\n```Plain\nClassify the text into neutral, negative or positive. Only answer the emotional tags.\nText: I think the vacation is okay.\nSentiment: neutral\nText: I think the food was okay.\nSentiment:\n```\n\n*回答* ：\n\n```Plain\nneutral\n```\n\n这次模型返回了`neutral`，这是你所寻找的具体标签。看起来，提示中提供的示例帮助模型在其输出中保持了具体性，并且没有多余的分析内容了，因为你强迫模型必须只能回答标签。\n\n不过上述例子展示了在英文当中的错误，但事实上，在中文方面，Qwen模型要做的更好一点：\n\n*提示词* ：\n\n```Plain\n将文本分为中性、否定或肯定。只回答情感标签。\n文本：我觉得这个假期还可以。\n标签：中性\n文本：这部电影一般\n标签：\n```\n\n*回答* ：\n\n```Plain\n中性\n```\n\n这或许是因为英文的大小写并不影响含义，而中文就没有这种顾虑，总之，对于Qwen模型，中文的指令遵从效果要好于英文。",
    "138": "一级标题：第四章 常见任务示例\n二级标题：代码生成\n内容：\nInstruct模型在代码生成方面也非常有效，得到特定条件的约束后，模型就可以根据我们的要求生成对应的代码， 我们所熟知的Copilot 就是一个很好的示例。通过巧妙的提示，你可以执行大量的代码生成任务。下面我们来看几个示例。\n\n我们可以先实现一个简单的用户欢迎程度，用Python代码实现，在提示词中加以约束：\n\n*提示词* ：\n\n```Plain\n按照下面的要求只生成python代码:\n/*\n打印出“hello world”\n*/\n```\n\n*输出* ：\n\n~~~Plain\n```python\nprint(\"hello world\")\n```\n~~~\n\n使用Qwen模型生成的结果中包含有`(```python …… ```)`，事实上，这是markdown语言中代码块的表述，出现在此的原因是Qwen对于生成代码会生成结构化回答，而这个结构化刚好是markdown格式，其实并不影响实际使用，我们只需要复制粘贴其中的代码部分即可。事实上，其他的代码语言均是如此，我们看下一个例子，实现一个 SQL 查询的描述，用于从两个表中检索属于 \"计算机科学系\" 的所有学生信息。\n\n*提示词* ：\n\n```Plain\n只使用MySQL查询来回答这个问题：\nTable departments, columns = [DepartmentId, DepartmentName]\nTable students, columns = [DepartmentId, StudentId, StudentName]\n为计算机科学系的所有学生创建一个MySQL查询\n```\n\n*回答* ：\n\n~~~Plain\n```sql\nSELECT students.StudentId, students.StudentName\nFROM students\nJOIN departments ON students.DepartmentId = departments.DepartmentId\nWHERE departments.DepartmentName = 'Computer Science';\n```\n~~~\n\n这非常令人印象深刻。在这种情况下，你提供了关于数据库模式的数据，并要求它生成一个有效的 MySQL 查询。",
    "139": "一级标题：第四章 常见任务示例\n二级标题：数学计算\n内容：\n当今大模型面临的最困难任务之一是需要某种形式的推理的任务。推理是最具吸引力的领域之一，因为有了推理，就可以从LLM中涌现出各种复杂的应用类型。\n\n目前，涉及数学能力的推理任务已经有了一些改进。 对于当前的大型语言模型来说，执行推理任务可能会有一些难度，但是在很多官方发布的Instruct模型，训练数据中都会有很多包含思考推理过程的数据，尤其是在数学推理模块，因为这通常意味着模型最终在各个领域的评测榜单里能占领高位。现在，我们将介绍几个基本示例来展示数学计算模块。\n\n*提示词* ：\n\n```Plain\n9.11和9.8哪个更大？\n```\n\n*回答* ：\n\n```Plain\n为了确定9.11和9.8之间哪个数字更大，我们可以从左到右逐个比较它们：\n\n1. 比较整数部分：\n-两个数字都有9的整数部分。\n\n2. 比较十分位：\n- 9.11在十分位有一个1。\n- 9.8在十分位有个8。\n\n由于1（来自9.11）大于8（来自9.8），我们可以得出9.11大于9.8的结论。\n\n因此，9.11大于9.8。\n```\n\n我们在前面的章节详细讲述思维链（CoT）的原理与使用，思维链只需要几行简单的提示，就可以大幅度提升模型的回答准确度，但是在这里，仅需要知道在没有任何提示词约束的情况下，对于数学问题，最新的模型基本都会有小部分推理过程，但是很可惜，就算模型自带了思考过程，答案也有几率回答错误，事实上，我们重新运行相同的提示词：\n\n*回答* ：\n\n```Plain\n要比较9.11和9.8的大小，我们可以按照小数点后的数字来逐位比较：\n\n- 9.11 和 9.8 都是两位小数。\n- 比较整数部分：两者都是9，相同。\n- 比较小数部分：\n  - 9.11 的小数部分是 0.11\n  - 9.8 的小数部分是 0.8\n\n接下来比较0.11和0.8，显然0.8大于0.11。\n\n因此，9.8 比 9.11 更大。\n```\n\n这次就回答正确了，这确实是一个让人比较焦虑的结果，我们当然希望每次运行都能正确。从上述例子可知，当思考的路径中产生错误的分析，那么结果错误的可能性就更大，但是如果我们在提示词的末尾添加一行`让我们一步一步思考`结果或许就大有不同：\n\n*提示词* ：\n\n```Plain\n9.11和9.8哪个更大？让我们一步一步思考\n```\n\n*回答* ：\n\n```Plain\n要比较9.11和9.8的大小，我们可以从以下几个步骤来分析：\n\n### 第一步：比较整数部分\n首先看两个数字的整数部分：\n- 9.11 的整数部分是 9\n- 9.8 的整数部分也是 9\n\n因为整数部分相同，我们需要进一步比较小数部分。\n\n### 第二步：比较小数部分\n接下来比较小数部分：\n- 9.11 的小数部分是 0.11\n- 9.8 的小数部分是 0.8\n\n显然，0.11 < 0.8。\n\n### 结论\n由于小数部分0.11小于0.8，所以9.11小于9.8。\n\n因此，9.8 比 9.11 更大。\n```\n\n从结果上看，不仅思考的步骤要比单纯的提问要更多，并且从整数部分到小数，都进行了细致的排查，从而形成了正确的思考链路，即使多次实验，最终结果出错的概率也大幅减少，这就是思维链的魅力所在，仅用一行提示就能改善模型性能，因为事实上一句简单的提示激活了模型的思考反思能力，这或许就是我们常说的“三思而后行”。\n\n另外，如果我们仅希望生成结果，那么我们也只需要对提示词加以约束：\n\n*提示词* ：\n\n```Plain\n只回答结果：9.11和9.8哪个更大？\n```\n\n*回答* ：\n\n```Plain\n9.8\n```\n\n最终结果达成，不过需要小小的提醒，生成的仍然是字符string格式哦，如果需要得到准确的数字，在实际代码中要添加转换代码，因为这通常用于评估模型在各个数据集上的评分。",
    "140": "一级标题：第四章 常见任务示例\n二级标题：聊天对话\n内容：\n我们已经做了很多关于大模型的实验，每一步都带来我们对于提示词新的认知，事实上，你可以通过提示词工程进行更有趣的实验，比如指导大语言模型系统如何表现，指定它的行为意图和身份。 当你在构建对话系统，如客户服务聊天机器人时，这尤其有用。\n\n下例中，你可以独立创建一个对话系统，该系统能够基于问题给出技术性和科学的回答。 你可以关注我们是如何通过指令明确地告诉模型应该如何表现。 这种应用场景有时也被称为*角色提示（Role Prompting）*。\n\n但是需要注意的是，`聊天`往往意味着需要对话的历史记录，我们需要将历史记录添加到提示词中，不过这并不难，还记得我们在推理代码中，构建的对话信息`message`吗？我们只需要将历史信息根据对话模板添加到提示词中就可以做到，话不多说，我们看下例子：\n\n*对话信息* ：\n\n```Plain\nsystem = \"你是一个专业的AI助手\"\nuser1=\"请记住我现在有一个数字13\"\nassistant1=\"好的，我记住了，现在有一个数字13\"\n\nuser2=\"然后把前面提到的数乘以3，请给出结果。\"\n```\n\n其中`system`是你为模型设定的角色信息，在例子中，我们简单设定为AI助手。第一轮对话分别是`user1`和`assistant1`，这是我们的历史信息，其中`user1`是你的问题，`assistant1`是AI助手的回答。第二轮对话以`user2`作为开始，我们需要AI助手回答`assistant2`的内容。这些信息通过模板保存在`message`中：\n\n*message* ：\n\n```Plain\nmessages = [\n    {\"role\": \"system\", \"content\": system},\n    {\"role\": \"user\", \"content\": user1},\n    {\"role\": \"assistant\", \"content\": assistant1},\n    {\"role\": \"user\", \"content\": user2}\n]\n```\n\n需要注意的是，无论历史对话进行了多少轮，你永远都是提问的一方，模型永远都是回答的一方，因此，我们需要确保最终是`user`的内容。同时历史对话并不会无限长，模型总有输入的极限，一旦超过这个极限，模型总会记得最新的回答，事实上，我们也可以称为“滑动窗口”，这是模型对记忆的保存机制。\n\n好了，我们看下回答的效果：\n\n*对话模板* ：\n\n```Python\n# 运行下面的代码，我们将知道输入到模型中的是什么样子的文本\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n<|im_start|>system\n你是一个专业的AI助手<|im_end|>\n<|im_start|>user\n请记住我现在有一个数字13<|im_end|>\n<|im_start|>assistant\n好的，我记住了，现在有一个数字13<|im_end|>\n<|im_start|>user\n然后把前面提到的数乘以3，请给出结果。<|im_end|>\n<|im_start|>assistant\n```\n\n从结果上看，我们真的将历史对话给到了模型，那么模型真的能够记住历史信息回答正确吗？\n\n*回答* ：\n\n```Plain\n好的，我们先将你提到的数字13乘以3。\n\n\\( 13 \\times 3 = 39 \\)\n\n所以结果是39。\n```\n\n回答正确！到这里，你已经学会简单地和模型聊天对话了，不过我们的AI研究助手听起来有点太技术性了，对吗？好的，让我们改变这种行为，让模型变得更有人情味儿点？实际上，我们可以让模型实现一个简单的`角色扮演`，主要是对`system`的改造，话不多说，我们看下示例：\n\n*提示词* ：\n\n```Plain\nsystem = \"你是个脾气暴躁的老书店老板，暗地里心软。你经营这家店40年了，闲聊会让你生气，但你总是记得老顾客最喜欢的书。\"\nuser1=\"打扰一下，你有大家都在谈论的那本新的科幻小说吗？”有蓝色封面的那本？\"\nassistant1=\"啊，最新的科幻大片。是的，我们有。在最上面的架子上，从后面数第三个。\"\n\nuser2=\"谢谢。顺便说一句，这地方很舒适。我已经走过一百次了，但从来没有进去过。\"\n```\n\n*回答* ：\n\n```Plain\n哈哈，年轻人，进来吧。坐下来翻翻书，享受你的时光。我这里的故事多的是。\n```\n\n看来老板是一个热情开朗的人呢～好了，我们通过为模型设定了一个角色模板，让模型互动更富有人情味儿了，我想我们应该取得了一些进展， 当然你也可以选择继续改进它，让回答更符合你的预期。",
    "141": "一级标题：第四章 常见任务示例\n二级标题：进阶数学问题\n内容：\n更复杂的数学问题如果单靠Instruct模型可能行不通，这跟人类一样，当我们在做高考数学题的时候，尤其是压轴题，我们读完题目总需要在草稿纸上不断的推演、不断的计算，我们并不能一气呵成的给出最准确的答案，也就是需要`思考`，模型同样也需要思考，而这正是推理型模型的优势所在，因为推理型模型经过大规模的训练，早已具备了反思和推理能力，无需我们添加CoT，模型就会通过长文本的思考给出最正确的答案。\n\n让我们先来看看普通的Instruct模型的回答：\n\n*提示词* ：\n\n```Plain\n早上8点，一列火车从A站出发，以每小时60英里的速度向东行驶。另一列火车上午9点从A站以东300英里的B站出发，以每小时75英里的速度向西行驶。这两列火车什么时候会合？\n```\n\n*回答* ：\n\n<details>\n<summary>点击展开完整回答内容</summary>\n\n```Plain\n要解决这个问题，我们可以通过计算两列火车的相对速度来确定它们何时会相遇。\n\n首先，我们知道两列火车的速度分别是：\n- 从A站出发的火车速度：60英里/小时\n- 从B站出发的火车速度：75英里/小时\n\n由于这两列火车在同一条直线上但方向相反，它们相向而行，因此它们相遇时的相对速度等于它们各自速度之和：\n\\[ 60 \\text{ 英里/小时} + 75 \\text{ 英里/小时} = 135 \\text{ 英里/小时} \\]\n\n现在我们知道两列火车之间的初始距离是300英里。我们可以用距离除以相对速度来计算两列火车相遇所需的时间：\n\\[ \\text{时间} = \\frac{\\text{距离}}{\\text{相对速度}} = \\frac{300 \\text{ 英里}}{135 \\text{ 英里/小时}} \\approx 2.22 \\text{ 小时} \\]\n\n这意味着两列火车将在大约2.22小时后相遇。\n\n接下来，我们需要确定具体时间。从上午8点开始算起，2.22小时大约等于2小时13分钟（因为 \\(0.22 \\times 60 \\approx 13\\) 分钟）。\n\n因此，两列火车会在上午10点13分左右相遇。\n```\n</details>\n\n<div style=\"background:#e7f5ff;color:#000;padding:12px 16px;border-left:4px solid #74c0fc;\">   事实上，该问题的答案应该是<strong>上午10 点 46 分 40 秒</strong>，没有思考的过程，模型给出的明显是错误的答案。那么如果是在Instruct模型基础上添加`让我们一步一步思考`这样的思维链模式呢？结果会如何？<br/></div>\n\n\n\n*回答* ：\n\n<details>\n<summary>点击展开完整回答内容</summary>\n\n```Plain\n为了解决这个问题，我们可以使用相对速度的概念来计算两列火车相遇的时间。\n\n首先，我们需要明确几点信息：\n1. 两列火车的初始位置：火车1在A站，火车2在B站，距离A站300英里。\n2. 速度：火车1的速度是每小时60英里，火车2的速度是每小时75英里。\n3. 时间差：火车1比火车2早出发1小时。\n\n由于火车1和火车2分别以相反的方向行驶，它们的相对速度就是它们速度的和，即 \\(60 + 75 = 135\\) 英里/小时。这意味着每小时两列火车之间会缩短135英里的距离。\n\n接下来，我们计算两列火车需要多长时间才能相遇。由于火车2在火车1出发后1小时才开始行驶，所以火车1已经行驶了1小时（即60英里）。\n\n火车1和火车2之间的初始距离是300英里。火车1已经行驶了60英里，因此剩余的距离为 \\(300 - 60 = 240\\) 英里。\n\n现在，我们要用剩余的距离除以它们的相对速度来计算相遇所需的时间：\n\\[ \\text{所需时间} = \\frac{\\text{剩余距离}}{\\text{相对速度}} = \\frac{240 \\text{英里}}{135 \\text{英里/小时}} \\]\n\n简化这个分数：\n\\[ \\frac{240}{135} = \\frac{16}{9} \\approx 1.777\\text{小时} \\]\n\n这意味着两列火车将在大约1小时46分钟之后相遇。由于火车1已经比火车2早出发了1小时，所以我们需要将1.777小时加到火车1的出发时间上：\n\\[ 8:00 + 1.777\\text{小时} \\approx 9:46 \\]\n\n因此，两列火车会在上午9点46分左右相遇。\n```\n</details>\n\n<div style=\"background:#e7f5ff;color:#000;padding:12px 16px;border-left:4px solid #74c0fc;\">\n看起来，添加了思维链的模型好比一位推理能力不错、却仍与满分有差距的学生；真正的差距还是来自模型本身。接下来我们用 Qwen3 系列的“推理型”模型来回答——该模型天然具备思考能力，其思考过程会包裹在 <code>&lt;think&gt;...&lt;/think&gt;</code> 标签里。如果想显式开启思考模式，需要在推理代码里把参数 <code>enable_thinking</code> 设为 <code>True</code>。\n</div>\n\n\n\n\n*代码* ：\n\n```Python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True\n)\n```\n\n*回答* ：\n\n<details>\n<summary>点击展开完整回答内容</summary>\n\n\n```Plain\n<think>\n嗯，这个问题看起来是关于两列火车相遇时间的，我得仔细想一想怎么解决。首先，题目是说早上8点，一列火车从A站出发，向东开，速度是每小时60英里。然后另一列火车在上午9点从A站以东300英里的B站出发，向西行驶，速度是每小时75英里。问这两列火车什么时候会合。\n首先，我需要明确两列火车的出发时间和位置。第一列火车是早上8点从A站出发，向东开，速度60mph。第二列火车是上午9点从B站出发，向西开，速度75mph。而B站位于A站以东300英里处，所以当第二列火车出发的时候，第一列火车已经出发了1小时，对吧？因为第二列火车是在9点出发，而第一列火车是8点出发，所以第一列火车在第二列火车出发时已经行驶了1小时，走了60英里，对吧？\n……\n不过，可能我哪里错了？或者有没有其他方法？\n这时候，第一列火车的位置是从A站出发向东行驶了60t英里。而第二列火车是从B站出发向西行驶，B站距离A站300英里，所以第二列火车的位置是从B站出发向西行驶了75*(t - 1)英里，因此它相对于A站的位置是300 - 75*(t - 1)英里。因为向西行驶，所以是从B站往西，也就是A站的东边减少。\n当两列火车相遇的时候，他们的位置应该相同，也就是：\n60t = 300 - 75(t - 1)\n\n解这个方程：\n\n60t = 300 -75t +75\n\n60t +75t = 300 +75\n\n135t = 375\n\nt = 375 / 135 = 25/9 ≈ 2.777...小时\n\n也就是2小时46分40秒，对吗？因为0.777...小时乘以60分钟是约46.666分钟，即46分40秒。所以，从早上8点开始，经过2小时46分40秒，就是10点46分40秒左右。\n</think>\n\n两列火车的相遇时间可以通过以下步骤计算：\n\n1. **设定变量**：设从第一列火车出发的8点开始，经过 $ t $ 小时后相遇。此时第一列火车行驶了 $ 60t $ 英里，第二列火车行驶了 $ 75(t - 1) $ 英里（因为第二列火车在9点出发，比第一列火车晚1小时）。\n\n2. **建立方程**：两列火车相遇时，它们的位置相同。第一列火车从A站出发向东行驶，第二列火车从B站出发向西行驶，B站距离A站300英里。因此：\n   $$\n   60t = 300 - 75(t - 1)\n   $$\n\n3. **解方程**：\n   $$\n   60t = 300 - 75t + 75 \\implies 135t = 375 \\implies t = \\frac{375}{135} = \\frac{25}{9} \\approx 2.777\\ \\text{小时}\n   $$\n\n4. **转换为具体时间**：$ \\frac{25}{9} $ 小时等于2小时46分40秒（因为 $ 0.777\\ \\text{小时} \\times 60 \\approx 46.67\\ \\text{分钟} $）。\n\n5. **计算相遇时间**：从8点开始经过2小时46分40秒，即 **10点46分40秒**。\n\n**答案**：两列火车在 **10点46分40秒** 时会合。\n```\n\n</details>\n\n\n<div style=\"background:#e7f5ff;color:#000;padding:12px 16px;border-left:4px solid #74c0fc;\">\n答案完全正确，而且比我们预想的思考还要完整，因为不仅有思考模块，还包含了反思过程，模型会反思答案的正确与否，然后重新计算。\n</div>\n\n\n\n不过，和我们考试一样，这是一个非常耗时的环节，在使用Instruct模型时，因为生成的内容不算太长，可能10秒内就能回答完，但是推理型模型由于思考部分太长，往往非常耗时，所以除非是类似于上个例子中特别复杂需要推理思考的问题，一般的问题使用Instruct模型即可。",
    "142": "一级标题：第四章 常见任务示例\n二级标题：微调模型评估\n内容：\n除了上述列举的各种任务类型，还可以通过设计提示词让大模型充当“裁判”的角色对已经微调好的模型进行打分，比如让微调后的模型回答一些问题，如果是数学问题可以直接对答案，而如果是一些文本类的回答，则可以让大模型对回答的准确性进行打分，充当“裁判”的大模型要求能力很强，这样才能在评估过程中客观评分，我们举一个简单的例子： *提示词* ：\n\n```Plain\n你是一位严格的评分老师。下面是一段「待评回答」。请只看它的事实准确性，忽略格式和语气。\n评分标准：\n0 分 = 完全不符合事实\n1 分 = 有明显事实错误\n2 分 = 有小错误，整体仍对\n3 分 = 完全正确\n\n只要回复一个数字（0/1/2/3）和一句理由，不要多余内容。\n\n### 待评估问题：\n巴黎现在有多少的人口？\n\n### 待评回答：\n根据2022年最新的统计数据，巴黎的总人口约为216万。\n```\n\n*回答* ：\n\n```Plain\n3\n```\n\n当然我们也可以让大模型判断回答结果和正确结果之间的相似程度作为评分标准：\n\n*提示词* ：\n\n```Plain\n你是评分员。请把下面的“标准答案”和“模型回答”对比，按 1–5 打分：\n5 分 = 含义完全一致\n4 分 = 基本一致，细节微差\n3 分 = 部分匹配\n2 分 = 少量相关\n1 分 = 完全不符\n\n只回复一个数字（1-5）和一句理由，不要多余内容。\n\n### 问题：\n光合作用需要什么？\n\n### 标准答案：\n阳光\n\n### 模型回答：\n光合作用是植物、藻类和某些细菌利用阳光将二氧化碳和水转化为氧气和葡萄糖的过程。在这个过程中，光合作用需要以下条件和成分：\n\n1. **光**：这是光合作用的直接能量来源。\n2. **二氧化碳（CO₂）**：从大气中吸收的二氧化碳用于合成有机物。\n3. **水（H₂O）**：通过叶绿体中的叶绿体膜上的水裂解过程产生氢离子和氧气。\n4. **叶绿素和其他色素**：叶绿素是主要的色素，能够吸收太阳光并将其转化为\n```\n\n*回答* ：\n\n```Plain\n1\n\n理由：模型回答虽然提到了光作为光合作用的条件之一，但并未明确指出光就是唯一的条件，并且详细说明了其他必要的物质和叶绿素的作用，而标准答案仅提及阳光这一单一条件。\n```\n\n从上面的例子可以看出，当大模型充当“裁判”后就可以替代人工做出合理的打分，这样的评估可以通过批量运行脚本，从而提高评估的效率。",
    "143": "一级标题：第五章 多模态提示词\n二级标题：无\n内容：\n多模态模型能够同时理解、处理、生成文字、图像、语音、视频等多种信息，多模态模型大致分为理解型模型和生成类模型，其中理解型模型输入多模态信息，输出是文字类型，而生成类模型输入可以是多模态信息，输出也是多模态类，比如让模型生成一张“阳光下的彩虹”的图像，多模态生成类模型就能够生成精美的图像。\n\n简单来说，单模态模型仅有文字，相当于人类仅能对话，看不到图像、听不到声音、说不出话，多模态模型相当于赋予模型人类的五感，它不仅能够去认识我们的这个文字，还能听懂我们的语音，看到我们的图片，理解我们的视频。\n\n本节我们将针对多模态模型设计提示词，当输入是由文字、图像、视频、音频等多种模态组成的信息时，如何引导AI生成我们理想的结果。\n\n使用的模型为Qwen系列模型，其中图像和视频都由Qwen2.5-vl模型实现。关于推理代码，我们可以从[modelscope官网](https://www.modelscope.cn/models/Qwen/Qwen2.5-VL-7B-Instruct)得知。",
    "144": "一级标题：第五章 多模态提示词\n二级标题：图像理解\n内容：\n对于多模态模型，提示词的设计需要注意下面几点：\n\n1. 准确清楚\n2. 必要情况下，对图像进行标记\n\n首先是**清楚的表达**，比如你在分析一张发票的时候，如果仅仅需要金额数据，但是没有表达清楚，那么可能默认输出所有的数据，比如：\n\n*提示词* ：\n\n<img src=\"./picture/fapiao.png\" alt=\"普通发票\" style=\"zoom:50%;\" />\n\n```Plain\n请提取发票中的信息，并按照json格式输出\n```\n\n*结果* ：\n\n```Plain\n{\n  \"发票号码\": \"243xxxx344\",\n  \"开票日期\": \"2024年01月02日\",\n  \"购买方信息\": {\n    \"名称\": \"xxx有限公司\",\n    \"统一社会信用代码/纳税人识别号\": \"xxxx\"\n  },\n  \"销售方信息\": {\n    \"名称\": \"中科视拓(南京)科技有限公司\",\n    \"统一社会信用代码/纳税人识别号\": \"91320191MA1XM5TX71\"\n  },\n  \"项目名称\": \"*信息技术服务*云服务器服务费\",\n  \"规格型号\": \"\",\n  \"单位\": \"项\",\n  \"数量\": \"1\",\n  \"单价\": \"4716.98113207547\",\n  \"金额\": \"4716.98\",\n  \"税率/征收率\": \"6%\",\n  \"税额\": \"283.02\",\n  \"合计\": \"￥4716.98\",\n  \"价税合计\": \"伍仟圆整 (小写): ￥5000.00\",\n  \"备注\": {\n    \"购方开户银行\": \"中国工商银行股份有限公司南京xxx支行\",\n    \"银行账号\": \"120xxxx7\",\n    \"销方开户银行\": \"中国工商银行股份有限公司南京xxx支行\",\n    \"银行账号\": \"7\"\n  }\n}\n```\n\n*准确的提示词* ：\n\n```Plain\n请提取发票中的给出的合计的金额，并按照json格式输出\n```\n\n*结果* ：\n\n```Plain\n{\n  \"合计金额\": \"4716.98\"\n}\n```\n\n其次，如果图像中元素过多，可以对我们需要询问的对象**单独框出**，然后让模型理解问题，比如：\n\n<img src=\"./picture/Agent.png\" alt=\"框出部分的Agent原理图\" style=\"zoom:50%;\" />\n\n\n\n*提示词* ：\n\n```Plain\n请你解释红框中的原理\n```\n\n*回答* ：\n\n<details>\n<summary>点击展开完整回答内容</summary>\n\n\n```Plain\n红框部分展示了大脑的内部工作原理，具体包括记忆、知识存储、决策制定和推理等过程。以下是这些部分的详细解释：\n\n1. **记忆（Memory）**：\n   - 大脑通过感知输入的信息来形成记忆。这些信息可以是视觉、听觉或其他感官输入。\n   - 记忆存储在大脑的不同区域，例如海马体负责短期记忆，而杏仁体则与情绪记忆有关。\n\n2. **知识（Knowledge）**：\n   - 知识是从记忆中提取并组织起来的信息。它包括事实、规则、经验和技能等。\n   - 知识可以通过学习（Learn）获得，并且可以被存储在大脑的长期记忆中。\n\n3. **决策制定（Decision Making）**：\n   - 决策制定是基于记忆和知识的过程，涉及到对信息进行评估和选择。\n   - 这个过程通常涉及多个步骤，包括问题识别、信息收集、分析、评估和选择行动方案。\n\n4. **推理（Reasoning）**：\n   - 推理是使用已有的知识和信息来推导出新的结论或解决方案的过程。\n   - 它可以是演绎推理（从一般到特殊），也可以是归纳推理（从特殊到一般）。\n\n5. **生成/转移（Generate/Transfer）**：\n   - 生成是指将新信息添加到记忆库中，或者创建新的知识结构。\n   - 转移则是指将已有的知识应用到新的情境中，以便做出适当的决策或采取行动。\n\n6. **总结（Summary）**：\n   - 总结是对大量信息进行简化和概括的过程，以便更好地理解和处理。\n   - 这有助于提高决策效率和准确性。\n\n7. **回忆（Recall）**：\n   - 回忆是从记忆中提取特定信息的过程。\n   - 这对于解决问题和执行任务至关重要。\n\n8. **学习（Learn）**：\n   - 学习是指通过经验获取新知识和技能的过程。\n   - 这可以通过观察、实践和反馈等方式实现。\n\n9. **检索（Retrieve）**：\n   - 检索是从记忆中找到所需信息的过程。\n   - 这对于快速响应和解决问题非常重要。\n\n综上所述，红框部分展示了大脑如何通过记忆、知识、决策制定和推理等过程来进行信息处理和决策制定。这些过程相互关联，共同构成了大脑的功能基础。\n```\n\n</details>\n\n<div style=\"background:#e7f5ff;color:#000;padding:12px 16px;border-left:4px solid #74c0fc;\">\n可以看到模型仅对于红框中的内容进行了分析和翻译，然后给出结果，这样框出指定位置然后提出问题的方式比修改提示词再询问要更加准确。\n</div>",
    "145": "一级标题：第五章 多模态提示词\n二级标题：视频分析\n内容：\n多模态模型也可以用来分析视频，不过在实际使用的时候，由于视频需要的资源都比较多，因此本地推理时显存占用一般承受不起，所以本节我们使用API接口实现，我们使用[阿里云百炼平台](https://bailian.console.aliyun.com/console?tab=model#/model-market)，通过调用视觉模型的API接口在本地做一个demo：\n\n1. 选择模型\n\n首先，我们在[模型广场](https://bailian.console.aliyun.com/console?tab=model#/model-market)中找到Qwen-VL系列模型，本次我们选择Max，\n\n<img src=\"./picture/vl_video_api.png\" alt=\"阿里云百炼平台vl模型\" style=\"zoom:80%;\" />\n\n点击**API参考**，然后找到视频输入\n\n<img src=\"./picture/video_api.png\" alt=\"vl模型视频输入\" style=\"zoom:80%;\" />\n\n不过该部分仅涉及视频中某帧对应的图像的分析，而不是连贯的视频的分析，如果想分析完整的视频，需要去[视觉理解](https://bailian.console.aliyun.com/console?tab=doc#/api/?type=model&url=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F2845871.html%2380dbf6ca8fh6s)中找到对应的API调用代码，其中接口和OpenAI兼容，我们只需要输入对应的视频的链接即可：\n\n```Python\n### 视频推理\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n    api_key=\"sk-\",\n    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n)\ncompletion = client.chat.completions.create(\n    model=\"qwen-vl-max-latest\",\n    messages=[\n        {\"role\": \"system\",\n         \"content\": [{\"type\": \"text\",\"text\": \"You are a helpful assistant.\"}]},\n        {\"role\": \"user\",\"content\": [{\n            # 直接传入视频文件时，请将type的值设置为video_url\n            # 使用OpenAI SDK时，视频文件默认每间隔0.5秒抽取一帧，且不支持修改，如需自定义抽帧频率，请使用DashScope SDK.\n            \"type\": \"video_url\",\n            \"video_url\": {\"url\": \"https://help-static-aliyun-doc.aliyuncs.com/file-manage-files/zh-CN/20241115/cqqkru/1.mp4\"}},\n            {\"type\": \"text\",\"text\": \"这段视频的内容是什么?\"}]\n         }]\n)\nprint(completion.choices[0].message.content)\n```\n\n*回答* ：\n\n```Plain\n这段视频展示了一位年轻女性的特写镜头。她有着短发，面带微笑，看起来非常开心和友好。她的穿着是一件粉色的针织开衫搭配白色的内搭，整体风格显得很清新自然。背景模糊，但可以看出是在户外，可能是一个校园或住宅区。\n\n视频中的女性表情生动，笑容灿烂，给人一种温暖和愉快的感觉。她的目光时而直视镜头，时而微微侧头，展现出不同的神态。整个视频传达出一种积极向上的氛围，可能是用于宣传、教育或其他正面主题的内容。\n\n此外，视频右上角有“通义·AI合成”的字样，表明这段视频可能是通过人工智能技术合成的。\n```\n\n至此，已完成了视频分析的任务。",
    "146": "一级标题：第五章 多模态提示词\n二级标题：音频分析\n内容：\n音频多模态大模型在短短两年间完成了从 **“听懂字”到“听懂声”** 的跨越。早期系统沿用ASR-TTS链路：先将语音转文字，再由LLM生成文本，最后合成语音。该方案成熟、易训，却丢失了音色、情绪、环境噪声等非符号信息，也无法回答“这段笑声是苦笑还是讪笑”这类需要声学特征的问题。\n\n新一代端到端音频大模型直接对离散化的声学token进行建模，把 **语音、音乐、环境声统一压缩成共享语义空间**。\n\n- 训练时，模型同时预测下一帧声学token和对应的文本token，实现跨模态对齐；\n- 推理时，可直接以声学条件做prompt，输出声学token后流式解码为波形，无需显式文字。\n\n其难点在于计算量：一秒音频可对应数百个声学token，需引入RVQ降采样、稀疏注意力、局部-全局双层Transformer等技巧，才能将上下文拉到分钟级。实际落地中，两种路线并非互斥。客服、导航等对精度敏感、延迟容忍高的场景仍倾向ASR-TTS，可控且易纠错；社交、创作、情感陪伴类应用更青睐端到端声学LLM，可保留副语言信息并生成更自然的韵律。\n\n未来趋势是“可路由混合架构”：系统先用轻量听写模型判断是否需要保留声学细节，简单请求走文本LLM，复杂情感请求走声学LLM，实现成本与体验的动态平衡。",
    "147": "一级标题：6.1 预训练阶段的合成数据\n二级标题：无\n内容：",
    "148": "一级标题：6.1 预训练阶段的合成数据\n二级标题：预训练阶段合成数据的重要性\n内容：\n我们知道，早期的大模型（204年前），大模型的性能提升着重于预训练阶段，早期提到的[Scaling Law法则](https://arxiv.org/pdf/2001.08361/1000)中，我们得知，模型规模、数据规模和模型性能之间呈现幂律增长的趋势，当模型参数量和训练数据规模同时增长的时候，模型性能有稳定的提升。下图展示了Scaling Law的图表原理[1]：\n\n<div style=\"display:flex;justify-content:center;\">\n  <figure style=\"text-align:center;margin:0;\">\n    <img src=\"./picture/scaling_law.png\" style=\"width:800px;\" alt=\"ScalingLaw原理图\">\n    <figcaption>Scaling Law原理图</figcaption>\n  </figure>\n</div>\n\n\n因此在当时的学者看来，训练数据越多，模型就能越大，模型表现就能越好，这不难理解，就跟人类18岁以前的学习一样，15-18岁必然比12岁左右懂得知识多，理解也会更深刻。\n\n在大模型发展的早期，OpenAI的GPT系列模型可谓是AI领域的领头羊，其中GPT-3，后面经过后训练得到的GPT-3.5不仅拥有超大规模的参数量（175B，当然现在的模型参数量甚至有1TB，但在当时已经是模型规模的极限），而且在零样本/少样本领域能力突出，拥有2k多tokens窗口的上下文理解能力，在各个领域的应用都非常显著。而能拥有如此强大的通用性能力，和预训练阶段庞大的数据规模密不可分，在GPT-3的论文里[Language Models are Few-Shot Learners](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)，作者给出GPT-3的训练数据分布[2]：\n\n<div style=\"display:flex;justify-content:center;\">\n  <figure style=\"text-align:center;margin:0;\">\n    <img src=\"./picture/gpt3_training_data.png\" style=\"width:800px;\" alt=\"GPT-3的训练数据分布\">\n    <figcaption>GPT-3的训练数据分布</figcaption>\n  </figure>\n</div>\n\n\n根据Scaling Law法则得知，这个规模的数据对应的模型规模175B已经是模型性能提升的极限，而在[洪永淼、汪寿阳：ChatGPT 与大模型将对经济学研究范式产生什么影响?](https://mp.weixin.qq.com/s?__biz=MzI0NzY3MzkzNw==&mid=2247485830&idx=1&sn=5a4111e768b12233909d1af0b096a1f4#:~:text=ChatGPT的第一个版本GPT-1，其参数数量为1.17亿，这是非常庞大的数量。在GPT-2版本中，模型参数数量从1.17亿上升到15亿，训练数据也增加了。在GPT-3版本中，参数数量达到1750亿个，并使用大约2%2F3互联网数据、整个维基百科以及2个大型图书馆数据进行训练。)一文中指出，GPT-3的预训练模型使用大约2/3互联网数据、整个维基百科以及2个大型图书馆数据进行训练，由于数据需要经过去重、清洗，事实上，GPT-3预训练使用的互联网数据规模基本是能获取的数据规模极限，因此模型规模也没有再有提升的空间。\n\n但是我们知道对于科学的探索是无止境的，在数据和模型规模都到达瓶颈的情况下，如何提升模型性能呢？\n\n[Textbooks Are All You Need](https://arxiv.org/pdf/2306.11644)这篇论文思考在预训练阶段如果对数据质量提升，能否提升模型性能。\n\n作者给出的答案是肯定的，并且开创性的在论文中提出，预训练阶段“教科书”级数据能够给模型带来非常不错的提升，并且认为后面的大模型的发展取决于合成数据，当然从现在看来，这并不是模型发展的未来，在2025年的今天，模型发展到了后训练阶段。不过即便如此，在当今社会，合成数据仍然是各个AI行业不可或缺的一环。\n\n我们接着回到论文的讨论，作者认为，在预训练阶段，数据集的质量集中在`多样性`、`去除噪声`以及`教科书`。\n\n`1、多样性`：\n\n这一点其实很好理解，所有的大模型在预训练阶段既要保证数据主题的广泛性，要涵盖尽可能多的知识点，也要保证数据不会有明显的重复。\n\n重复数据对于模型训练来说基本是毁灭性的打击，如果数据中存在大量重复内容（例如同一篇文章被多次收录），模型会优先 “记住” 这些重复信息，而非学习背后的通用模式，泛化能力降低，同时会人为放大某类信息的占比，导致模型误以为这类信息更重要或更常见。\n\n`2、去除噪声`：\n\n我们知道训练大规模语言模型，预训练数据大多来源于互联网数据，在互联网中存在大量会干扰模型学习有效规律、导致模型学到错误模式或者降低泛化能力的内容，比如网页中存在的各种营销类文本、HTML 标签（如`<div>`、`<p>`）、表格乱码，同时可能存在大量无意义内容，比如很多的随机字符如 （“qwertyuiop”）、重复堆砌的短句（如 “加油加油加油……”）、空白或乱码（如 “�￥%”）。\n\n在常见任务示例中，我们使用Qwen2.5-base模型有可能会出现大量无意义重复表情包，这很有可能是预训练数据集中存在的噪声未被及时清理并且训练过拟合。因此去除噪声是保证数据质量的又一关键要素。\n\n不过需要注意的是，高质量的判定并不包含Toxic信息，也就是有毒内容，因为即便是涉及安全类信息，只要不是重复性高、多样化程度低的数据其实都算高质量数据，而有毒内容的剔除往往是后训练阶段处理。\n\n`3、教科书数据`：\n\n其实我认为这一概念的提出和当时学者对于大模型的定位有关，在攻克了自然语言处理任务后，大模型充当问答辅助工具或者对话工具，而如果跟现实联系起来，大模型就很像是经验丰富的“老师”，那要想成为这样的老师，学习“教科书”数据显然能够大幅度提升模型的能力，但是这类数据在网络上其实并不多，大多数还是类似于科普类专业数据。即便将所有的“教科书”数据提取出来并数据清洗去重，得到的结果其实也有偏向，比如数学类比较多，文学类比较少，这样训练出来的模型很有可能发生过拟合的现象。\n\n基于此，Phi论文[3]的作者认为可以用大模型合成“教科书”数据，一来GPT-3拥有广泛的知识储备和问答能力，合成的数据自然在`多样性`上不会有问题，并且合成的`噪声`也不会很多；其次因为强大的零样本/少样本学习能力，模型能够很好的理解提示词并生成高质量的回答，只要设定好对应的`受众群体`，比如对于面向年幼儿童的教科书，内容需要使用非常简单、日常的语言和短句，以便10岁左右的孩子能够轻松理解；对于面向专业人士和研究人员的教科书，内容则需要深入探讨主题，包括对最新研究成果和领域内辩论的批判性分析；而对于面向高中生的教科书，内容需要平衡教育的严谨性和可访问性。使用的语言和例子应该能够与青少年学生产生共鸣，同时激发他们对日常生活相关性的好奇心。对于不同的`受众群体`，提示词的构建也会有相应的调整，在[huggingface的博客](https://huggingface.co/blog/zh/cosmopedia)中提到了*少儿、专业人士和研究人员以及高中生生成相同主题的教科书的提示：*[4]\n\n<div style=\"display:flex;justify-content:center;\">\n  <figure style=\"text-align:center;margin:0;\">\n    <img src=\"./picture/huggingface_phi_data.png\" style=\"width:800px;\" alt=\"少儿、专业人士和研究人员以及高中生生成相同主题的教科书的提示\">\n    <figcaption>少儿、专业人士和研究人员以及高中生生成相同主题的教科书的提示</figcaption>\n  </figure>\n</div>\n\n\n而面对不同的群体，哪怕是同一个知识点，不同群体之间的关联度其实很低，你将化学元素周期表跟小孩子讲，他们大概率连字都不认识，而对于高中生而言，这只不过是他们日常试题的一部分，因此这样生成的数据，即便背后的知识一致，也不会导致重复性数据，从而训练过拟合，而这样生成的数据，规模是成倍增长的，并且由于大部分知识可以从网络知识中获取，那么“教科书”数据可以确保数据质量和规模。\n\n不过，为了保证模型不会完全在“教课”这一条路上走到黑，Phi的作者将网络数据和教科书数据混合训练模型，成果相当不错，不过模型规模只有3B，因为Scaling Law法则的影响，毕竟数据规模就那么大，模型规模也大不到哪里去。那我们就提出疑问了，既然“教科书”数据这么强，何不把所有的网络数据改造成合成数据？这样数据规模扩大了，那模型规模不也能接着扩大？\n\n答案是不行。事实上预训练阶段模型训练的其实是“常识”，对于网络数据源，其中的专业知识类是必不可少的，如果转换成“教科书”，训练出来的模型就会偏向于各个阶段的教育，这其实也是一种降低泛化能力的行为。那如果将“教科书”数据和大规模网络数据混合呢？\n\n答案也是不行，因为就像是数学类比较多，文学类比较少的这种有明确占比的网络中的教科书数据，如果只是简单的叠加这类数据，事实上是增加“教科书”数据的占比，那么模型训练的结果必然是偏向于这一类的，从而导致通用泛化能力降低。\n\n讲了这么多，其实对于预训练阶段，使用大模型合成数据确实能够提升模型的整体性能，而这类数据需要注意`多样性`、`去除噪声`以及可以适当采用`教科书数据`，这些都是保证预训练阶段数据质量的关键。",
    "149": "一级标题：6.1 预训练阶段的合成数据\n二级标题：预训练数据举例\n内容：\n由于Phi论文中并没有明确提及如何合成的数据，huggingface团队为复现[Phi-1.5](https://arxiv.org/abs/2309.05463)过程中所遇到的挑战及其解决方案，构建了包含数十亿词元的合成数据集[cosmopedia](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia)，当然还有采样了100k条数据的[cosmopedia-100k](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia-100k)。\n\n<div style=\"display:flex;justify-content:center;\">\n  <figure style=\"text-align:center;margin:0;\">\n    <img src=\"./picture/cosmopedia_data.png\" style=\"width:800px;\" alt=\"cosmopedia数据集示例\">\n    <figcaption>cosmopedia数据集示例</figcaption>\n  </figure>\n</div>\n\n\n该数据集首先在网络上收集大量数据，其中`seed_data`就代表词源信息，然后由于受众群体`audience`的不一样，根据不同的任务类型`format`，提示词`prompt`会进行相应的构建，然后利用GPT-3.5生成相应的回答，也就是我们最终需要预训练数据`text`。\n\n我们可以用Qwen2.5-3B-Instruct模型进行一波尝试，就比如采用cosmopedia数据第一条中的prompt转换成中文形式输出相应的预训练数据，点击可以查看完整数据：\n\n<details>\n<summary>目标受众是小学生时提示词与数据</summary>\n\n*提示词* ：\n\n```Plain\n以下是一个网页的摘录：\n\n#贴现率计算器\n\n由博士候选人Tibor Pál创建\n\nArturo Barrantes和Adena Benn评论\n\n由博士候选人Tibor Pál创建\nArturo Barrantes和Adena Benn评论\n基于Cipra， T.金融与保险公式（2010）的研究\n最后更新:2023年2月2日\n\n我们创建了这个贴现率计算器来帮助您估计给定支付流的贴现率。更具体地说，您可以计算给定现值和未来值的贴现率，也可以计算附加支付流（例如年金）的贴现率。\n\n请继续阅读并了解以下内容：\n•贴现率的定义是什么？\n•贴现率公式是什么？\n•如何计算贴现率？\n要通过贴现率计算现值，您可能需要查看我们的现值计算器。\n\n贴现率的定义是什么？\n\n贴现率是贴现现金流量（DCF）分析中用于确定未来现金流量现值的利率。贴现率是一个重要的比较基础，因为它是\n\n创建一个与上面的片段相关的教育片段，目标受众是小学生。不应该使用像电磁学和集成这样复杂的大学课程，因为小学通常不会教这些课程。如果这就是这个片段的内容，那就找一个更简单的科学替代方法来解释，并使用日常示例。例如，如果主题是“线性代数”，你可以讨论如何将物体排成一行或列来帮助解决难题。\n\n避免使用专业术语和LaTeX，只讨论简单的小学水平的主题。马上开始教育部分。\n```\n\n*数据* ：\n\n```Plain\n好的，让我们为小学生们设计一个关于贴现率的简单教育片段。我们可以用一些日常生活中熟悉的概念来解释这个概念。\n\n大家好！今天我们要聊聊一个有趣的概念——贴现率。想象一下，你有一笔钱存起来，比如你的压岁钱，或者从爸爸妈妈那里得到的零花钱。这些钱放在口袋里不动，随着时间的推移，它们会变多吗？当然不会，因为它们只是在银行里放着，银行会给你一点小利息作为回报。这个小利息就是贴现率！",
    "150": "一级标题：6.1 预训练阶段的合成数据\n二级标题：贴现率的定义\n内容：\n贴现率就像是银行给你的“租金”。当你把钱存到银行里时，银行会给你一定的利息，这个利息就叫做贴现率。如果你把钱存得越久，银行就会给你越多的利息。所以，贴现率就像是一把尺子，用来测量你的钱随着时间的增长。",
    "151": "一级标题：6.1 预训练阶段的合成数据\n二级标题：贴现率公式\n内容：\n虽然这个公式可能看起来有点复杂，但其实很好理解。贴现率公式可以帮助我们算出未来某一天，这笔钱会变成多少。比如，如果你现在有10元钱，银行给你1%的贴现率，那么一年后你就有10元 + 1元 = 11元。这就像你在银行存了一年的钱，银行给了你1元的利息。",
    "152": "一级标题：6.1 预训练阶段的合成数据\n二级标题：如何计算贴现率\n内容：\n假设你有一个好朋友，他答应过你一年后送你一个玩具，这个玩具现在值50元。为了公平起见，你想知道这个玩具在未来一年内会变成多少钱，这样才能决定现在是否应该把100元存下来。这里，你就可以用贴现率来计算。\n\n1. 首先，你需要知道这个玩具现在的价值（称为现值），也就是50元。\n2. 然后，你需要知道你希望这个玩具在未来的价值（称为未来值），比如100元。\n3. 最后，你用贴现率来算出这个玩具在未来一年内会变成多少钱。",
    "153": "一级标题：6.1 预训练阶段的合成数据\n二级标题：小练习\n内容：\n想象一下，你现在有10元钱，你想存到银行里，让银行给你一定的贴现率，让你的钱变多。假设银行给你2%的贴现率，一年后你会有多少钱呢？（答案：10元 + 10元 × 2% = 10元 + 0.2元 = 10.2元）\n\n好了，今天的分享就到这里啦！记住，贴现率就像是银行给你的“租金”，帮你把现在的东西变成未来的东西。希望你下次听到这个词时，能立刻想到这个有趣的概念哦！\n```\n\n</details>\n\n\n<details>\n<summary>目标受众是专业人员时提示词与数据</summary>\n\n而如果我们将提示词中关于受众群体更改为专业人员，情况又会如何，下面的例子仍采用上述提示词，前半部分完全相同，因此以省略符号替代：\n\n*提示词* ：\n\n```Plain\n……\n\n创建一个与上面的片段相关的教育片段，目标受众是专业研究人员，他们学习了关于这些课程的内容，可以用专业术语来进行科普。\n请马上开始教育部分。\n```\n\n*数据* ：\n\n```Plain",
    "154": "一级标题：6.1 预训练阶段的合成数据\n二级标题：贴现率及其在金融与保险中的应用\n内容：",
    "155": "一级标题：6.1 预训练阶段的合成数据\n二级标题：贴现率的定义\n内容：\n贴现率（Discount Rate）在金融与保险领域具有重要地位，它是一种用来衡量未来现金流价值的比率。在进行投资决策、评估项目收益或是评估资产价值时，贴现率被广泛应用于现值（Present Value, PV）的计算。贴现率本质上是一个折现率，它表示了将未来一定时期内的一系列现金流折算到现在时点的价值。\n\n在贴现现金流（Discounted Cash Flow, DCF）分析中，贴现率通常被视为一项无风险收益率，用于调整预期未来现金流至当前的价值。贴现率的高低直接影响到现金流现值的大小，从而对投资决策产生重大影响。",
    "156": "一级标题：6.1 预训练阶段的合成数据\n二级标题：贴现率公式\n内容：\n贴现率的核心计算公式为：\n\\[ PV = \\frac{CF}{(1+r)^n} \\]\n其中，\n- \\( PV \\) 是现值，\n- \\( CF \\) 是未来某时刻的现金流，\n- \\( r \\) 是贴现率（折现率），\n- \\( n \\) 是时间（通常以年为单位）。\n\n通过上述公式，我们可以计算出某一特定未来时刻的现金流在当前时点的价值。在实际操作中，为了简化计算过程，有时会采用复利贴现率，即：\n\\[ PV = CF \\times (1 + r)^{-n} \\]",
    "157": "一级标题：6.1 预训练阶段的合成数据\n二级标题：如何计算贴现率\n内容：\n在金融模型中，贴现率的选择对最终结果至关重要。通常情况下，贴现率的选择会基于市场条件、行业标准或特定项目的预期风险水平。对于投资者而言，贴现率一般由投资者的风险承受能力和预期回报率决定；而对于企业而言，则可能基于其资本成本或机会成本来设定。\n\n贴现率的确定可以利用多种方法，包括但不限于：\n1. **加权平均资本成本（WACC）**：这是综合考虑不同资本来源成本的一种方法，常用于企业内部项目评估。\n2. **风险调整贴现率（Risk-Adjusted Discount Rate, RADR）**：这种方法考虑了特定项目的风险溢价，使得贴现率能够反映项目的额外风险。\n3. **市场贴现率**：基于当前市场的整体风险偏好来设定贴现率，适用于一些标准化的项目评估。",
    "158": "一级标题：6.1 预训练阶段的合成数据\n二级标题：现值计算器的应用\n内容：\n为了方便计算，我们提供了一个贴现率计算器，该工具可以帮助您根据给定的现值、未来值以及附加支付流（如年金）来计算相应的贴现率。通过现值计算器，您可以更直观地理解贴现率在实际应用中的重要性及其如何影响现金流的价值评估。\n```\n\n</details>\n\n\n<div style=\"background:#e7f5ff;color:#000;padding:12px 16px;border-left:4px solid #74c0fc;\">   从上述提示词的举例类比可以看出，同样的数据源，当提示词中受众目标更改，最终生成的预训练数据完全不同，相互之间关联度很低，专业人员所面对的一大堆公式，在受众群体是小学生时一点都没有提到，那么数据重复性就无从谈起，而且由于是大模型合成的数据，生成的文案在质量上有所保障，同时也可以确保规模，哪怕是3100k条数据也能保证多样性。<br></div>",
    "159": "一级标题：6.1 预训练阶段的合成数据\n二级标题：合成预训练数据代码实践\n内容：\n其实构建Phi模型的团队在论文[5] [6]里仅说明了数据集的构成来源以及数量，但是并没有详细说明数据的内容、如何处理数据的，因此Huggingface团队生成含数十亿词元的合成数据集以复现 [Phi-1.5](https://arxiv.org/abs/2309.05463) 过程中所遇到的挑战及其解决方案，由此最终创建了 [Cosmopedia](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia) 合成数据集[7]。该团队根据论文中提供的信息，从网络以及各个教育平台获得“种子数据”，然后由“种子数据”通过大模型合成扩大到3000万条的规模。\n\n该团队获得的“种子数据源”分别是网络数据和教育数据，其中网络数据占比最高，高达83%，还有纯粹的教育数据由16%，最后还有其他少量指令数据。\n\n<div style=\"display:flex;justify-content:center;\">\n  <figure style=\"text-align:center;margin:0;\">\n    <img src=\"./picture/seed_data.png\" style=\"width:800px;\" alt=\"种子数据来源\">\n    <figcaption>cosmopedia数据集示例</figcaption>\n  </figure>\n</div>\n\n\n该huggingface团队还提供了复现代码[8]，其中将合成步骤简单总结为三步，分别是：\n\n1. 合成prompt\n2. 根据prompt生成数据\n3. 数据筛选、去重等\n\n我们提供了简易的生成代码，链接[在这](https://gitlab.115lab.club:9000/lixinyu/prompt_engineering_tutorial/-/tree/main/5.synthetic_data/pretrain_data_generation?ref_type=heads)，代码构成如下：\n\n```Plain\npretrain_data_generation/\n├── generation.py             # 数据生成\n├── utils.py                  # openai接口设计以及其他工具等\n├── data/                     # 生成的数据保存地址\n└── data_process/               # 数据处理\n    ├── minhash_logs/           # 生成的哈希去重log\n    ├── minhash_results/        # 哈希去重结果\n    ├── data_depulication.py    # 数据去重\n    └── data_format_conversion.py  # 数据格式转换\n```\n\n下面我们按照步骤依次执行：\n\n### 1、合成prompt\n\n由于数据集包含大量的网络数据，但是huggingface团队提供的比如生成web网络数据的prompt的[代码](https://github.com/huggingface/cosmopedia/blob/main/prompts/web_samples/build_web_prompts.py#L48)里，数据集无法获取，不过该合成prompt的思想还是提示词+网络数据（上下文），具体的prompt可以参考[官方的提示词](https://github.com/huggingface/cosmopedia/blob/main/prompts/web_samples/build_web_prompts.py#L6)：\n\n*提示词模板：*\n\n```Plain\n\"wikihow\":\n\"\"\"Here is an extract from a webpage: \"<INSERT_EXTRACT>\".\n\nWrite a long and very detailed tutorial that could be part of WikiHow whose title is related to the extract above<ADD_TOPIC>. Include in depth explanations for each step and how it helps achieve the desired outcome, inluding key tips and guidelines.\nEnsure clarity and practicality, allowing readers to easily follow and apply the instructions. Do not use images.\"\"\",\n\n```\n\n`<INSERT_EXTRACT>`是网络数据放入的地方。\n\n为了方便起见，我们直接获取[HuggingFaceTB/cosmopedia-100k](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia-100k)的prompt就行（因为我们只需要学习如何生成的思想，具体操作可以查阅网络数据填充对应的地方批量生成prompt）。\n\n我们下载数据集到本地，可以使用`modelscope`下载数据集到本地\n\n```Bash\nmodelscope download --dataset swift/cosmopedia-100k data/train-00000-of-00002.parquet --local_dir ./dir\n```\n\n然后查看第一条数据：\n\n<img src=\"./picture/first_data.png\" alt=\"官方数据举例展示\" style=\"zoom:80%;\" />\n\n我们使用prompt作为我们后续生成的模板即可。\n\n\n### 2、根据prompt生成数据\n\n为快速、高效的生成数据，我们采用vllm框架实现，使用Qwen2.5-3B-Base模型，在开头环境安装和平台准备中，我们提醒道，因为vllm把本地的GPU完全利用，因此显存占用会比较高，3B的模型需要37-38GB左右，如果资源受限，可以使用更小的模型尝试，不过生成效果就不一定很好了。\n\n我们先开启vllm，模型采用本地保存的模型地址：\n\n```Bash\nvllm serve /home/lixinyu/weights/Qwen2.5-3B\n```\n\n然后运行生成代码：\n\n```Bash\npython generation.py\n```\n\n<div style=\"background:#fff3cd;color:#5b3200;padding:12px 16px;border-left:4px solid #ffeaa7;\">   ⚠️ <strong>注意</strong><br/>   不过需要注意的是，我们仅为了提供示例，下载.parquet格式数据，读取的时候也针对.parquet格式，如果你采用的是完整数据，请<strong>调整下面的代码</strong>： </div>\n\n<img src=\"./picture/parquet_data_process.png\" alt=\"数据处理\" style=\"zoom:80%;\" />\n\n我们的例子中只生成了20条数据，只需要两分钟时间，生成结果如下：\n\n<img src=\"./picture/data_process_success.png\" alt=\"生成20条数据\" style=\"zoom:80%;\" />\n\n<img src=\"./picture/data_process_example.png\" alt=\"生成20条数据举例\" style=\"zoom:80%;\" />\n\n接下来我们看下如何处理数据。\n\n\n### 3、数据筛选、去重等\n\n首先关于数据筛选，huggingface团队给出的[HuggingFaceTB/cosmopedia-100k](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia-100k)数据集已经经过了筛选环节，我们只是使用筛选后的数据集进行生成，因此这里不多赘述，有兴趣了解的朋友可以查看[官方给出的web数据筛选代码](https://github.com/huggingface/cosmopedia/blob/main/prompts/web_samples/filter_and_classify_clusters.py)，其实就是预设题目筛选，我们主要来实现数据去重。\n\ncosmopedia团队利用[datatrove库](https://github.com/huggingface/datatrove)中的Minhash实现数据去重，[代码在此](https://github.com/huggingface/cosmopedia/tree/main/deduplication)，然后我们的代码在这👉[ours](https://github.com/828Tina/PromptEngineeringCourse/tree/main/5.synthetic_data/pretrain_data_generation)。\n\n因为我们不需要Slurm集群来实现大规模数据去重，仅仅实现少量数据，因此我们使用本地的服务器就行，对于官方代码中的所有`SlurmPipelineExecutor`改成`LocalPipelineExecutor`，具体原理可参考[datatrove的本地pipeline](https://github.com/huggingface/datatrove?tab=readme-ov-file#localpipelineexecutor)设置。\n\n这段代码通过 Minhash 算法，也就是哈希去重的方法实现文本去重，哈希去重通过将任意长度文本转化为固定长度哈希值，既能大幅压缩数据规模，又能利用 “相同文本生成相同哈希值” 的特性快速判断重复，避免直接比对原始文本的冗余计算。其计算速度快且存储成本低，非常适合海量数据场景，像 Minhash 这类算法还能捕捉文本相似性，不仅检测完全重复，还能识别高度相似内容。同时，结合分桶、聚类等策略可进一步减少比对次数，显著提升大规模数据处理的效率，最终实现高效、精准的重复内容识别与过滤。\n\n整个流程分四个阶段完成重复检测与过滤。\n\n- 首先配置 Minhash 参数，包括哈希精度、桶数量和每个桶的哈希数，这些参数直接影响去重精度和效率。MinhashDedupSignature 组件为每条英文文本数据生成独特签名然后保存，签名作为文本的紧凑表示，在保留特征的同时减少数据量，且通过多进程并行处理提升效率。\n- 第二阶段进行签名分桶匹配，从签名文件夹读取数据，按配置的桶数量将相似签名文本归入同一桶中。这种分桶策略缩小了后续比较范围，避免全量两两比对，大幅提高处理效率。\n- 第三阶段基于桶结果聚类，从桶文件夹读取数据，将重复文本聚合成簇，确定需移除的重复数据 ID，明确重复文本对象，为最终过滤做准备。\n- 第四阶段完成重复过滤：这一阶段再次读取原始数据，指定文本字段为 “generated_text”（我们的数据保存到这里，当然也可以命名其他字段）；然后依据 remove_ids 信息过滤重复数据，被移除数据由 exclusion_writer 保存到 removed 文件夹，剩余非重复数据单独保存。\n\n运行下面的代码👇：\n\n```bash\npython data_depulication.py\n```\n\n<img src=\"./picture/data_depulication_success.png\" alt=\"数据去重成功结果\" style=\"zoom:80%;\" />\n\n当看到上图表示完成数据去重，并且成功保存好数据，保存好的文件是gz压缩包，我们可以执行下面的代码👇，将压缩包文件转换成jsonl文件，这样你就完成了最终的数据去重操作。\n\n```Bash\npython data_format_conversion.py\n```\n\n其实从上面的结果中，我们可以知道，所有数据都没有重复的，这是当然的，因为：\n\n1. 我们的数据量很少，仅作为例子，20条数据想重复都比较难\n2. 我们随机从100k条（当然我们只下载了50k条）数据，从中随机选择20条prompt，主题重复的概率极低，那生成的数据的重复的概率也很低\n\n<div style=\"background:#e8f5e9;color:#000;padding:12px 16px;border-left:4px solid #81c784;\">\n  ✅ 我们完成所有的步骤仅为了完整实现利用大模型实现预训练合成数据的操作，如果要进行大规模数据合成需要收集<strong>足量的种子源数据（互联网数据）</strong>，然后参考我们的教程的思想。\n</div>",
    "160": "一级标题：6.1 预训练阶段的合成数据\n二级标题：参考文献\n内容：\n[1].[Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361)\n\n[2].[GPT-1, GPT-2, GPT-3, GPT-3.5, GPT-4论文内容解读](https://blog.csdn.net/BGoodHabit/article/details/130134446?ops_request_misc=%257B%2522request%255Fid%2522%253A%25226ee3a9ef15f6a61581e883f22d069f12%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fblog.%2522%257D&request_id=6ee3a9ef15f6a61581e883f22d069f12&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~first_rank_ecpm_v1~rank_v31_ecpm-1-130134446-null-null.nonecase&utm_term=gpt&spm=1018.2226.3001.4450)\n\n\n[3].[Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/pdf/2309.05463)\n\n[4].[Cosmopedia: how to create large-scale synthetic data for pre-training](https://huggingface.co/blog/zh/cosmopedia)\n\n[5].[Textbooks Are All You Need](https://arxiv.org/pdf/2306.11644)\n\n[6].[Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/pdf/2309.05463)\n\n[7].[Cosmopedia: how to create large-scale synthetic data for pre-training](https://huggingface.co/blog/zh/cosmopedia)\n\n[8].[https://github.com/huggingface/cosmopedia](https://github.com/huggingface/cosmopedia?tab=readme-ov-file)",
    "161": "一级标题：6.2 微调阶段的合成数据\n二级标题：无\n内容：",
    "162": "一级标题：6.2 微调阶段的合成数据\n二级标题：微调阶段合成数据的重要性\n内容：\n理解微调阶段为什么需要合成数据，其实就是去了解为什么大模型需要微调、微调阶段需要什么样的数据。\n\n大模型经过预训练后，已经具备了基础的语言理解和生成能力，但这些能力更多是通用层面的。在实际应用中，不同场景对模型的要求千差万别，比如企业需要模型能精准处理法律合同审查，金融机构希望模型能高效分析市场动态，医疗机构则期待模型能辅助疾病诊断。这时候，**微调就成了让大模型 “术业有专攻” 的关键步骤**，通过在特定任务或领域的数据上进一步训练，让模型适配具体需求，提升在目标场景下的性能。\n\n而微调的效果，很大程度上取决于数据的质量和适配性。微调阶段对数据有着明确且严苛的要求。首先，**数据质量必须足够高**，需要准确、规范、无歧义，避免错误信息误导模型学习。其次，在格式上，微调数据基本以问答格式为主，因为这种格式能直接对应模型的交互场景，让模型学习到 “输入问题→输出答案” 的映射逻辑。对于更复杂的场景，多轮对话格式的数据也必不可少，它能帮助模型理解上下文关联，提升连续交互能力。\n\n更重要的是，微调数据往往需要专注于某一特定领域，比如法律、金融、医学等。以法律领域为例，可能需要大量 “如何认定合同无效”“离婚财产分割的法律依据是什么” 这类专有问答数据；金融领域则需要 “股票期权的风险如何评估”“企业债券发行的流程是什么” 等专业内容。这些领域专有问答数据，是让模型掌握专业知识、形成领域思维的核心素材。\n\n然而，**现实情况是网上这类高质量的领域专有问答数据非常稀少**。早期获得这类数据更多的依赖于人工标注，但是面对日益扩大的模型规模和任务需求，人工标注不仅耗时耗力，而且人工标注多多少少都会有偏向性从而无法保证数据的`丰富度`，因此最初大家都在往预训练阶段发力。\n\n随着[Stanford Alpaca: An Instruction-following LLaMA Model](https://arxiv.org/pdf/2212.10560)论文发布，由预训练GPT-3通过设定的提示词合成指令数据，然后清洗、去重等，最终用合成的指令数据对Llama模型进行微调，得到的Llama-Instruct模型在各个测试数据上都取得了不错的效果。\n\n下面我们详细解释论文中数据是如何生成的。\n\n<div style=\"display:flex;justify-content:center;\">\n  <figure style=\"text-align:center;margin:0;\">\n    <img src=\"./picture/instruct_data_generation.png\" style=\"width:800px;\" alt=\"alpaca指令微调数据集合成原理流程图\">\n    <figcaption>alpaca指令微调合成数据原理流程图</figcaption>\n  </figure>\n</div>\n\n\n上图是Alpaca论文中的数据集合成完整流程[5]，完整的数据合成流程分为四步，分别是1）生成任务指令，2）判断指令是否属于分类任务，3）采用输入优先或输出优先的方式进行实例生成，4）过滤低质量数据\n\n1. **生成任务指令**：首先需要人工编写多个种子任务，这些种子任务其实就是正常的问答，不过要添加任务主题，还有任务类型，比如下面的[例子](https://github.com/tatsu-lab/stanford_alpaca/blob/main/seed_tasks.jsonl)：\n\n```json\n{\n    \"id\": \"seed_task_0\",\n    \"name\": \"breakfast_suggestion\",\n    \"instruction\": \"Is there anything I can eat for a breakfast that doesn't include eggs, yet includes protein, and has roughly 700-1000 calories?\",\n    \"instances\": [{\"input\": \"\", \"output\": \"Yes, you can have 1 oatmeal banana protein shake and 4 strips of bacon. The oatmeal banana protein shake may contain 1/2 cup oatmeal, 60 grams whey protein powder, 1/2 medium banana, 1tbsp flaxseed oil and 1/2 cup watter, totalling about 550 calories. The 4 strips of bacon contains about 200 calories.\"}],\n    \"is_classification\": false}\n```\n\n`name`是主题，`is_classification`判断任务类型是否属于分类任务。论文中给出的种子数量仅有175条，这些种子任务目的是为了后续模型生成的时候有参照的模板，类似于`few-shot`的例子部分。由于任务种类较少，因此第一步是让大模型模仿并扩展任务指令，从任务池中采样8个任务，其中6个是种子任务的，2个是新生成的（任务池随着每次迭代更新数据，因此后续会有新生成的指令任务），\n\n<div style=\"display:flex;justify-content:center;\">\n  <figure style=\"text-align:center;margin:0;\">\n    <img src=\"./picture/instruct_generate_task.png\" style=\"width:600px;\" alt=\"指令生成task提示词数量\">\n    <figcaption>指令生成task提示词数量</figcaption>\n  </figure>\n</div>\n\n\n作为few-shot的例子部分，从task9开始让大模型生成新的task和instruction作为新的指令[5]，然后生成、数据过滤、去重清洗、加入到任务池中反复执行，从而扩展了大量的任务指令。\n\n2. **判断指令是否属于分类任务**：分类任务要先生成output，也就是标签，再生成input；不是分类任务的话先生成input再生成output（分类任务先生成标签是为了确保input不会偏离标签，因为本身由于instruction 的不同，模型生成的input有偏向，先生成了output确保生成的input不会偏离output标签；非分类任务由于output是跟着input走的，而不是像label一样是固定的，因此先生成input然后输出output）。这里需要注意的是，input和output都是GPT-3生成的，因为你的目标是生成数据集，而不是和模型问答。\n\n3. **采用输入优先或输出优先的方式进行实例生成**：第二步判断了是否属于分类任务，当明确了任务类型后，就能使用大模型生成对应任务类型的数据，比如我们看个例子：\n\n   ```text\n   # 分类任务\n\n   Given the classification task definition and the class labels, generate an input that\n   corresponds to each of the class labels. If the task doesn’t require input, just generate the\n   correct class label.\n\n   Task: Classify the sentiment of the sentence into positive, negative, or mixed.\n\n   Output(Class label): mixed\n   Input(Sentence): I enjoy the flavor of the restaurant but their service is too slow.\n\n   Output(Class label): Positive\n   Input(Sentence): I had a great day today. The weather was beautiful and I spent time with friends.\n\n   Output(Class label): Negative\n   Input(Sentence): I was really disappointed by the latest superhero movie. I would not recommend it.\n\n   # 非分类任务\n\n   Instruction: Given an address and city, come up\n   with the zip code.\n   Input:\n   Address: 123 Main Street, City: San Francisco\n   Output: 94105\n   ```\n\n4. **过滤低质量数据**：这一步通过过滤、去重等操作，将新的数据投放到数据池作为后续的数据生成数据池。过滤用的是指令过滤，方法是计算新生成的指令与现有指令之间的相似性（如ROUGE-L相似度）。如果新指令与现有指令的相似度超过某个阈值（如0.7），则认为该指令是重复的，将其过滤掉；关键词过滤检查指令中是否包含某些特定关键词（如“image”、“picture”、“graph”等），这些关键词通常表示任务超出了语言模型的处理范围；重复性检查生成的实例是否与现有实例完全相同，或者输入相同但输出不同；质量检查通过启发式规则（如指令长度、输入长度、输出长度等）来识别无效或低质量的生成内容。例如，指令过长或过短，输出是输入的重复等。\n\n<div style=\"background:#e7f5ff;color:#000;padding:12px 16px;border-left:4px solid #74c0fc;\">   综上所述，在微调阶段，合成数据是非常重要的，因为大模型合成的指令数据不仅在质量上有所保障，并且可以弥补人工标注的弊端，合成大量数据。<br/></div>",
    "163": "一级标题：6.2 微调阶段的合成数据\n二级标题：微调数据举例\n内容：\n但是微调虽然要求的数据规模不大， 但是对质量的要求很高。我们在前文已经分析了生成微调数据的完整流程，而生成的数据集就是我们在入门大模型微调时经常使用的数据集[Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca)，总计52k条数据，所有的数据都是由GPT-3预训练模型生成，如果想复现生成数据集的代码，查看[官方给的代码](https://github.com/tatsu-lab/stanford_alpaca/blob/main/generate_instruction.py)即可。\n\n一般的，指令微调的数据集包含三个部分：指令、输入、输出：\n\n```Plain\ninstruction：指令\ninput：输入\noutput：输出\n```\n\n我们看下经典的Alpaca数据集格式：\n\n<div style=\"display:flex;justify-content:center;\">\n  <figure style=\"text-align:center;margin:0;\">\n    <img src=\"./picture/alpaca_data_example.png\" style=\"width:700px;\" alt=\"alpaca数据集示例\">\n    <figcaption>alpaca数据集示例</figcaption>\n  </figure>\n</div>\n\n\n其中每一部分含义如下：\n\n- `instruction`：描述模型应执行的任务。52K 条指令中的每一条都是唯一的。\n- `input`：可选的上下文或任务输入。例如，当指令为“总结以下文章”时，输入就是文章本身。大约 40% 的示例包含输入。\n- `output`：由`text-davinci-003`（GPT-3）模型来生成的指令的答案。\n- `text`：`instruction`，`input`并使用作者用于微调其模型的[提示模板](https://github.com/tatsu-lab/stanford_alpaca#data-release)`output`进行格式化。",
    "164": "一级标题：6.2 微调阶段的合成数据\n二级标题：合成微调数据代码实践\n内容：\n关于微调数据的合成，提供了完整的合成instruct数据的[代码](https://github.com/tatsu-lab/stanford_alpaca/tree/main)，完整的生成流程我们在前文已经讲过，我们再简单总结下：\n\n1. 生成任务指令\n2. 判断指令是否属于分类任务\n3. 采用输入优先或输出优先的方式进行实例生成\n4. 过滤低质量数据\n\n完整的四步在[生成代码](https://github.com/tatsu-lab/stanford_alpaca/blob/main/generate_instruction.py)里，具体看`generate_instruction_following_data`部分，基本按照生成流程的顺序实现，openai对应的API接口处理代码在[工具](https://github.com/tatsu-lab/stanford_alpaca/blob/main/utils.py)里，代码原理也很简单，我们就不再赘述。\n\n我们提供了简易的生成代码，链接[在这](https://gitlab.115lab.club:9000/lixinyu/prompt_engineering_tutorial/-/tree/main/5.synthetic_data/instruct_data_gengeration?ref_type=heads)，代码构成如下：\n\n```Plain\ninstruct_data_gengeration/\n├── generate_instruction.py            # 数据生成\n├── utils.py                           # openai接口设计以及其他工具等\n├── prompt.txt                         # 生成instruction的提示词\n├── seed_tasks.jsonl                   # 种子任务\n└── data/                              # 生成的数据保存地址\n```\n\n不过因为我们仍然用vllm框架来实现高效生成数据的方法，而官方的代码时间有点久，很多包进行了更新，不一定适配原始的代码，因此我们把需要改动的地方着重强调下。\n\n因为我们希望使用本地保存的模型，重点是[model_name_or_path](https://github.com/tatsu-lab/stanford_alpaca/blob/main/generate_instruction.py#L114)换成我们的**本地的模型地址**，无论是本地模型Qwen还是默认的GPT系列的text-davinci-003，其实都适配OpenAI的JSON输入接口，因此[该函数的整体逻辑](https://github.com/tatsu-lab/stanford_alpaca/blob/main/utils.py#L39)不需要进行修改，但是由于openai这个包有更新，细节部分需要修改。\n\n1. 首先是[openai的Completion](https://github.com/tatsu-lab/stanford_alpaca/blob/main/utils.py#L106)换成了`openai.OpenAI.completions`，这里需要注意。\n2. 其次是我们没有走openai的调用API的方法，走的是本地服务器，因此需要把url改成本地的端口地址，一般情况下，默认是8000，代码如下：\n\n```Python\nclient = openai.OpenAI(\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\n    api_key=\"EMPTY\",\n    base_url=\"http://127.0.0.1:8000/v1\",\n)\n```\n\n因此第一步的completions需要改成如下形式：\n\n```Python\ncompletion_batch = client.completions.create(prompt=prompt_batch, **shared_kwargs)\n```\n\n3. 最后[openai删掉了openai_object](https://github.com/tatsu-lab/stanford_alpaca/blob/main/utils.py#L13)，不过这一步基本只是规定输出格式，因此直接删掉所有关联的代码即可\n\n具体的修改好的代码可以参考👉[ours](https://github.com/828Tina/PromptEngineeringCourse/tree/main/5.synthetic_data/instruct_data_gengeration)\n\n我们先开启vllm，模型采用本地保存的模型地址：\n\n```Bash\nvllm serve /home/lixinyu/weights/Qwen2.5-3B\n```\n\n然后我们运行下面的代码，就可以合成微调数据了：\n\n```Python\npython -m generate_instruction generate_instruction_following_data \\\n  --output_dir ./data \\\n  --num_instructions_to_generate 10 \\\n  --model_name /home/lixinyu/weights/Qwen2.5-3B \\\n  --request_batch_size 2\n```\n\n合成的数据如下：\n\n<img src=\"./picture/instruct_data_result.png\" alt=\"合成instruct数据集示例\" style=\"zoom:80%;\" />\n\n<div style=\"background:#e8f5e9;color:#000;padding:12px 16px;border-left:4px solid #81c784;\">\n  ✅ 这里我们看下，most_similar_instructions后面有很多的分数，avg_similarity_score也有，我们简单讲述每个分数的含义：</br>\n    1. most_similar_instructions的分数：数值是ROUGE-L 分数（范围 0-1），表示<strong>新生成的指令与已有指令中最相似的 10 条指令</strong>（num_instructions_to_generate）之间的相似度。</br>\n    2. avg_similarity_score的分数：是<strong>所有已有指令与新指令</strong>相似度的平均值</br>\n</div>",
    "165": "一级标题：6.3 推理合成数据\n二级标题：无\n内容：",
    "166": "一级标题：6.3 推理合成数据\n二级标题：推理合成数据的重要性\n内容：\n推理数据，其实可以叫做具备“思考能力”的数据。“思考能力”其实是近一年左右逐渐发展的概念，主要应用于后训练阶段，希望模型在具备基本的问答能力的同时，多加一个思考模块，这个思想是从DeepSeek-R1爆火开始。\n\n具备“思考能力”数据集，顾名思义，在原始问答对基础上，增加了思考模块的数据。在前文我们提到随着模型规模的扩大，互联网数据几乎被全部用于预训练，即使有合成数据的帮助，在2024年也到了瓶颈，而2025年年初，DeepSeek凭借R1模型爆火了一把，R1凭借其独特的思考模块和能力，还有独特的训练方式，稳居当时开源模型的榜首，而也正是因为其思考能力，让合成数据再一次突破了数据规模的瓶颈，在原始问答对基础上只是增加think部分，模型性能就有很大的提升，尤其表现在数学推理等任务当中。\n\n“思考型”数据基本只能由合成数据构成，因为互联网基本不会存在这种数据，而人工标注也基本不可能，简单的问答对或许还有实现的希望，增加或错或对的思考过程，基本只能由大模型本身的生成能力才能实现。\n\n在DeepSeek-R1发布的[技术报告](https://arxiv.org/pdf/2501.12948)中，R1的基线模型是DeepSeek-V3，经过多步合成数据，通过微调的方式对V3进行微调，从而获得R1，而合成的数据大部分是V3通过GRPO强化学习方式生成的推理数据，外加少部分非推理数据，使得V3具备思考能力，具体的流程可参考下图[6]：\n\n<div style=\"display:flex;justify-content:center;\">\n  <figure style=\"text-align:center;margin:0;\">\n    <img src=\"./picture/grpo.png\" style=\"width:900px;\" alt=\"GRPO原理图\">\n    <figcaption>GRPO原理图</figcaption>\n  </figure>\n</div>\n\n\n其中我们所说的“思考型”数据，在R1的整体生成流程中，GRPO强化学习阶段，模型生成而来，然后用生成的数据对V3进行微调，最终得到的R1。\n\n在GRPO阶段，从V3到R1-Zero的过程中，DeepSeek团队对system提示词加以改造：\n\n<div style=\"display:flex;justify-content:center;\">\n  <figure style=\"text-align:center;margin:0;\">\n    <img src=\"./picture/grpo_system_prompt.png\" style=\"width:700px;\" alt=\"GRPO的system提示词设计\">\n    <figcaption>GRPO的system提示词设计</figcaption>\n  </figure>\n</div>\n\n\n上图是GRPO过程中提示词的内容[7]，可以看到，虽然提示中明确要求在 `<think>` 标签内写出推理过程，但并未对推理过程的具体形式做任何规定。\n\n在强化学习阶段，他们基于规则设计了两类奖励：\n\n1. **准确度奖励（Accuracy rewards）**：通过测试答案的正确性来给予奖励。\n2. **格式奖励（Format rewards）**：对使用 `<thinking>` 和 `<answer>` 标签的行为给予奖励。\n\n对于那些导致答案正确的所有决策——无论是特定的 token 序列还是推理步骤——都会在训练中获得使其更有可能被采纳的权重调整。\n\n而对于那些导致答案错误的所有决策，则会在训练中获得使其更不可能被采纳的权重调整。\n\n<div style=\"display:flex;justify-content:center;\">\n  <figure style=\"text-align:center;margin:0;\">\n    <img src=\"./picture/r1_zero.png\" style=\"width:700px;\" alt=\"R1-Zero生成原理\">\n    <figcaption>R1-Zero生成原理</figcaption>\n  </figure>\n</div>\n\n\n有意思的是，该提示并没有给出 `<think>` 过程应当如何呈现的示例，只是要求在 `<think>` 标签内进行思考，无需更多细节。通过向模型提供与思维链（Chain-of-Thought）相关的间接奖励，模型逐渐自主学会：当推理过程更长、更复杂时，答案更可能是正确的。\n\n<div style=\"display:flex;justify-content:center;\">\n  <figure style=\"text-align:center;margin:0;\">\n    <img src=\"./picture/grpo_training_steps.png\" style=\"width:700px;\" alt=\"模型训练过程\">\n    <figcaption>模型训练过程文本长度变化</figcaption>\n  </figure>\n</div>\n\n\n通过这样的训练流程，研究人员发现，模型能够自发地探索出最优的链式推理模式，并展现出如自我反思、自我验证等高级推理能力。\n\n不过，这种做法仍存在一个显著缺陷：其输出的可读性不佳，而且有时会混用多种语言，这是仅用强化学习导致的弊端，没有微调阶段对回答模式加以限制，推理时错误的概率就会比较高。为了解决这个问题，团队转而研究另一种思路，在后续GRPO学习中添加了语言类的奖励信号，同时因为V3-Base强大的生成能力，通过V3-Base合成的“思考型”数据作为微调时所用的微调数据，对预训练模型进行微调，就能让模型快速学会这种思考方式。\n\n总结下来，“思考型”数据只能由模型生成，当然质量问题是基础，不过可能模型生成的“思考型”数据也不一定质量很高，因为可能存在大量重复思考过程，更重要的是，现有的数据中基本没有包含思考过程的数据。\n\n因此合成数据对于提高模型思考推理能力是必要的，想要让模型具备思考能力，要么通过强化训练自己合成数据自己微调，这对于模型的规模要求较高，因为小规模模型不一定有强大的能力；要么通过知识蒸馏，把大模型具备的思考能力迁移到小规模模型中。",
    "167": "一级标题：6.3 推理合成数据\n二级标题：推理合成数据举例\n内容：\n虽然DeepSeek技术报告中的80w条数据并未开源，但是训练数据的格式基本包含下面三个部分：\n\n```Plain\ninput：请说说三原色分别是什么\nreasoning_content：好的，用户……\ncontent：三原色分别是……\n```\n\n其中：\n\n- `input`：问答对中的问题部分\n- `reasoning_content`：这部分是模型的思考部分，其实在输出中由`<think></think>`特殊字符包裹的部分，里面包含模型的推理、反思等内容，是一段非常长的文本\n- `content`：该部分是模型的最终输出内容，也包含在输出中，通常在`</think>`特殊字符后面，作为模型给出的标准回答，模型在输出的时候可以通过对话模板自动检测出特殊字符，从而提取模型最终的输出\n\n我们可以在huggingface社区中找到开源的[基于R1生成的SFT数据](https://huggingface.co/datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k)：\n\n<div style=\"display:flex;justify-content:center;\">\n  <figure style=\"text-align:center;margin:0;\">\n    <img src=\"./picture/reasoning_data_example.png\" style=\"width:700px;\" alt=\"推理数据集示例\">\n    <figcaption>推理数据集示例</figcaption>\n  </figure>\n</div>\n\n\n该数据集为中文开源蒸馏满血R1的数据集，数据集中不仅包含math数据，还包括大量的通用类型数据，总数量为110K。\n\n由于强化学习对资源要求比较高，并且训练时间通常非常漫长，想要通过GPRO在小模型上复现R1其实不太现实，因此如果想快速实现R1的功能，可以采用蒸馏的方法，该数据集可以很好的实现在小模型上蒸馏R1的能力，有兴趣的朋友可以利用该数据集对小模型比如Qwen系列7B以下的模型进行SFT，从而让规模较小的模型实现思考推理能力。",
    "168": "一级标题：6.3 推理合成数据\n二级标题：推理合成数据代码实践\n内容：\n由于推理数据需要本身就具备思考过程的模型来实现，我们所熟悉的deepseek-r1还有Qwen3都有思考模块，我们选择Qwen3，为什么选择Qwen而不是DeepSeek，因为我们使用阿里百炼平台通过调用API的形式调用模型来生成数据，Qwen3能快点，DeeSeek-R1有点慢而已，用R1也绝对没有任何问题。\n\n我们提供了简易的生成代码，链接[在这](https://gitlab.115lab.club:9000/lixinyu/prompt_engineering_tutorial/-/tree/main/5.synthetic_data/inference_data_generation?ref_type=heads)，代码构成如下：\n\n```Plain\ninstruct_data_gengeration/\n├── generate.py            # 数据生成\n└── data/                  # 生成的数据保存地址\n```\n\n为方便起见，我们直接选择[alpaca中文版](https://huggingface.co/datasets/llm-wizard/alpaca-gpt4-data-zh)的指令和输入作为整体的输入数据，让Qwen3去生成思考+回答，作为我们的推理数据。我们简单分成下面两步：\n\n1. 下载Alpaca数据\n2. 调用Qwen3生成推理数据\n\n我们直接运行下面的代码，就可以完整的生成推理数据：\n\n```Python\npython generation.py\n```\n\n因为我们只是举例说明，我们只生成了10条数据，批量生成数据的话可以修改数量。\n\n<img src=\"./picture/reasoning_data_results.png\" alt=\"合成reasoning数据集示例\" style=\"zoom:80%;\" />",
    "169": "一级标题：第六章 合成数据\n二级标题：无\n内容：",
    "170": "一级标题：第六章 合成数据\n二级标题：为什么需要合成数据？\n内容：\n在了解为什么需要合成数据前，我们先简单了解下何为“合成数据”。\n\n合成数据（Synthetic Data）是指用算法、生成模型或仿真程序“造”出来的数据，它模仿真实世界数据的统计分布和结构，但并不直接来源于真实观测，在此教程里，合成数据是由大模型根据不同的提示词生成的数据。\n\n为什么需要合成数据，其实核心的核心是数据的`质量问题`，我们知道，对于大模型来说，优秀的大模型往往需要经过预训练、后训练，后训练包含微调、强化训练等，关于每个阶段训练使用的数据，其`数据质量`对于每个阶段来说有些微的不同，不过每个阶段的共同点是都需要数据的`丰富度`。",
    "171": "一级标题：第六章 合成数据\n二级标题：课程导航\n内容：\n<div style=\"background:#e7f5ff;color:#000;padding:12px 16px;border-left:4px solid #74c0fc;\">\n本节我们将分别详细探讨各个阶段为何需要合成数据，并举例开源的一些合成的数据的例子，最后我们会对如何实现合成数据做出详细的教程，每一个模块都根据预训练、微调、推理数据分类讨论，下面是我们每一个章节的链接👇：\n</div>\n\n| 教程章节   | 状态 |\n|:--------|:------|\n| [6.1.预训练合成数据](./1.pretrain_data.md)   |  ✅  |\n| [6.2.微调合成数据](./2.instruct_data.md)   | ✅  |\n| [6.3.推理合成数据](./3.reasoning_data.md)   | ✅   |",
    "172": "一级标题：第七章 RAG检索\n二级标题：无\n内容：\n当我们向大模型询问大模型不知道的知识的时候，大模型可能会因为“幻觉”即使不知道答案，毕竟大模型在训练时候使用过的数据集通常来说不会紧跟时事训练，大部分的知识在预训练阶段灌输，在微调阶段进行回答的调试。因此大模型会胡乱用相似的知识为我们做出回答，但是很明显不会是我们想要的，那有什么方法是可以不用随时训练大模型就能让大模型回答正确呢？答案是RAG。\n\n大模型 RAG（检索增强生成）是一种结合大语言模型与检索技术的混合系统，其核心原理是在生成回答前，先从外部知识库中检索与问题相关的信息，再将这些精准信息作为上下文输入大模型，辅助其生成回答，**大模型只充当了回答的角色**。这种模式的优势显著，既能增强回答的准确性，有效减少大模型常见的 “幻觉” 问题，又能利用最新数据弥补模型训练数据的滞后性，同时降低对模型参数量的依赖以节省计算成本。不过，RAG 也存在一定劣势，检索质量直接决定结果优劣，高度依赖知识库的完整性与准确性；在多轮对话中上下文管理较为复杂，容易出现信息冗余或遗漏；且对长文档的处理效率较低，可能因检索范围有限而遗漏关键内容。\n\n我们可以看下RAG原理：\n\n<div style=\"display:flex;justify-content:center;\">\n  <figure style=\"text-align:center;margin:0;\">\n    <img src=\"./picture/rag_theory.png\" style=\"width:700px;\" alt=\"RAG原理\">\n    <figcaption>RAG原理</figcaption>\n  </figure>\n</div>\n\n\n我们简单的把RAG过程分割成三个部分，根据RAG的英文全称是Retrieval-Augmented Generation，理论上应该分割成`检索`、`增强`、`生成`，不过`检索`部分可以类比成`存储+检索`，多加了一个词，是不是原理就更清晰了呢？我们拆分下：\n\n1. `存储+检索`：将文档内容分割、转换成向量格式、检索相似性较高的文档切片\n2. `增强`：设计提示词，将用户的问题+文档切片内容结合作为查询内容\n3. `生成`：使用大模型对`增强`后的提示词生成响应\n\n简单讲完RAG原理，虽然感觉听懂了，但是还是有点抽象对吗？那么我们接下来用一个简单的代码示例来完整讲述RAG：",
    "173": "一级标题：第七章 RAG检索\n二级标题：构建向量数据库\n内容：\n首先我们构建一个向量数据库，用来存储文档切片转换成的向量内容。该数据库函数包含添加向量、检索向量两部分功能，其中检索向量我们需要使用embedding模型计算向量相似度，embedding模型我们使用`Qwen3-embedding`模型，Qwen3提供了0.6B、4B、8B等，由于我们就是简单做一个RAG，因此我们使用最小的0.6B模型，对资源消耗不是很高，大约2-3GB的显存占用，但是注意，不要重复运行加载代码，显存会累积占用，多次运行加载代码会耗费很多资源。\n\n话不多说，我们看下代码：\n\n```Python\n### 1、加载embedding模型\nfrom sentence_transformers import SentenceTransformer\n\nembedding_model_path = \"/your/path/of/Qwen3-Embedding-0.6B\"\nembedding_model = SentenceTransformer(embedding_model_path)\n```\n\n<div style=\"background:#fff3cd;color:#000;padding:12px 16px;border-left:4px solid #ffeaa7;\">   ⚠️ 加载模型的代码不能重复运行，否则会累积显存 </div>\n\n```Python\n### 2、构建向量数据库\nimport numpy as np\nfrom typing import List, Dict, Optional\n\nclass VectorStorage:\n    \"\"\"简单向量存储类，仅负责向量的存储功能\"\"\"\n\n    def __init__(self):\n        \"\"\"初始化向量存储\"\"\"\n        self.vectors = []  # 存储向量\n        self.metadata = []  # 存储向量对应的元数据\n        self.dim = None  # 向量维度\n\n    def add(self, vector: List[float], metadata: Optional[Dict] = None) -> int:\n        \"\"\"\n        添加向量到存储\n\n        参数:\n            vector: 要添加的向量\n            metadata: 向量相关的元数据\n\n        返回:\n            向量在存储中的索引\n        \"\"\"\n\n        # 检查向量维度\n        if self.dim is None:\n            self.dim = len(vector)\n        elif len(vector) != self.dim:\n            raise ValueError(f\"向量维度必须为{self.dim}\")\n\n        # 保存向量和元数据\n        self.vectors.append(vector)\n        self.metadata.append(metadata or {})\n\n        # 返回向量索引\n        return len(self.vectors) - 1\n\n    def get(self, index: int) -> tuple[np.ndarray, dict]:\n        \"\"\"\n        获取指定索引的向量和元数据\n\n        参数:\n            index: 向量索引\n\n        返回:\n            元组 (向量, 元数据)\n        \"\"\"\n        if 0 <= index < len(self.vectors):\n            return self.vectors[index], self.metadata[index]\n        raise IndexError(\"索引超出范围\")\n    def search(self, query: str, k: int = 2) -> List[tuple[int, float, dict]]:\n        \"\"\"        搜索最相似的向量\n        参数:\n            query_vector: 查询向量\n            k: 返回的相似向量数量\n        返回:\n            包含元组的列表，每个元组格式为 (索引, 相似度, 元数据)\n        \"\"\"\n        # 输入的文本转换成向量格式\n        query_vector = embedding_model.encode([query], prompt_name=\"query\")\n\n        # 计算相似度\n        similarities = embedding_model.similarity(query_vector, self.vectors)\n        similarities = similarities.squeeze()\n\n        # 限制 k 值不超过向量总数\n        k = min(k, len(self.vectors))\n\n        # 获取最相似的 k 个向量的索引（使用 PyTorch 方法）\n        top_k_values, top_k_indices = similarities.topk(k, largest=True)\n\n        # 返回结果（将张量转换为 Python 列表）\n        return [\n        (top_k_indices[i].item(), top_k_values[i].item(), self.metadata[top_k_indices[i].item()])\n        for i in range(k)\n        ]\n\n    def __len__(self) -> int:\n        \"\"\"返回存储中向量的数量\"\"\"\n        return len(self.vectors)\n```\n\n其中`add`是添加向量的部分，`search`是检索向量，`get`仅是确定下每一个序号对应的向量内容，不参与实际使用。\n\n我们存入一个向量内容：\n\n```Python\n### 存入向量数据库\nvb= VectorStorage()\n\nquery=\"你好\"\nquery_vector=[1,2,3]\n\n# 添加到向量数据库中\nvb.add(query_vector,{\"index\":1,\"chunk\":query})\n\n# 我们查看下向量\nvector1, meta1 = vb.get(1)\nprint(f\"向量1: {vector1}, 元数据: {meta1}\")\n\n# 获取向量数量\nprint(f\"存储的向量数量: {len(vb)}\")\n```\n\n*回答* ：\n\n```Plain\n向量1: [1,2,3], 元数据: {\"index\":1,\"chunk\":\"你好\"}\n存储的向量数量: 1\n```\n\n非常好！你现在已经成功构建了一个向量数据库，那么接下来我们来学习如何处理文档，因为通常来说，文档的内容都会很长，将其转换为向量显然不合理，因为每一个embedding模型能够处理的文本长度都是有限的，比如Qwen3的embedding模型：\n\n<img src=\"./picture/qwen3_list.png\" alt=\"Qwen3的Embedding模型列表\" style=\"zoom:80%;\" />\n\n能够处理的序列长度为32K，超过这个长度就无法词嵌入了，因此我们要对文档进行分块处理。如何处理我们接着往下看。",
    "174": "一级标题：第七章 RAG检索\n二级标题：文档分块处理\n内容：\n前面我们知道，首先embedding能够处理的文本长度有限。其次呢，如果不分块，一段文字的内容过多，显然我们检索出来的文本可能包含内容会很多很杂，但是我们希望结果会更精准些，比如，如果我要问一条新闻里张三做了什么事情，但是如果检索出来的切片里不仅有张三做的事情，还有李四、王五、赵六等等，大模型就需要做额外的判断，那么生成的时候效率就会降低。\n\n因此我们可以按照句号来划分、或者按照字数来划分，比如500字分成一条信息，我们举一个简单的例子：\n\n```Python\n### 3、文档加载和分块\nimport PyPDF2\nfrom typing import List, Tuple, Dict\nimport re\n\ndef read_pdf(file_path: str) -> str:\n    \"\"\"\n    读取PDF文件内容\n\n    参数:\n        file_path: PDF文件路径\n\n    返回:\n        PDF文件中的文本内容\n    \"\"\"\n    try:\n        with open(file_path, 'rb') as file:\n            reader = PyPDF2.PdfReader(file)\n            text = \"\"\n            for page in reader.pages:\n                text += page.extract_text()\n            return text\n    except Exception as e:\n        print(f\"读取PDF文件时出错: {e}\")\n        return \"\"\n\ndef split_pdf(text, max_len=200):\n    # 按句子优先分割\n    chunks = re.split(r'[。！？]', text)\n    # 合并短句到200字以内\n    result, buf = [], \"\"\n    for s in chunks:\n        if len(buf + s) <= max_len:\n            buf += s\n        else:\n            if buf: result.append(buf)\n            buf = s\n    if buf: result.append(buf)\n    return result\n\nfile_path = \"./liulangdiqiu2.pdf\"\nchunks = split_pdf(read_pdf(file_path), max_len=200)\nchunks\n```\n\n该例中，我们按照。！？为优先划分，200字为次优先限制，将一个简单的文本划分了7条信息：\n\n<details>\n<summary>完整信息</summary>\n\n```Plain\n['《流浪地球2》是由郭帆执导的科幻灾难电影，于2023年上映，故事围绕《流浪地球》\\n前作展开，以计划建造1万座行星发动机的时代为背景\\n在不远的未来，太阳急速衰老膨胀，即将吞噬太阳系，地球面临灭顶之灾为应对危机，地\\n球各国成立联合政府，提出数百个自救计划，其中“移山计划”“方舟计划”“逐月计划”\\n和“数字生命计划”进入论证阶段',\n '“移山计划”由中国提出，旨在建造1万座行星\\n发动机推动地球前往新家园；“方舟计划”是美国提议的在地球同步轨道建立空间站以带\\n领人类逃离；“逐月计划”由俄罗斯提出，想改造月球为逃生舱，后因月球结构等问题并\\n入“移山计划”；“数字生命计划”则是将人类意识数字化，实现永生，但最终被伦理委\\n员会禁止经过考量，“移山计划”被选定，人类开始着手建造行星发动机，同时准备建\\n造卫星发动机以放逐月球，摆脱月球引力',\n '\\n然而，计划推进过程中危机四伏2044年，太空电梯基地遭遇危机，处在9万公里高度\\n的方舟空间站爆炸坠落，引发连锁反应，导致太空电梯基地被摧毁，流浪地球计划面临重大\\n挑战影片中，满腔赤诚的刘培强（吴京饰）历经层层考验成为航天员大队的一员，他与韩\\n朵朵（王智饰）在此过程中相知相恋，而刘培强也在后续故事中为了地球和家人，不断经历\\n着艰难抉择与挑战',\n '\\n另一边，量子科学家图恒宇（刘德华饰）则与“数字生命计划”紧密相关他致力于将女\\n儿图丫丫的意识数字化，即使该计划被禁止，他也未放弃在一系列意外后，图恒宇自己也\\n意外进入数字世界，以一种特殊的方式继续参与到拯救地球的行动中，他在数字空间中的经\\n历，也为影片增添了一层神秘的“元宇宙”色彩',\n '\\n联合政府中国代表周喆直（李雪健饰）则在国际舞台上，为“移山计划”的推进四处奔走，\\n面对各方压力和危机，他始终坚定信念，相信人类能够团结起来拯救地球，展现出了强大的\\n责任感与使命感\\n随着故事发展，月球发动机建造完成，但却因未知原因被启动，月球开始向地球逼近，引发\\n了全球性的灾难，地震、海啸等灾害频发，人类再次面临生死考验为了阻止月球撞击地球，\\n人类决定启动行星发动机，利用其推力将月球推离',\n '在这一过程中，无数普通人与主角们一\\n起，克服了重重困难，最终成功放逐月球，为地球开启流浪之旅奠定了基础',\n '\\n《流浪地球2》通过展现刘培强、图恒宇、周喆直等众多角色的经历，以及全球人类在末\\n日危机下的挣扎与抗争，呈现了一个宏大而震撼的科幻世界，探讨了人类面对绝境时的生存\\n选择、亲情、责任与勇气等主题，传达出“人类命运共同体”理念和“没有人的文明，\\n毫无意义”的深刻内涵，以其壮观的视效和动人的情节，成为中国科幻电影的重要代表作']\n```\n\n</details>\n\n这里我简单的用《流浪地球2》的新闻作为例子，总长只有1000多字，按照上面的代码，我们分成了7条信息。\n\n那么知道了如何对文档进行分块，我们需要将分块后的文档切片按顺序保存到向量数据库里，由于要保存成向量格式，因此需要使用embedding模型实现词嵌入的转换，然后将向量保存。\n\n我们看下实现代码：\n\n```Python\n### 4、载入向量数据库\nvb = VectorStorage()\n\nembedding_pdf = embedding_model.encode(chunks, prompt_name=\"document\")\n\nfor i,emb in enumerate(embedding_pdf):\n    vb.add(emb, {\"index\": i, \"chunk\": chunks[i]})\n\n# 获取向量数量\nprint(f\"存储的向量数量: {len(vb)}\")\n存储的向量数量: 7\n```\n\n好了，到这里我们已经成功将分割后的文档保存在向量数据库里，接下来我们要实现`检索`功能，这个是RAG比较重要的环节。",
    "175": "一级标题：第七章 RAG检索\n二级标题：检索向量\n内容：\n关于`检索`，最重要的是根据用户的问题找到相似度最高的几个对应的文档切片，最重要的当然就是相似度计算，原始的我们通常采用余弦相似度，不过Qwen3提供了embedding模型直接可以计算相似度，我们可以在其[官网](https://www.modelscope.cn/models/Qwen/Qwen3-Embedding-0.6B)找到推理代码，为了方便起见，我们使用`sentence-transformers`库来实现相似度计算，代码如下：\n\n```Python\nfrom sentence_transformers import SentenceTransformer\n\n# Load the model\nmodel = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")\n\n# The queries and documents to embed\nqueries = [\n    \"What is the capital of China?\",\n    \"Explain gravity\",\n]\ndocuments = [\n    \"The capital of China is Beijing.\",\n    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n]\n\n\nquery_embeddings = model.encode(queries, prompt_name=\"query\")\ndocument_embeddings = model.encode(documents)\n\n# Compute the (cosine) similarity between the query and document embeddings\nsimilarity = model.similarity(query_embeddings, document_embeddings)\nprint(similarity)\n```\n\n然后我们根据检测出来相似度最高的**两条信息**作为最终的文档内容，当然你也可以把选出来的文档切片数量扩大，能确保模型收到的信息完整一些，不过最好不要太多，一般**5条信息**以内就可以，信息太多，模型处理速度也会下降，生成效率就会降低。\n\n至于如何检索，我们使用向量数据库里的`search`函数：\n\n```Python\n### 5、检索向量数据库\nquery=\"《流浪地球2》是由谁导演的？\"\n\nresults = vb.search(query, k=2)\nprint(results)\n```\n\n*回答* ：\n\n```JSON\n[(0, 0.8075547814369202, {'index': 0, 'chunk': '《流浪地球2》是由郭帆执导的科幻灾难电影，于2023年上映，故事围绕《流浪地球》\\n前作展开，以计划建造1万座行星发动机的时代为背景\\n在不远的未来，太阳急速衰老膨胀，即将吞噬太阳系，地球面临灭顶之灾为应对危机，地\\n球各国成立联合政府，提出数百个自救计划，其中“移山计划”“方舟计划”“逐月计划”\\n和“数字生命计划”进入论证阶段'}),\n(6, 0.7484183311462402, {'index': 6, 'chunk': '\\n《流浪地球2》通过展现刘培强、图恒宇、周喆直等众多角色的经历，以及全球人类在末\\n日危机下的挣扎与抗争，呈现了一个宏大而震撼的科幻世界，探讨了人类面对绝境时的生存\\n选择、亲情、责任与勇气等主题，传达出“人类命运共同体”理念和“没有人的文明，\\n毫无意义”的深刻内涵，以其壮观的视效和动人的情节，成为中国科幻电影的重要代表作'})]\n```\n\n那么你已经完成了检索部分的任务，接下来，我们将讲述`增强和生成`部分的任务。",
    "176": "一级标题：第七章 RAG检索\n二级标题：提示词增强\n内容：\n这一部分简单来说是通过构建提示词将大模型指令与检索的信息结合用于后续大模型生成任务实现，具体而言，通过调整提示词结构，明确要求模型优先基于检索到的向量数据库内容生成回答，限定对外部知识的依赖范围；同时，在提示词中加入逻辑引导（如 “对比上下文信息”“引用具体内容”），促使模型更精准地关联检索结果与问题，减少脱离知识库的 “幻觉” 输出。\n\n此外，提示词可规范回答格式（如标注信息来源），强化模型对检索内容的利用效率，从而提升回答的准确性与可靠性。\n\n比如，我们将检索的信息和提示词指令结合：\n\n*提示词* ：\n\n```Python\n### 6、提示词增强\nquery=query\ncontext = \"\\n\".join([f\"{i+1}. {result[2]['chunk']}\" for i, result in enumerate(results)])\n\nprompt=f\"\"\"\n你是一个非常专业的AI智能助手，请你根据下面的“问题”，结合给出的“文本”内容，生成合理的回答，如果文本中没有相关内容，请回答“无法回答”。\n问题: {query}\n文本: {context}\n回答:\n\"\"\"\n\nprint(prompt.format(query=query, context=context))\n```\n\n*回答* ：\n\n```Plain\n你是一个非常专业的AI智能助手，请你根据下面的“问题”，结合给出的“文本”内容，生成合理的回答，如果文本中没有相关内容，请回答“无法回答”。\n问题: 《流浪地球2》是由谁导演的？\n文本: 1. 《流浪地球2》是由郭帆执导的科幻灾难电影，于2023年上映，故事围绕《流浪地球》\n前作展开，以计划建造1万座行星发动机的时代为背景\n在不远的未来，太阳急速衰老膨胀，即将吞噬太阳系，地球面临灭顶之灾为应对危机，地\n球各国成立联合政府，提出数百个自救计划，其中“移山计划”“方舟计划”“逐月计划”\n和“数字生命计划”进入论证阶段\n2.\n《流浪地球2》通过展现刘培强、图恒宇、周喆直等众多角色的经历，以及全球人类在末\n日危机下的挣扎与抗争，呈现了一个宏大而震撼的科幻世界，探讨了人类面对绝境时的生存\n选择、亲情、责任与勇气等主题，传达出“人类命运共同体”理念和“没有人的文明，\n毫无意义”的深刻内涵，以其壮观的视效和动人的情节，成为中国科幻电影的重要代表作\n回答:\n```\n\n规范了提示词后，相信大模型生成的结果会更加准确。",
    "177": "一级标题：第七章 RAG检索\n二级标题：大模型生成\n内容：\n这部分内容呢就是老生常谈的推理部分了，有了提示词后，直接让模型生成对应的结果，如果问的问题和文档切片不相关，大模型就会明确说“无法回答”，能有效避免大模型幻觉，具体推理代码在前面的章节中有提到，我们使用的是huggingface的对应推理代码，这里我们只展示提示词部分内容：\n\n*提示词* ：\n\n```Plain\n你是一个非常专业的AI智能助手，请你根据下面的“问题”，结合给出的“文本”内容，生成合理的回答，如果文本中没有相关内容，请回答“无法回答”。\n问题: 《流浪地球2》是由谁导演的？\n文本: 1. 《流浪地球2》是由郭帆执导的科幻灾难电影，于2023年上映，故事围绕《流浪地球》\n前作展开，以计划建造1万座行星发动机的时代为背景\n在不远的未来，太阳急速衰老膨胀，即将吞噬太阳系，地球面临灭顶之灾为应对危机，地\n球各国成立联合政府，提出数百个自救计划，其中“移山计划”“方舟计划”“逐月计划”\n和“数字生命计划”进入论证阶段\n2.\n《流浪地球2》通过展现刘培强、图恒宇、周喆直等众多角色的经历，以及全球人类在末\n日危机下的挣扎与抗争，呈现了一个宏大而震撼的科幻世界，探讨了人类面对绝境时的生存\n选择、亲情、责任与勇气等主题，传达出“人类命运共同体”理念和“没有人的文明，\n毫无意义”的深刻内涵，以其壮观的视效和动人的情节，成为中国科幻电影的重要代表作\n回答:\n```\n\n*回答* ：\n\n```Plain\n《流浪地球2》是由郭帆执导的。\n```\n\n如果问的问题和文档无关呢？\n\n*提示词* ：\n\n```Plain\n你是一个非常专业的AI智能助手，请你根据下面的“问题”，结合给出的“文本”内容，生成合理的回答，如果文本中没有相关内容，请回答“无法回答”。\n问题: 《黑客帝国》是由谁导演的？\n文本: 1. 《流浪地球2》是由郭帆执导的科幻灾难电影，于2023年上映，故事围绕《流浪地球》\n前作展开，以计划建造1万座行星发动机的时代为背景\n在不远的未来，太阳急速衰老膨胀，即将吞噬太阳系，地球面临灭顶之灾为应对危机，地\n球各国成立联合政府，提出数百个自救计划，其中“移山计划”“方舟计划”“逐月计划”\n和“数字生命计划”进入论证阶段\n2.\n另一边，量子科学家图恒宇（刘德华饰）则与“数字生命计划”紧密相关他致力于将女\n儿图丫丫的意识数字化，即使该计划被禁止，他也未放弃在一系列意外后，图恒宇自己也\n意外进入数字世界，以一种特殊的方式继续参与到拯救地球的行动中，他在数字空间中的经\n历，也为影片增添了一层神秘的“元宇宙”色彩\n回答:\n```\n\n*回答* ：\n\n```Plain\n无法回答。根据提供的文本内容，没有提到《黑客帝国》这部电影及其导演信息。文本主要描述了《流浪地球2》的相关信息以及一个科幻灾难电影的故事梗概。\n```\n\n因为在检索向量部分，我们并没有限制检索分数，因此会将排名前两个的文档切片全部作为提示词部分，虽然和《黑客帝国》没有任何关系，而为了回复效率，我们当然可以给检索限制一个分数，比如0.5以上才能作为输出，这样一旦出现差异过大的，大模型就能直接“无法回答”，无需再分析文本。",
    "178": "一级标题：8.1 函数调用实践\n二级标题：无\n内容：\n大模型的函数调用（Function calling），其实就是工具调用，在整个`agent`的结构中，工具的调用是很重要的一环，在感知环境和执行大模型给出的决策时都充当了重要的角色，下面我们利用天气查询的函数调用来举例说明。\n\n在函数调用中，我们使用大模型目的是为了让它判断出用户当前的问题是否触及了“查询天气”、“查询时间”等情况，最终输出的是调用的函数名字，通过对应函数中搜索天气的举动，得到准确的天气数据，由于这些数据多而繁杂，可以将其交给大模型加工处理，最终就能输出成我们需要的对话。\n\n<div style=\"display:flex;justify-content:center;\">\n  <figure style=\"text-align:center;margin:0;\">\n    <img src=\"./picture/function_calling_theory.png\" style=\"width:700px;\" alt=\"函数调用原理\">\n    <figcaption>函数调用原理</figcaption>\n  </figure>\n</div>\n\n\n那么首先我们来设计工具函数，这里仅举例，因此没有调用天气API，如果想深入了解如何操作，可以查看[阿里云平台](https://help.aliyun.com/zh/model-studio/qwen-function-calling)并且查看如何利用API调用天气数据。\n\n1. 定义工具函数\n\n*代码* ：\n\n```Python\n# 模拟查询天气，不用API调用，直接输出{地点}今天是{天气}\ndef get_current_weather(arguments):\n    # 定义备选天气选项（固定好的）\n    weather_conditions=[\"晴天\", \"阴天\", \"小雨\", \"大雨\", \"雪天\", \"多云\"]\n    # 随机选择一个天气\n    weather = random.choice(weather_conditions)\n    # 获取地点信息\n    location = arguments[\"location\"]\n    # 输出内容\n    return f\"{location}今天是{weather}\"\n\n# 模拟查询当前时间工具，不使用API调用，直接输出现在是{时间}\ndef get_current_time(arguments):\n    # 获取当前时间\n    now = datetime.now()\n    # 格式化时间为字符串\n    current_time = now.strftime(\"%Y年%m月%d日 %H:%M:%S\")\n    # 输出内容\n    return f\"现在是{current_time}\"\n```\n\n2. 创建tools数组\n\n人类在选择工具之前，需要对工具有全面的了解，包括工具的功能、何时使用以及输入参数等。大模型也需要这些信息才能更准确地选择工具。通常来说，对于现在的大模型都会有对应的对话模板，其中会添加Function Calling的模板转换，那么我们只需要按照json模板要求的格式对输入参数处理即可，具体的json模板可以在[Qwen官网](https://qwen.readthedocs.io/zh-cn/latest/framework/function_call.html)找到，其他的模型可能会有不同，需要查找对应的官网文档教程来设计，这里我们仅看Qwen的json格式模板：\n\n```JSON\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_current_weather\",\n        \"description\": \"当你想查询指定城市的天气时非常有用。\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"城市或县区，比如北京市、杭州市、余杭区等。\",\n                }\n            },\n            \"required\": [\"location\"]\n        }\n    }\n}\n```\n\n其中：\n\n- `type`字段固定为`\"function\"`；\n- `function`字段为 Object 类型；\n  - `name`字段为自定义的工具函数名称，建议使用与函数相同的名称，如`get_current_weather`或`get_current_time`；\n  - `description`字段是对工具函数功能的描述，大模型会参考该字段来选择是否使用该工具函数。\n  - **`parameters`**字段是对工具函数入参的描述，类型是 Object ，大模型会参考该字段来进行入参的提取。如果工具函数不需要输入参数，则无需指定`parameters`参数。\n    - `type`字段固定为`\"object\"`；\n    - `properties`字段描述了入参的名称、数据类型与描述，为 Object 类型，Key 值为入参的名称，Value 值为入参的数据类型与描述；\n    - `required`字段指定哪些参数为必填项，为 Array 类型。\n\n**`parameters`**是我们需要的主要的参数设置，比如天气就需要“location”，股票需要“company”，新闻需要“name”、“time”、“matter”等等，名称随你设置，但是一般都需要`type`，并且你需要设置下哪个参数是必须要大模型提供的，也就是`required`。\n\n3. 融入对话模板\n\n尽管在[创建 tools 数组](https://help.aliyun.com/zh/model-studio/qwen-function-calling#b7c8a0e72a9d0)时已经对工具的作用与何时使用工具进行了描述，但在 System Message 中强调何时调用工具通常会提高工具调用的准确率。也就是说我们需要对message中的system进行修改：\n\n*提示词* ：\n\n```Plain\n你是一个很有帮助的助手。如果用户提问关于天气的问题，请调用 ‘get_current_weather’ 函数;\n如果用户提问关于时间的问题，请调用‘get_current_time’函数。\n请以友好的语气回答问题。\n```\n\n然后我们从[apply_chat_template](https://github.com/huggingface/transformers/blob/main/src/transformers/tokenization_utils_base.py#L1522)中看到，tools需要列表格式，其中包含函数调用的json模板，需要多少函数就写多少即可，然后在模板中添加，我们可以看下输出的效果：\n\n*代码* ：\n\n```Python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    tools=tools,\n)\n```\n\n*输出* ：\n\n```Plain\n<|im_start|>system\n你是一个很有帮助的助手。如果用户提问关于天气的问题，请调用 ‘get_current_weather’ 函数;\n     如果用户提问关于时间的问题，请调用‘get_current_time’函数。\n     请以友好的语气回答问题。\n\n# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>\n{\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\", \"description\": \"当你想查询指定城市的天气时非常有用。\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"城市或县区，比如北京市、杭州市、余杭区等。\"}}, \"required\": [\"location\"]}}}\n{\"type\": \"function\", \"function\": {\"name\": \"get_current_time\", \"description\": \"当你想知道现在的时间时非常有用。\"}}\n</tools>\n\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\n</tool_call><|im_end|>\n<|im_start|>user\n你好，今天天气如何<|im_end|>\n<|im_start|>assistant\n```\n\n可以看到function calling部分在模板中体现，而一旦我们问到天气、时间相关的问题的时候，就能触发相应的函数，最终模型输出的是调用的是具体哪一个函数名称，当然如果我们询问触及多个函数，也会返回多个函数，那么话不多说，我们下面看下实际效果：\n\n4. 运行\n\n*提示词* ：\n\n```Plain\n北京现在天气如何？\n```\n\n*输出* ：\n\n```Plain\n<tool_call>\n{\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"北京市\"}}\n</tool_call>\n```\n\n如果触及多个函数呢？\n\n*提示词* ：\n\n```Plain\n你好，现在几点了，北京天气如何\n```\n\n*回答* ：\n\n```Plain\n<tool_call>\n{\"name\": \"get_current_time\", \"arguments\": {}}\n</tool_call>\n<tool_call>\n{\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"北京市\"}}\n</tool_call>\n```\n\n当然，模型给出的仅是需要调用的函数名称，我们还需要调用对应的函数运行，比如天气、时间，会给出具体的数据列表，而这往往不好直接输出，这时可以将给出的数据交给大模型进行进一步的处理：\n\n*提示词* ：\n\n```Plain\n用户提问的内容是你好，现在几点了，北京天气如何\n模型给出的回答是[{'name': 'get_current_time', 'arguments': {}, 'output': '现在是2025年07月17日 16:25:05'}, {'name': 'get_current_weather', 'arguments': {'location': '北京市'}, 'output': '北京市今天是晴天'}]\n请你根据用户提问的内容和模型给出的回答，组合成合适的回答用来回答用户的问题。\n```\n\n*回答* ：\n\n```Plain\n您好！现在是2025年07月17日 16:25:05。同时，根据最新的天气信息，北京市今天是晴天。如果您还有其他问题需要了解的，欢迎随时询问！\n```\n\n那么很好！你已经成功完成了函数调用的整个流程，这其实是agent中工具起到的作用，贯穿了agent的整个流程。\n\n下面我们提供了几个案例，有兴趣的小伙伴可以尝试实现，需要注意的是对于实时更新的一些资料，使用函数调用会有很好的效果，因为可以有效的避免大模型幻觉。\n\n那么什么时候需要函数调用，什么时候没必要用函数调用，我们用一句话来理解，“能用规则 100% 搞定的，就别劳驾大模型；凡是输入语义多变、输出结构复杂、需要动态组合外部能力的场景，就上函数调用。”",
    "179": "一级标题：8.2 MCP实践\n二级标题：无\n内容：\nMCP全名是Model Context Protocol，也就是模型上下文协议，2024年11月底，由 Anthropic 推出的一种开放标准，旨在统一大模型与外部数据源和工具之间的通信协议。MCP 的主要目的在于解决当前 AI 模型因数据孤岛限制而无法充分发挥潜力的难题，MCP 使得 AI 应用能够安全地访问和操作本地及远程数据，为 AI 应用提供了连接万物的接口。\n\n在大模型领域里，简单说就是能让多个模型像搭积木一样配合工作的平台。它能把不同功能的模型（比如擅长聊天的、分析图片的）整合到一起，让它们分工合作解决复杂问题。比如用户问一个需要文字生成又要图片分析的问题时，MCP 会自动调合适的模型来处理，不用人工一个个操作。这样做的好处很明显，既能发挥每个模型的长处，又能快速应对多样需求，还能节省重复开发的功夫，让大模型的应用更灵活、效率更高，普通用户不用懂技术也能用好各种模型能力。\n\n听上去MCP好像跟函数调用，也就是Function Calling也没什么区别啊，事实上MCP可以当作是函数调用的延伸，不过函数调用需要你多次配置大模型完成相应的步骤，但是MCP直接省掉这些步骤，比如查询一次车票，你只需要输入你需要查询的信息，这些信息可以隐藏在一句话里，经过MCP会直接输出结果，而不需要函数调用过程中繁琐的步骤，同时也可以调用多个MCP服务，只要输入检测出来对应的信息，经过处理后就能调用对应的MCP从而得到响应。\n\n我们看一个简单的例子，在这个例子中，我们使用[阿里云百炼平台中的MCP广场](https://bailian.console.aliyun.com/?tab=mcp#/mcp-market)，选择三个MCP服务，分别是当日油价、当日高铁车票、还有论文查询三个比较常用的功能，下面我们一步一步来实现：\n\n1. 开通MCP服务\n\n首先，我们想要`当日油价`、`当日高铁车票`、还有`论文查询`这三个功能的话，我们需要在MCP广场找到对应的服务并且开通。\n\n<img src=\"./picture/mcp_ground.png\" alt=\"阿里云百炼MCP广场\" style=\"zoom:80%;\" />\n\n<img src=\"./picture/mcp_examples.png\" alt=\"MCP使用例子\" style=\"zoom:80%;\" />\n\n有些服务在开通的时候需要输入敏感信息，这些需要创建 KMS 凭据加密这些信息，但是本例中选择的三个都不需要，可以直接开通。\n\n2. MCP服务添加到智能体中\n\n然后我们将开通好的MCP服务嵌入到我们准备的智能体中，在[百炼应用管理](https://bailian.console.aliyun.com/?tab=app#/app-center)中找到智能体应用功能，创建一个新的智能体应用：\n\n<img src=\"./picture/mcp_test.png\" style=\"zoom:80%;\" />\n\n在智能体配置中，我们可以设置我们的提示词指令，让模型能够更加精准的从我们的话中判断对应的信息：\n\n<img src=\"./picture/prompt_mcp_example.png\" alt=\"智能体提示词设计\" style=\"zoom:80%;\" />\n\n*提示词* ：\n\n```Plain\n# 角色\n你是一位优秀的人工智能助手，擅长处理和回答各种问题，并能够通过查询MCP（Memory and Context Processing）来获取相关信息。",
    "180": "一级标题：8.2 MCP实践\n二级标题：技能\n内容：",
    "181": "一级标题：8.2 MCP实践\n二级标题：技能 1: 回答用户问题\n内容：\n- **任务**：根据用户提出的问题，提供准确且详细的答案。\n  - 如果问题可以直接回答，直接给出答案。\n  - 如果问题需要查询MCP中的信息，请先进行查询，然后将查询到的信息进行加工处理后回答用户。",
    "182": "一级标题：8.2 MCP实践\n二级标题：技能 2: 查询MCP\n内容：\n- **任务**：当用户的问题需要更多信息支持时，调用MCP进行查询。\n  - 根据用户的问题，确定需要查询的具体内容。\n  - 使用MCP工具获取相关信息。\n  - 对查询到的信息进行加工处理，确保答案准确、简洁且易于理解。",
    "183": "一级标题：8.2 MCP实践\n二级标题：技能 3: 信息加工处理\n内容：\n- **任务**：对从MCP中查询到的信息进行加工处理，以便更好地回答用户问题。\n  - 摘取关键信息，去除冗余部分。\n  - 将信息整理成条理清晰的答案。\n  - 确保答案的准确性和相关性。",
    "184": "一级标题：8.2 MCP实践\n二级标题：限制\n内容：\n- 只回答与用户问题相关的内容。\n- 在使用MCP查询信息时，确保只查询与用户问题相关的信息。\n- 始终保持答案的准确性和简洁性，避免提供过多无关信息。\n- 在加工处理信息时，确保不改变信息的原意，保持信息的准确性。\n```\n\n智能体在回答时可以调用多个 MCP 服务，你可以根据自身需要，同时添加至多 20个 **MCP 服务**。\n\n<img src=\"./picture/multi_mcp_examples.png\" alt=\"多个MCP实践例子\" style=\"zoom:80%;\" />\n\n上面我们开通了三个服务，所以直接从`已开通`的服务找起。\n\n3. 使用添加好MCP服务的智能体进行对话\n\n然后我们就可以试试看我们的提问是否都用上了MCP了：\n\n<img src=\"./picture/mcp_single_example.png\" alt=\"油价查询MCP实践例子\" style=\"zoom:80%;\" />\n\n如果是正常对话，不涉及MCP的场景，模型会正常输出，但是一旦涉及模型无法回答的内容，而刚好我们嵌入的MCP服务能够帮助提供信息，模型就会调用相应的服务，并且将回答整理、精炼，给出我们想要的答案。\n\n<img src=\"./picture/mcp_api_port.png\" alt=\"MCP的API接口\" style=\"zoom:80%;\" />\n\n右图是油价MCP的调用情况，需要注意，每一个MCP都有对应的输入和输出接口，我们从左图中看到，查询油价只需要一个输入参数，也就是全国31省份的名称，大模型会从我们的话中提取省份名称信息，然后给出对应的答案。这个例子比较简单，我们看下稍微复杂一点的。\n\n<img src=\"./picture/mcp_process.png\" alt=\"MCP的调用流程\" style=\"zoom:80%;\" />\n\n*提示词* ：\n\n```Plain\n你能告诉我明天北京到上海的高铁动车都有哪些吗？需要出发时间从早到晚排列，只告诉我早上9点到15点之间的列车信息\n```\n\n*回答* ：\n\n```Plain\n根据查询结果，明天（2025年7月19日）从北京到上海的高铁和动车信息如下，仅展示早上9点到15点之间的列车：\nG3（实际车次：24000000G30B）\n出发站：北京（BJP）\n到达站：上海（SHH）\n出发时间：07:40\n到达时间：12:32\n历时：04:52\n二等座：有票，673元\n商务座：剩余2张票，2350元\nG3（实际车次：24000000G30B）\n出发站：北京南（VNP）\n到达站：上海（SHH）\n出发时间：08:00\n到达时间：12:32\n历时：04:32\n二等座：无票\n商务座：剩余2张票，2337元\nG111（实际车次：240000G1111I）\n出发站：北京南（VNP）\n到达站：上海虹桥（AOH）\n出发时间：08:16\n到达时间：14:11\n历时：05:55\n一等座：有票，1060元\n二等座：有票，662元\n商务座：剩余10张票，2318元\nG113（实际车次：240000G1131E）\n出发站：北京南（VNP）\n到达站：上海虹桥（AOH）\n出发时间：08:39\n到达时间：15:01\n历时：06:22\n一等座：有票，1028元\n二等座：有票，644元\n商务座：剩余8张票，2314元\n以上是符合您要求的时间段内的高铁和动车信息。如果您需要更详细的座位信息或购票帮助，请告诉我！\n```\n\n当我们查询车票的时候，会调用多个接口，这些接口都是12306车票查询中的接口，我们找到MCP广场中12306车票查询的服务工具：\n\n<img src=\"./picture/specific_usage.png\" alt=\"MCP对话使用\" style=\"zoom:80%;\" />\n\n我们使用到的三个接口在右图中，分别是显示时间、定位每个城市到底有哪些车站、根据车站名称搜索对应时间的车票信息，不过我们在实际使用的时候不需要经过这么多步的输入，根据信息的传递，我们发现其实该MCP输入接口只需要时间、你的出发地点、你的终点站是哪里就可以了，这些信息需要在你的问话中出现，而且对于城市名称，最好不要是“北京站”、“成都站”这种，直接写“北京”、“成都”这种城市名称即可，否则大模型可能会识别错误。\n\n4. API调用\n\n那么完成了上面说的步骤后，我们可以通过API渠道转换成python语言，在本地运行，我们找到[接口文档](https://bailian.console.aliyun.com/?tab=api#/api/?type=app&url=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F2846133.html)\n\n<img src=\"./picture/mcp_api_usage.png\" alt=\"MCP的API调用\" style=\"zoom:80%;\" />\n\n按照文档中提供的信息，我们就实现在本地运行该agent，并且也可以调用MCP服务了。\n\n*代码* ：\n\n```Python\nimport os\nfrom http import HTTPStatus\nfrom dashscope import Application\n\nresponse = Application.call(\n    # 若没有配置环境变量，可用百炼API Key将下行替换为：api_key=\"sk-xxx\"。但不建议在生产环境中直接将API Key硬编码到代码中，以减少API Key泄露风险。\n    api_key=\"sk-\",\n    app_id='9f0',# 替换为实际的应用 ID\n    prompt='请告诉我今天北京的油价是多少？')\n\nif response.status_code != HTTPStatus.OK:\n    print(f'request_id={response.request_id}')\n    print(f'code={response.status_code}')\n    print(f'message={response.message}')\n    print(f'请参考文档：https://help.aliyun.com/zh/model-studio/developer-reference/error-code')\nelse:\n    print(response)\n```\n\n*输出* ：\n\n```Plain\n{\"status_code\": 200, \"request_id\": \"……\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": \"根据查询结果，今天（2025年7月18日）北京市的油价如下：\\n\\n- 92号汽油：6.79元/升  \\n- 95号汽油：7.73元/升  \\n- 98号汽油：9.23元/升  \\n- 0号柴油：6.95元/升  \\n\\n以上价格仅供参考，实际以加油站为准。\", \"finish_reason\": \"stop\", \"session_id\": \"……\", \"thoughts\": null, \"doc_references\": null}, \"usage\": {\"models\": [{\"model_id\": \"qwen3-8b\", \"input_tokens\": 6771, \"output_tokens\": 115}]}}\n```\n\n`text`中的内容就是我们需要的结果。\n\n还有一个关于论文的查找，相对来说就比较简单，该服务既可以批量查找论文，如下图左图所示，也可以根据你提供的论文ID查找对应的下载链接，如下图右图所示。\n\n<img src=\"./picture/arxiv_search.png\" alt=\"论文搜索MCP使用\" style=\"zoom:80%;\" />\n\n在对话中如果我要找一个论文的下载地址，不用再去费力搜索，直接提问大模型，它就会给你答案：\n\n<img src=\"./picture/arxiv_search_reasult.png\" alt=\"论文搜索MCP使用\" style=\"zoom:80%;\" />",
    "185": "一级标题：8.3 多Agents简介\n二级标题：无\n内容：\n在大模型领域，多 agents 指的是多个具备独立能力的 “智能体” 协同工作的系统。每个 agent 就像一个专业助手，有自己的技能范围，它们通过沟通协作完成复杂任务，而不是单个模型单打独斗。\n\n这类系统的应用场景很广，比如复杂数据分析时，数据收集 agent 负责找资料，分析 agent 处理数据，总结 agent 生成报告；或者在电商运营中，选品 agent 推荐商品，客服 agent 处理咨询，营销 agent 制定推广方案，形成完整工作流。\n\n<div style=\"display:flex;justify-content:center;\">\n  <figure style=\"text-align:center;margin:0;\">\n    <img src=\"./picture/multi_agents.png\" style=\"width:800px;\" alt=\"多agents原理\">\n    <figcaption>多agents原理</figcaption>\n  </figure>\n</div>\n\n\n\n该图出自[Agents: An Open-source Framework for Autonomous Language Agents](https://arxiv.org/pdf/2309.07870v3)，图中SOP（标准作业流程）是多 agents 运行的关键。它就像提前设定好的 “工作手册”，明确每个 agent 的职责、操作步骤和协作规则。比如客户投诉处理的 SOP 会规定：先由接待 agent 记录问题，再转交对应领域的专业 agent 解决，最后由质检 agent 确认结果，确保流程规范高效，避免混乱。\n\n多 agents 与提示词的关系紧密又有区别。提示词是用户对单个模型的指令，而多 agents 系统中，提示词不仅有用户给的初始需求，还包括 agents 之间的 “内部指令”。用户只需给出最终目标提示，系统会自动生成每个 agent 的任务提示词。比如用户说 “写一份市场分析”，系统会拆解出给数据 agent 的 “收集行业数据” 提示、给分析 agent 的 “提炼趋势” 提示等，让提示词更精准地适配不同分工，既保留了提示词的引导作用，又通过多 agent 协作放大了其效果，让复杂任务处理更流畅。",
    "186": "一级标题：第八章 Agent实践\n二级标题：无\n内容：\n在前面的例子中，我们学习了如何设计提示词完成各类任务，也包括简单的RAG检索，但是每一类任务都是基于我们与大模型的一问一答的互动，我们无形中把这种对话当成和“人”的对话，这样其实是限制了大模型能够做到的事情，毕竟“人”除了对话，也能决策、动手、反思等等，事实上大模型也能做到这些，而这就涉及到了另外一个专业词汇`agent`。\n\n什么是`agent`？或者说什么是大模型`agent`？\n\n大模型`agent`，作为一种人工智能体，是具备环境感知能力、自主理解、决策制定及执行行动能力的智能实体。简而言之，它是构建于大模型之上的计算机程序，能够模拟独立思考过程，灵活调用各类工具，逐步达成预设目标的智能存在。通俗点来说，大模型充当了大脑的部分，可以决策、感知、反思，但是真正的行动需要各类工具的辅助来达成各类目标。**将这些功能全部组合起来，以大模型为核心，就构成了agent。**\n\n我们可以查看 OpenAI 研究员 Lilian Weng的个人博客文章[LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/)，这篇博客不仅形象又具体的讲述了`agent`的结构，也对相关的研究做了总结，这里我们简单的分解下`agent`。\n\n<div style=\"display:flex;justify-content:center;\">\n  <figure style=\"text-align:center;margin:0;\">\n    <img src=\"./picture/agent_theory.png\" style=\"width:700px;\" alt=\"agent原理图\">\n    <figcaption>agent原理图</figcaption>\n  </figure>\n</div>\n\n\n\n如上图所示，Agent 共由4个关键部分组成：规划（Planning）、记忆（Memory）、工具（Tools）、行动（Action），下面详细剖析。\n\n1. 规划（Planning）\n\n\"规划\"是agent的思维模型。类比人类，面对任务，我们先构思解决方案，拆解为子任务，评估工具，执行中反思调整，并考量终止时机。通过提示词工程，比如：ReAct、CoT 推理模式，可赋予agent类似思维模式，精准拆解复杂任务，依次经过大模型决策规划，从而分布解决问题，实现更准确的规划。\n\n2. 记忆（Memory）\n\n记忆，即信息的存储与回忆。agent可以模拟人类，人类对于记忆的保存有长期记忆，也有短期记忆，长期记忆通常来说规模较大，而且更多需要后期多次调用，比如用户信息、业务数据等，因此可以保存在数据库或者知识库中；短期记忆通常涉及临时对话信息，用于大模型上下文对话中，如果任务终止，那么短期记忆就会清空。\n\n3. 工具（Tools）\n\nagent需要依托工具来感知环境、执行决策等。对于感知环境，比如通常大模型的内部训练好的信息相比于时间和新闻的更新迭代差了很长时间的，最新的一些新闻、论文资料大模型是不清楚的，因此需要通过工具调用知识库或者链接等读取信息，才能做出回答，类似于RAG；执行决策就很好理解了，如果你需要大模型帮你定一张周六的餐厅的订单，在大模型检索好信息后，订票的操作需要调用链接完成，这一步就不是大模型能做的，而是要工具实现。\n\n4. 行动（Action）\n\nagent经过大模型的规划、记忆的读取、工具的调用，最终是要实现和用户的互动，完成用户的目标，也就是实现输入到输出的转化。比如：智能客服回复、查询天气预报、AI 机器人抓起物体等等。\n\n经过对`agent`结构的拆解，我们知道构建一个agent需要做些什么，其中每一部分都需要利用提示词划定任务细节，从而更好的辅助大模型实现每一部分的细节。那么接下来，我们举一个简单的例子构建一个agent。\n\n\n本节我们将会带大家实现函数调用、MCP的简单应用，并且讲述多Agents原理，下面是我们每一个章节的链接👇：\n\n| 教程章节   | 状态 |\n|:--------|:------|\n| [8.1.函数调用实战](./1.function_calling.md)   |  ✅  |\n| [8.2.MCP应用实战](./2.mcp_usage.md)   | ✅  |\n| [8.3.多Agents原理](./3.multi_agents.md)   | ✅   |",
    "187": "一级标题：懒人专属：拖入文档=拥有AI文档助手？RAG怎么做？SwanLab文档助手方案开源了！（附详细代码）\n二级标题：无\n内容：\n**作者**：李晨 、王超\n\n**机构**：新疆大学丝路多语言认知计算国际合作联合实验室研究生、情感机器实习研究员\n\n**联系邮箱**：lichen@stu.xju.edu.cn、wangchao@stu.xju.edu.cn\n\n---\n\n**\"随着官方文档API的爆炸式增长，用户阅读较为困难，——今天，我们直接用SwanLab+开箱即用的RAG框架**，带你30分钟搭建一个**服务级文档助手**（附完整源码和在线Demo，无需复杂配置，10分钟即可本地跑通！）\n\nAI文档助手在线体验链接：[https://chat.swanlab.cn/](https://chat.swanlab.cn/)\n\n方案开源Github仓库链接：https://github.com/EmotionMachine/swanlab-rag\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./picture/rag-demo.gif\" alt=\"rag-demo\"  width=\"800\" />\n  <figcaption>Swanlab官方文档Demo演示</figcaption>\n  </figure>\n</div>",
    "188": "一级标题：懒人专属：拖入文档=拥有AI文档助手？RAG怎么做？SwanLab文档助手方案开源了！（附详细代码）\n二级标题：摘要\n内容：\n在信息爆炸的时代，开发者需要快速从技术文档中获取准确信息。SwanLab官方文档内容丰富，但手动查找耗时且效率低下。**为了让开发者能够更快速、更精准地从SwanLab官方文档中获取信息**，我们开发了一款AI文档助手，基于**检索增强生成（RAG）架构**。\n\n本文介绍了Swanlab官方文档助手的详细实现过程，不依赖任何现有Agent框架，妥妥“**纯手工打造**”。本文将会从数据准备、文档检索、模型生成三个方面去介绍。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./picture/rag-structure.png\" alt=\"image-20250813112441893\" width=\"800\" />\n  <figcaption>Swanlab文档助手技术架构简介</figcaption>\n  </figure>\n</div>",
    "189": "一级标题：懒人专属：拖入文档=拥有AI文档助手？RAG怎么做？SwanLab文档助手方案开源了！（附详细代码）\n二级标题：目录\n内容：\n[[toc]]",
    "190": "一级标题：懒人专属：拖入文档=拥有AI文档助手？RAG怎么做？SwanLab文档助手方案开源了！（附详细代码）\n二级标题：数据准备\n内容：\n本节主要介绍**向量数据库**的详细构建方案。\n\n### 文档检索\n\n AI助手的知识基础来源于最新、最全的官方文档。首先需要让 AI 拥有“阅读”所有 SwanLab 官方文档的能力。**GitHub作为开发者最熟悉的平台，降低学习成本**。笔者采用**GitHub API作为获取文档的主要技术，技术可信度为一方面，更重要的是确保获取到的资料具有实时性与完整性。**\n\n实现过程**使用Python编写了一个网络爬虫脚本，通过调用 GitHub API，自动扫描 SwanLab 文档仓库，获取所有 Markdown 文件的元数据**，如标题、下载链接和对应的官网页面地址保存到`JSON`文件中。工作流程如下所示：\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./picture/rag-github_api.png\" alt=\"image-20250724121726401\"  width=\"800\" />\n  <figcaption>GitHub API 调用流程</figcaption>\n  </figure>\n</div>\n\n\n`swanlab.json`包含文件名、文档地址、网页地址和文档主题，具体如下所示：\n\n```json\n{\n      \"theme\": \"cli-swanlab-convert\",\n      \"url\": \"https://raw.githubusercontent.com/SwanHubX/SwanLab-Docs/main/zh/api/cli-swanlab-convert.md\",\n      \"html_url\": \"https://docs.swanlab.cn/api/cli-swanlab-convert.html\",\n      \"title\": \"swanlab convert\"\n }\n```\n\n### Github API及其使用方法\n\nGitHub API 是一种用于与 GitHub 平台交互的编程接口，允许开发者通过编写代码来访问 GitHub 上的资源和数据。https://api.github.com/repos/ 是其中一个重要的端点，用户可以对 GitHub 上的代码仓库资源进行获取。\n\nAPI 的核心功能是为用户提供一种以编程方式访问和操作 GitHub 平台数据的方法，而不是通过用户界面手动操作。通过 API工具，开发者可以获取仓库的详细信息，创建新仓库，修改现有仓库的设置，甚至删除仓库。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./picture/rag-git.png\" alt=\"image-20250724121726401\"  width=\"800\" />\n  <figcaption>自动化Git仓库</figcaption>\n  </figure>\n</div>\n\n在代码中Github API的具体使用方法：\n\n```python\n#scrape_swanlab_docs_Internet.py\n\n#认证与请求头，通过 Authorization: token {token} 请求头进行认证，提高 API 速率限制（未认证用户每小时 60 次请求，认证用户每小时 5000 次）。\ngithub_token = \"\"  # 填写为你的GitHub Token\nheaders['Authorization'] = f'token {github_token}'\n# 使用requests 库配置重试机制\nsession = requests.Session()\nresponse = session.get(\"https://api.github.com/rate_limit\", headers=headers, timeout=60)\nresponse.raise_for_status()\ncontents = response.json()\n\n\"\"\"\n检索\nArgs:\n   repo_url (str): GitHub仓库目录URL（例如 'https://github.com/SwanHubX/SwanLab-Docs/tree/main/zh'）\n   base_html_url (str): SwanLab文档网站根URL（例如 'https://docs.swanlab.cn'）\n\"\"\"\nfor item in contents:\n    if item['type'] == 'file' and item['name'].endswith('.md'):\n        theme = item['name'].replace('.md', '')\n        md_url = item['download_url']\n        relative_path = md_url.split('/main/zh/')[-1]\n        html_filename = filename_map.get(theme, theme)\n        html_path = relative_path.replace(theme + '.md', html_filename + '.html')\n        html_url = f\"{base_html_url}/{html_path}\"\n        title = get_markdown_title(md_url, headers, session)\n\n        docs.append({\n            \"theme\": theme,\n            \"url\": md_url,\n            \"html_url\": html_url,\n            \"title\": title\n            })\n            logger.info(\n                f\"{'  ' * depth}Added file: {theme}, Markdown: {md_url}, HTML: {html_url}, Title: {title}\")\n    elif item['type'] == 'dir':\n           logger.info(f\"{'  ' * depth}Entering directory: {item['path']}\")\n            scan_directory(item['url'], docs, depth + 1)\n```\n\n### 文档解析与分块合并\n\n 在上一步我们得到了所有文档的“地址列表”（`JSON` 文件）。接下来，我们需要“按图索骥”，访问每一个链接，异步地抓取所有 Markdown 的原始内容，使用LLM对每个文档根据内容进行文档解析与分块处理，采取其重要部分用于构建知识库。\n\n首先笔者先介绍一下分块策略毕竟笔者采用的具体分块策略，为什么需要分快？分块涉及将文本划分为可管理的单元或“块”，以实现高效处理。这种分割对于语义搜索、信息检索和生成式 AI 应用等任务都至关重要。每个块都保留上下文和语义完整性，以确保结果连贯。分块策略在检索增强生成（RAG）方法中起着至关重要的作用，它使文档能够被划分为可管理的部分，同时保持上下文。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./picture/rag-chunk.png\" alt=\"image-20250724173035496\" width=\"400\" />\n  <figcaption>Chunk策略示意图</figcaption>\n  </figure>\n</div>\n\n在笔者实际的测试中，直接对长文本进行向量化存在诸多问题，存在语义信息的稀释、计算资源的耗费以及检索效率的降低等。将长文本进行合理的分块处理，成为构建高效向量数据库的前提。在可以提高检索准确率同时，也确保召回的内容与用户的查询意图更加契合，进而提升大模型生成答案的质量。最终笔者采取的方案为根据文档的二级标题进行分块处理，在引入宝贵的结构元数据同时，也实现了分块粒度的平衡，保持了语义的完整性和专注性。当用户的查询与某一子主题高度相关时，模型能够精准召回包含该标题和对应内容的文本块，避免了不相关信息的干扰，也提升了答案的可信度和可用性。在笔者不断的摸索中，检索效率和生成效果之间取得了理想的平衡。\n\n将分块后的内容合并到一起，这样每个知识块都包含了相对完整的主题与内容。实现这个逻辑的代码也非常简单：\n\n```python\n# create_vector.py\ndef split_text(text: str, separator: str = \"",
    "191": "\") -> list[str]:\n    \"\"\"\n    分割文本：根据指定的分隔符分割文本。\n    \"\"\"\n    chunks = text.split(separator)\n    # 过滤掉可能存在的空字符串\n    return [chunk.strip() for chunk in chunks if chunk.strip()]\n```\n\n配置文件和分块提示词如`config.py`和`prompt.py`所示。在处理时，我们用一个特殊的分隔符 `",
    "192": "` 将文档内容按照二级标题的内容隔开，形成一个完整的Swanlab文档知识库。\n\n```yaml\n#config.yaml`\nllm:\n  api_type: \"openai\"  # 或 \"groq\" 等\n  model: \"Qwen/Qwen2.5-32B-Instruct\"  # 或 \"GPT-4o\"等\n  api_key: \"\"    #填写api_key\n  base_url: \"\"  # LLM服务URL\n#prompt.py\nllm = LLM()\nprompt = f\"\"\"\n分析以下 Markdown 内容，把内容进行分块整理。\n格式如下所示：\n一级标题：(文档标题)\n二级标题（文档二级标题）：内容",
    "193": "一级标题：(文档标题)\n二级标题（文档二级标题）：内容",
    "194": "......",
    "195": "参考内容：{content}\n\"\"\"\n```\n\n### 文档编码\n\n现在我们有了一堆文本块，但计算机不理解文字，只懂数字。我们需要一个“翻译官”，把每个文本块翻译成一个由数字组成的“向量”（Vector）。这个过程就是 **Embedding**。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./picture/rag-embedding.png\" alt=\"image-20250724173738310\" width=\"800\" />\n  <figcaption>生成词向量</figcaption>\n  </figure>\n</div>\n\n调用API提供的 `Embedding` 模型来完成这个任务。调用的过程也相对比较简单，提供API_KEY和Base_URL等关键参数即可，具体实现方法如下代码所示。对于这样的每一个文本块，获得了一个`Embedding`后的向量。\n\n```python\n# create_vector.py\ndef get_embeddings_from_api(texts: list[str], batch_size: int = 16) -> list[list[float]]:\n    all_embeddings = []\n    headers = {\n        \"Authorization\": f\"Bearer {API_KEY}\",\n        \"Content-Type\": \"application/json\"\n    }\n    url = f\"{BASE_URL.rstrip('/')}/embeddings\"\n\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i + batch_size]\n        payload = {\n            \"model\": EMBEDDING_MODEL,\n            \"input\": batch\n        }\n        # ... 发起请求并获取返回的向量 ...\n        response_data = response.json()\n        batch_embeddings = [item['embedding'] for item in response_data['data']]\n        all_embeddings.extend(batch_embeddings)\n\n    return all_embeddings\n```\n\n拿到所有向量后，如何管理这些向量？笔者选择 Facebook 开源的 `FAISS` 库来构建一个向量数据库，支持快速处理大规模数据，并且支持在高维空间中进行相似性搜索。可以把`FAISS` 想象成一个专门为向量设计的高效“图书馆”，能极快地找到与给定向量最相似的几个向量。\n\n至此，我们的“Swanlab官方文档向量知识库”已经完全准备就绪了！我们就可以进行文档检索了!",
    "196": "一级标题：懒人专属：拖入文档=拥有AI文档助手？RAG怎么做？SwanLab文档助手方案开源了！（附详细代码）\n二级标题：文档检索（RAG）\n内容：\n`RAG`（`Retrieval-Augmented Generation`，检索增强生成） 是一种结合了信息检索技术与语言生成模型的人工智能技术。该技术通过从外部知识库中检索相关信息，并将其作为提示（Prompt）输入给大型语言模型（LLMs），以增强模型处理知识密集型任务的能力，如问答、文本摘要、内容生成等。RAG模型由Facebook AI Research（FAIR）团队于2020年首次提出，并迅速成为大模型应用中的热门方案。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./picture/rag-rag.png\" alt=\"image-20250725151046751\"  width=\"800\" />\n  <figcaption>文档检索流程示意图</figcaption>\n  </figure>\n</div>\n\n数据准备是离线完成的，而问答检索则是用户与 AI 实时交互的过程。这个过程就像让 AI 进行一场“开卷考试”，我们先把相关的参考资料找出来，再让它根据这些官方资料来回答与解决用户问题。\n\n### 检索出相关文档\n\n当用户提出一个问题时，我们首先用和之前完全相同的 `Embedding` 模型，将用户的问题也转换成一个向量。我们拿着这个“问题向量”，采用混合检索（向量检索+关键词检索）的策略去 FAISS 向量数据库里进行搜索，命令它：“找出与我这个向量在空间距离上最接近文本块向量！\"。至于为什么采用这种方式，会在混合检索中进行详细讨论分析。再此之前笔者先带领读者先回顾一下向量检索和关键词检索的核心逻辑。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./picture/rag-faiss.png\" alt=\"image-20250724145848506\" width=\"800\" />\n  <figcaption>FAISS向量数据库</figcaption>\n  </figure>\n</div>\n\n#### 向量检索\n\n`Faiss`（Facebook AI Similarity Search）是由 Facebook AI 团队开源的高效相似性搜索和聚类库，专为稠密向量的快速检索设计。它支持十亿级别的向量搜索，广泛应用于推荐系统、图像检索和自然语言处理等领域。\n\nFaiss 提供多种算法来处理任意大小的向量集，支持高效的近似最近邻（ANN）搜索和聚类。其核心算法包括倒排索引（IVF）和乘积量化（PQ），并通过 GPU 加速进一步提升性能。最重要的原因，`Faiss` 提供了与 Python 的无缝接口，便于与 `Numpy` 集成的同时，也笔者在不使用任何架构使用纯`Python`提供了便捷。\n\n\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./picture/rag-faiss2.png\" alt=\"image-20250725172624004\" width=\"800\" />\n  <figcaption>FAISS</figcaption>\n  </figure>\n</div>\n\n向量检索能够跨越字面障碍，理解用户的真实意图。即使用户的提问方式和文档的写法完全不同，只要意思相近，也能够轻易找到。能帮助用户发现他们没直接提问、但高度相关的内容，提供更具探索性的结果。\n\n向量检索的实现如下所示：\n\n```python\n# chat_logic.py\n# 1. 将用户问题转换为向量\nquery_embedding = self._get_query_embedding(question)\nquery_vector = np.array([query_embedding]).astype('float32')\n\n# 2. 在FAISS中执行搜索，k=10表示寻找最相似的10个,可以动态调整\ndistances, indices = self.index.search(query_vector, k=10)\n\n# 3. 根据返回的索引号，找到原始的文本块\nretrieved_chunks = [self.index_to_chunk[str(i)] for i in indices[0]]\n```\n\n#### 关键词检索\n\n关键词检索（`Keyword Search`），也称为词法检索（`Lexical Search`），是一种基于精确词汇匹配的传统信息检索方法。\n\n其核心机制依赖于倒排索引结构（Inverted Index），该结构由词汇表（Lexicon）和对应的发布列表（Posting List）组成，通过两者的映射关系实现高效查找。在处理流程上，首先进行查询解析，通过词法分析提取查询项，并可选地应用停用词过滤等技术优化检索过程。这种检索方式的特点在于对查询关键词与文档词汇的精确匹配，具有计算效率高、实现简单等优势。\n\n```python\n#使用正则表达式（re模块）进行关键词检索，同样选择寻找最相似的10个文本快\ndef _keyword_search(self, query:str, k: int = 10):\n    keywords = [word for word in re.split(r'\\W+', query.lower()) if word]\n    if not keywords:\n        return []\n\n    all_chunks = list(self.index_to_chunk.values())\n    chunk_scores = {}\n\n    for chunk in all_chunks:\n        score = 0\n        chunk_lower = chunk.lower()\n        for keyword in keywords:\n            score += chunk_lower.count(keyword)\n\n        if score > 0:\n            chunk_scores[chunk] = score\n\n    sorted_chunks = sorted(chunk_scores.items(), key=lambda item: item[1], reverse=True)\n    return [chunk for chunk, score in sorted_chunks[:k]]\n```\n\n#### 混合检索\n\n笔者采用混合检索（向量检索+关键词检索），核心原因这两种检索方式优势互补，向量检索的“智能”和“深度”来理解用户意图，同时用关键词检索的“精准”和“可靠”来抓住关键细节。能够实现1+1 > 2的效果，亦是当今构建先进RAG系统的主流和与最佳实践方式。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./picture/rag-fusion_search.png\" alt=\"image-20250725180644828\"  width=\"800\" />\n  <figcaption>混合检索示意图</figcaption>\n  </figure>\n</div>\n\n采用混合检索技术的优势：\n\n- 确保高召回率 (`Recall`): 混合检索能最大程度地确保所有可能相关的文档都被“召回”。向量检索负责召回那些“意思对”的文档，而关键词检索则像一张安全网，确保那些包含“特定关键词”的文档不被遗漏。\n- 提升答案的精准度 (`Precision`): 通过向大模型提供一个既包含宏观概念理解、又包含微观关键细节的上下文，大模型能够基于更丰富、更无偏见的信息，生成更可靠、更精确的答案。\n- 应对复杂查询场景:\n  - 当用户提问模糊时：“我的实验结果好像不对劲”，向量检索能发挥巨大作用，找到关于“调试”、“数据异常”、“结果分析”等概念的文档。\n  - 当用户提问精确时：“`swanlab.log` 函数怎么用？”，关键词检索能确保包含 `swanlab.log` 的文档被精准找到，而向量检索则能补充一些关于“如何记录日志”、“最佳实践”等相关上下文。\n\n将两种检索出来的文本块进行去重处理：\n\n```python\nfor chunk in all_retrieved:\n    processed_chunk = chunk\n    # 检查chunk是否为列表，如果是，则将其合并为字符串\n    if isinstance(chunk, list):\n        processed_chunk = \"\\n\".join(chunk)\n    # 确保处理后的chunk是字符串且不为空\n    if isinstance(processed_chunk, str) and processed_chunk:\n        combined_chunks[processed_chunk] = None\n```\n\n被检索出来的文本块，是为 LLM 精心准备的背景知识，也是解决用户问题最佳内容。",
    "197": "一级标题：懒人专属：拖入文档=拥有AI文档助手？RAG怎么做？SwanLab文档助手方案开源了！（附详细代码）\n二级标题：模型生成\n内容：\n### API选用与时延测试\nAPI选用方面，为了提供给用户更好的体验，选用阿里云百炼API。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./picture/rag-swanlab.png\" alt=\"image-20250812142111725\" width=\"800\" />\n  <figcaption></figcaption>\n  </figure>\n</div>\n\n模型选用`Qwen3-30B`作为项目的LLM，考量的角度聚焦为指令遵循、生成质量与首`Token`响应时间三个方面。相较于其他模型的LLM模型，笔者在测试过程中发现`Qwen`系列在指令遵循更加精准。针对生成质量的稳定性，按理说参数量越大的模型稳定性越好，费用是硬伤。折中考量在测试中`Qwen3-30B`也能达到预期的效果（用户在使用的过程的可以选择适合的模型）。首`Token`响应关系到用户的体验，这里选用“**阿里云百炼平台**”，首token响应时间远远超出笔者的预期，平均响应时间控制在1s。\n\n阿里云百炼地址：[大模型服务平台百炼_企业级大模型开发平台_百炼AI应用构建-阿里云](https://www.aliyun.com/product/bailian)\n\n百炼平台上提供Qwen所有系列的模型，用户可以在上面选择合适的模型。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./picture/rag-aliyun.png\" alt=\"image-20250812142111725\" width=\"800\" />\n  <figcaption></figcaption>\n  </figure>\n</div>\n\n这里笔者使用**Evalscope**框架也对其进行时延测试，测试脚本如下：\n\n```bash\nevalscope perf \\\n --parallel 1 \\\n --url  https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \\\n --api-key \"填写api_key\"\\\n --model qwen3-30b-a3b-instruct-2507 \\\n --log-every-n-query 5 \\\n --connect-timeout 6000 \\\n --read-timeout 6000 \\\n --max-tokens 2048 \\\n --min-tokens 2048 \\\n --api openai \\\n --dataset openqa \\\n --number 5\\\n --swanlab-api-key \"填入Swanlab_key完成推理可视化\"\\\n --stream\n```\n\n测试结果如下图所示：\n\n| 吞吐量指标名称          | 值      |\n| ----------------------- | ------- |\n| 输出token吞吐量 (tok/s) | 91.8166 |\n| 总token吞吐量 (tok/s)   | 95.4285 |\n| 请求吞吐量 (req/s)      | 0.1235  |\n\n| 延迟指标名称               | 值     |\n| -------------------------- | ------ |\n| 平均延迟 (秒)              | 8.0971 |\n| 首token平均到达时间 (秒)   | 0.4173 |\n| 每个输出token平均时间 (秒) | 0.0102 |\n| 平均包延迟 (秒)            | 0.0403 |\n\n快速使用阿里云百炼API示例：\n\n```python\n#配置API的相关参数\nimport os\nfrom openai import OpenAI\n\n\nclient = OpenAI(\n    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n)\n\n#Test\ncompletion = client.chat.completions.create(\n    # 模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n    model=\"qwen3-30b-a3b-instruct-2507\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"你是谁？\"},\n    ],\n    # Qwen3模型通过enable_thinking参数控制思考过程（开源版默认True，商业版默认False）\n    # 使用Qwen3开源版模型时，若未启用流式输出，请将下行取消注释，否则会报错\n    # extra_body={\"enable_thinking\": False},\n)\n```\n\n### 构建Prompt\n\n在笔者的实际测试中，发现直接把“背景知识”和“问题”扔给LLM的效果通常不好。下图是笔者在玩耍文生图模型过程中发现提示词的重要性。为此笔者精心设计一个“提示词（`Prompt`）\"，相当于给 LLM设定好角色和答题格式。让LLM的回答更加人性化，条例清晰。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./picture/rag-prompt.png\" alt=\"image-20250812142111725\" width=\"800\" />\n  <figcaption>Prompt的重要性</figcaption>\n  </figure>\n</div>\n\n笔者的 Prompt 模板大概是这样的：\n\n```python\nprompt = f\"\"\"\n        # Role\n        You are a documentation Q&A assistant for the SwanLab open-source project. SwanLab is a product developed by Emotion \t\t\t\tMachine (Beijing) Technology Co., Ltd.\n\n        # Instructions\n        1. Answer the [question] based on the provided [background knowledge]. If no relevant information is found, notify the \t\t\t\tuser that no reference material was located and provide suggestions based on your existing knowledge.\n        2. When answering, you may use some emojis in your narrative style to make it more human-like.\n        3. After answering, you can guide the user to ask some related follow-up questions.\n        4. If you encounter incomplete web addresses, please prepend \"https://docs.swanlab.cn/\" and replace \".md\" with \".html\".\n        5. Also consider whether the user's [historical questions] are relevant to the current question and incorporate that \t\t\t\tinto your response.\n\n        ---\n        [Historical Questions]\n        {history_prompt}\n        ---\n        [Background Knowledge]\n        {context}\n        ---\n        # Current Question\n        [Question]\n        {question}\n        ---\n        Your response:\n        \"\"\"\n```\n\n这个精心构造的 Prompt，将所有必要信息都提供给了大模型，引导它做出最准确与最全面的回答。最后LLM基于回答返回参考的文档用于前端展示。\n\n### 模型生成\n\n最后一步，将这个完整的 Prompt 发送给一个强大的大语言模型（本项目使用的LLM为阿里云百联提供的`qwen3-30b-a3b-instruct-2507`模型），它会综合所有信息，生成一段流畅、准确的回答。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./picture/rag-qwen3.png\" alt=\"image-20250724151955944\" width=\"800\" />\n  <figcaption></figcaption>\n  </figure>\n</div>\n\n还增加了一个小细节：从被检索到的知识块中返回的“一级标题”，并利用我们最早爬取到的 `html_url`，为用户生成一个可点击的“参考资料”列表，让答案更有权威性。为用户提供更多的参考与指南。",
    "198": "一级标题：懒人专属：拖入文档=拥有AI文档助手？RAG怎么做？SwanLab文档助手方案开源了！（附详细代码）\n二级标题：快速使用\n内容：\nSwanlab官网中提供两个文档助手入口，或者直接访问链接[SwanLab AI文档助手](https://chat.swanlab.cn/)，如下图所示（一个在导航栏右侧，一个在主界面）。用户在使用的过程种如遇BUG情况，请联系笔者反馈并提供您宝贵的意见。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./picture/rag-login.png\" alt=\"image-20250812170254538\"  width=\"800\" />\n  <figcaption></figcaption>\n  </figure>\n</div>\n\n测试中面对较为复杂的问题，文档助手能够准确回答问题，并且提供出准确的文档参考链接。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./picture/rag-test.png\" alt=\"image-20250812170254538\"  width=\"800\" />\n  <figcaption></figcaption>\n  </figure>\n</div>\n\n这里笔者问了一个非常刁钻的问题，文档中没有明确指出`leader`的联系方式，但是也能够在内容中总结出正确的联系方式。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./picture/rag-test2.png\" alt=\"image-20250812170254538\"  width=\"800\" />\n  <figcaption></figcaption>\n  </figure>\n</div>\n\n更多详细内容请参考：\n\nAI文档助手在线体验链接：[https://chat.swanlab.cn/](https://chat.swanlab.cn/)\n\n方案开源Github仓库链接：https://github.com/EmotionMachine/swanlab-rag",
    "199": "一级标题：第九章 项目实战\n二级标题：无\n内容：\n实战是解锁真知的唯一钥匙。 再精妙的构思、再前沿的理论，未经实战的淬炼都如同纸上谈兵。接下来，我们将深入实战环节，通过一步步拆解项目 与详细阐述，手把手实现如何将知识转化为可执行、可验证的解决方案。\n\n\n\n\n\n📖下面是我们本节的课程目录：\n\n\n\n| 教程章节   | 核心内容 |\n\n|:--------|:------|\n\n| [9.1.Swanlab-RAG实战](./1.swanlab-rag.md)   | Swanlab-RAG实战项目 |",
    "200": "一级标题：懒人专属：拖入文档=拥有AI文档助手？RAG怎么做？SwanLab文档助手方案开源了！（附详细代码）\n二级标题：无\n内容：\n**作者**：李晨 、王超\n\n**机构**：新疆大学丝路多语言认知计算国际合作联合实验室研究生、情感机器实习研究员\n\n**联系邮箱**：lichen@stu.xju.edu.cn、wangchao@stu.xju.edu.cn\n\n---\n\n**\"随着官方文档API的爆炸式增长，用户阅读较为困难，——今天，我们直接用SwanLab+开箱即用的RAG框架**，带你30分钟搭建一个**服务级文档助手**（附完整源码和在线Demo，无需复杂配置，10分钟即可本地跑通！）\n\nAI文档助手在线体验链接：[https://chat.swanlab.cn/](https://chat.swanlab.cn/)\n\n方案开源Github仓库链接：https://github.com/EmotionMachine/swanlab-rag\n\nSwabnlab官方提示词课程链接:fire:：[https://docs.swanlab.cn/course/prompt_engineering_course/01-preface/README.html](https://docs.swanlab.cn/course/prompt_engineering_course/01-preface/README.html)\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./assets/rag-demo.gif\" alt=\"rag-demo\"  width=\"800\" />\n  <figcaption>Swanlab官方文档Demo演示</figcaption>\n  </figure>\n</div>",
    "201": "一级标题：懒人专属：拖入文档=拥有AI文档助手？RAG怎么做？SwanLab文档助手方案开源了！（附详细代码）\n二级标题：摘要\n内容：\n在信息爆炸的时代，开发者需要快速从技术文档中获取准确信息。SwanLab官方文档内容丰富，但手动查找耗时且效率低下。**为了让开发者能够更快速、更精准地从SwanLab官方文档中获取信息**，我们开发了一款AI文档助手，基于**检索增强生成（RAG）架构**。\n\n本文介绍了Swanlab官方文档助手的详细实现过程，不依赖任何现有Agent框架，妥妥“**纯手工打造**”。本文将会从数据准备、文档检索、模型生成三个方面去介绍。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./assets/rag-structure.png\" alt=\"image-20250813112441893\" width=\"800\" />\n  <figcaption>Swanlab文档助手技术架构简介</figcaption>\n  </figure>\n</div>\n\n------",
    "202": "一级标题：懒人专属：拖入文档=拥有AI文档助手？RAG怎么做？SwanLab文档助手方案开源了！（附详细代码）\n二级标题：目录\n内容：\n[[toc]]",
    "203": "一级标题：懒人专属：拖入文档=拥有AI文档助手？RAG怎么做？SwanLab文档助手方案开源了！（附详细代码）\n二级标题：数据准备\n内容：\n本节主要介绍**向量数据库**的详细构建方案。\n\n### 文档检索\n\n AI助手的知识基础来源于最新、最全的官方文档。首先需要让 AI 拥有“阅读”所有 SwanLab 官方文档的能力。**GitHub作为开发者最熟悉的平台，降低学习成本**。笔者采用**GitHub API作为获取文档的主要技术，技术可信度为一方面，更重要的是确保获取到的资料具有实时性与完整性。**\n\n实现过程**使用Python编写了一个网络爬虫脚本，通过调用 GitHub API，自动扫描 SwanLab 文档仓库，获取所有 Markdown 文件的元数据**，如标题、下载链接和对应的官网页面地址保存到`JSON`文件中。工作流程如下所示：\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./assets/rag-github_api.png\" alt=\"image-20250724121726401\"  width=\"800\" />\n  <figcaption>GitHub API 调用流程</figcaption>\n  </figure>\n</div>\n\n​\n\n`swanlab.json`包含文件名、文档地址、网页地址和文档主题，具体如下所示：\n\n```json\n{\n      \"theme\": \"cli-swanlab-convert\",\n      \"url\": \"https://raw.githubusercontent.com/SwanHubX/SwanLab-Docs/main/zh/api/cli-swanlab-convert.md\",\n      \"html_url\": \"https://docs.swanlab.cn/api/cli-swanlab-convert.html\",\n      \"title\": \"swanlab convert\"\n }\n```\n\n### Github API及其使用方法\n\nGitHub API 是一种用于与 GitHub 平台交互的编程接口，允许开发者通过编写代码来访问 GitHub 上的资源和数据。https://api.github.com/repos/ 是其中一个重要的端点，用户可以对 GitHub 上的代码仓库资源进行获取。\n\nAPI 的核心功能是为用户提供一种以编程方式访问和操作 GitHub 平台数据的方法，而不是通过用户界面手动操作。通过 API工具，开发者可以获取仓库的详细信息，创建新仓库，修改现有仓库的设置，甚至删除仓库。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./assets/rag-git.png\" alt=\"image-20250724121726401\"  width=\"800\" />\n  <figcaption>自动化Git仓库</figcaption>\n  </figure>\n</div>\n\n在代码中Github API的具体使用方法：\n\n```python\n#scrape_swanlab_docs_Internet.py\n\n#认证与请求头，通过 Authorization: token {token} 请求头进行认证，提高 API 速率限制（未认证用户每小时 60 次请求，认证用户每小时 5000 次）。\ngithub_token = \"\"  # 填写为你的GitHub Token\nheaders['Authorization'] = f'token {github_token}'\n# 使用requests 库配置重试机制\nsession = requests.Session()\nresponse = session.get(\"https://api.github.com/rate_limit\", headers=headers, timeout=60)\nresponse.raise_for_status()\ncontents = response.json()\n\n\"\"\"\n检索\nArgs:\n   repo_url (str): GitHub仓库目录URL（例如 'https://github.com/SwanHubX/SwanLab-Docs/tree/main/zh'）\n   base_html_url (str): SwanLab文档网站根URL（例如 'https://docs.swanlab.cn'）\n\"\"\"\nfor item in contents:\n    if item['type'] == 'file' and item['name'].endswith('.md'):\n        theme = item['name'].replace('.md', '')\n        md_url = item['download_url']\n        relative_path = md_url.split('/main/zh/')[-1]\n        html_filename = filename_map.get(theme, theme)\n        html_path = relative_path.replace(theme + '.md', html_filename + '.html')\n        html_url = f\"{base_html_url}/{html_path}\"\n        title = get_markdown_title(md_url, headers, session)\n\n        docs.append({\n            \"theme\": theme,\n            \"url\": md_url,\n            \"html_url\": html_url,\n            \"title\": title\n            })\n            logger.info(\n                f\"{'  ' * depth}Added file: {theme}, Markdown: {md_url}, HTML: {html_url}, Title: {title}\")\n    elif item['type'] == 'dir':\n           logger.info(f\"{'  ' * depth}Entering directory: {item['path']}\")\n            scan_directory(item['url'], docs, depth + 1)\n```\n\n### 文档解析与分块合并\n\n 在上一步我们得到了所有文档的“地址列表”（`JSON` 文件）。接下来，我们需要“按图索骥”，访问每一个链接，异步地抓取所有 Markdown 的原始内容，使用LLM对每个文档根据内容进行文档解析与分块处理，采取其重要部分用于构建知识库。\n\n首先笔者先介绍一下分块策略毕竟笔者采用的具体分块策略，为什么需要分快？分块涉及将文本划分为可管理的单元或“块”，以实现高效处理。这种分割对于语义搜索、信息检索和生成式 AI 应用等任务都至关重要。每个块都保留上下文和语义完整性，以确保结果连贯。分块策略在检索增强生成（RAG）方法中起着至关重要的作用，它使文档能够被划分为可管理的部分，同时保持上下文。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./assets/rag-chunk.png\" alt=\"image-20250724173035496\" width=\"400\" />\n  <figcaption>Chunk策略示意图</figcaption>\n  </figure>\n</div>\n\n在笔者实际的测试中，直接对长文本进行向量化存在诸多问题，存在语义信息的稀释、计算资源的耗费以及检索效率的降低等。将长文本进行合理的分块处理，成为构建高效向量数据库的前提。在可以提高检索准确率同时，也确保召回的内容与用户的查询意图更加契合，进而提升大模型生成答案的质量。最终笔者采取的方案为根据文档的二级标题进行分块处理，在引入宝贵的结构元数据同时，也实现了分块粒度的平衡，保持了语义的完整性和专注性。当用户的查询与某一子主题高度相关时，模型能够精准召回包含该标题和对应内容的文本块，避免了不相关信息的干扰，也提升了答案的可信度和可用性。在笔者不断的摸索中，检索效率和生成效果之间取得了理想的平衡。\n\n将分块后的内容合并到一起，这样每个知识块都包含了相对完整的主题与内容。实现这个逻辑的代码也非常简单：\n\n```python\n# create_vector.py\ndef split_text(text: str, separator: str = \"",
    "204": "\") -> list[str]:\n    \"\"\"\n    分割文本：根据指定的分隔符分割文本。\n    \"\"\"\n    chunks = text.split(separator)\n    # 过滤掉可能存在的空字符串\n    return [chunk.strip() for chunk in chunks if chunk.strip()]\n```\n\n配置文件和分块提示词如`config.py`和`prompt.py`所示。在处理时，我们用一个特殊的分隔符 `",
    "205": "` 将文档内容按照二级标题的内容隔开，形成一个完整的Swanlab文档知识库。\n\n```yaml\n#config.yaml`\nllm:\n  api_type: \"openai\"  # 或 \"groq\" 等\n  model: \"Qwen/Qwen2.5-32B-Instruct\"  # 或 \"GPT-4o\"等\n  api_key: \"\"    #填写api_key\n  base_url: \"\"  # LLM服务URL\n#prompt.py\nllm = LLM()\nprompt = f\"\"\"\n分析以下 Markdown 内容，把内容进行分块整理。\n格式如下所示：\n一级标题：(文档标题)\n二级标题（文档二级标题）：内容",
    "206": "一级标题：(文档标题)\n二级标题（文档二级标题）：内容",
    "207": "......",
    "208": "参考内容：{content}\n\"\"\"\n```\n\n### 文档编码\n\n现在我们有了一堆文本块，但计算机不理解文字，只懂数字。我们需要一个“翻译官”，把每个文本块翻译成一个由数字组成的“向量”（Vector）。这个过程就是 **Embedding**。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./assets/rag-embedding.png\" alt=\"image-20250724173738310\" width=\"800\" />\n  <figcaption>生成词向量</figcaption>\n  </figure>\n</div>\n\n调用API提供的 `Embedding` 模型来完成这个任务。调用的过程也相对比较简单，提供API_KEY和Base_URL等关键参数即可，具体实现方法如下代码所示。对于这样的每一个文本块，获得了一个`Embedding`后的向量。\n\n```python\n# create_vector.py\ndef get_embeddings_from_api(texts: list[str], batch_size: int = 16) -> list[list[float]]:\n    all_embeddings = []\n    headers = {\n        \"Authorization\": f\"Bearer {API_KEY}\",\n        \"Content-Type\": \"application/json\"\n    }\n    url = f\"{BASE_URL.rstrip('/')}/embeddings\"\n\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i + batch_size]\n        payload = {\n            \"model\": EMBEDDING_MODEL,\n            \"input\": batch\n        }\n        # ... 发起请求并获取返回的向量 ...\n        response_data = response.json()\n        batch_embeddings = [item['embedding'] for item in response_data['data']]\n        all_embeddings.extend(batch_embeddings)\n\n    return all_embeddings\n```\n\n拿到所有向量后，如何管理这些向量？笔者选择 Facebook 开源的 `FAISS` 库来构建一个向量数据库，支持快速处理大规模数据，并且支持在高维空间中进行相似性搜索。可以把`FAISS` 想象成一个专门为向量设计的高效“图书馆”，能极快地找到与给定向量最相似的几个向量。\n\n至此，我们的“Swanlab官方文档向量知识库”已经完全准备就绪了！我们就可以进行文档检索了!\n\n------",
    "209": "一级标题：懒人专属：拖入文档=拥有AI文档助手？RAG怎么做？SwanLab文档助手方案开源了！（附详细代码）\n二级标题：文档检索（RAG）\n内容：\n`RAG`（`Retrieval-Augmented Generation`，检索增强生成） 是一种结合了信息检索技术与语言生成模型的人工智能技术。该技术通过从外部知识库中检索相关信息，并将其作为提示（Prompt）输入给大型语言模型（LLMs），以增强模型处理知识密集型任务的能力，如问答、文本摘要、内容生成等。RAG模型由Facebook AI Research（FAIR）团队于2020年首次提出，并迅速成为大模型应用中的热门方案。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./assets/rag-rag.png\" alt=\"image-20250725151046751\"  width=\"800\" />\n  <figcaption>文档检索流程示意图</figcaption>\n  </figure>\n</div>\n\n数据准备是离线完成的，而问答检索则是用户与 AI 实时交互的过程。这个过程就像让 AI 进行一场“开卷考试”，我们先把相关的参考资料找出来，再让它根据这些官方资料来回答与解决用户问题。\n\n### 检索出相关文档\n\n当用户提出一个问题时，我们首先用和之前完全相同的 `Embedding` 模型，将用户的问题也转换成一个向量。我们拿着这个“问题向量”，采用混合检索（向量检索+关键词检索）的策略去 FAISS 向量数据库里进行搜索，命令它：“找出与我这个向量在空间距离上最接近文本块向量！\"。至于为什么采用这种方式，会在混合检索中进行详细讨论分析。再此之前笔者先带领读者先回顾一下向量检索和关键词检索的核心逻辑。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./assets/rag-faiss.png\" alt=\"image-20250724145848506\" width=\"800\" />\n  <figcaption>FAISS向量数据库</figcaption>\n  </figure>\n</div>\n\n#### 向量检索\n\n`Faiss`（Facebook AI Similarity Search）是由 Facebook AI 团队开源的高效相似性搜索和聚类库，专为稠密向量的快速检索设计。它支持十亿级别的向量搜索，广泛应用于推荐系统、图像检索和自然语言处理等领域。\n\nFaiss 提供多种算法来处理任意大小的向量集，支持高效的近似最近邻（ANN）搜索和聚类。其核心算法包括倒排索引（IVF）和乘积量化（PQ），并通过 GPU 加速进一步提升性能。最重要的原因，`Faiss` 提供了与 Python 的无缝接口，便于与 `Numpy` 集成的同时，也笔者在不使用任何架构使用纯`Python`提供了便捷。\n\n\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./assets/rag-faiss2.png\" alt=\"image-20250725172624004\" width=\"800\" />\n  <figcaption>FAISS</figcaption>\n  </figure>\n</div>\n\n向量检索能够跨越字面障碍，理解用户的真实意图。即使用户的提问方式和文档的写法完全不同，只要意思相近，也能够轻易找到。能帮助用户发现他们没直接提问、但高度相关的内容，提供更具探索性的结果。\n\n向量检索的实现如下所示：\n\n```python\n# chat_logic.py\n# 1. 将用户问题转换为向量\nquery_embedding = self._get_query_embedding(question)\nquery_vector = np.array([query_embedding]).astype('float32')\n\n# 2. 在FAISS中执行搜索，k=10表示寻找最相似的10个,可以动态调整\ndistances, indices = self.index.search(query_vector, k=10)\n\n# 3. 根据返回的索引号，找到原始的文本块\nretrieved_chunks = [self.index_to_chunk[str(i)] for i in indices[0]]\n```\n\n#### 关键词检索\n\n关键词检索（`Keyword Search`），也称为词法检索（`Lexical Search`），是一种基于精确词汇匹配的传统信息检索方法。\n\n其核心机制依赖于倒排索引结构（Inverted Index），该结构由词汇表（Lexicon）和对应的发布列表（Posting List）组成，通过两者的映射关系实现高效查找。在处理流程上，首先进行查询解析，通过词法分析提取查询项，并可选地应用停用词过滤等技术优化检索过程。这种检索方式的特点在于对查询关键词与文档词汇的精确匹配，具有计算效率高、实现简单等优势。\n\n```python\n#使用正则表达式（re模块）进行关键词检索，同样选择寻找最相似的10个文本快\ndef _keyword_search(self, query:str, k: int = 10):\n    keywords = [word for word in re.split(r'\\W+', query.lower()) if word]\n    if not keywords:\n        return []\n\n    all_chunks = list(self.index_to_chunk.values())\n    chunk_scores = {}\n\n    for chunk in all_chunks:\n        score = 0\n        chunk_lower = chunk.lower()\n        for keyword in keywords:\n            score += chunk_lower.count(keyword)\n\n        if score > 0:\n            chunk_scores[chunk] = score\n\n    sorted_chunks = sorted(chunk_scores.items(), key=lambda item: item[1], reverse=True)\n    return [chunk for chunk, score in sorted_chunks[:k]]\n```\n\n#### 混合检索\n\n笔者采用混合检索（向量检索+关键词检索），核心原因这两种检索方式优势互补，向量检索的“智能”和“深度”来理解用户意图，同时用关键词检索的“精准”和“可靠”来抓住关键细节。能够实现1+1 > 2的效果，亦是当今构建先进RAG系统的主流和与最佳实践方式。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./assets/rag-fusion_search.png\" alt=\"image-20250725180644828\"  width=\"800\" />\n  <figcaption>混合检索示意图</figcaption>\n  </figure>\n</div>\n\n采用混合检索技术的优势：\n\n- 确保高召回率 (`Recall`): 混合检索能最大程度地确保所有可能相关的文档都被“召回”。向量检索负责召回那些“意思对”的文档，而关键词检索则像一张安全网，确保那些包含“特定关键词”的文档不被遗漏。\n- 提升答案的精准度 (`Precision`): 通过向大模型提供一个既包含宏观概念理解、又包含微观关键细节的上下文，大模型能够基于更丰富、更无偏见的信息，生成更可靠、更精确的答案。\n- 应对复杂查询场景:\n  - 当用户提问模糊时：“我的实验结果好像不对劲”，向量检索能发挥巨大作用，找到关于“调试”、“数据异常”、“结果分析”等概念的文档。\n  - 当用户提问精确时：“`swanlab.log` 函数怎么用？”，关键词检索能确保包含 `swanlab.log` 的文档被精准找到，而向量检索则能补充一些关于“如何记录日志”、“最佳实践”等相关上下文。\n\n将两种检索出来的文本块进行去重处理：\n\n```python\nfor chunk in all_retrieved:\n    processed_chunk = chunk\n    # 检查chunk是否为列表，如果是，则将其合并为字符串\n    if isinstance(chunk, list):\n        processed_chunk = \"\\n\".join(chunk)\n    # 确保处理后的chunk是字符串且不为空\n    if isinstance(processed_chunk, str) and processed_chunk:\n        combined_chunks[processed_chunk] = None\n```\n\n被检索出来的文本块，是为 LLM 精心准备的背景知识，也是解决用户问题最佳内容。\n\n------",
    "210": "一级标题：懒人专属：拖入文档=拥有AI文档助手？RAG怎么做？SwanLab文档助手方案开源了！（附详细代码）\n二级标题：模型生成\n内容：\n### API选用与时延测试\n\nAPI选用方面，为了提供给用户更好的体验，选用阿里云百炼API。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./assets/rag-swanlab.png\" alt=\"image-20250812142111725\" width=\"800\" />\n  <figcaption></figcaption>\n  </figure>\n</div>\n\n模型选用`Qwen3-30B`作为项目的LLM，考量的角度聚焦为指令遵循、生成质量与首`Token`响应时间三个方面。相较于其他模型的LLM模型，笔者在测试过程中发现`Qwen`系列在指令遵循更加精准。针对生成质量的稳定性，按理说参数量越大的模型稳定性越好，费用是硬伤。折中考量在测试中`Qwen3-30B`也能达到预期的效果（用户在使用的过程的可以选择适合的模型）。首`Token`响应关系到用户的体验，这里选用“**阿里云百炼平台**”，首token响应时间远远超出笔者的预期，平均响应时间控制在1s。\n\n阿里云百炼地址：[大模型服务平台百炼_企业级大模型开发平台_百炼AI应用构建-阿里云](https://www.aliyun.com/product/bailian)\n\n百炼平台上提供Qwen所有系列的模型，用户可以在上面选择合适的模型。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./assets/rag-aliyun.png\" alt=\"image-20250812142111725\" width=\"800\" />\n  <figcaption></figcaption>\n  </figure>\n</div>\n\n这里笔者使用**Evalscope**框架也对其进行时延测试，测试脚本如下：\n\n```bash\nevalscope perf \\\n --parallel 1 \\\n --url  https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \\\n --api-key \"填写api_key\"\\\n --model qwen3-30b-a3b-instruct-2507 \\\n --log-every-n-query 5 \\\n --connect-timeout 6000 \\\n --read-timeout 6000 \\\n --max-tokens 2048 \\\n --min-tokens 2048 \\\n --api openai \\\n --dataset openqa \\\n --number 5\\\n --swanlab-api-key \"填入Swanlab_key完成推理可视化\"\\\n --stream\n```\n\n测试结果如下图所示：\n\n| 吞吐量指标名称          | 值      |\n| ----------------------- | ------- |\n| 输出token吞吐量 (tok/s) | 91.8166 |\n| 总token吞吐量 (tok/s)   | 95.4285 |\n| 请求吞吐量 (req/s)      | 0.1235  |\n\n| 延迟指标名称               | 值     |\n| -------------------------- | ------ |\n| 平均延迟 (秒)              | 8.0971 |\n| 首token平均到达时间 (秒)   | 0.4173 |\n| 每个输出token平均时间 (秒) | 0.0102 |\n| 平均包延迟 (秒)            | 0.0403 |\n\n快速使用阿里云百炼API示例：\n\n```python\n#配置API的相关参数\nimport os\nfrom openai import OpenAI\n\n\nclient = OpenAI(\n    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n)\n\n#Test\ncompletion = client.chat.completions.create(\n    # 模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n    model=\"qwen3-30b-a3b-instruct-2507\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"你是谁？\"},\n    ],\n    # Qwen3模型通过enable_thinking参数控制思考过程（开源版默认True，商业版默认False）\n    # 使用Qwen3开源版模型时，若未启用流式输出，请将下行取消注释，否则会报错\n    # extra_body={\"enable_thinking\": False},\n)\n```\n\n### 构建Prompt\n\n在笔者的实际测试中，发现直接把“背景知识”和“问题”扔给LLM的效果通常不好。下图是笔者在玩耍文生图模型过程中发现提示词的重要性。为此笔者精心设计一个“提示词（`Prompt`）\"，相当于给 LLM设定好角色和答题格式。让LLM的回答更加人性化，条例清晰。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./assets/rag-prompt.png\" alt=\"image-20250812142111725\" width=\"800\" />\n  <figcaption>Prompt的重要性</figcaption>\n  </figure>\n</div>\n\n笔者的 Prompt 模板大概是这样的：\n\n```python\nprompt = f\"\"\"\n        # Role\n        You are a documentation Q&A assistant for the SwanLab open-source project. SwanLab is a product developed by Emotion \t\t\t\tMachine (Beijing) Technology Co., Ltd.\n\n        # Instructions\n        1. Answer the [question] based on the provided [background knowledge]. If no relevant information is found, notify the \t\t\t\tuser that no reference material was located and provide suggestions based on your existing knowledge.\n        2. When answering, you may use some emojis in your narrative style to make it more human-like.\n        3. After answering, you can guide the user to ask some related follow-up questions.\n        4. If you encounter incomplete web addresses, please prepend \"https://docs.swanlab.cn/\" and replace \".md\" with \".html\".\n        5. Also consider whether the user's [historical questions] are relevant to the current question and incorporate that \t\t\t\tinto your response.\n\n        ---\n        [Historical Questions]\n        {history_prompt}\n        ---\n        [Background Knowledge]\n        {context}\n        ---\n        # Current Question\n        [Question]\n        {question}\n        ---\n        Your response:\n        \"\"\"\n```\n\n这个精心构造的 Prompt，将所有必要信息都提供给了大模型，引导它做出最准确与最全面的回答。最后LLM基于回答返回参考的文档用于前端展示。\n\n### 模型生成\n\n最后一步，将这个完整的 Prompt 发送给一个强大的大语言模型（本项目使用的LLM为阿里云百联提供的`qwen3-30b-a3b-instruct-2507`模型），它会综合所有信息，生成一段流畅、准确的回答。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./assets/rag-qwen3.png\" alt=\"image-20250724151955944\" width=\"800\" />\n  <figcaption></figcaption>\n  </figure>\n</div>\n\n还增加了一个小细节：从被检索到的知识块中返回的“一级标题”，并利用我们最早爬取到的 `html_url`，为用户生成一个可点击的“参考资料”列表，让答案更有权威性。为用户提供更多的参考与指南。",
    "211": "一级标题：懒人专属：拖入文档=拥有AI文档助手？RAG怎么做？SwanLab文档助手方案开源了！（附详细代码）\n二级标题：快速使用\n内容：\nSwanlab官网中提供两个文档助手入口，或者直接访问链接[SwanLab AI文档助手](https://chat.swanlab.cn/)，如下图所示（一个在导航栏右侧，一个在主界面）。用户在使用的过程种如遇BUG情况，请联系笔者反馈并提供您宝贵的意见。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./assets/rag-login.png\" alt=\"image-20250812170254538\"  width=\"800\" />\n  <figcaption></figcaption>\n  </figure>\n</div>\n\n测试中面对较为复杂的问题，文档助手能够准确回答问题，并且提供出准确的文档参考链接。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./assets/rag-test.png\" alt=\"image-20250812170254538\"  width=\"800\" />\n  <figcaption></figcaption>\n  </figure>\n</div>\n\n这里笔者问了一个非常刁钻的问题，文档中没有明确指出`leader`的联系方式，但是也能够在内容中总结出正确的联系方式。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./assets/rag-test2.png\" alt=\"image-20250812170254538\"  width=\"800\" />\n  <figcaption></figcaption>\n  </figure>\n</div>\n\n---\n更多详细内容请参考：\n\nAI文档助手在线体验链接：[https://chat.swanlab.cn/](https://chat.swanlab.cn/)\n\n方案开源Github仓库链接：https://github.com/EmotionMachine/swanlab-rag\n\nSwabnlab官方提示词课程链接:fire:：[https://docs.swanlab.cn/course/prompt_engineering_course/01-preface/README.html](https://docs.swanlab.cn/course/prompt_engineering_course/01-preface/README.html)",
    "212": "一级标题：音频分类\n二级标题：无\n内容：\n:::info\n音频分类、音频处理入门\n:::\n\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/PyTorch\\_Audio\\_Classification-simple/charts)\n\n音频分类任务是指将音频信号按照其内容的类别归属进行划分。例如，区分一段音频是音乐、语音、环境声音（如鸟鸣、雨声、机器运转声）还是动物叫声等。其目的是通过自动分类的方式，高效地对大量音频数据进行组织、检索和理解。\n\n![alt text](/assets/examples/audio_classification/example-audio-classification-1.png)\n\n在现在音频分类的应用场景，比较多的是在音频标注、音频推荐这一块。同时，这也是一个非常好的入门音频模型训练的任务。\n\n在本文中，我们会基于PyTorch框架，使用 ResNet系列模型在 GTZAN 数据集上进行训练，同时使用[SwanLab](https://swanlab.cn)监控训练过程、评估模型效果。\n\n* Github：[https://github.com/Zeyi-Lin/PyTorch-Audio-Classification](https://github.com/Zeyi-Lin/PyTorch-Audio-Classification)\n* 数据集：[https://pan.baidu.com/s/14CTI_9MD1vXCqyVxmAbeMw?pwd=1a9e](https://pan.baidu.com/s/14CTI_9MD1vXCqyVxmAbeMw?pwd=1a9e) 提取码: 1a9e\n* SwanLab实验日志：[https://swanlab.cn/@ZeyiLin/PyTorch\\_Audio\\_Classification-simple/charts](https://swanlab.cn/@ZeyiLin/PyTorch\\_Audio\\_Classification-simple/charts)\n* 更多实验日志：[https://swanlab.cn/@ZeyiLin/PyTorch\\_Audio\\_Classification/charts](https://swanlab.cn/@ZeyiLin/PyTorch\\_Audio\\_Classification/charts)",
    "213": "一级标题：音频分类\n二级标题：1. 音频分类逻辑\n内容：\n本教程对音频分类任务的逻辑如下：\n\n1. 载入音频数据集，数据集为音频WAV文件与对应的标签\n2. 以8:2的比例划分训练集和测试集\n3. 使用`torchaudio`库，将音频文件转换为梅尔频谱图，本质将其转换为图像分类任务\n4. 使用ResNet模型对梅尔频谱图进行训练迭代\n5. 使用SwanLab记录训练和测试阶段的loss、acc变化，并对比不同实验之间的效果差异",
    "214": "一级标题：音频分类\n二级标题：2. 环境安装\n内容：\n本案例基于**Python>=3.8**，请在您的计算机上安装好Python。\n\n我们需要安装以下这几个Python库：\n\n```python\ntorch\ntorchvision\ntorchaudio\nswanlab\npandas\nscikit-learn\n```\n\n一键安装命令：\n\n```shellscript\npip install torch torchvision torchaudio swanlab pandas scikit-learn\n```",
    "215": "一级标题：音频分类\n二级标题：3. GTZAN数据集准备\n内容：\n本任务使用的数据集为GTZAN，这是一个在音乐流派识别研究中常用的公开数据集。GTZAN数据集包含 1000 个音频片段，每个音频片段的时长为 30 秒，共分为 10 种音乐流派：包括布鲁斯（Blues）、古典（Classical）、乡村（Country）、迪斯科（Disco）、嘻哈（Hip Hop）、爵士（Jazz）、金属（Metal）、流行（Pop）、雷鬼（Reggae）、摇滚（Rock），且每种流派都有 100 个音频片段。\n\n![alt text](/assets/examples/audio_classification/example-audio-classification-2.png)\n\nGTZAN数据集是在 2000-2001 年从各种来源收集的，包括个人 CD、收音机、麦克风录音等，代表了各种录音条件下的声音。\n\n**数据下载方式（大小1.4GB）：**\n\n1. 百度网盘下载：链接: [https://pan.baidu.com/s/14CTI\\_9MD1vXCqyVxmAbeMw?pwd=1a9e](https://pan.baidu.com/s/14CTI\\_9MD1vXCqyVxmAbeMw?pwd=1a9e) 提取码: 1a9e\n2. 通过Kaggle下载：[https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification](https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification)\n3. 在Hyper超神经网站下载BT种子进行下载：[https://hyper.ai/cn/datasets/32001](https://hyper.ai/cn/datasets/32001)\n\n> 注意，数据集中有一个音频是损坏的，在百度网盘版本里已经将其剔除。\n\n下载完成后，解压到项目根目录下即可。",
    "216": "一级标题：音频分类\n二级标题：4. 生成数据集CSV文件\n内容：\n我们将数据集中的音频文件路径和对应的标签，处理成一个`audio_dataset.csv`文件，其中第一列为文件路径，第二列为标签：\n\n（这一部分先不执行，在完整代码里会带上）\n\n```python\nimport os\nimport pandas as pd\n\ndef create_dataset_csv():\n    # 数据集根目录\n    data_dir = './GTZAN/genres_original'\n    data = []\n\n    # 遍历所有子目录\n    for label in os.listdir(data_dir):\n        label_dir = os.path.join(data_dir, label)\n        if os.path.isdir(label_dir):\n            # 遍历子目录中的所有wav文件\n            for audio_file in os.listdir(label_dir):\n                if audio_file.endswith('.wav'):\n                    audio_path = os.path.join(label_dir, audio_file)\n                    data.append([audio_path, label])\n\n    # 创建DataFrame并保存为CSV\n    df = pd.DataFrame(data, columns=['path', 'label'])\n    df.to_csv('audio_dataset.csv', index=False)\n    return df\n\n\n# 生成或加载数据集CSV文件\nif not os.path.exists('audio_dataset.csv'):\n    df = create_dataset_csv()\nelse:\n    df = pd.read_csv('audio_dataset.csv')\n```\n\n处理后，你会在根目录下看到一个`audio_dataset.csv`文件：\n\n![alt text](/assets/examples/audio_classification/example-audio-classification-3.png)",
    "217": "一级标题：音频分类\n二级标题：5. 配置训练跟踪工具SwanLab\n内容：\nSwanLab 是一款开源、轻量的 AI 实验跟踪工具，提供了一个跟踪、比较、和协作实验的平台。SwanLab 提供了友好的 API 和漂亮的界面，结合了超参数跟踪、指标记录、在线协作、实验链接分享等功能，让您可以快速跟踪 AI 实验、可视化过程、记录超参数，并分享给伙伴。\n\n![alt text](/assets/examples/audio_classification/example-audio-classification-4.png)\n\n配置SwanLab的方式很简单：\n\n1. 注册一个账号：[https://swanlab.cn](https://swanlab.cn)\n2. 在安装好swanlab后（pip install swanlab），登录：\n\n```bash\nswanlab login\n```\n\n在提示输入API Key时，去[设置页面](https://swanlab.cn/settings/overview)复制API Key，粘贴后按回车即可。\n\n![alt text](/assets/examples/audio_classification/example-audio-classification-5.png)",
    "218": "一级标题：音频分类\n二级标题：6. 完整代码\n内容：\n开始训练时的目录结构：\n\n```\n|--- train.py\n|--- GTZAN\n```\n\ntrain.py做的事情包括：\n\n1. 生成数据集csv文件\n2. 加载数据集和resnet18模型（ImageNet预训练）\n3. 训练20个epoch，每个epoch进行训练和评估\n4. 记录loss和acc，以及学习率的变化情况，在swanlab中可视化\n\n\n\ntrain.py：\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchaudio\nimport torchvision.models as models\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport swanlab\n\n\ndef create_dataset_csv():\n    # 数据集根目录\n    data_dir = './GTZAN/genres_original'\n    data = []\n\n    # 遍历所有子目录\n    for label in os.listdir(data_dir):\n        label_dir = os.path.join(data_dir, label)\n        if os.path.isdir(label_dir):\n            # 遍历子目录中的所有wav文件\n            for audio_file in os.listdir(label_dir):\n                if audio_file.endswith('.wav'):\n                    audio_path = os.path.join(label_dir, audio_file)\n                    data.append([audio_path, label])\n\n    # 创建DataFrame并保存为CSV\n    df = pd.DataFrame(data, columns=['path', 'label'])\n    df.to_csv('audio_dataset.csv', index=False)\n    return df\n\n# 自定义数据集类\nclass AudioDataset(Dataset):\n    def __init__(self, df, resize, train_mode=True):\n        self.audio_paths = df['path'].values\n        # 将标签转换为数值\n        self.label_to_idx = {label: idx for idx, label in enumerate(df['label'].unique())}\n        self.labels = [self.label_to_idx[label] for label in df['label'].values]\n        self.resize = resize\n        self.train_mode = train_mode  # 添加训练模式标志\n    def __len__(self):\n        return len(self.audio_paths)\n\n    def __getitem__(self, idx):\n        # 加载音频文件\n        waveform, sample_rate = torchaudio.load(self.audio_paths[idx])\n\n        # 将音频转换为梅尔频谱图\n        transform = torchaudio.transforms.MelSpectrogram(\n            sample_rate=sample_rate,\n            n_fft=2048,\n            hop_length=640,\n            n_mels=128\n        )\n        mel_spectrogram = transform(waveform)\n\n        # 确保数值在合理范围内\n        mel_spectrogram = torch.clamp(mel_spectrogram, min=0)\n\n        # 转换为3通道图像格式 (为了适配ResNet)\n        mel_spectrogram = mel_spectrogram.repeat(3, 1, 1)\n\n        # 确保尺寸一致\n        resize = torch.nn.AdaptiveAvgPool2d((self.resize, self.resize))\n        mel_spectrogram = resize(mel_spectrogram)\n\n        return mel_spectrogram, self.labels[idx]\n\n# 修改ResNet模型\nclass AudioClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super(AudioClassifier, self).__init__()\n        # 加载预训练的ResNet\n        self.resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n        # 修改最后的全连接层\n        self.resnet.fc = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        return self.resnet(x)\n\n# 训练函数\ndef train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n\n        for i, (inputs, labels) in enumerate(train_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            loss.backward()\n\n            optimizer.step()\n            optimizer.zero_grad()\n\n            running_loss += loss.item()\n\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n        train_loss = running_loss/len(train_loader)\n        train_acc = 100.*correct/total\n\n        # 验证阶段\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n\n        val_loss = val_loss/len(val_loader)\n        val_acc = 100.*correct/total\n\n        current_lr = optimizer.param_groups[0]['lr']\n\n        # 记录训练和验证指标\n        swanlab.log({\n            \"train/loss\": train_loss,\n            \"train/acc\": train_acc,\n            \"val/loss\": val_loss,\n            \"val/acc\": val_acc,\n            \"train/epoch\": epoch,\n            \"train/lr\": current_lr\n        })\n\n        print(f'Epoch {epoch+1}:')\n        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n        print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n        print(f'Learning Rate: {current_lr:.6f}')\n\n# 主函数\ndef main():\n    # 设置设备\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    run = swanlab.init(\n        project=\"PyTorch_Audio_Classification-simple\",\n        experiment_name=\"resnet18\",\n        config={\n            \"batch_size\": 16,\n            \"learning_rate\": 1e-4,\n            \"num_epochs\": 20,\n            \"resize\": 224,\n        },\n    )\n\n    # 生成或加载数据集CSV文件\n    if not os.path.exists('audio_dataset.csv'):\n        df = create_dataset_csv()\n    else:\n        df = pd.read_csv('audio_dataset.csv')\n\n    # 划分训练集和验证集\n    train_df = pd.DataFrame()\n    val_df = pd.DataFrame()\n\n    for label in df['label'].unique():\n        label_df = df[df['label'] == label]\n        label_train, label_val = train_test_split(label_df, test_size=0.2, random_state=42)\n        train_df = pd.concat([train_df, label_train])\n        val_df = pd.concat([val_df, label_val])\n\n    # 创建数据集和数据加载器\n    train_dataset = AudioDataset(train_df, resize=run.config.resize, train_mode=True)\n    val_dataset = AudioDataset(val_df, resize=run.config.resize, train_mode=False)\n\n    train_loader = DataLoader(train_dataset, batch_size=run.config.batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n\n    # 创建模型\n    num_classes = len(df['label'].unique())  # 根据实际分类数量设置\n    print(\"num_classes\", num_classes)\n    model = AudioClassifier(num_classes).to(device)\n\n    # 定义损失函数和优化器\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=run.config.learning_rate)\n\n    # 训练模型\n    train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=run.config.num_epochs, device=device)\n\nif __name__ == \"__main__\":\n    main()\n```\n\n看到下面的输出，则代表训练开始：\n\n![alt text](/assets/examples/audio_classification/example-audio-classification-6.png)\n\n访问打印的swanlab链接，可以看到训练的全过程：\n\n![alt text](/assets/examples/audio_classification/example-audio-classification-7.png)\n\n可以看到Reset18模型，且不加任何策略的条件下，在训练集的准确率为99.5%，验证集的准确率最高为71.5%，val loss在第3个epoch开始反而在上升，呈现「过拟合」的趋势。",
    "219": "一级标题：音频分类\n二级标题：7. 进阶代码\n内容：\n下面是我训出验证集准确率87.5%的实验，具体策略包括：\n\n1. 将模型换成resnext101\\_32x8d\n2. 将梅尔顿图的resize提高到512\n3. 增加warmup策略\n4. 增加时间遮蔽、频率屏蔽、高斯噪声、随机响度这四种数据增强策略\n5. 增加学习率梯度衰减策略\n\n![alt text](/assets/examples/audio_classification/example-audio-classification-8.png)\n\n进阶代码（需要24GB显存，如果要降低显存消耗的话，可以调低batch\\_size）：\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchaudio\nimport torchvision.models as models\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport swanlab\nimport random\nimport numpy as np\n\n# 设置随机种子\ndef set_seed(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndef create_dataset_csv():\n    # 数据集根目录\n    data_dir = './GTZAN/genres_original'\n    data = []\n\n    # 遍历所有子目录\n    for label in os.listdir(data_dir):\n        label_dir = os.path.join(data_dir, label)\n        if os.path.isdir(label_dir):\n            # 遍历子目录中的所有wav文件\n            for audio_file in os.listdir(label_dir):\n                if audio_file.endswith('.wav'):\n                    audio_path = os.path.join(label_dir, audio_file)\n                    data.append([audio_path, label])\n\n    # 创建DataFrame并保存为CSV\n    df = pd.DataFrame(data, columns=['path', 'label'])\n    df.to_csv('audio_dataset.csv', index=False)\n    return df\n\n# 自定义数据集类\nclass AudioDataset(Dataset):\n    def __init__(self, df, resize, train_mode=True):\n        self.audio_paths = df['path'].values\n        # 将标签转换为数值\n        self.label_to_idx = {label: idx for idx, label in enumerate(df['label'].unique())}\n        self.labels = [self.label_to_idx[label] for label in df['label'].values]\n        self.resize = resize\n        self.train_mode = train_mode  # 添加训练模式标志\n    def __len__(self):\n        return len(self.audio_paths)\n\n    def __getitem__(self, idx):\n        # 加载音频文件\n        waveform, sample_rate = torchaudio.load(self.audio_paths[idx])\n\n        # 将音频转换为梅尔频谱图\n        transform = torchaudio.transforms.MelSpectrogram(\n            sample_rate=sample_rate,\n            n_fft=2048,\n            hop_length=640,\n            n_mels=128\n        )\n        mel_spectrogram = transform(waveform)\n\n        # 仅在训练模式下进行数据增强\n        if self.train_mode:\n            # 1. 时间遮蔽 (Time Masking)：通过随机选择一个时间步，然后遮蔽掉20个时间步\n            time_mask = torchaudio.transforms.TimeMasking(time_mask_param=20)\n            mel_spectrogram = time_mask(mel_spectrogram)\n\n            # 2. 频率遮蔽 (Frequency Masking)：通过随机选择一个频率步，然后遮蔽掉20个频率步\n            freq_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param=20)\n            mel_spectrogram = freq_mask(mel_spectrogram)\n\n            # 3. 随机增加高斯噪声\n            if random.random() < 0.5:\n                noise = torch.randn_like(mel_spectrogram) * 0.01\n                mel_spectrogram = mel_spectrogram + noise\n\n            # 4. 随机调整响度\n            if random.random() < 0.5:\n                gain = random.uniform(0.8, 1.2)\n                mel_spectrogram = mel_spectrogram * gain\n\n        # 确保数值在合理范围内\n        mel_spectrogram = torch.clamp(mel_spectrogram, min=0)\n\n        # 转换为3通道图像格式 (为了适配ResNet)\n        mel_spectrogram = mel_spectrogram.repeat(3, 1, 1)\n\n        # 确保尺寸一致\n        resize = torch.nn.AdaptiveAvgPool2d((self.resize, self.resize))\n        mel_spectrogram = resize(mel_spectrogram)\n\n        return mel_spectrogram, self.labels[idx]\n\n# 修改ResNet模型\nclass AudioClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super(AudioClassifier, self).__init__()\n        # 加载预训练的ResNet\n        self.resnet = models.resnext101_32x8d(weights=models.ResNeXt101_32X8D_Weights.IMAGENET1K_V1)\n        # 修改最后的全连接层\n        self.resnet.fc = nn.Linear(2048, num_classes)\n\n    def forward(self, x):\n        return self.resnet(x)\n\n# 训练函数\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, run):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n\n        # 前5个epoch进行warmup\n        if epoch < 5:\n            warmup_factor = (epoch + 1) / 5\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = run.config.learning_rate * warmup_factor\n\n        # optimizer.zero_grad()  # 移到循环外部\n\n        for i, (inputs, labels) in enumerate(train_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n\n            optimizer.step()\n            optimizer.zero_grad()\n\n            running_loss += loss.item()\n\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n        train_loss = running_loss\n        train_acc = 100.*correct/total\n\n        # 验证阶段\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n\n        val_loss = val_loss/len(val_loader)\n        val_acc = 100.*correct/total\n\n        # 只在warmup结束后使用学习率调度器\n        if epoch >= 5:\n            scheduler.step()\n        current_lr = optimizer.param_groups[0]['lr']\n\n        # 记录训练和验证指标\n        swanlab.log({\n            \"train/loss\": train_loss,\n            \"train/acc\": train_acc,\n            \"val/loss\": val_loss,\n            \"val/acc\": val_acc,\n            \"train/epoch\": epoch,\n            \"train/lr\": current_lr\n        })\n\n        print(f'Epoch {epoch+1}:')\n        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n        print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n        print(f'Learning Rate: {current_lr:.6f}')\n\n# 主函数\ndef main():\n    # 设置随机种子\n    set_seed(42)\n\n    # 设置设备\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    run = swanlab.init(\n        project=\"PyTorch_Audio_Classification-simple\",\n        experiment_name=\"😄resnext101_32x8d\",\n        config={\n            \"batch_size\": 16,\n            \"learning_rate\": 1e-4,\n            \"num_epochs\": 30,\n            \"resize\": 512,\n            \"weight_decay\": 0  # 添加到配置中\n        },\n    )\n\n    # 生成或加载数据集CSV文件\n    if not os.path.exists('audio_dataset.csv'):\n        df = create_dataset_csv()\n    else:\n        df = pd.read_csv('audio_dataset.csv')\n\n    # 划分训练集和验证集\n    train_df = pd.DataFrame()\n    val_df = pd.DataFrame()\n\n    for label in df['label'].unique():\n        label_df = df[df['label'] == label]\n        label_train, label_val = train_test_split(label_df, test_size=0.2, random_state=42)\n        train_df = pd.concat([train_df, label_train])\n        val_df = pd.concat([val_df, label_val])\n\n    # 创建数据集和数据加载器\n    train_dataset = AudioDataset(train_df, resize=run.config.resize, train_mode=True)\n    val_dataset = AudioDataset(val_df, resize=run.config.resize, train_mode=False)\n\n    train_loader = DataLoader(train_dataset, batch_size=run.config.batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n\n    # 创建模型\n    num_classes = len(df['label'].unique())  # 根据实际分类数量设置\n    model = AudioClassifier(num_classes).to(device)\n\n    # 定义损失函数和优化器\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(\n        model.parameters(),\n        lr=run.config.learning_rate,\n        weight_decay=run.config.weight_decay\n    )  # Adam优化器\n\n    # 添加学习率调度器\n    scheduler = optim.lr_scheduler.StepLR(\n        optimizer,\n        step_size=10,  # 在第10个epoch衰减\n        gamma=0.1,     # 衰减率为0.1\n        verbose=True\n    )\n\n    # 训练模型\n    train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n                num_epochs=run.config.num_epochs, device=device, run=run)\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n![alt text](/assets/examples/audio_classification/example-audio-classification-9.png)\n\n可以看到提升的非常明显\n\n> 期待有训练师能把eval acc刷上90！",
    "220": "一级标题：音频分类\n二级标题：8. 相关链接\n内容：\n* Github：[https://github.com/Zeyi-Lin/PyTorch-Audio-Classification](https://github.com/Zeyi-Lin/PyTorch-Audio-Classification)\n* 数据集：[https://pan.baidu.com/s/14CTI_9MD1vXCqyVxmAbeMw?pwd=1a9e](https://pan.baidu.com/s/14CTI_9MD1vXCqyVxmAbeMw?pwd=1a9e) 提取码: 1a9e\n* SwanLab实验日志：[https://swanlab.cn/@ZeyiLin/PyTorch\\_Audio\\_Classification-simple/charts](https://swanlab.cn/@ZeyiLin/PyTorch\\_Audio\\_Classification-simple/charts)\n* 更多实验日志：[https://swanlab.cn/@ZeyiLin/PyTorch\\_Audio\\_Classification/charts](https://swanlab.cn/@ZeyiLin/PyTorch\\_Audio\\_Classification/charts)\n* SwanLab官网：[https://swanlab.cn](https://swanlab.cn)",
    "221": "一级标题：BERT文本分类\n二级标题：无\n内容：\n:::info\n自然语言处理、文本分类、机器学习入门\n:::\n\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/BERT/charts)\n\n[在线Demo](https://swanlab.cn/@ZeyiLin/BERT/charts) ｜ [知乎](https://zhuanlan.zhihu.com/p/699441531)  | [美团外卖评论分类](https://zhuanlan.zhihu.com/p/701460910)",
    "222": "一级标题：BERT文本分类\n二级标题：概述\n内容：\n**BERT**（Bidirectional Encoder Representations from Transformers）是由Google提出的一种自然语言处理预训练模型，广泛应用于各种自然语言处理任务。BERT 通过在大规模语料库上进行预训练，能够捕捉词汇之间的上下文关系，从而在很多任务上取得了优秀的效果。\n\n在这个任务中，我们将使用 BERT 模型对 IMDB 电影评论进行情感分类，具体来说是将电影评论分类为“正面”或“负面”。\n\n![IMDB](/assets/example-bert-1.png)\n\n**IMDB 电影评论数据集**包含50,000条电影评论，分为25,000条训练数据和25,000条测试数据，每部分又包含50%正面评论和50%负面评论。我们将使用预训练的 BERT 模型，通过微调(finetuning)的方式，来对这些评论进行情感分类。",
    "223": "一级标题：BERT文本分类\n二级标题：环境安装\n内容：\n本案例基于`Python>=3.8`，请在您的计算机上安装好Python。 环境依赖：\n\n```txt\ntransformers\ndatasets\nswanlab\n```\n\n快速安装命令：\n\n```bash\npip install transformers datasets swanlab\n```\n\n> 本文的代码测试于transformers==4.41.0、datasets==2.19.1、swanlab==0.3.3",
    "224": "一级标题：BERT文本分类\n二级标题：完整代码\n内容：\n```python\n\"\"\"\n用预训练的Bert模型微调IMDB数据集，并使用SwanLabCallback回调函数将结果上传到SwanLab。\nIMDB数据集的1是positive，0是negative。\n\"\"\"\n\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom swanlab.integration.transformers import SwanLabCallback\nimport swanlab\n\ndef predict(text, model, tokenizer, CLASS_NAME):\n    inputs = tokenizer(text, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        predicted_class = torch.argmax(logits).item()\n\n    print(f\"Input Text: {text}\")\n    print(f\"Predicted class: {int(predicted_class)} {CLASS_NAME[int(predicted_class)]}\")\n    return int(predicted_class)\n\n# 加载IMDB数据集\ndataset = load_dataset('imdb')\n\n# 加载预训练的BERT tokenizer\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n\n# 定义tokenize函数\ndef tokenize(batch):\n    return tokenizer(batch['text'], padding=True, truncation=True)\n\n# 对数据集进行tokenization\ntokenized_datasets = dataset.map(tokenize, batched=True)\n\n# 设置模型输入格式\ntokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\ntokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n\n# 加载预训练的BERT模型\nmodel = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n\n# 设置训练参数\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    eval_strategy='epoch',\n    save_strategy='epoch',\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    logging_first_step=100,\n    # 总的训练轮数\n    num_train_epochs=3,\n    weight_decay=0.01,\n    report_to=\"none\",\n    # 单卡训练\n)\n\nCLASS_NAME = {0: \"negative\", 1: \"positive\"}\n\n# 设置swanlab回调函数\nswanlab_callback = SwanLabCallback(project='BERT',\n                                   experiment_name='BERT-IMDB',\n                                   config={'dataset': 'IMDB', \"CLASS_NAME\": CLASS_NAME})\n\n# 定义Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['test'],\n    callbacks=[swanlab_callback],\n)\n\n# 训练模型\ntrainer.train()\n\n# 保存模型\nmodel.save_pretrained('./sentiment_model')\ntokenizer.save_pretrained('./sentiment_model')\n\n# 测试模型\ntest_reviews = [\n    \"I absolutely loved this movie! The storyline was captivating and the acting was top-notch. A must-watch for everyone.\",\n    \"This movie was a complete waste of time. The plot was predictable and the characters were poorly developed.\",\n    \"An excellent film with a heartwarming story. The performances were outstanding, especially the lead actor.\",\n    \"I found the movie to be quite boring. It dragged on and didn't really go anywhere. Not recommended.\",\n    \"A masterpiece! The director did an amazing job bringing this story to life. The visuals were stunning.\",\n    \"Terrible movie. The script was awful and the acting was even worse. I can't believe I sat through the whole thing.\",\n    \"A delightful film with a perfect mix of humor and drama. The cast was great and the dialogue was witty.\",\n    \"I was very disappointed with this movie. It had so much potential, but it just fell flat. The ending was particularly bad.\",\n    \"One of the best movies I've seen this year. The story was original and the performances were incredibly moving.\",\n    \"I didn't enjoy this movie at all. It was confusing and the pacing was off. Definitely not worth watching.\"\n]\n\nmodel.to('cpu')\ntext_list = []\nfor review in test_reviews:\n    label = predict(review, model, tokenizer, CLASS_NAME)\n    text_list.append(swanlab.Text(review, caption=f\"{label}-{CLASS_NAME[label]}\"))\n\nif text_list:\n    swanlab.log({\"predict\": text_list})\n\nswanlab.finish()\n```",
    "225": "一级标题：BERT文本分类\n二级标题：演示效果\n内容：\n![](/assets/example-bert-2.png)",
    "226": "一级标题：猫狗分类\n二级标题：无\n内容：\n:::info\n图像分类、机器学习入门、RGB图像、自定义数据集\n:::\n\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/Cats_Dogs_Classification/runs/jzo93k112f15pmx14vtxf/chart)\n\n[知乎](https://zhuanlan.zhihu.com/p/676430630)\n\n猫狗分类是计算机视觉最基础的任务之一——如果说完成MNIST手写体识别是实现CV的“Hello World”，那猫狗分类就是旅程的下一站～。这篇文章我将带大家使用SwanLab、PyTorch、Gradio三个开源工具，完成从数据集准备、代码编写、可视化训练到构建Demo网页的全过程。\n\n![](./cats_dogs/01.png)\n\n- 实验过程可看这个网页：[猫狗分类｜SwanLab](https://swanlab.cn/@ZeyiLin/Cats_Dogs_Classification/runs/jzo93k112f15pmx14vtxf/chart)\n- 代码：[Github](https://github.com/Zeyi-Lin/Resnet50-cats_vs_dogs)\n- 在线Demo：[HuggingFace](https://huggingface.co/spaces/TheEeeeLin/Resnet50-cats_vs_dogs)\n- 数据集：[百度云](https://pan.baidu.com/s/1qYa13SxFM0AirzDyFMy0mQ) 提取码: 1ybm\n- 三个开源库：[SwanLab](https://github.com/swanhubx/swanlab)、[Gradio](https://github.com/gradio-app/gradio)、[PyTorch](https://github.com/pytorch/pytorch)",
    "227": "一级标题：猫狗分类\n二级标题：1. 准备部分\n内容：\n### 1.1 安装Python库\n需要安装下面这4个库：\n```bash\ntorch>=1.12.0\ntorchvision>=0.13.0\nswanlab\ngradio\n```\n安装命令：\n```bash\npip install torch>=1.12.0 torchvision>=0.13.0 swanlab gradio\n```\n\n### 1.2 创建文件目录\n现在打开1个文件夹，新建下面这5个文件：\n\n![在这里插入图片描述](./cats_dogs/02.png)\n\n它们各自的作用分别是：\n| 文件 | 用途 |\n| --- | --- |\n| `checkpoint` | 这个文件夹用于存储训练过程中生成的模型权重。 |\n| `datasets` | 这个文件夹用于放置数据集。 |\n| `app.py` | 运行Gradio Demo的Python脚本。 |\n| `load_datasets.py` | 负责载入数据集，包含了数据的预处理、加载等步骤，确保数据以适当的格式提供给模型使用。 |\n| `train.py` | 模型训练的核心脚本。它包含了模型的载入、训练循环、损失函数的选择、优化器的配置等关键组成部分，用于指导如何使用数据来训练模型。 |\n\n### 1.3 下载猫狗分类数据集\n\n数据集来源是Modelscope上的[猫狗分类数据集](https://modelscope.cn/datasets/tany0699/cats_and_dogs/summary)，包含275张图像的数据集和70张图像的测试集，一共不到10MB。\n我对数据做了一些整理，所以更推荐使用下面的百度网盘链接下载：\n> 百度网盘：链接: https://pan.baidu.com/s/1qYa13SxFM0AirzDyFMy0mQ 提取码: 1ybm\n\n![在这里插入图片描述](./cats_dogs/03.png)\n\n将数据集放入`datasets`文件夹：\n\n![在这里插入图片描述](./cats_dogs/04.png)\n\nok，现在我们开始训练部分！\n> ps：如果你想要用更大规模的数据来训练猫狗分类模型，请前往文末的相关链接。",
    "228": "一级标题：猫狗分类\n二级标题：2. 训练部分\n内容：\nps：如果想直接看完整代码和效果，可直接跳转到第2.9。\n\n### 2.1 load_datasets.py\n我们首先需要创建1个类`DatasetLoader`，它的作用是完成数据集的读取和预处理，我们将它写在`load_datasets.py`中。\n在写这个类之前，先分析一下数据集。\n在datasets目录下，`train.csv`和`val.csv`分别记录了训练集和测试集的图像相对路径（第一列是图像的相对路径，第二列是标签，0代表猫，1代表狗）：\n![在这里插入图片描述](./cats_dogs/05.png)\n![左图作为train.csv，右图为train文件夹中的cat文件夹中的图像](./cats_dogs/06.png)\n左图作为train.csv，右图为train文件夹中的cat文件夹中的图像。\n\n那么我们的目标就很明确：\n1. 解析这两个csv文件，获取图像相对路径和标签\n2. 根据相对路径读取图像\n3. 对图像做预处理\n4. 返回预处理后的图像和对应标签\n\n明确了目标后，现在我们开始写`DatasetLoader`类：\n\n```python\nimport csv\nimport os\nfrom torchvision import transforms\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n\nclass DatasetLoader(Dataset):\n    def __init__(self, csv_path):\n        self.csv_file = csv_path\n        with open(self.csv_file, 'r') as file:\n            self.data = list(csv.reader(file))\n\n        self.current_dir = os.path.dirname(os.path.abspath(__file__))\n\n    def preprocess_image(self, image_path):\n        full_path = os.path.join(self.current_dir, 'datasets', image_path)\n        image = Image.open(full_path)\n        image_transform = transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n        return image_transform(image)\n\n    def __getitem__(self, index):\n        image_path, label = self.data[index]\n        image = self.preprocess_image(image_path)\n        return image, int(label)\n\n    def __len__(self):\n        return len(self.data)\n   ```\n\n`DatasetLoader`类由四个部分组成：\n1. `__init__`：包含1个输入参数csv_path，在外部传入`csv_path`后，将读取后的数据存入`self.data`中。`self.current_dir`则是获取了当前代码所在目录的绝对路径，为后续读取图像做准备。\n\n2. `preprocess_image`：此函数用于图像预处理。首先，它构造图像文件的绝对路径，然后使用PIL库打开图像。接着，定义了一系列图像变换：调整图像大小至256x256、转换图像为张量、对图像进行标准化处理，最终，返回预处理后的图像。\n\n3. `__getitem__`：当数据集类被循环调用时，`__getitem__`方法会返回指定索引index的数据，即图像和标签。首先，它根据索引从`self.data`中取出图像路径和标签。然后，调用`preprocess_image`方法来处理图像数据。最后，将处理后的图像数据和标签转换为整型后返回。\n\n4. `__len__`：用于返回数据集的总图像数量。\n\n### 2.2 载入数据集\n> 从本节开始，代码将写在train.py中。\n```python\nfrom torch.utils.data import DataLoader\nfrom load_datasets import DatasetLoader\n\nbatch_size = 8\n\nTrainDataset = DatasetLoader(\"datasets/train.csv\")\nValDataset = DatasetLoader(\"datasets/val.csv\")\nTrainDataLoader = DataLoader(TrainDataset, batch_size=batch_size, shuffle=True)\nValDataLoader = DataLoader(ValDataset, batch_size=batch_size, shuffle=False)\n```\n\n我们传入那两个csv文件的路径实例化`DatasetLoader`类，然后用PyTorch的`DataLoader`做一层封装。`DataLoader`可以再传入两个参数：\n- `batch_size`：定义了每个数据批次包含多少张图像。在深度学习中，我们通常不会一次性地处理所有数据，而是将数据划分为小批次。这有助于模型更快地学习，并且还可以节省内存。在这里我们定义batch_size = 8，即每个批次将包含8个图像。\n- `shuffle`：定义了是否在每个循环轮次（epoch）开始时随机打乱数据。这通常用于训练数据集以保证每个epoch的数据顺序不同，从而帮助模型更好地泛化。如果设置为True，那么在每个epoch开始时，数据将被打乱。在这里我们让训练时打乱，测试时不打乱。\n\n### 2.3 载入ResNet50模型\n\n模型我们选用经典的**ResNet50**，模型的具体原理本文就不细说了，重点放在工程实现上。\n我们使用**torchvision**来创建1个resnet50模型，并载入在Imagenet1k数据集上预训练好的权重：\n\n```python\nfrom torchvision.models import ResNet50_Weights\n\n# 加载预训练的ResNet50模型\nmodel = torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n```\n\n因为猫狗分类是个2分类任务，而torchvision提供的resnet50默认是1000分类，所以我们需要把模型最后的全连接层的输出维度替换为2：\n\n```python\nfrom torchvision.models import ResNet50_Weights\n\nnum_classes=2\n\n# 加载预训练的ResNet50模型\nmodel = torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n\n# 将全连接层的输出维度替换为num_classes\nin_features = model.fc.in_features\nmodel.fc = torch.nn.Linear(in_features, num_classes)\n```\n\n### 2.4 设置cuda/mps/cpu\n如果你的电脑是**英伟达显卡**，那么cuda可以极大加速你的训练；\n如果你的电脑是**Macbook Apple Sillicon（M系列芯片）**，那么mps同样可以极大加速你的训练；\n如果都不是，那就选用cpu：\n```python\n#检测是否支持mps\ntry:\n    use_mps = torch.backends.mps.is_available()\nexcept AttributeError:\n    use_mps = False\n\n#检测是否支持cuda\nif torch.cuda.is_available():\n    device = \"cuda\"\nelif use_mps:\n    device = \"mps\"\nelse:\n    device = \"cpu\"\n```\n\n将模型加载到对应的device中：\n\n```python\nmodel.to(torch.device(device))\n```\n\n### 2.5 设置超参数、优化器、损失函数\n\n**超参数**\n设置训练轮次为20轮，学习率为1e-4，训练批次为8，分类数为2分类。\n\n```python\nnum_epochs = 20\nlr = 1e-4\nbatch_size = 8\nnum_classes = 2\n```\n### 损失函数与优化器\n设置损失函数为交叉熵损失，优化器为Adam。\n\n```python\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n```\n\n### 2.6 初始化SwanLab\n\n在训练中我们使用`swanlab`库作为实验管理与指标可视化工具。\n[swanlab](https://github.com/SwanHubX/SwanLab)是一个类似Tensorboard的开源训练图表可视化库，有着更轻量的体积与更友好的API，除了能记录指标，还能自动记录训练的logging、硬件环境、Python环境、训练时间等信息。\n\n![在这里插入图片描述](./cats_dogs/07.png)\n\n#### 2.6.1 设置初始化配置参数\nswanlab库使用`swanlab.init`设置实验名、实验介绍、记录超参数以及日志文件的保存位置。\n后续打开可视化看板需要根据日志文件完成。\n\n```python\nimport swanlab\n\nswanlab.init(\n    # 设置实验名\n    experiment_name=\"ResNet50\",\n    # 设置实验介绍\n    description=\"Train ResNet50 for cat and dog classification.\",\n    # 记录超参数\n    config={\n        \"model\": \"resnet50\",\n        \"optim\": \"Adam\",\n        \"lr\": lr,\n        \"batch_size\": batch_size,\n        \"num_epochs\": num_epochs,\n        \"num_class\": num_classes,\n        \"device\": device,\n    }\n)\n```\n\n#### 2.6.2 跟踪关键指标\nswanlab库使用`swanlab.log`来记录关键指标，具体使用案例见2.7和2.8。\n\n### 2.7 训练函数\n\n我们定义1个训练函数`train`：\n```python\ndef train(model, device, train_dataloader, optimizer, criterion, epoch):\n    model.train()\n    for iter, (inputs, labels) in enumerate(train_loader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        print('Epoch [{}/{}], Iteration [{}/{}], Loss: {:.4f}'.format(epoch, num_epochs, iter + 1, len(TrainDataLoader),\n                                                                      loss.item()))\n        swanlab.log({\"train_loss\": loss.item()})\n```\n\n训练的逻辑很简单：我们循环调用`train_dataloader`，每次取出1个batch_size的图像和标签，传入到resnet50模型中得到预测结果，将结果和标签传入损失函数中计算交叉熵损失，最后根据损失计算反向传播，Adam优化器执行模型参数更新，循环往复。\n在训练中我们最关心的指标是损失值`loss`，所以我们用`swanlab.log`跟踪它的变化。\n\n### 2.8 测试函数\n我们定义1个测试函数`test`：\n```python\ndef test(model, device, test_dataloader, epoch):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in test_dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    accuracy = correct / total * 100\n    print('Accuracy: {:.2f}%'.format(accuracy))\n    swanlab.log({\"test_acc\": accuracy})\n```\n\n测试的逻辑同样很简单：我们循环调用`test_dataloader`，将测试集的图像传入到resnet50模型中得到预测结果，与标签进行对比，计算整体的准确率。\n在测试中我们最关心的指标是准确率`accuracy`，所以我们用`swanlab.log`跟踪它的变化。\n\n### 2.9 完整训练代码\n\n我们一共训练`num_epochs`轮，每4轮进行测试，并在最后保存权重文件：\n\n```python\nfor epoch in range(1, num_epochs + 1):\n    train(model, device, TrainDataLoader, optimizer, criterion, epoch)\n    if epoch % 4 == 0:\n        accuracy = test(model, device, ValDataLoader, epoch)\n\nif not os.path.exists(\"checkpoint\"):\n    os.makedirs(\"checkpoint\")\ntorch.save(model.state_dict(), 'checkpoint/latest_checkpoint.pth')\nprint(\"Training complete\")\n```\n\n组合后的完整`train.py`代码：\n\n```python\nimport torch\nimport torchvision\nfrom torchvision.models import ResNet50_Weights\nimport swanlab\nfrom torch.utils.data import DataLoader\nfrom load_datasets import DatasetLoader\nimport os\n\n# 定义训练函数\ndef train(model, device, train_dataloader, optimizer, criterion, epoch):\n    model.train()\n    for iter, (inputs, labels) in enumerate(train_dataloader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        print('Epoch [{}/{}], Iteration [{}/{}], Loss: {:.4f}'.format(epoch, num_epochs, iter + 1, len(TrainDataLoader),\n                                                                      loss.item()))\n        swanlab.log({\"train_loss\": loss.item()})\n\n\n# 定义测试函数\ndef test(model, device, test_dataloader, epoch):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in test_dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    accuracy = correct / total * 100\n    print('Accuracy: {:.2f}%'.format(accuracy))\n    swanlab.log({\"test_acc\": accuracy})\n\n\nif __name__ == \"__main__\":\n    num_epochs = 20\n    lr = 1e-4\n    batch_size = 8\n    num_classes = 2\n\n    # 设置device\n    try:\n        use_mps = torch.backends.mps.is_available()\n    except AttributeError:\n        use_mps = False\n\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    elif use_mps:\n        device = \"mps\"\n    else:\n        device = \"cpu\"\n\n    # 初始化swanlab\n    swanlab.init(\n        experiment_name=\"ResNet50\",\n        description=\"Train ResNet50 for cat and dog classification.\",\n        config={\n            \"model\": \"resnet50\",\n            \"optim\": \"Adam\",\n            \"lr\": lr,\n            \"batch_size\": batch_size,\n            \"num_epochs\": num_epochs,\n            \"num_class\": num_classes,\n            \"device\": device,\n        }\n    )\n\n    TrainDataset = DatasetLoader(\"datasets/train.csv\")\n    ValDataset = DatasetLoader(\"datasets/val.csv\")\n    TrainDataLoader = DataLoader(TrainDataset, batch_size=batch_size, shuffle=True)\n    ValDataLoader = DataLoader(ValDataset, batch_size=batch_size, shuffle=False)\n\n    # 载入ResNet50模型\n    model = torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n\n    # 将全连接层替换为2分类\n    in_features = model.fc.in_features\n    model.fc = torch.nn.Linear(in_features, num_classes)\n\n    model.to(torch.device(device))\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    # 开始训练\n    for epoch in range(1, num_epochs + 1):\n        train(model, device, TrainDataLoader, optimizer, criterion, epoch)  # Train for one epoch\n\n        if epoch % 4 == 0:  # Test every 4 epochs\n            accuracy = test(model, device, ValDataLoader, epoch)\n\n    # 保存权重文件\n    if not os.path.exists(\"checkpoint\"):\n        os.makedirs(\"checkpoint\")\n    torch.save(model.state_dict(), 'checkpoint/latest_checkpoint.pth')\n    print(\"Training complete\")\n```\n\n### 2.10 开始训练！\n\n🔥实验过程可看这个网页：[猫狗分类｜SwanLab](https://swanlab.cn/@ZeyiLin/Cats_Dogs_Classification/runs/jzo93k112f15pmx14vtxf/chart)\n如果你第一次使用SwanLab，你需要先登录账号，在终端输入：\n\n```bash\nswanlab login\n```\n\n会让你填一个API Key，去[SwanLab](https://swanlab.cn)官网登录一下账号，在设置页面复制API Key，粘贴过来就可以：\n\n![在这里插入图片描述](./cats_dogs/08.png)\n\n然后，我们运行`train.py`：\n\n![在这里插入图片描述](./cats_dogs/09.png)\n\n这时候你会在看到在开头会给到你两个链接，我们点击第一个，里面包含了这个项目的信息和一个对比实验表格：\n\n![在这里插入图片描述](./cats_dogs/10.png)\n\n我们点开1个进行中的实验，会看到train_loss和test_acc整体的变化曲线，以及我们测试集里的图像和它们对应的预测标签：\n\n![在这里插入图片描述](./cats_dogs/11.png)\n\n切换到实验卡片，这里记录了实验的各种信息，包括超参数、最终的实验指标、实验状态、训练时长、Git仓库链接、主机名、操作系统、Python版本、硬件配置等等。\n\n![在这里插入图片描述](./cats_dogs/12.png)\n\n可以看到模型在中已经达到了100%的测试准确率，但是在最后反而拉了——这可能因为过拟合、也可能是常规的波动，就看后续如何优化啦～\n![在这里插入图片描述](./cats_dogs/13.png)",
    "229": "一级标题：猫狗分类\n二级标题：3. Gradio演示程序\n内容：\nGradio是一个开源的Python库，旨在帮助数据科学家、研究人员和从事机器学习领域的开发人员快速创建和共享用于机器学习模型的用户界面。\n在这里我们使用Gradio来构建一个猫狗分类的Demo界面，编写`app.py`程序：\n```python\nimport gradio as gr\nimport torch\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nimport torchvision\n\n\n# 加载与训练中使用的相同结构的模型\ndef load_model(checkpoint_path, num_classes):\n    # 加载预训练的ResNet50模型\n    try:\n        use_mps = torch.backends.mps.is_available()\n    except AttributeError:\n        use_mps = False\n\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    elif use_mps:\n        device = \"mps\"\n    else:\n        device = \"cpu\"\n\n    model = torchvision.models.resnet50(weights=None)\n    in_features = model.fc.in_features\n    model.fc = torch.nn.Linear(in_features, num_classes)\n    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n    model.eval()  # Set model to evaluation mode\n    return model\n\n\n# 加载图像并执行必要的转换的函数\ndef process_image(image, image_size):\n    # Define the same transforms as used during training\n    preprocessing = transforms.Compose([\n        transforms.Resize((image_size, image_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n    image = preprocessing(image).unsqueeze(0)\n    return image\n\n\n# 预测图像类别并返回概率的函数\ndef predict(image):\n    classes = {'0': 'cat', '1': 'dog'}  # Update or extend this dictionary based on your actual classes\n    image = process_image(image, 256)  # Using the image size from training\n    with torch.no_grad():\n        outputs = model(image)\n        probabilities = F.softmax(outputs, dim=1).squeeze()  # Apply softmax to get probabilities\n    # Mapping class labels to probabilities\n    class_probabilities = {classes[str(i)]: float(prob) for i, prob in enumerate(probabilities)}\n    return class_probabilities\n\n\n# 定义到您的模型权重的路径\ncheckpoint_path = 'checkpoint/latest_checkpoint.pth'\nnum_classes = 2\nmodel = load_model(checkpoint_path, num_classes)\n\n# 定义Gradio Interface\niface = gr.Interface(\n    fn=predict,\n    inputs=gr.Image(type=\"pil\"),\n    outputs=gr.Label(num_top_classes=num_classes),\n    title=\"Cat vs Dog Classifier\",\n)\n\nif __name__ == \"__main__\":\n    iface.launch()\n```\n\n运行程序后，会出现以下输出：\n\n![在这里插入图片描述](./cats_dogs/14.png)\n点开链接，出现猫狗分类的Demo网页：\n\n![在这里插入图片描述](./cats_dogs/15.png)\n\n用猫和狗的图片试试：\n\n![在这里插入图片描述](./cats_dogs/16.png)\n\n![在这里插入图片描述](./cats_dogs/17.png)\n\n效果很完美！\n\n至此，我们完成了用PyTorch、SwanLab、Gradio三个开源工具训练1个猫狗分类模型的全部过程，更多想了解的可以参考相关链接或评论此文章。\n\n如果有帮助，请点个赞和收藏吧～",
    "230": "一级标题：猫狗分类\n二级标题：4. 相关链接\n内容：\n- 在线看实验过程：[猫狗分类 · SwanLab](https://swanlab.cn/@ZeyiLin/Cats_Dogs_Classification/runs/jzo93k112f15pmx14vtxf/chart)\n- SwanLab：[Github](https://github.com/SwanHubX/SwanLab)\n- 猫狗分类代码：[Github](https://github.com/xiaolin199912/Resnet50-cats_vs_dogs)\n- 在线Demo：[HuggingFace](https://huggingface.co/spaces/TheEeeeLin/Resnet50-cats_vs_dogs)\n- 猫狗分类数据集（300张图像）：[ModelScope](https://modelscope.cn/datasets/tany0699/cats_and_dogs/summary)\n  - 百度云下载：[链接](https://pan.baidu.com/s/1qYa13SxFM0AirzDyFMy0mQ) 提取码: 1ybm\n- 猫狗分类数据集（10k张图像）：[ModelScope](https://modelscope.cn/datasets/XCsunny/cat_vs_dog_class/summary)",
    "231": "一级标题：CIFAR10 图像分类\n二级标题：无\n内容：\n:::info\n图像分类、机器学习入门\n:::\n\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/CIFAR10/runs/5q3sh20ni2zs6p28ja8qm/chart)",
    "232": "一级标题：CIFAR10 图像分类\n二级标题：概述\n内容：\nCIFAR-10是一个经典的图像分类数据集，包含60,000张32×32像素的彩色图像，分为10个类别（如飞机、汽车、鸟类等），其中50,000张用于训练，10,000张用于测试。\n\n![](./cifar10/dataset.png)\n\nCIFAR-10常被用于图像分类训练任务。该任务是构建模型对输入图像进行10分类，输出每个类别的概率。由于图像分辨率低、背景复杂且数据量有限，该数据集常被用于测试模型的泛化能力和特征提取效果，成为深度学习入门基准。典型方法包括CNN（如ResNet、AlexNet），配合数据增强和交叉熵损失优化，最高准确率可达95%以上。CIFAR-10的轻量级特性使其广泛用于教学和研究，并衍生出更复杂的变体（如CIFAR-100）。\n\nCIFAR-10 包含来自 10 个类别的图像。这些类别包括：\n\n- 飞机 (airplane)\n- 汽车 (automobile)\n- 鸟类 (bird)\n- 猫 (cat)\n- 鹿 (deer)\n- 狗 (dog)\n- 青蛙 (frog)\n- 马 (horse)\n- 船 (ship)\n- 卡车 (truck)\n\n本案例主要：\n\n- 使用`pytorch`进行[ResNet50](https://arxiv.org/abs/1512.03385)(残差神经网络)网络的构建、模型训练与评估\n- 使用`swanlab` 跟踪超参数、记录指标和可视化监控整个训练周期",
    "233": "一级标题：CIFAR10 图像分类\n二级标题：环境安装\n内容：\n本案例基于`Python>=3.8`，请在您的计算机上安装好Python。\n环境依赖：\n```\ntorch\ntorchvision\nswanlab\n```\n快速安装命令：\n```bash\npip install torch torchvision swanlab\n```",
    "234": "一级标题：CIFAR10 图像分类\n二级标题：完整代码\n内容：\n```python\nimport os\nimport random\nimport numpy as np\nimport torch\nfrom torch import nn, optim, utils\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.transforms import ToTensor, Compose, Resize, Lambda\nimport swanlab\n\ndef set_seed(seed=42):\n    \"\"\"设置所有随机种子以确保可重复性\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    # 设置CUDA的随机种子\n    if torch.cuda.is_available():\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n# 捕获并可视化前20张图像\ndef log_images(loader, num_images=16):\n    images_logged = 0\n    logged_images = []\n    for images, labels in loader:\n        # images: batch of images, labels: batch of labels\n        for i in range(images.shape[0]):\n            if images_logged < num_images:\n                # 使用swanlab.Image将图像转换为wandb可视化格式\n                logged_images.append(swanlab.Image(images[i], caption=f\"Label: {labels[i]}\", size=(128, 128)))\n                images_logged += 1\n            else:\n                break\n        if images_logged >= num_images:\n            break\n    swanlab.log({\"Preview/CIFAR10\": logged_images})\n\n\nif __name__ == \"__main__\":\n    # 设置随机种子\n    set_seed(42)\n\n    # 设置device\n    try:\n        use_mps = torch.backends.mps.is_available()\n    except AttributeError:\n        use_mps = False\n\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    elif use_mps:\n        device = \"mps\"\n    else:\n        device = \"cpu\"\n\n    # 初始化swanlab\n    run = swanlab.init(\n        project=\"CIFAR10\",\n        experiment_name=\"resnet50-pretrained\",\n        config={\n            \"model\": \"Resnet50\",\n            \"optim\": \"Adam\",\n            \"lr\": 1e-4,\n            \"batch_size\": 32,\n            \"num_epochs\": 5,\n            \"train_dataset_num\": 45000,\n            \"val_dataset_num\": 5000,\n            \"device\": device,\n            \"num_classes\": 10,\n        },\n    )\n\n    # 定义转换：调整大小并转换为3通道\n    transform = Compose([\n        ToTensor(),\n        Resize((224, 224), antialias=True),  # ResNet期望224x224的输入\n        # Lambda(lambda x: x.repeat(3, 1, 1))  # 将单通道转换为3通道\n    ])\n\n    # 设置训练集、验证集和测试集\n    dataset = CIFAR10(os.getcwd(), train=True, download=True, transform=transform)\n\n    # 确保划分数量正确\n    total_size = len(dataset)  # 应该是50000\n    train_dataset, val_dataset = utils.data.random_split(\n        dataset,\n        [run.config.train_dataset_num, run.config.val_dataset_num],\n        generator=torch.Generator().manual_seed(42)  # 保持划分的随机性一致\n    )\n\n    train_loader = utils.data.DataLoader(train_dataset, batch_size=run.config.batch_size, shuffle=True)\n    val_loader = utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False)\n\n    # 初始化模型、损失函数和优化器\n    if run.config.model == \"Resnet18\":\n        from torchvision.models import resnet18\n        model = resnet18(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n    elif run.config.model == \"Resnet34\":\n        from torchvision.models import resnet34\n        model = resnet34(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n    elif run.config.model == \"Resnet50\":\n        from torchvision.models import resnet50\n        model = resnet50(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n    elif run.config.model == \"Resnet101\":\n        from torchvision.models import resnet101\n        model = resnet101(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n    elif run.config.model == \"Resnet152\":\n        from torchvision.models import resnet152\n        model = resnet152(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n\n    model.to(torch.device(device))\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=run.config.lr)\n\n    # （可选）看一下数据集的前16张图像\n    log_images(train_loader, 8)\n\n    # 开始训练\n    for epoch in range(1, run.config.num_epochs+1):\n        swanlab.log({\"train/epoch\": epoch}, step=epoch)\n        model.train()  # 确保模型处于训练模式\n        train_correct = 0\n        train_total = 0\n\n        # 训练循环\n        for iter, batch in enumerate(train_loader):\n            x, y = batch\n            x, y = x.to(device), y.to(device)\n            optimizer.zero_grad()\n            output = model(x)\n            loss = criterion(output, y)\n            loss.backward()\n            optimizer.step()\n\n            # 计算训练准确率\n            _, predicted = torch.max(output, 1)\n            train_total += y.size(0)\n            train_correct += (predicted == y).sum().item()\n\n            if iter % 40 == 0:\n                print(\n                    f\"Epoch [{epoch}/{run.config.num_epochs}], Iteration [{iter + 1}/{len(train_loader)}], Loss: {loss.item()}\"\n                )\n                swanlab.log({\"train/loss\": loss.item()}, step=(epoch - 1) * len(train_loader) + iter)\n\n        # 记录每个epoch的训练准确率\n        train_accuracy = train_correct / train_total\n        swanlab.log({\"train/acc\": train_accuracy}, step=(epoch - 1) * len(train_loader) + iter)\n\n        # 评估\n        model.eval()\n        correct = 0\n        total = 0\n        val_loss = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                x, y = batch\n                x, y = x.to(device), y.to(device)\n                output = model(x)\n                # 计算验证损失\n                loss = criterion(output, y)\n                val_loss += loss.item()\n                # 计算验证准确率\n                _, predicted = torch.max(output, 1)\n                total += y.size(0)\n                correct += (predicted == y).sum().item()\n\n        accuracy = correct / total\n        avg_val_loss = val_loss / len(val_loader)\n        swanlab.log({\n            \"val/acc\": accuracy,\n            \"val/loss\": avg_val_loss,\n            }, step=(epoch - 1) * len(train_loader) + iter)\n```",
    "235": "一级标题：CIFAR10 图像分类\n二级标题：切换其他ResNet模型\n内容：\n上面的代码支持切换以下ResNet模型：\n- ResNet18\n- ResNet34\n- ResNet50\n- ResNet101\n- ResNet152\n\n切换方式非常简单，只需要将`config`的`model`参数修改为对应的模型名称即可，如切换为ResNet50：\n\n```python (5)\n    # 初始化swanlab\n    run = swanlab.init(\n        ...\n        config={\n            \"model\": \"Resnet50\",\n        ...\n        },\n    )\n```\n\n- `config`是如何发挥作用的？ 👉 [设置实验配置](/guide_cloud/experiment_track/set-experiment-config)",
    "236": "一级标题：CIFAR10 图像分类\n二级标题：效果演示\n内容：\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/CIFAR10/runs/5q3sh20ni2zs6p28ja8qm/chart)\n\n![](./cifar10/show.png)",
    "237": "一级标题：DQN-CartPole\n二级标题：无\n内容：\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/RL-All-In-One/runs/vjbnl6y3l99k0sqrd0f2s/chart)\n\n\n> 训练过程：[RL-All-In-One](https://swanlab.cn/@ZeyiLin/RL-All-In-One/runs/vjbnl6y3l99k0sqrd0f2s/chart)\n>\n> 代码：[Zeyi-Lin/SwanBook-RL](https://github.com/Zeyi-Lin/SwanBook-RL/blob/main/dqn-cartpole.py)\n>\n> 硬件环境：纯CPU可训，实测M1 Max训练3分30秒",
    "238": "一级标题：DQN-CartPole\n二级标题：一、什么是DQN？\n内容：\nDQN（Deep Q-Network，深度Q网络）是Q-Learning的**深度学习扩展**，通过神经网络替代Q表的方式来解决高维状态空间问题（例如图像输入），开启了**深度强化学习时代**。它在2013年由DeepMind提出，并在**Atari**游戏上取得了突破性表现。\n\n![image](https://imagebed-1301372061.cos.ap-beijing.myqcloud.com/blogs/image-20250207132403-hte9grx.png)\n\n传统的Q-Learning方法很好，但是Q表是个离散的结构，无法处理状态是连续的任务；以及一些状态空间巨大的任务（比如视频游戏），Q表的开销也是无法接受的，所以DQN应运而生。DQN用神经网络（称为QNet）**近似Q函数**，输入状态S，输出所有动作的Q值。\n\n**DQN还做了以下改进：**\n\n1. **经验回放（Experience Replay）** ：存储历史经验(st,at,rt+1,st+1)(st,at,rt+1,st+1)到缓冲区，训练时随机采样，打破数据相关性。\n2. **目标网络（Target Network）** ：使用独立的网络计算目标Q值，减少训练波动。\n3. **端到端训练**：直接从原始输入（如像素）学习，无需人工设计状态特征。\n\n具体DQN原理本文不做过多赘述，结合本文提供的代码和网上其他教程/DeepSeek R1学习，会有更好效果。",
    "239": "一级标题：DQN-CartPole\n二级标题：二、什么是CartPole推车倒立摆任务？\n内容：\n**CartPole（推车倒立摆）**  是强化学习中经典的基准测试任务，因为其直观可视、方便调试、状态和动作空间小等特性，常用于入门教学和算法验证。它的目标是训练一个智能体（agent）通过左右移动小车，使车顶的杆子尽可能长时间保持竖直不倒。\n\n![image](https://imagebed-1301372061.cos.ap-beijing.myqcloud.com/blogs/20250207134541.png)\n\n* **环境**：小车（cart）可以在水平轨道上左右移动，顶部通过关节连接一根自由摆动的杆子（pole）。\n* **目标**：通过左右移动小车，使杆子的倾斜角度不超出阈值（±12°或±15°），同时小车不超出轨道范围（如轨道长度的±2.4单位）。简单理解为，就是杆子不会倒下里，小车不会飞出屏幕。\n* **状态**：状态空间包含4个连续变量，分别是小车位置（x），小车速度（v），杆子角度（θ），杆子角速度（ω）\n* **动作**：动作空间只有2个离线动作，分别是0（向左移动）或1（向右移动）\n* **奖励机制**：每成功保持杆子不倒+1分，目前是让奖励最大化，即杆子永远不倒\n\n使用`gymnasium`库，启动cartpole环境非常容易，下面是一个简单的示例代码：\n\n```python\nimport gymnasium as gym\n\nenv = gym.make(\"CartPole-v1\")\nstate = env.reset()\ndone = False\n\nwhile not done:\n    action = 0 if state[2] < 0 else 1  # 根据杆子角度简单决策\n    next_state, reward, done, _ = env.step(action)\n    state = next_state\n    env.render()\n```",
    "240": "一级标题：DQN-CartPole\n二级标题：三、安装环境\n内容：\n首先你需要1个Python>=3.8的环境，然后安装下面的库：\n\n```txt\nswanlab\ngymnasium\nnumpy\ntorch\npygame\nmoviepy\n```\n\n一键安装命令：\n\n```bash\npip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\npip install swanlab gymnasium numpy torch pygame moviepy\n```",
    "241": "一级标题：DQN-CartPole\n二级标题：四、定义QNet\n内容：\nDQN使用神经网络来近似QLearning中的Q表，这个神经网络被称为QNetwork。\n\nQNetwork的输入是状态向量，输出是动作向量，这里用一个非常简单的神经网络：\n\n```python\nclass QNetwork(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(state_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, action_dim)\n        )\n        self.to(device)  # 将网络移到指定设备\n\n    def forward(self, x):\n        return self.fc(x)\n```",
    "242": "一级标题：DQN-CartPole\n二级标题：五、定义DQNAgent\n内容：\nDQNAgent定义了一系列强化学习训练的行为，代码略长，我拿部分内容进行解读：\n\n### 初始配置\n\n```python\nclass DQNAgent:\n    def __init__(self, state_dim, action_dim):\n        self.q_net = QNetwork(state_dim, action_dim)       # 当前网络\n        self.target_net = QNetwork(state_dim, action_dim)  # 目标网络\n        self.target_net.load_state_dict(self.q_net.state_dict())  # 将目标网络和当前网络初始化一致，避免网络不一致导致的训练波动\n        self.optimizer = optim.Adam(self.q_net.parameters(), lr=1e-3)\n        self.replay_buffer = deque(maxlen=10000)           # 经验回放缓冲区\n\t\tself.update_target_freq = 100\n```\n\nDQN会定义2个神经网络，分别是q_net和target_net，结构是完全相同的。训练过程中，target_net负责计算预期值，即 **reward + target_net(next_state).max(1)[0]** ，q_net负责计算当前值，训练时将两个值送到MSELoss里计算差值，反向传播后更新q_net的参数；每过update_target_freq步，将q_net的参数赋给target_net。\n\n优化器使用Adam；经验回访缓冲区是最大长度为10000的队列，用于存储历史经验用于训练。\n\n### 动作选择（ε-贪婪策略）\n\n动作选择的ε-贪婪策略，指的是在当前状态下，选择下一个动作时的两种方式：\n\nA. 随机选择1个动作，这种被称为探索\n\nB. 按照先前训练得到的知识选择动作。\n\n在强化学习训练中，每一步会以epsilon（即ε）的概率选择A，否则选择B：\n\n```python\n    def choose_action(self, state):\n        if np.random.rand() < self.epsilon:\n            return np.random.randint(0, 2)  # CartPole有2个动作（左/右）\n        else:\n            state_tensor = torch.FloatTensor(state).to(device)\n            q_values = self.q_net(state_tensor)\n            return q_values.cpu().detach().numpy().argmax()\n```\n\n在训练中，开始时以高概率随机探索环境，逐渐转向利用学到的知识。",
    "243": "一级标题：DQN-CartPole\n二级标题：六、完整代码\n内容：\n**下面是DQN训练的完整代码，做了这些事：**\n\n1. 开启gymnasium中的CartPole环境\n2. QAgent按照ε-贪婪策略选择动作，更新状态，训练模型更新q_net参数\n3. 每隔固定的步数，同步target_net的参数\n4. 一共训练600轮，每10轮会进行一次评估，并使用swanlab记录参数\n5. 保存评估时最高reward的模型权重\n6. 使用了经验回放与ε衰减策略\n7. 训练完成后，进行测试，并保存测试视频到本地目录下\n\n**完整代码如下：**\n\n```python\nimport gymnasium as gym\nfrom gymnasium.wrappers import RecordVideo\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom collections import deque\nimport random\nimport swanlab\nimport os\n\n# 设置随机数种子\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# 定义Q网络\nclass QNetwork(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(state_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, action_dim)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n\n# DQN Agent\nclass DQNAgent:\n    def __init__(self, state_dim, action_dim):\n        self.q_net = QNetwork(state_dim, action_dim)       # 当前网络\n        self.target_net = QNetwork(state_dim, action_dim)  # 目标网络\n        self.target_net.load_state_dict(self.q_net.state_dict())  # 将目标网络和当前网络初始化一致，避免网络不一致导致的训练波动\n        self.best_net = QNetwork(state_dim, action_dim)\n        self.optimizer = optim.Adam(self.q_net.parameters(), lr=1e-3)\n        self.replay_buffer = deque(maxlen=10000)           # 经验回放缓冲区\n        self.batch_size = 64\n        self.gamma = 0.99\n        self.epsilon = 0.1\n        self.update_target_freq = 100  # 目标网络更新频率\n        self.step_count = 0\n        self.best_reward = 0\n        self.best_avg_reward = 0\n        self.eval_episodes = 5  # 评估时的episode数量\n\n    def choose_action(self, state):\n        if np.random.rand() < self.epsilon:\n            return np.random.randint(0, 2)  # CartPole有2个动作（左/右）\n        else:\n            state_tensor = torch.FloatTensor(state)\n            q_values = self.q_net(state_tensor)\n            return q_values.cpu().detach().numpy().argmax()\n\n    def store_experience(self, state, action, reward, next_state, done):\n        self.replay_buffer.append((state, action, reward, next_state, done))\n\n    def train(self):\n        if len(self.replay_buffer) < self.batch_size:\n            return\n\n        # 从缓冲区随机采样\n        batch = random.sample(self.replay_buffer, self.batch_size)\n        states, actions, rewards, next_states, dones = zip(*batch)\n\n        states = torch.FloatTensor(np.array(states))\n        actions = torch.LongTensor(actions)\n        rewards = torch.FloatTensor(rewards)\n        next_states = torch.FloatTensor(np.array(next_states))\n        dones = torch.FloatTensor(dones)\n\n        # 计算当前Q值\n        current_q = self.q_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n\n        # 计算目标Q值（使用目标网络）\n        with torch.no_grad():\n            next_q = self.target_net(next_states).max(1)[0]\n            target_q = rewards + self.gamma * next_q * (1 - dones)\n\n        # 计算损失并更新网络\n        loss = nn.MSELoss()(current_q, target_q)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        # 定期更新目标网络\n        self.step_count += 1\n        if self.step_count % self.update_target_freq == 0:\n            # 使用深拷贝更新目标网络参数\n            self.target_net.load_state_dict({\n                k: v.clone() for k, v in self.q_net.state_dict().items()\n            })\n\n    def save_model(self, path=\"./output/best_model.pth\"):\n        if not os.path.exists(\"./output\"):\n            os.makedirs(\"./output\")\n        torch.save(self.q_net.state_dict(), path)\n        print(f\"Model saved to {path}\")\n\n    def evaluate(self, env):\n        \"\"\"评估当前模型的性能\"\"\"\n        original_epsilon = self.epsilon\n        self.epsilon = 0  # 关闭探索\n        total_rewards = []\n\n        for _ in range(self.eval_episodes):\n            state = env.reset()[0]\n            episode_reward = 0\n            while True:\n                action = self.choose_action(state)\n                next_state, reward, done, _, _ = env.step(action)\n                episode_reward += reward\n                state = next_state\n                if done or episode_reward > 2e4:\n                    break\n            total_rewards.append(episode_reward)\n\n        self.epsilon = original_epsilon  # 恢复探索\n        return np.mean(total_rewards)\n\n# 训练过程\nenv = gym.make('CartPole-v1')\nstate_dim = env.observation_space.shape[0]\naction_dim = env.action_space.n\nagent = DQNAgent(state_dim, action_dim)\n\n\n# 初始化SwanLab日志记录器\nswanlab.init(\n    project=\"RL-All-In-One\",\n    experiment_name=\"DQN-CartPole-v1\",\n    config={\n        \"state_dim\": state_dim,\n        \"action_dim\": action_dim,\n        \"batch_size\": agent.batch_size,\n        \"gamma\": agent.gamma,\n        \"epsilon\": agent.epsilon,\n        \"update_target_freq\": agent.update_target_freq,\n        \"replay_buffer_size\": agent.replay_buffer.maxlen,\n        \"learning_rate\": agent.optimizer.param_groups[0]['lr'],\n        \"episode\": 600,\n        \"epsilon_start\": 1.0,\n        \"epsilon_end\": 0.01,\n        \"epsilon_decay\": 0.995,\n    },\n    description=\"增加了初始化目标网络和当前网络一致，避免网络不一致导致的训练波动\"\n)\n\n# ========== 训练阶段 ==========\n\nagent.epsilon = swanlab.config[\"epsilon_start\"]\n\nfor episode in range(swanlab.config[\"episode\"]):\n    state = env.reset()[0]\n    total_reward = 0\n\n    while True:\n        action = agent.choose_action(state)\n        next_state, reward, done, _, _ = env.step(action)\n        agent.store_experience(state, action, reward, next_state, done)\n        agent.train()\n\n        total_reward += reward\n        state = next_state\n        if done or total_reward > 2e4:\n            break\n\n    # epsilon是探索系数，随着每一轮训练，epsilon 逐渐减小\n    agent.epsilon = max(swanlab.config[\"epsilon_end\"], agent.epsilon * swanlab.config[\"epsilon_decay\"])\n\n    # 每10个episode评估一次模型\n    if episode % 10 == 0:\n        eval_env = gym.make('CartPole-v1')\n        avg_reward = agent.evaluate(eval_env)\n        eval_env.close()\n\n        if avg_reward > agent.best_avg_reward:\n            agent.best_avg_reward = avg_reward\n            # 深拷贝当前最优模型的参数\n            agent.best_net.load_state_dict({k: v.clone() for k, v in agent.q_net.state_dict().items()})\n            agent.save_model(path=f\"./output/best_model.pth\")\n            print(f\"New best model saved with average reward: {avg_reward}\")\n\n    print(f\"Episode: {episode}, Train Reward: {total_reward}, Best Eval Avg Reward: {agent.best_avg_reward}\")\n\n    swanlab.log(\n        {\n            \"train/reward\": total_reward,\n            \"eval/best_avg_reward\": agent.best_avg_reward,\n            \"train/epsilon\": agent.epsilon\n        },\n        step=episode,\n    )\n\n# 测试并录制视频\nagent.epsilon = 0  # 关闭探索策略\ntest_env = gym.make('CartPole-v1', render_mode='rgb_array')\ntest_env = RecordVideo(test_env, \"./dqn_videos\", episode_trigger=lambda x: True)  # 保存所有测试回合\nagent.q_net.load_state_dict(agent.best_net.state_dict())  # 使用最佳模型\n\nfor episode in range(3):  # 录制3个测试回合\n    state = test_env.reset()[0]\n    total_reward = 0\n    steps = 0\n\n    while True:\n        action = agent.choose_action(state)\n        next_state, reward, done, _, _ = test_env.step(action)\n        total_reward += reward\n        state = next_state\n        steps += 1\n\n        # 限制每个episode最多1500步,约30秒,防止录制时间过长\n        if done or steps >= 1500:\n            break\n\n    print(f\"Test Episode: {episode}, Reward: {total_reward}\")\n\ntest_env.close()\n```\n\n---\n\n训练用的是SwanLab的记录过程，能更好地分析和总结知识。\n\n在开始训练之前，如果你没有使用过[SwanLab](https://swanlab.cn)，需要去它的官网（[https://swanlab.cn](https://swanlab.cn)）注册一下，然后按下面的步骤复制API Key：\n\n![image](https://imagebed-1301372061.cos.ap-beijing.myqcloud.com/blogs/20250207150845.png)\n\n接下来打开命令行，敲下面的命令：\n\n```python\nswanlab login\n```\n\n在弹出的提示中，把API Key粘贴进去（粘贴进去不会显示任何东西，放心这是正常的），然后按回车，登录完毕！\n\n然后，就可以运行训练代码了。",
    "244": "一级标题：DQN-CartPole\n二级标题：七、训练结果\n内容：\n训练过程可以看：[RL-All-In-One - SwanLab](https://swanlab.cn/@ZeyiLin/RL-All-In-One/runs/vjbnl6y3l99k0sqrd0f2s/chart)\n\n我的机器是Macbook M1 Max，大概训练了3分30秒。\n\n![image](https://imagebed-1301372061.cos.ap-beijing.myqcloud.com/blogs/20250207151927.png)\n\n可以看到train的reward波动的很厉害，因为随机探索的缘故，但eval（关闭随机探索）可以看到是很快达到了20000分的上限。\n\n下面是训练好的Agent控制倒立摆的30s视频：\n\n<video controls src=\"/assets/rl-video-episode-0.mp4\"></video>",
    "245": "一级标题：FashionMNIST\n二级标题：无\n内容：\n:::info\n图像分类、机器学习入门、灰度图像\n:::\n\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/FashionMNIST/overview)",
    "246": "一级标题：FashionMNIST\n二级标题：概述\n内容：\nFashionMNIST 是一个广泛用于测试机器学习算法的图像数据集，特别是在图像识别领域。它由 Zalando 发布，旨在替代传统的 MNIST 数据集，后者主要包含手写数字的图片。FashionMNIST 的设计初衷是提供一个稍微更具挑战性的问题，同时保持与原始 MNIST 数据集相同的图像大小（28x28 像素）和结构（训练集60,000张图片，测试集10,000张图片）。\n\n![fashion-mnist](/assets/example-fashionmnist.png)\n\nFashionMNIST 包含来自 10 个类别的服装和鞋类商品的灰度图像。这些类别包括：\n\n1. T恤/上衣（T-shirt/top）\n2. 裤子（Trouser）\n3. 套头衫（Pullover）\n4. 裙子（Dress）\n5. 外套（Coat）\n6. 凉鞋（Sandal）\n7. 衬衫（Shirt）\n8. 运动鞋（Sneaker）\n9. 包（Bag）\n10. 短靴（Ankle boot）\n\n每个类别都有相同数量的图像，使得这个数据集成为一个平衡的数据集。这些图像的简单性和标准化尺寸使得 FashionMNIST 成为计算机视觉和机器学习领域入门级的理想选择。数据集被广泛用于教育和研究，用于测试各种图像识别方法的效果。\n\n本案例主要：\n\n- 使用`pytorch`进行[ResNet34](https://arxiv.org/abs/1512.03385)(残差神经网络)网络的构建、模型训练与评估\n- 使用`swanlab` 跟踪超参数、记录指标和可视化监控整个训练周期",
    "247": "一级标题：FashionMNIST\n二级标题：环境安装\n内容：\n本案例基于`Python>=3.8`，请在您的计算机上安装好Python。\n环境依赖：\n```\ntorch\ntorchvision\nswanlab\n```\n快速安装命令：\n```bash\npip install torch torchvision swanlab\n```",
    "248": "一级标题：FashionMNIST\n二级标题：完整代码\n内容：\n```python\nimport os\nimport random\nimport numpy as np\nimport torch\nfrom torch import nn, optim, utils\nfrom torchvision.datasets import FashionMNIST\nfrom torchvision.transforms import ToTensor, Compose, Resize, Lambda\nimport swanlab\n\n\ndef set_seed(seed=42):\n    \"\"\"设置所有随机种子以确保可重复性\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    # 设置CUDA的随机种子\n    if torch.cuda.is_available():\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n# 捕获并可视化前20张图像\ndef log_images(loader, num_images=16):\n    images_logged = 0\n    logged_images = []\n    for images, labels in loader:\n        # images: batch of images, labels: batch of labels\n        for i in range(images.shape[0]):\n            if images_logged < num_images:\n                # 使用swanlab.Image将图像转换为wandb可视化格式\n                logged_images.append(swanlab.Image(images[i], caption=f\"Label: {labels[i]}\", size=(128, 128)))\n                images_logged += 1\n            else:\n                break\n        if images_logged >= num_images:\n            break\n    swanlab.log({\"Preview/FashionMNIST\": logged_images})\n\n\nif __name__ == \"__main__\":\n    # 设置随机种子\n    set_seed(42)\n\n    # 设置device\n    try:\n        use_mps = torch.backends.mps.is_available()\n    except AttributeError:\n        use_mps = False\n\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    elif use_mps:\n        device = \"mps\"\n    else:\n        device = \"cpu\"\n\n    # 初始化swanlab\n    run = swanlab.init(\n        project=\"FashionMNIST\",\n        experiment_name=\"resnet50\",\n        config={\n            \"model\": \"Resnet50\",\n            \"optim\": \"Adam\",\n            \"lr\": 1e-4,\n            \"batch_size\": 32,\n            \"num_epochs\": 10,\n            \"train_dataset_num\": 55000,\n            \"val_dataset_num\": 5000,\n            \"device\": device,\n            \"num_classes\": 10,\n        },\n    )\n\n    # 定义转换：调整大小并转换为3通道\n    transform = Compose([\n        ToTensor(),\n        Resize((224, 224), antialias=True),  # ResNet期望224x224的输入\n        Lambda(lambda x: x.repeat(3, 1, 1))  # 将单通道转换为3通道\n    ])\n\n    # 设置训练集、验证集和测试集\n    dataset = FashionMNIST(os.getcwd(), train=True, download=True, transform=transform)\n    train_dataset, val_dataset = utils.data.random_split(\n        dataset, [run.config.train_dataset_num, run.config.val_dataset_num]\n    )\n\n    train_loader = utils.data.DataLoader(train_dataset, batch_size=run.config.batch_size, shuffle=True)\n    val_loader = utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False)\n\n    # 初始化模型、损失函数和优化器\n    if run.config.model == \"Resnet18\":\n        from torchvision.models import resnet18\n        model = resnet18(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n    elif run.config.model == \"Resnet34\":\n        from torchvision.models import resnet34\n        model = resnet34(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n    elif run.config.model == \"Resnet50\":\n        from torchvision.models import resnet50\n        model = resnet50(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n    elif run.config.model == \"Resnet101\":\n        from torchvision.models import resnet101\n        model = resnet101(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n    elif run.config.model == \"Resnet152\":\n        from torchvision.models import resnet152\n        model = resnet152(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n\n    model.to(torch.device(device))\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=run.config.lr)\n\n    # （可选）看一下数据集的前8张图像\n    log_images(train_loader, 8)\n\n    # 开始训练\n    for epoch in range(1, run.config.num_epochs+1):\n        swanlab.log({\"train/epoch\": epoch}, step=epoch)\n        model.train()  # 确保模型处于训练模式\n        train_correct = 0\n        train_total = 0\n\n        # 训练循环\n        for iter, batch in enumerate(train_loader):\n            x, y = batch\n            x, y = x.to(device), y.to(device)\n            optimizer.zero_grad()\n            output = model(x)\n            loss = criterion(output, y)\n            loss.backward()\n            optimizer.step()\n\n            # 计算训练准确率\n            _, predicted = torch.max(output, 1)\n            train_total += y.size(0)\n            train_correct += (predicted == y).sum().item()\n\n            if iter % 40 == 0:\n                print(\n                    f\"Epoch [{epoch}/{run.config.num_epochs}], Iteration [{iter + 1}/{len(train_loader)}], Loss: {loss.item()}\"\n                )\n                swanlab.log({\"train/loss\": loss.item()}, step=(epoch - 1) * len(train_loader) + iter)\n\n        # 记录每个epoch的训练准确率\n        train_accuracy = train_correct / train_total\n        swanlab.log({\"train/acc\": train_accuracy}, step=(epoch - 1) * len(train_loader) + iter)\n\n        #\n        model.eval()\n        correct = 0\n        total = 0\n        val_loss = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                x, y = batch\n                x, y = x.to(device), y.to(device)\n                output = model(x)\n                # 计算验证损失\n                loss = criterion(output, y)\n                val_loss += loss.item()\n                # 计算验证准确率\n                _, predicted = torch.max(output, 1)\n                total += y.size(0)\n                correct += (predicted == y).sum().item()\n\n        accuracy = correct / total\n        avg_val_loss = val_loss / len(val_loader)\n        swanlab.log({\n            \"val/acc\": accuracy,\n            \"val/loss\": avg_val_loss,\n            }, step=(epoch - 1) * len(train_loader) + iter)\n```",
    "249": "一级标题：FashionMNIST\n二级标题：切换其他ResNet模型\n内容：\n上面的代码支持切换以下ResNet模型：\n- ResNet18\n- ResNet34\n- ResNet50\n- ResNet101\n- ResNet152\n\n切换方式非常简单，只需要将`config`的`model`参数修改为对应的模型名称即可，如切换为ResNet50：\n\n```python (5)\n    # 初始化swanlab\n    run = swanlab.init(\n        ...\n        config={\n            \"model\": \"Resnet50\",\n        ...\n        },\n    )\n```\n\n- `config`是如何发挥作用的？ 👉 [设置实验配置](/guide_cloud/experiment_track/set-experiment-config)",
    "250": "一级标题：FashionMNIST\n二级标题：效果演示\n内容：\n![](/assets/example-fashionmnist-show.jpg)",
    "251": "一级标题：使用ChatGLM4进行大模型指令遵从微调（附代码和测试脚本）\n二级标题：无\n内容：\n[![](/assets/badge1.svg)](https://swanlab.cn/@ShaohonChen/chatglm-finetune/)\n\n作者：情感机器实验室-陈少宏 邮箱：<shaohon_chen@115lab.club>\n\n[[toc]]",
    "252": "一级标题：使用ChatGLM4进行大模型指令遵从微调（附代码和测试脚本）\n二级标题：摘要\n内容：\n![instruct](./images/glm4-instruct/instruct.png)\n\n本教程主要实现了一个大模型的指令遵从微调方法。为了便于实现，减少代码量，本文使用了🤗HuggingFace的TRL框架实现。该框架除了支持SFT外，对DPO、PPO、GRPO等流行的强化微调算法都有很好的支持。\n\n虽然使用框架能够极大的减少工作量，但是不可避免的为新手学习带来了困扰。因此本教程会尽量附上完整的文档引用来帮助读者进一步学习框架。诚然从使用pytorch实现微调过程能够极大的提升对过程的理解，社区也有相当多优秀的项目。但是笔者仍推荐大家多使用框架来完成训练，这样可以减少大量的时间来让大家更专注于创新。\n\n因此本教程建议对🤗HuggingFace Transformers框架有一定基础的读者阅读～。\n\n注意：由于ChatGLM的模型相对较大，实际运行大概需要显存>=16G\n\n🎉 **SwanLab被官方集成进入了🤗HuggingFace Transformers：** 如果本地环境安装了SwanLab会默认开启！也可以通过`report_to=\"swanlab\"`开启训练跟踪。\n\n![swanlabxhuggingface](./images/glm4-instruct/swanlabxhuggingface.png)\n\n**参考资料：**\n\n* 智谱AI官网：[https://www.zhipuai.cn/](https://www.zhipuai.cn/)\n\n* ChatGLM-9B基座模型：[https://huggingface.co/THUDM/glm-4-9b-hf](https://huggingface.co/THUDM/glm-4-9b-hf/tree/main)\n\n* ChatGLM-9B-Chat模型：[https://huggingface.co/THUDM/glm-4-9b-chat-hf](https://huggingface.co/THUDM/glm-4-9b-chat-hf/tree/main)\n\n* Alpaca数据集中文版：[https://huggingface.co/datasets/llamafactory/alpaca_gpt4_zh](https://huggingface.co/datasets/llamafactory/alpaca_gpt4_zh)\n\n* 本博客开源项目链接：[https://github.com/SwanHubX/glm4-finetune](https://github.com/SwanHubX/glm4-finetune)\n\n* SwanLab训练日志查看：[https://swanlab.cn/@ShaohonChen/chatglm-finetune/](https://swanlab.cn/@ShaohonChen/chatglm-finetune/)",
    "253": "一级标题：使用ChatGLM4进行大模型指令遵从微调（附代码和测试脚本）\n二级标题：TRL包介绍+环境准备\n内容：\n![trl](./images/glm4-instruct/trl.png)\n\n本教程使用[🤗HuggingFace TRL](https://huggingface.co/docs/trl/index)框架来完成微调代码的实现。TRL是一个强大且便于使用的微调框架，除了支持SFT外，也能轻松的通过接口调用DPO、PPO、GRPO等流行的强化微调算法。此外也完美兼容Transformers架构。\n\n首先是安装本教程的环境，安装命令如下：\n\n```bash\npip install transformers trl datasets peft swanlab\n```\n\n其中`transformers trl peft`用于模型的加载和训练，`datasets`用于导入数据集，`swanlab`用于对训练过程可视化跟踪。\n\n下面列举一个简单的微调案例来介绍HF TRL框架的使用方法：\n\n```python\nfrom datasets import load_dataset\nfrom trl import SFTConfig, SFTTrainer\n\ndataset = load_dataset(\"stanfordnlp/imdb\", split=\"train\")   # 设置微调数据集，此处使用IMDB电影评论分类数据\n\ntraining_args = SFTConfig(  # 设置微调参数\n    max_length=512,\n    output_dir=\"/tmp\",\n)\ntrainer = SFTTrainer(   # 设置模型，此处使用facebook的opt-350M，参数量比较小便于下载\n    \"facebook/opt-350m\",\n    train_dataset=dataset,\n    args=training_args,\n)\ntrainer.train() # 开始训练，流程和TRL一样\n```\n\n上面的代码来自HF官方文档[https://huggingface.co/docs/trl/sft_trainer](https://huggingface.co/docs/trl/sft_trainer)，增加了注释便于读者理解。\n\n简单来说TRL包的使用方法和Transformers类似，不过多了两步：\n\n* 导入`SFTConfig`模块，这个模块基于`transformers`的`TrainingArguments`，不过针对SFT引入了一点额外的参数，以及lora的支持参数\n\n* 导入`SFTTrainer`模块，这个模块包含了SFT的代码实现，还有一些对`peft`的lora支持和数据集格式转换代码。\n\n后文将完整的介绍如何使用TRL包完成大模型的指令遵从功能。",
    "254": "一级标题：使用ChatGLM4进行大模型指令遵从微调（附代码和测试脚本）\n二级标题：GLM4介绍+模型准备\n内容：\n![chatglm_history](images/glm4-instruct/chatglm_history.png)\n\nGLM-4-9B是[智谱AI](https://www.zhipuai.cn/)推出的最新一代预训练模型GLM-4系列中的开源版本。ChatGLM发布了多个版本，其中GLM-4-9B是第四代基座模型，其微调版本GLM-4-9B-Chat具备网页浏览、代码执行、自定义工具调用（Function Call）和长文本推理（支持最大 128K 上下文）等高级功能。\n\n本教程使用GLM-4-9B模型进行指令遵从功能微调，并使用SwanLab进行模型的结果跟踪。\n\n⚠️注意：ChatGLM为了配合Huggingface Transformers更新，发布了两个版本权重`THUDM/glm-4-9b`和`THUDM/glm-4-9b-hf`，后者对应更为新版本的transformers，因此本教程使用后者的权重。\n\n本教程以经提供好了下载模型的脚本，下载模型的方法如下：\n\n```bash\nhuggingface-cli download --local-dir ./weights/glm-4-9b-hf THUDM/glm-4-9b-hf\n```\n\n模型将会下载在项目目录下的`./weights/glm-4-9b-hf`中\n\n下面列举一个使用`transformers`加载ChatGLM模型并进行推理的代码：\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ndevice = \"cuda\"\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/glm-4-9b-chat-hf\")\nmodel = AutoModelForCausalLM.from_pretrained(\"THUDM/glm-4-9b-chat-hf\").eval().to(device)\ninputs = tokenizer.encode(\"我是ChatGLM，是\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n```\n\n由于是基座模型，没经过微调，因此模型只会完成`\"我是ChatGLM，是\"`这段文本的后续补全，运行后会生成如下代码：\n\n```bash\nLoading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.35it/s]\n[gMASK]<sop>我是ChatGLM，是人工智能助手。我是ChatGLM，是人工智能助手。我是ChatGLM，是人工智能助手\n```\n\n当然上面的例子是一个基座模型推理的例子，该模型只能进行文本生成，如果希望使用对话能力，还是需要加载已经微调好的对话模型，代码如下：\n\n```python\nfrom transformers import pipeline\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"你是谁\"},\n]\npipe = pipeline(\"text-generation\", model=\"THUDM/glm-4-9b-chat-hf\")\nprint(pipe(messages))\n```\n\n此处我们换了种推理接口，直接使用pipeline完成推理，运行后将会生成如下信息\n\n```bash\nLoading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.24it/s]\nDevice set to use cuda:0\n[{'generated_text': [{'role': 'user', 'content': '你是谁'}, {'role': 'assistant', 'content': '\\n我是一个人工智能助手，名为 ChatGLM。我是基于清华大学 KEG 实验室和'}]}]\n```\n\n使用`print(model)`将模型的结构打印出来，展示如下：\n\n```text\nGlmForCausalLM(\n  (model): GlmModel(\n    (embed_tokens): Embedding(151552, 4096, padding_idx=151329)\n    (layers): ModuleList(\n      (0-39): 40 x GlmDecoderLayer(\n        (self_attn): GlmAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n          (k_proj): Linear(in_features=4096, out_features=256, bias=True)\n          (v_proj): Linear(in_features=4096, out_features=256, bias=True)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n        )\n        (mlp): GlmMLP(\n          (gate_up_proj): Linear(in_features=4096, out_features=27392, bias=False)\n          (down_proj): Linear(in_features=13696, out_features=4096, bias=False)\n          (activation_fn): SiLU()\n        )\n        (input_layernorm): GlmRMSNorm((4096,), eps=1.5625e-07)\n        (post_attention_layernorm): GlmRMSNorm((4096,), eps=1.5625e-07)\n      )\n    )\n    (norm): GlmRMSNorm((4096,), eps=1.5625e-07)\n    (rotary_emb): GlmRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=151552, bias=False)\n)\n```\n\n可以看到GLM模型的层数达到了惊人的40层😂，因此本身使用Lora进行微调时其可训练参数会比其他模型大一些。",
    "255": "一级标题：使用ChatGLM4进行大模型指令遵从微调（附代码和测试脚本）\n二级标题：数据集准备\n内容：\n数据集我已经提前包括在了github项目当中，可以直接使用如下命令下载完整的实验代码\n\n```bash\ngit clone https://github.com/SwanHubX/glm4-finetune.git\n```\n\n如果只想下载数据集，可以直接下载如下文件：\n\n```bash\nwget https://github.com/SwanHubX/glm4-finetune/blob/main/data/alpaca_gpt4_data_zh.json\n```\n\n也可以通过🤗huggingface上下载：[https://huggingface.co/datasets/llamafactory/alpaca_gpt4_zh](https://huggingface.co/datasets/llamafactory/alpaca_gpt4_zh)",
    "256": "一级标题：使用ChatGLM4进行大模型指令遵从微调（附代码和测试脚本）\n二级标题：代码说明+超参数调整\n内容：\n完整的微调代码公开在了GitHub上，使用如下命令即可下载\n\n```bash\ngit clone https://github.com/SwanHubX/glm4-finetune.git\n```\n\n文章的附件中也有完整的实现代码[#代码附件](#附件完整代码)\n\n本文接下来重点介绍各个代码的功能模块\n\n加载模型的超参数设置，这里可以重点关注lora参数的设置，本文lora参数参考了ChatGLM官方微调代码的lora参数设置\n\n这里要注意学习率为5e-4，如果是全量微调要小一个数量级。\n\n```python",
    "257": "# Model kwargs",
    "258": "@dataclass\nclass ChatGLM4ModelConfig(ModelConfig):\n    model_name_or_path: Optional[str] = field(\n        default=\"./weights/glm-4-9b-hf\",\n        metadata={\n            \"help\": \"Model checkpoint for weights initialization. default used glm4\"\n        },\n    )\n    torch_dtype: Optional[str] = field(\n        default=\"bfloat16\",\n        metadata={\n            \"help\": \"Override the default `torch.dtype` and load the model under this dtype.\",\n            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n        },\n    )\n    use_peft: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to use PEFT for training. Default true\"},\n    )\n    lora_r: int = field(\n        default=8,\n        metadata={\"help\": \"LoRA R value.\"},\n    )\n    lora_alpha: int = field(\n        default=32,\n        metadata={\"help\": \"LoRA alpha.\"},\n    )\n    lora_dropout: float = field(\n        default=0.1,\n        metadata={\"help\": \"LoRA dropout.\"},\n    )\n    lora_target_modules: Optional[list[str]] = field(\n        default_factory=lambda: [\"q_proj\", \"k_proj\", \"v_proj\"],\n        metadata={\"help\": \"LoRA target modules.\"},\n    )\n```\n\n数据集超参数设置，这里比较简单，只是加载了本地的数据集\n\n```python",
    "259": "# Datasets kwargs",
    "260": "@dataclass\nclass DataTrainingArguments:\n    data_files: Optional[str] = field(\n        default=\"./data/alpaca_gpt4_data_zh.json.json\",\n        metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"},\n    )\n```\n\n不过为了方便读者理解数据集长什么样，仍旧提供数据集展示脚本\n\n```python\nimport datasets\nraw_dataset=datasets.load_dataset(\"json\", data_files=\"data/glaive_toolcall_zh_1k.json\")\nprint(raw_dataset)\n\"\"\"打印内容\nDatasetDict({\n    train: Dataset({\n        features: ['instruction', 'input', 'output'],\n        num_rows: 42677\n    })\n})\n\"\"\"\n```\n\n可以看到数据一共有1000条，并且包括`'conversations', 'tools'`两个字段\n\n进一步选取其中一条打印：\n\n```python\nprint(raw_dataset[\"train\"][0])\n```\n\n输出如下：\n\n```json\n{\n    \"instruction\": \"保持健康的三个提示。\",\n    \"input\": \"\",\n    \"output\": \"以下是保持健康的三个提示：\\n\\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\\n\\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\\n\\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。\"\n}\n```\n\n这里大家会注意到为什么会有Instruct和input两部分。实际上早期针对指令遵从的研究是为了获得一个通用的任务处理模型（比如既能做翻译又能做计算这样），因此我们通常把对任务的描述放到instruct中，将实际的任务文本放在input中。\n但是随着ChatGPT这种通用的AI助理出现，大家已经逐渐习惯直接下指令让其执行了。因此instruct和prompt的这种分离就显得没那么有必要了。实际上无论分离和不分离模型的本质都是根据前文补后文。因此分离不分离对模型的最终结果不会有太大影响，无非就是格式的不同。\n现在的开源Chat大语言模型流行把“人设”放在“system prompt”中，把用户的指令放在input中，因此后文我们会将Alpaca数据集处理成更适应于主流Chat的格式。\n\nChatGLM提供的推荐输入微调数据结构如下：\n\n```json\n{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"类型#裤*材质#牛仔布*风格#性感\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"3x1的这款牛仔裤采用浅白的牛仔面料为裤身材质，其柔然的手感和细腻的质地，在穿着舒适的同时，透露着清纯甜美的个性气质。除此之外，流畅的裤身剪裁将性感的腿部曲线彰显的淋漓尽致，不失为一款随性出街的必备单品。\"\n    }\n  ]\n}\n```\n\n这里可能有一定经验的读者会说，不对呀，我们从0训练我们当然可以定义自己的数据结构。这么想是对的，但是让我们能够直接使用ChatGLM原生的`chat_template`，我还是建议咱们遵守chatglm官方定义的数据格式，这么做的话既能兼容ChatGLM的很多工具，又能充分利用官方定义的special_token。\n\n我们可以通过HuggingFace上开源的`glm-4-9b-chat-hf`的`tokenizer_config.json`中可以找到他们的原生`chat_template`，下面的脚本提供一个打印`chat_template`的代码\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ndevice = \"cuda\"\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/glm-4-9b-chat-hf\")\nprint(tokenizer.chat_template)\n```\n\n获取tokenizer配置的链接[https://huggingface.co/THUDM/glm-4-9b-chat-hf/blob/main/tokenizer_config.json](https://huggingface.co/THUDM/glm-4-9b-chat-hf/blob/main/tokenizer_config.json)\n\n这里我们简单打印一下转换完成后数据集最终的一个效果，参考脚本如下：\n\n```python\ndef formatting_func(example):\n    \"\"\"\n    process data format\n    \"\"\"\n    prompt = example[\"instruction\"]\n    if len(example[\"input\"]) != 0:\n        prompt += \"\\n\\n\" + example[\"input\"]\n    conversations = [\n        {\"role\": \"user\", \"content\": prompt},\n        {\"role\": \"assistant\", \"content\": example[\"output\"]},\n    ]\n    output_text = tokenizer.apply_chat_template(\n        conversation=conversations, tokenize=False\n    )\n    return output_text\n```\n\n输出效果如下，以下字段便是实际运用于模型微调时，输入给模型的数据样式：\n\n```text\n[gMASK]<sop><|user|>\n保持健康的三个提示。<|assistant|>\n以下是保持健康的三个提示：\n\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\n\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\n\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。\n```\n\n最后便是训练的超参数设置和训练过程的实现，这里由于数据规模比较小，我们训练600个steps，每个GPU实际batch大小为1*4：\n\n```python",
    "261": "# Train kwargs",
    "262": "@dataclass\nclass MySFTConfig(SFTConfig):\n    output_dir: Optional[str] = field(\n        default=\"./output/lora-glm4-9b-alpaca\",\n        metadata={\n            \"help\": \"The output directory where the model predictions and checkpoints will be written. Defaults to 'lora-glm4-9b-toolcall' if not provided.\"\n        },\n    )\n    num_train_epochs: float = field(\n        default=3.0, metadata={\"help\": \"Total number of training epochs to perform.\"}\n    )\n    per_device_train_batch_size: int = field(\n        default=2,\n        metadata={\"help\": \"Batch size per GPU/TPU/MPS/NPU core/CPU for training.\"},\n    )\n    per_device_eval_batch_size: int = field(\n        default=4,\n        metadata={\"help\": \"Batch size per GPU/TPU/MPS/NPU core/CPU for evaluation.\"},\n    )\n    gradient_accumulation_steps: int = field(\n        default=1,\n        metadata={\n            \"help\": \"Number of updates steps to accumulate before performing a backward/update pass.\"\n        },\n    )\n    learning_rate: float = field(\n        default=5e-4, metadata={\"help\": \"The initial learning rate for AdamW.\"}\n    )\n    bf16: bool = field(\n        default=True,\n        metadata={\n            \"help\": (\n                \"Whether to use bf16 (mixed) precision instead of 32-bit. Requires Ampere or higher NVIDIA\"\n                \" architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change.\"\n            )\n        },\n    )\n    bf16_full_eval: bool = field(\n        default=True,\n        metadata={\n            \"help\": (\n                \"Whether to use full bfloat16 evaluation instead of 32-bit. This is an experimental API and it may\"\n                \" change.\"\n            )\n        },\n    )\n    max_seq_length: Optional[int] = field(\n        default=512,\n        metadata={\n            \"help\": \"Maximum length of the tokenized sequence. Sequences longer than `max_seq_length` are truncated \"\n            \"from the right. If `None`, no truncation is applied. When packing is enabled, this value sets the \"\n            \"sequence length.\"\n        },\n    )\n    eval_strategy: Union[str] = field(\n        default=\"steps\",\n        metadata={\"help\": \"The evaluation strategy to use.\"},\n    )\n    eval_steps: Optional[float] = field(\n        default=0.1,\n        metadata={\n            \"help\": (\n                \"Run an evaluation every X steps. Should be an integer or a float in range `[0,1)`. \"\n                \"If smaller than 1, will be interpreted as ratio of total training steps.\"\n            )\n        },\n    )\n    logging_steps: float = field(\n        default=10,\n        metadata={\n            \"help\": (\n                \"Log every X updates steps. Should be an integer or a float in range `[0,1)`. \"\n                \"If smaller than 1, will be interpreted as ratio of total training steps.\"\n            )\n        },\n    )\n    save_steps: float = field(\n        default=0.1,\n        metadata={\n            \"help\": (\n                \"Save checkpoint every X updates steps. Should be an integer or a float in range `[0,1)`. \"\n                \"If smaller than 1, will be interpreted as ratio of total training steps.\"\n            )\n        },\n    )\n```\n\n训练的流程这块如下,使用HF TRL后流程变得非常简洁。\n\n```python",
    "263": "# Training",
    "264": "trainer = SFTTrainer(\n    model=model_args.model_name_or_path,\n    args=training_args,\n    data_collator=None,\n    train_dataset=raw_datasets[\"train\"],\n    eval_dataset=(\n        raw_datasets[\"test\"] if training_args.eval_strategy != \"no\" else None\n    ),\n    processing_class=tokenizer,\n    peft_config=get_peft_config(model_args),\n    formatting_func=formatting_func,\n    callbacks=[SavePredictCallback()],\n)\ntrainer.train()\n```",
    "265": "一级标题：使用ChatGLM4进行大模型指令遵从微调（附代码和测试脚本）\n二级标题：启动训练+效果评测\n内容：\n本代码在实现训练时默认是开启[SwanLab](https://swanlab.cn)的。SwanLab被官方集成进入了🤗HuggingFace Transformers。可以通过`report_to=\"swanlab\"`开启训练跟踪。如果本地环境安装了SwanLab会默认开启！\n\n启动训练的命令如下：\n\n```bash\npython instruct_train.py\n```\n\n可以看到如下启动信息\n\n![train](images/glm4-instruct/train.png)\n\n如果没登录SwanLab可能会弹出登录提示，这里推荐选择1并在[https://swanlab.cn](https://swanlab.cn)完成注册。即可在线查看到训练进展。\n\n登陆命令如下\n\n```bash\nswanlab login\n```\n\n点击打印出的链接即可通过看板查看训练日志：\n\n![swanlab](images/glm4-instruct/swanlab.png)\n\n通过配置`callback`，SwanLab还能自动记录模型的预测输出，代码和效果如下：\n\n```python",
    "266": "# Print prediction text callback",
    "267": "class SavePredictCallback(TrainerCallback):\n    def __init__(self, num_steps=10):\n        self.num_steps = num_steps\n\n    def on_save(self, args, state, control, model, processing_class, **kwargs):\n        if state.is_world_process_zero:\n            tokenizer = processing_class\n            batch_test_message = [\n                [{\"role\": \"user\", \"content\": \"你好，告诉我你的名字。\"}],\n                [{\"role\": \"user\", \"content\": \"告诉我1+2等于多少？\"}],\n            ]\n            batch_inputs_text = tokenizer.apply_chat_template(\n                batch_test_message,\n                return_tensors=\"pt\",\n                return_dict=True,\n                padding=True,\n                padding_side=\"left\",\n                add_generation_prompt=True,\n            ).to(model.device)\n\n            # print(batch_inputs_text)\n            outputs = model.generate(**batch_inputs_text, max_new_tokens=512)\n            batch_reponse = tokenizer.batch_decode(\n                outputs, skip_special_tokens=False\n            )\n            log_text_list = [swanlab.Text(response) for response in batch_reponse]\n            swanlab.log({\"Prediction\": log_text_list}, step=state.global_step)\n```\n\n![swanlab-text](images/glm4-instruct/swanlab-text.png)\n\n**多卡实验**\n\n如果你的卡数比较多，推荐使用多卡训练来极大提升训练速度！首先安装huggingface accelerate和deepspeed来方便的开启zero2多卡训练：\n\n```bash\npip install accelerate deepspeed\n```\n\n接下来使用如下命令来开启多卡训练（默认8GPU，可更改num_processes参数为实际卡数）：\n\n```bash\naccelerate launch --num_processes 8 --config_file configs/zero2.yaml instruct_train.py\n```\n\n关于zero2的详细设置在`configs/zero2.yaml`中。\n\n模型将会保存在`output/lora-glm4-9b-alpaca`，由于笔者的硬盘空间有限，因此仅仅保存Lora权重，推理加载时也要记得加载原始模型。\n\n**推理+效果对比**\n\n可以通过使用如下命令进行命令行聊天：\n\n```bash\nbash chat_cli.py\n```\n\n效果如下，我个人感觉有点overfit，因此建议大家使用早一点的checkpoints来做推理：\n\n![chat_cli](images/glm4-instruct/chat_cli.png)",
    "268": "一级标题：使用ChatGLM4进行大模型指令遵从微调（附代码和测试脚本）\n二级标题：附件：完整代码\n内容：\n完整代码如下，推荐还是通过使用github获得完整的代码\n\n[https://github.com/SwanHubX/glm4-finetune](https://github.com/SwanHubX/glm4-finetune)\n\n记得帮忙点个star🌟\n\n```python\n\"\"\"\nRefer: https://huggingface.co/docs/trl/sft_trainer#add-special-tokens-for-chat-format for more advance tools\n\"\"\"\n\nimport argparse\nfrom typing import Optional, Union, List\nfrom dataclasses import dataclass, field\n\nimport datasets\nfrom transformers import AutoTokenizer, TrainerCallback\nfrom trl import (\n    ModelConfig,\n    SFTConfig,\n    SFTTrainer,\n    TrlParser,\n    get_kbit_device_map,\n    get_peft_config,\n    get_quantization_config,\n)\nimport swanlab",
    "269": "# Model kwargs",
    "270": "@dataclass\nclass ChatGLM4ModelConfig(ModelConfig):\n    model_name_or_path: Optional[str] = field(\n        default=\"./weights/glm-4-9b-hf\",\n        metadata={\n            \"help\": \"Model checkpoint for weights initialization. default used glm4\"\n        },\n    )\n    torch_dtype: Optional[str] = field(\n        default=\"bfloat16\",\n        metadata={\n            \"help\": \"Override the default `torch.dtype` and load the model under this dtype.\",\n            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n        },\n    )\n    use_peft: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to use PEFT for training. Default true\"},\n    )\n    lora_r: int = field(\n        default=8,\n        metadata={\"help\": \"LoRA R value.\"},\n    )\n    lora_alpha: int = field(\n        default=32,\n        metadata={\"help\": \"LoRA alpha.\"},\n    )\n    lora_dropout: float = field(\n        default=0.1,\n        metadata={\"help\": \"LoRA dropout.\"},\n    )\n    lora_target_modules: Optional[list[str]] = field(\n        default_factory=lambda: [\"q_proj\", \"k_proj\", \"v_proj\"],\n        metadata={\"help\": \"LoRA target modules.\"},\n    )",
    "271": "# Datasets kwargs",
    "272": "@dataclass\nclass DataTrainingArguments:\n    data_files: Optional[str] = field(\n        default=\"./data/alpaca_gpt4_data_zh.json\",\n        metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"},\n    )",
    "273": "# Train kwargs",
    "274": "@dataclass\nclass MySFTConfig(SFTConfig):\n    output_dir: Optional[str] = field(\n        default=\"./output/lora-glm4-9b-alpaca\",\n        metadata={\n            \"help\": \"The output directory where the model predictions and checkpoints will be written. Defaults to 'lora-glm4-9b-toolcall' if not provided.\"\n        },\n    )\n    num_train_epochs: float = field(\n        default=3.0, metadata={\"help\": \"Total number of training epochs to perform.\"}\n    )\n    per_device_train_batch_size: int = field(\n        default=2,\n        metadata={\"help\": \"Batch size per GPU/TPU/MPS/NPU core/CPU for training.\"},\n    )\n    per_device_eval_batch_size: int = field(\n        default=4,\n        metadata={\"help\": \"Batch size per GPU/TPU/MPS/NPU core/CPU for evaluation.\"},\n    )\n    gradient_accumulation_steps: int = field(\n        default=1,\n        metadata={\n            \"help\": \"Number of updates steps to accumulate before performing a backward/update pass.\"\n        },\n    )\n    learning_rate: float = field(\n        default=5e-4, metadata={\"help\": \"The initial learning rate for AdamW.\"}\n    )\n    bf16: bool = field(\n        default=True,\n        metadata={\n            \"help\": (\n                \"Whether to use bf16 (mixed) precision instead of 32-bit. Requires Ampere or higher NVIDIA\"\n                \" architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change.\"\n            )\n        },\n    )\n    bf16_full_eval: bool = field(\n        default=True,\n        metadata={\n            \"help\": (\n                \"Whether to use full bfloat16 evaluation instead of 32-bit. This is an experimental API and it may\"\n                \" change.\"\n            )\n        },\n    )\n    max_seq_length: Optional[int] = field(\n        default=512,\n        metadata={\n            \"help\": \"Maximum length of the tokenized sequence. Sequences longer than `max_seq_length` are truncated \"\n            \"from the right. If `None`, no truncation is applied. When packing is enabled, this value sets the \"\n            \"sequence length.\"\n        },\n    )\n    eval_strategy: Union[str] = field(\n        default=\"steps\",\n        metadata={\"help\": \"The evaluation strategy to use.\"},\n    )\n    eval_steps: Optional[float] = field(\n        default=0.1,\n        metadata={\n            \"help\": (\n                \"Run an evaluation every X steps. Should be an integer or a float in range `[0,1)`. \"\n                \"If smaller than 1, will be interpreted as ratio of total training steps.\"\n            )\n        },\n    )\n    logging_steps: float = field(\n        default=10,\n        metadata={\n            \"help\": (\n                \"Log every X updates steps. Should be an integer or a float in range `[0,1)`. \"\n                \"If smaller than 1, will be interpreted as ratio of total training steps.\"\n            )\n        },\n    )\n    save_steps: float = field(\n        default=0.1,\n        metadata={\n            \"help\": (\n                \"Save checkpoint every X updates steps. Should be an integer or a float in range `[0,1)`. \"\n                \"If smaller than 1, will be interpreted as ratio of total training steps.\"\n            )\n        },\n    )",
    "275": "# Print prediction text callback",
    "276": "class SavePredictCallback(TrainerCallback):\n    def __init__(self, num_steps=10):\n        self.num_steps = num_steps\n\n    def on_save(self, args, state, control, model, processing_class, **kwargs):\n        if state.is_world_process_zero:\n            tokenizer = processing_class\n            batch_test_message = [\n                [{\"role\": \"user\", \"content\": \"你好，告诉我你的名字。\"}],\n                [{\"role\": \"user\", \"content\": \"告诉我1+2等于多少？\"}],\n            ]\n            batch_inputs_text = tokenizer.apply_chat_template(\n                batch_test_message,\n                return_tensors=\"pt\",\n                return_dict=True,\n                padding=True,\n                padding_side=\"left\",\n                add_generation_prompt=True,\n            ).to(model.device)\n\n            # print(batch_inputs_text)\n            outputs = model.generate(**batch_inputs_text, max_new_tokens=512)\n            batch_reponse = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n            log_text_list = [swanlab.Text(response) for response in batch_reponse]\n            swanlab.log({\"Prediction\": log_text_list}, step=state.global_step)\n\n\ndef main(model_args, data_args, training_args):",
    "277": "# Model init kwargs & Tokenizer",
    "278": "quantization_config = get_quantization_config(model_args)\n    model_kwargs = dict(\n        trust_remote_code=model_args.trust_remote_code,\n        attn_implementation=model_args.attn_implementation,\n        torch_dtype=model_args.torch_dtype,\n        use_cache=False if training_args.gradient_checkpointing else True,\n        device_map=get_kbit_device_map() if quantization_config is not None else None,\n        quantization_config=quantization_config,\n    )\n    training_args.model_init_kwargs = model_kwargs\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_args.model_name_or_path,\n        trust_remote_code=model_args.trust_remote_code,\n        use_fast=True,\n    )\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    if tokenizer.chat_template is None:\n        tokenizer.chat_template = \"[gMASK]<sop>{% for item in messages %}{% if item['tools'] is defined %}<|system|>\\n你是一个名为 ChatGLM 的人工智能助手。你是基于智谱AI训练的语言模型 GLM-4 模型开发的，你的任务是针对用户的问题和要求提供适当的答复和支持。\\n\\n# 可用工具{% set tools = item['tools'] %}{% for tool in tools %}{% if tool['type'] == 'function' %}\\n\\n## {{ tool['function']['name'] }}\\n\\n{{ tool['function'] | tojson(indent=4) }}\\n在调用上述函数时，请使用 Json 格式表示调用的参数。{% elif tool['type'] == 'python' %}\\n\\n## python\\n\\n当你向 `python` 发送包含 Python 代码的消息时，该代码将会在一个有状态的 Jupyter notebook 环境中执行。\\n`python` 返回代码执行的输出，或在执行 60 秒后返回超时。\\n`/mnt/data` 将会持久化存储你的文件。在此会话中，`python` 无法访问互联网。不要使用 `python` 进行任何网络请求或者在线 API 调用，这些在线内容的访问将不会成功。{% elif tool['type'] == 'simple_browser' %}\\n\\n## simple_browser\\n\\n你可以使用 `simple_browser` 工具。该工具支持以下函数：\\n`search(query: str, recency_days: int)`：使用搜索引擎进行查询并显示结果，可以使用 `recency_days` 参数控制搜索内容的时效性。\\n`mclick(ids: list[int])`：获取一系列指定 id 的页面内容。每次调用时，须选择3-10个页面。选择多个角度的页面，同时尽可能选择可信任的信息来源。考虑到部分页面是无法加载的，你也可以多打开一些可能有用的页面而不用担心内容过多。\\n`open_url(url: str)`：打开指定的 URL。\\n\\n使用 `【{引用 id}†{引用文本}】` 来引用内容。\\n\\n操作步骤：1. 使用 `search` 来获得信息列表; 2. 使用 `mclick` 来获取指定 ID 页面的内容; 3. 根据获得的内容进行回复。在回复中应当引用信息来源。\\n 如果用户提供了 URL，也可以用 `open_url` 直接打开页面。\\n如果初次搜索结果没有找到合适的信息，也可以再次使用 `search` 进行搜索。{% elif tool['type'] == 'cogview' %}\\n\\n## cogview\\n\\n如果用户的请求中包含了对图像的描述，你可以使用 `cogview` 来生成图像并展示给用户。你需要向 `cogview` 发送图像描述，规则：\\n- 发送给 `cogview` 的消息必须使用英语。用户的中文描述必须完全翻译为英语。\\n- 应当尽可能详细地描述图像生成的需求，需求描述约 100 英文单词。\\n- 保持用户原始描述的意图。不要虚构内容或者没见过的人物。\\n- 如无特殊说明，所在地为中国，持有中国立场并遵循中国社会主义价值观。{% endif %}{% endfor %}{% endif %}{% if item['content'] %}<|{{ item['role'] }}|>{{ item['metadata'] }}\\n{{ item['content'] }}{% endif %}{% endfor %}{% if add_generation_prompt %}<|assistant|>{% endif %}\"",
    "279": "# Dataset",
    "280": "raw_datasets = datasets.load_dataset(\"json\", data_files=data_args.data_files)\n    raw_datasets = raw_datasets[\"train\"].train_test_split(0.05)  # split train test data\n\n    def formatting_func(example):\n        \"\"\"\n        process data format\n        \"\"\"\n        prompt = example[\"instruction\"]\n        if len(example[\"input\"]) != 0:\n            prompt += \"\\n\\n\" + example[\"input\"]\n        conversations = [\n            {\"role\": \"user\", \"content\": prompt},\n            {\"role\": \"assistant\", \"content\": example[\"output\"]},\n        ]\n        output_text = tokenizer.apply_chat_template(\n            conversation=conversations, tokenize=False\n        )\n        return output_text",
    "281": "# Training",
    "282": "trainer = SFTTrainer(\n        model=model_args.model_name_or_path,\n        args=training_args,\n        data_collator=None,\n        train_dataset=raw_datasets[\"train\"],\n        eval_dataset=(\n            raw_datasets[\"test\"] if training_args.eval_strategy != \"no\" else None\n        ),\n        processing_class=tokenizer,\n        peft_config=get_peft_config(model_args),\n        formatting_func=formatting_func,\n        callbacks=[SavePredictCallback()],\n    )\n    trainer.train()\n\n    # Save\n    trainer.save_model(training_args.output_dir)\n\n\ndef make_parser(subparsers: argparse._SubParsersAction = None):\n    dataclass_types = (ChatGLM4ModelConfig, DataTrainingArguments, MySFTConfig)\n    if subparsers is not None:\n        parser = subparsers.add_parser(\n            \"sft\", help=\"Run the SFT training script\", dataclass_types=dataclass_types\n        )\n    else:\n        parser = TrlParser(dataclass_types)\n    return parser\n\n\nif __name__ == \"__main__\":\n    parser = make_parser()\n    model_args, data_args, training_args = parser.parse_args_and_config()\n    main(model_args, data_args, training_args)\n```",
    "283": "一级标题：Hello World\n二级标题：无\n内容：\n[![](/assets/colab.svg)](https://colab.research.google.com/drive/1RWsrY_1bS8ECzaHvYtLb_1eBkkdzekR3?usp=sharing)\n\n这是一个入门案例，是一个最简的深度学习训练模拟。",
    "284": "一级标题：Hello World\n二级标题：环境准备\n内容：\n```bash\npip install swanlab\n```",
    "285": "一级标题：Hello World\n二级标题：完整代码\n内容：\n```python\nimport swanlab\nimport random\n\noffset = random.random() / 5\n\n# 初始化SwanLab\nrun = swanlab.init(\n    project=\"my-project\",\n    config={\n        \"learning_rate\": 0.01,\n        \"epochs\": 10,\n    },\n)\n\n# 模拟训练过程\nfor epoch in range(2, run.config.epochs):\n    acc = 1 - 2**-epoch - random.random() / epoch - offset\n    loss = 2**-epoch + random.random() / epoch + offset\n    print(f\"epoch={epoch}, accuracy={acc}, loss={loss}\")\n\n    swanlab.log({\"accuracy\": acc, \"loss\": loss})  # 记录指标\n```",
    "286": "一级标题：LSTM股票预测\n二级标题：无\n内容：\n:::info\n时间序列、量化交易、时序模型\n:::\n\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/Google-Stock-Prediction/runs/0c2ci59aje4rb54r2z4y5/chart)\n\n[在线Demo](https://swanlab.cn/@ZeyiLin/Google-Stock-Prediction/runs/0c2ci59aje4rb54r2z4y5/chart) | [知乎教程](https://zhuanlan.zhihu.com/p/702114810)",
    "287": "一级标题：LSTM股票预测\n二级标题：概述\n内容：\nLSTM（Long Short-Term Memory），即长短时记忆网络，是一种特殊的RNN（递归神经网络），它改进了传统RNN在处理长序列数据时存在的梯度消失和梯度爆炸问题。LSTM由Hochreiter和Schmidhuber于1997年提出，已成为处理**时间序列数据**的经典模型之一。\n\n![](/assets/example-lstm-1.png)\n\n股票预测任务指的是根据一支股票的过去一段时间的数据，通过AI模型预测现在以及未来的股价变化，也是一种实用的时间序列任务。这里我们使用2016～2021年的Google股价数据数据集来进行训练和推理。",
    "288": "一级标题：LSTM股票预测\n二级标题：环境安装\n内容：\n本案例基于`Python>=3.8`，请在您的计算机上安装好Python。 环境依赖：\n\n```txt\npandas\ntorch\nmatplotlib\nswanlab\nscikit-learn\n```\n\n快速安装命令：\n\n```bash\npip install pandas torch matplotlib swanlab scikit-learn\n```\n\n> 本代码测试于torch==2.3.0、pandas==2.0.3、matplotlib==3.8.2、swanlab==0.3.8、scikit-learn==1.3.2",
    "289": "一级标题：LSTM股票预测\n二级标题：完整代码\n内容：\n请先在[Kaggle](https://www.kaggle.com/datasets/shreenidhihipparagi/google-stock-prediction)下载Google Stock Prediction数据集到根目录下。\n\n```python\nimport os\nimport swanlab\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nfrom copy import deepcopy as dc\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nclass LSTMModel(nn.Module):\n    \"\"\"\n    定义模型类\n    \"\"\"\n    def __init__(self, input_size=1, hidden_size1=50, hidden_size2=64, fc1_size=32, fc2_size=16, output_size=1):\n        super(LSTMModel, self).__init__()\n        self.lstm1 = nn.LSTM(input_size, hidden_size1, batch_first=True)\n        self.lstm2 = nn.LSTM(hidden_size1, hidden_size2, batch_first=True)\n        self.fc1 = nn.Linear(hidden_size2, fc1_size)\n        self.fc2 = nn.Linear(fc1_size, fc2_size)\n        self.fc3 = nn.Linear(fc2_size, output_size)\n\n    def forward(self, x):\n        x, _ = self.lstm1(x)\n        x, _ = self.lstm2(x)\n        x = self.fc1(x[:, -1, :])\n        x = self.fc2(x)\n        x = self.fc3(x)\n        return x\n\n\nclass TimeSeriesDataset(Dataset):\n    \"\"\"\n    定义数据集类\n    \"\"\"\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, i):\n        return self.X[i], self.y[i]\n\n\ndef prepare_dataframe_for_lstm(df, n_steps):\n    \"\"\"\n    处理数据集，使其适用于LSTM模型\n    \"\"\"\n    df = dc(df)\n    df['date'] = pd.to_datetime(df['date'])\n    df.set_index('date', inplace=True)\n\n    for i in range(1, n_steps+1):\n        df[f'close(t-{i})'] = df['close'].shift(i)\n\n    df.dropna(inplace=True)\n    return df\n\n\ndef get_dataset(file_path, lookback, split_ratio=0.9):\n    \"\"\"\n    归一化数据、划分训练集和测试集\n    \"\"\"\n    data = pd.read_csv(file_path)\n    data = data[['date','close']]\n\n    shifted_df_as_np = prepare_dataframe_for_lstm(data, lookback)\n\n    scaler = MinMaxScaler(feature_range=(-1,1))\n    shifted_df_as_np = scaler.fit_transform(shifted_df_as_np)\n\n    X = shifted_df_as_np[:, 1:]\n    y = shifted_df_as_np[:, 0]\n\n    X = dc(np.flip(X,axis=1))\n\n    # 划分训练集和测试集\n    split_index = int(len(X) * split_ratio)\n\n    X_train = X[:split_index]\n    X_test = X[split_index:]\n\n    y_train = y[:split_index]\n    y_test = y[split_index:]\n\n    X_train = X_train.reshape((-1, lookback, 1))\n    X_test = X_test.reshape((-1, lookback, 1))\n\n    y_train = y_train.reshape((-1, 1))\n    y_test = y_test.reshape((-1, 1))\n\n    # 转换为Tensor\n    X_train = torch.tensor(X_train).float()\n    y_train = torch.tensor(y_train).float()\n    X_test = torch.tensor(X_test).float()\n    y_test = torch.tensor(y_test).float()\n\n    return scaler, X_train, X_test, y_train, y_test\n\n\ndef train(model, train_loader, optimizer, criterion):\n        model.train()\n        running_loss = 0\n        # 训练\n        for i, batch in enumerate(train_loader):\n            x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n            y_pred = model(x_batch)\n            loss = criterion(y_pred, y_batch)\n            running_loss += loss.item()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        avg_loss_epoch = running_loss / len(train_loader)\n        print(f'Epoch: {epoch}, Batch: {i}, Avg. Loss: {avg_loss_epoch}')\n        swanlab.log({\"train/loss\": running_loss}, step=epoch)\n        running_loss = 0\n\n\ndef validate(model, test_loader, criterion, epoch):\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for _, batch in enumerate(test_loader):\n            x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n            y_pred = model(x_batch)\n            loss = criterion(y_pred, y_batch)\n            val_loss += loss.item()\n        avg_val_loss = val_loss / len(test_loader)\n        print(f'Epoch: {epoch}, Validation Loss: {avg_val_loss}')\n        swanlab.log({\"val/loss\": avg_val_loss}, step=epoch)\n\n\ndef inverse_transform_and_extract(scaler, data, lookback):\n    dummies = np.zeros((data.shape[0], lookback + 1))\n    dummies[:, 0] = data.flatten()\n    return dc(scaler.inverse_transform(dummies)[:, 0])\n\n\ndef plot_predictions(actual, predicted, title, xlabel='Date', ylabel='Close Price'):\n    \"\"\"\n    绘制最后的股价预测与真实值的对比图\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.plot(actual, color='red', label='Actual Close Price')\n    plt.plot(predicted, color='blue', label='Predicted Close Price', alpha=0.5)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(title)\n    plt.legend()\n    return swanlab.Image(plt, caption=title)\n\n\ndef visualize_predictions(train_predictions, val_predictions, scaler, y_train, y_test, lookback):\n    train_predictions = inverse_transform_and_extract(scaler, train_predictions, lookback)\n    val_predictions = inverse_transform_and_extract(scaler, val_predictions, lookback)\n    new_y_train = inverse_transform_and_extract(scaler, y_train, lookback)\n    new_y_test = inverse_transform_and_extract(scaler, y_test, lookback)\n\n    plt_image = []\n    plt_image.append(plot_predictions(new_y_train, train_predictions, '(TrainSet) Google Stock Price Prediction with LSTM'))\n    plt_image.append(plot_predictions(new_y_test, val_predictions, '(TestSet) Google Stock Price Prediction with LSTM'))\n\n    swanlab.log({\"Prediction\": plt_image})\n\n\nif __name__ == '__main__':\n    # ------------------- 初始化一个SwanLab实验 -------------------\n    swanlab.init(\n        project='Google-Stock-Prediction',\n        experiment_name=\"LSTM\",\n        description=\"根据前7天的数据预测下一日股价\",\n        config={\n            \"learning_rate\": 1e-3,\n            \"epochs\": 100,\n            \"batch_size\": 32,\n            \"lookback\": 7,\n            \"spilt_ratio\": 0.9,\n            \"save_path\": \"./checkpoint\",\n            \"optimizer\": \"Adam\",\n            \"device\": 'cuda' if torch.cuda.is_available() else 'cpu',\n        },\n    )\n\n    config = swanlab.config\n    device = torch.device(config.device)\n\n    # ------------------- 定义数据集 -------------------\n    scaler, X_train, X_test, y_train, y_test = get_dataset(file_path='./GOOG.csv',\n                                                           lookback=config.lookback,\n                                                           split_ratio=config.spilt_ratio,)\n\n    train_dataset = TimeSeriesDataset(X_train, y_train)\n    test_dataset = TimeSeriesDataset(X_test, y_test)\n\n    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n\n    # ------------------- 定义模型、超参数 -------------------\n    model = LSTMModel(input_size=1, output_size=1)\n\n    model = model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n    criterion = nn.MSELoss()\n\n    # ------------------- 训练与验证 -------------------\n    for epoch in range(1, config.epochs+1):\n        train(model, train_loader, optimizer, criterion)\n        validate(model, test_loader, criterion, epoch)\n\n    # ------------------- 使用最佳模型推理，与生成可视化结果 -------------------\n    with torch.no_grad():\n        model.eval()\n        train_predictions = model(X_train.to(device)).to('cpu').numpy()\n        val_predictions = model(X_test.to(device)).to('cpu').numpy()\n        visualize_predictions(train_predictions, val_predictions, scaler, y_train, y_test, config.lookback)\n\n    # ------------------- 保存模型 -------------------\n    model_save_path = os.path.join(config.save_path, 'lstm.pth')\n    if not os.path.exists(config.save_path):\n        os.makedirs(config.save_path)\n    torch.save(model.state_dict(), model_save_path)\n```",
    "290": "一级标题：LSTM股票预测\n二级标题：演示效果\n内容：\n![](/assets/example-lstm-2.png)",
    "291": "一级标题：MNIST手写体识别\n二级标题：无\n内容：\n:::info\n图像分类、机器学习入门、灰度图像\n:::\n\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/MNIST-example/runs/4plp6w0qehoqpt0uq2tcy/chart)\n\n[![](/assets/colab.svg)](https://colab.research.google.com/drive/1Au8aXxU2o0QNWSzGXGsTdHggighXQMNu?usp=sharing)",
    "292": "一级标题：MNIST手写体识别\n二级标题：概述\n内容：\nMNIST手写体识别是深度学习最经典的入门任务之一，由 LeCun 等人提出。\n该任务基于[MNIST数据集](https://paperswithcode.com/dataset/mnist)，研究者通过构建机器学习模型，来识别10个手写数字（0～9）。\n\n![mnist](/assets/mnist.jpg)\n\n本案例主要：\n- 使用`pytorch`进行CNN（卷积神经网络）的构建、模型训练与评估\n- 使用`swanlab`跟踪超参数、记录指标和可视化监控整个训练周期",
    "293": "一级标题：MNIST手写体识别\n二级标题：环境安装\n内容：\n本案例基于`Python>=3.8`，请在您的计算机上安装好Python。\n环境依赖：\n```\ntorch\ntorchvision\nswanlab\n```\n快速安装命令：\n```bash\npip install torch torchvision swanlab\n```",
    "294": "一级标题：MNIST手写体识别\n二级标题：完整代码\n内容：\n```python\nimport os\nimport torch\nfrom torch import nn, optim, utils\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nimport swanlab\n\n# CNN网络构建\nclass ConvNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # 1,28x28\n        self.conv1 = nn.Conv2d(1, 10, 5)  # 10, 24x24\n        self.conv2 = nn.Conv2d(10, 20, 3)  # 128, 10x10\n        self.fc1 = nn.Linear(20 * 10 * 10, 500)\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x):\n        in_size = x.size(0)\n        out = self.conv1(x)  # 24\n        out = F.relu(out)\n        out = F.max_pool2d(out, 2, 2)  # 12\n        out = self.conv2(out)  # 10\n        out = F.relu(out)\n        out = out.view(in_size, -1)\n        out = self.fc1(out)\n        out = F.relu(out)\n        out = self.fc2(out)\n        out = F.log_softmax(out, dim=1)\n        return out\n\n\n# 捕获并可视化前20张图像\ndef log_images(loader, num_images=16):\n    images_logged = 0\n    logged_images = []\n    for images, labels in loader:\n        # images: batch of images, labels: batch of labels\n        for i in range(images.shape[0]):\n            if images_logged < num_images:\n                # 使用swanlab.Image将图像转换为wandb可视化格式\n                logged_images.append(swanlab.Image(images[i], caption=f\"Label: {labels[i]}\"))\n                images_logged += 1\n            else:\n                break\n        if images_logged >= num_images:\n            break\n    swanlab.log({\"MNIST-Preview\": logged_images})\n\n\ndef train(model, device, train_dataloader, optimizer, criterion, epoch, num_epochs):\n    model.train()\n    # 1. 循环调用train_dataloader，每次取出1个batch_size的图像和标签\n    for iter, (inputs, labels) in enumerate(train_dataloader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        # 2. 传入到resnet18模型中得到预测结果\n        outputs = model(inputs)\n        # 3. 将结果和标签传入损失函数中计算交叉熵损失\n        loss = criterion(outputs, labels)\n        # 4. 根据损失计算反向传播\n        loss.backward()\n        # 5. 优化器执行模型参数更新\n        optimizer.step()\n        print('Epoch [{}/{}], Iteration [{}/{}], Loss: {:.4f}'.format(epoch, num_epochs, iter + 1, len(train_dataloader),\n                                                                      loss.item()))\n        # 6. 每20次迭代，用SwanLab记录一下loss的变化\n        if iter % 20 == 0:\n            swanlab.log({\"train/loss\": loss.item()})\n\ndef test(model, device, val_dataloader, epoch):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        # 1. 循环调用val_dataloader，每次取出1个batch_size的图像和标签\n        for inputs, labels in val_dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            # 2. 传入到resnet18模型中得到预测结果\n            outputs = model(inputs)\n            # 3. 获得预测的数字\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            # 4. 计算与标签一致的预测结果的数量\n            correct += (predicted == labels).sum().item()\n\n        # 5. 得到最终的测试准确率\n        accuracy = correct / total\n        # 6. 用SwanLab记录一下准确率的变化\n        swanlab.log({\"val/accuracy\": accuracy}, step=epoch)\n\n\nif __name__ == \"__main__\":\n\n    #检测是否支持mps\n    try:\n        use_mps = torch.backends.mps.is_available()\n    except AttributeError:\n        use_mps = False\n\n    #检测是否支持cuda\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    elif use_mps:\n        device = \"mps\"\n    else:\n        device = \"cpu\"\n\n    # 初始化swanlab\n    run = swanlab.init(\n        project=\"MNIST-example\",\n        experiment_name=\"PlainCNN\",\n        config={\n            \"model\": \"ResNet18\",\n            \"optim\": \"Adam\",\n            \"lr\": 1e-4,\n            \"batch_size\": 256,\n            \"num_epochs\": 10,\n            \"device\": device,\n        },\n    )\n\n    # 设置MNIST训练集和验证集\n    dataset = MNIST(os.getcwd(), train=True, download=True, transform=ToTensor())\n    train_dataset, val_dataset = utils.data.random_split(dataset, [55000, 5000])\n\n    train_dataloader = utils.data.DataLoader(train_dataset, batch_size=run.config.batch_size, shuffle=True)\n    val_dataloader = utils.data.DataLoader(val_dataset, batch_size=8, shuffle=False)\n\n    # （可选）看一下数据集的前16张图像\n    log_images(train_dataloader, 16)\n\n    # 初始化模型\n    model = ConvNet()\n    model.to(torch.device(device))\n\n    # 打印模型\n    print(model)\n\n    # 定义损失函数和优化器\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=run.config.lr)\n\n    # 开始训练和测试循环\n    for epoch in range(1, run.config.num_epochs+1):\n        swanlab.log({\"train/epoch\": epoch}, step=epoch)\n        train(model, device, train_dataloader, optimizer, criterion, epoch, run.config.num_epochs)\n        if epoch % 2 == 0:\n            test(model, device, val_dataloader, epoch)\n\n    # 保存模型\n    # 如果不存在checkpoint文件夹，则自动创建一个\n    if not os.path.exists(\"checkpoint\"):\n        os.makedirs(\"checkpoint\")\n    torch.save(model.state_dict(), 'checkpoint/latest_checkpoint.pth')\n```",
    "295": "一级标题：MNIST手写体识别\n二级标题：效果演示\n内容：\n![mnist](/assets/example-mnist.jpg)",
    "296": "一级标题：Qwen2命名实体识别\n二级标题：无\n内容：\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/Qwen2-NER-fintune/runs/9gdyrkna1rxjjmz0nks2c/chart)\n\n[Qwen2](https://modelscope.cn/models/qwen/Qwen2-1.5B-Instruct/summary)是通义千问团队最近开源的大语言模型，由阿里云通义实验室研发。\n\n以Qwen2作为基座大模型，通过**指令微调**的方式做高精度的命名实体识别（NER），是学习入门**LLM微调**、建立大模型认知的非常好的任务。\n\n![](./ner/01.png)\n\n> 使用LoRA方法训练，1.5B模型对显存要求不高，10GB左右就可以跑。\n\n在本文中，我们会使用 [Qwen2-1.5b-Instruct](https://modelscope.cn/models/qwen/Qwen2-1.5B-Instruct/summary) 模型在 [中文NER](https://huggingface.co/datasets/qgyd2021/chinese_ner_sft) 数据集上做指令微调训练，同时使用[SwanLab](https://swanlab.cn)监控训练过程、评估模型效果。\n\n- 代码：完整代码直接看本文第5节 或 [Github](https://github.com/Zeyi-Lin/LLM-Finetune)\n- 实验日志过程：[Qwen2-1.5B-NER-Fintune - SwanLab](https://swanlab.cn/@ZeyiLin/Qwen2-NER-fintune/runs/9gdyrkna1rxjjmz0nks2c/chart)\n- 模型：[Modelscope](https://modelscope.cn/models/qwen/Qwen2-1.5B-Instruct/summary)\n- 数据集：[chinese_ner_sft](https://huggingface.co/datasets/qgyd2021/chinese_ner_sft)\n- SwanLab：[https://swanlab.cn](https://swanlab.cn)",
    "297": "一级标题：Qwen2命名实体识别\n二级标题：知识点1：什么是指令微调？\n内容：\n大模型指令微调（Instruction Tuning）是一种针对大型预训练语言模型的微调技术，**其核心目的是增强模型理解和执行特定指令的能力，使模型能够根据用户提供的自然语言指令准确、恰当地生成相应的输出或执行相关任务。**\n\n指令微调特别关注于提升模型在**遵循指令**方面的一致性和准确性，从而拓宽模型在各种应用场景中的泛化能力和实用性。\n\n在实际应用中，我的理解是，指令微调更多**把LLM看作一个更智能、更强大的传统NLP模型（比如Bert）**，来实现更高精度的NLP任务。所以这类任务的应用场景覆盖了以往NLP模型的场景，甚至很多团队拿它来**标注互联网数据**。",
    "298": "一级标题：Qwen2命名实体识别\n二级标题：知识点2：什么是命名实体识别？\n内容：\n命名实体识别 (NER) 是一种NLP技术，主要用于识别和分类文本中提到的重要信息（关键词）。这些实体可以是人名、地名、机构名、日期、时间、货币值等等。 NER 的目标是将文本中的非结构化信息转换为结构化信息，以便计算机能够更容易地理解和处理。\n\n![](./ner/02.png)\n\nNER 也是一项非常实用的技术，包括在互联网数据标注、搜索引擎、推荐系统、知识图谱、医疗保健等诸多领域有广泛应用。",
    "299": "一级标题：Qwen2命名实体识别\n二级标题：1.环境安装\n内容：\n本案例基于`Python>=3.8`，请在您的计算机上安装好Python，并且有一张英伟达显卡（显存要求并不高，大概10GB左右就可以跑）。\n\n我们需要安装以下这几个Python库，在这之前，请确保你的环境内已安装好了**pytorch**以及**CUDA**：\n\n```txt\nswanlab\nmodelscope\ntransformers\ndatasets\npeft\naccelerate\npandas\n```\n\n一键安装命令：\n\n```bash\npip install swanlab modelscope transformers datasets peft pandas accelerate\n```\n\n> 本案例测试于modelscope==1.14.0、transformers==4.41.2、datasets==2.18.0、peft==0.11.1、accelerate==0.30.1、swanlab==0.3.11",
    "300": "一级标题：Qwen2命名实体识别\n二级标题：2.准备数据集\n内容：\n本案例使用的是HuggingFace上的[chinese_ner_sft](https://huggingface.co/datasets/qgyd2021/chinese_ner_sft)数据集，该数据集主要被用于训练命名实体识别模型。\n\n![alt text](./ner/03.png)\n\nchinese_ner_sft由不同来源、不同类型的几十万条数据组成，应该是我见过收录最齐全的中文NER数据集。\n\n这次训练我们不需要用到它的全部数据，只取其中的CCFBDCI数据集（中文命名实体识别算法鲁棒性评测数据集）进行训练，该数据集包含LOC（地点）、GPE（地理）、ORG（组织）和PER（人名）四种实体类型标注，每条数据的例子如下：\n\n```json\n{\n  \"text\": \"今天亚太经合组织第十二届部长级会议在这里开幕，中国外交部部长唐家璇、外经贸部部长石广生出席了会议。\",\n  \"entities\": [\n    {\n        \"start_idx\": 23,\n        \"end_idx\": 25,\n        \"entity_text\": \"中国\",\n        \"entity_label\": \"GPE\",\n        \"entity_names\": [\"地缘政治实体\", \"政治实体\", \"地理实体\", \"社会实体\"]},\n        {\n            \"start_idx\": 25,\n            \"end_idx\": 28,\n            \"entity_text\": \"外交部\",\n            \"entity_label\": \"ORG\",\n            \"entity_names\": [\"组织\", \"团体\", \"机构\"]\n        },\n        {\n            \"start_idx\": 30,\n            \"end_idx\": 33,\n            \"entity_text\": \"唐家璇\",\n            \"entity_label\": \"PER\",\n            \"entity_names\": [\"人名\", \"姓名\"]\n        },\n        ...\n    ],\n\"data_source\": \"CCFBDCI\"\n}\n```\n\n其中`text`是输入的文本，`entities`是文本抽取出的实体。我们的目标是希望微调后的大模型能够根据由`text`组成的提示词，预测出一个json格式的实体信息：\n\n```txt\n输入：今天亚太经合组织第十二届部长级会议在这里开幕，中国外交部部长唐家璇、外经贸部部长石广生出席了会议。\n\n大模型输出：{\"entity_text\":\"中国\", \"entity_label\":\"组织\"}{\"entity_text\":\"唐家璇\", \"entity_label\":\"人名\"}...\n```\n\n---\n\n现在我们将数据集下载到本地目录。下载方式是前往[chinese_ner_sft - huggingface](https://huggingface.co/datasets/qgyd2021/chinese_ner_sft/tree/main/data)下载`ccfbdci.jsonl`到项目根目录下即可：\n\n![alt text](./ner/04.png)",
    "301": "一级标题：Qwen2命名实体识别\n二级标题：3. 加载模型\n内容：\n这里我们使用modelscope下载Qwen2-1.5B-Instruct模型（modelscope在国内，所以直接用下面的代码自动下载即可，不用担心速度和稳定性问题），然后把它加载到Transformers中进行训练：\n\n```python\nfrom modelscope import snapshot_download, AutoTokenizer\nfrom transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n\nmodel_id = \"qwen/Qwen2-1.5B-Instruct\"\nmodel_dir = \"./qwen/Qwen2-1___5B-Instruct\"\n\n# 在modelscope上下载Qwen模型到本地目录下\nmodel_dir = snapshot_download(model_id, cache_dir=\"./\", revision=\"master\")\n\n# Transformers加载模型权重\ntokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\nmodel.enable_input_require_grads()  # 开启梯度检查点时，要执行该方法\n```",
    "302": "一级标题：Qwen2命名实体识别\n二级标题：4. 配置训练可视化工具\n内容：\n我们使用SwanLab来监控整个训练过程，并评估最终的模型效果。\n\n这里直接使用SwanLab和Transformers的集成来实现：\n\n```python\nfrom swanlab.integration.huggingface import SwanLabCallback\n\nswanlab_callback = SwanLabCallback(...)\n\ntrainer = Trainer(\n    ...\n    callbacks=[swanlab_callback],\n)\n\n```\n\n如果你是第一次使用SwanLab，那么还需要去[https://swanlab.cn](https://swanlab.cn)上注册一个账号，在**用户设置**页面复制你的API Key，然后在训练开始时粘贴进去即可：\n\n![](./ner/05.png)",
    "303": "一级标题：Qwen2命名实体识别\n二级标题：5. 完整代码\n内容：\n开始训练时的目录结构：\n\n```txt\n|--- train.py\n|--- train.jsonl\n|--- test.jsonl\n```\n\ntrain.py:\n\n```python\nimport json\nimport pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom modelscope import snapshot_download, AutoTokenizer\nfrom swanlab.integration.huggingface import SwanLabCallback\nfrom peft import LoraConfig, TaskType, get_peft_model\nfrom transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\nimport os\nimport swanlab\n\n\ndef dataset_jsonl_transfer(origin_path, new_path):\n    \"\"\"\n    将原始数据集转换为大模型微调所需数据格式的新数据集\n    \"\"\"\n    messages = []\n\n    # 读取旧的JSONL文件\n    with open(origin_path, \"r\") as file:\n        for line in file:\n            # 解析每一行的json数据\n            data = json.loads(line)\n            input_text = data[\"text\"]\n            entities = data[\"entities\"]\n            match_names = [\"地点\", \"人名\", \"地理实体\", \"组织\"]\n\n            entity_sentence = \"\"\n            for entity in entities:\n                entity_json = dict(entity)\n                entity_text = entity_json[\"entity_text\"]\n                entity_names = entity_json[\"entity_names\"]\n                for name in entity_names:\n                    if name in match_names:\n                        entity_label = name\n                        break\n\n                entity_sentence += f\"\"\"{{\"entity_text\": \"{entity_text}\", \"entity_label\": \"{entity_label}\"}}\"\"\"\n\n            if entity_sentence == \"\":\n                entity_sentence = \"没有找到任何实体\"\n\n            message = {\n                \"instruction\": \"\"\"你是一个文本实体识别领域的专家，你需要从给定的句子中提取 地点; 人名; 地理实体; 组织 实体. 以 json 格式输出, 如 {\"entity_text\": \"南京\", \"entity_label\": \"地理实体\"} 注意: 1. 输出的每一行都必须是正确的 json 字符串. 2. 找不到任何实体时, 输出\"没有找到任何实体\". \"\"\",\n                \"input\": f\"文本:{input_text}\",\n                \"output\": entity_sentence,\n            }\n\n            messages.append(message)\n\n    # 保存重构后的JSONL文件\n    with open(new_path, \"w\", encoding=\"utf-8\") as file:\n        for message in messages:\n            file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n\n\ndef process_func(example):\n    \"\"\"\n    将数据集进行预处理\n    \"\"\"\n\n    MAX_LENGTH = 384\n    input_ids, attention_mask, labels = [], [], []\n    system_prompt = \"\"\"你是一个文本实体识别领域的专家，你需要从给定的句子中提取 地点; 人名; 地理实体; 组织 实体. 以 json 格式输出, 如 {\"entity_text\": \"南京\", \"entity_label\": \"地理实体\"} 注意: 1. 输出的每一行都必须是正确的 json 字符串. 2. 找不到任何实体时, 输出\"没有找到任何实体\".\"\"\"\n\n    instruction = tokenizer(\n        f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{example['input']}<|im_end|>\\n<|im_start|>assistant\\n\",\n        add_special_tokens=False,\n    )\n    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n    attention_mask = (\n        instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n    )\n    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n        input_ids = input_ids[:MAX_LENGTH]\n        attention_mask = attention_mask[:MAX_LENGTH]\n        labels = labels[:MAX_LENGTH]\n    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n\n\ndef predict(messages, model, tokenizer):\n    device = \"cuda\"\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\n    generated_ids = model.generate(\n        model_inputs.input_ids,\n        max_new_tokens=512\n    )\n    generated_ids = [\n        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    print(response)\n\n    return response\n\n\nmodel_id = \"qwen/Qwen2-1.5B-Instruct\"\nmodel_dir = \"./qwen/Qwen2-1___5B-Instruct\"\n\n# 在modelscope上下载Qwen模型到本地目录下\nmodel_dir = snapshot_download(model_id, cache_dir=\"./\", revision=\"master\")\n\n# Transformers加载模型权重\ntokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\nmodel.enable_input_require_grads()  # 开启梯度检查点时，要执行该方法\n\n# 加载、处理数据集和测试集\ntrain_dataset_path = \"ccfbdci.jsonl\"\ntrain_jsonl_new_path = \"ccf_train.jsonl\"\n\nif not os.path.exists(train_jsonl_new_path):\n    dataset_jsonl_transfer(train_dataset_path, train_jsonl_new_path)\n\n# 得到训练集\ntotal_df = pd.read_json(train_jsonl_new_path, lines=True)\ntrain_df = total_df[int(len(total_df) * 0.1):]\ntrain_ds = Dataset.from_pandas(train_df)\ntrain_dataset = train_ds.map(process_func, remove_columns=train_ds.column_names)\n\n\nconfig = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    inference_mode=False,  # 训练模式\n    r=8,  # Lora 秩\n    lora_alpha=32,  # Lora alaph，具体作用参见 Lora 原理\n    lora_dropout=0.1,  # Dropout 比例\n)\n\nmodel = get_peft_model(model, config)\n\nargs = TrainingArguments(\n    output_dir=\"./output/Qwen2-NER\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,\n    logging_steps=10,\n    num_train_epochs=2,\n    save_steps=100,\n    learning_rate=1e-4,\n    save_on_each_node=True,\n    gradient_checkpointing=True,\n    report_to=\"none\",\n)\n\nswanlab_callback = SwanLabCallback(\n    project=\"Qwen2-NER-fintune\",\n    experiment_name=\"Qwen2-1.5B-Instruct\",\n    description=\"使用通义千问Qwen2-1.5B-Instruct模型在NER数据集上微调，实现关键实体识别任务。\",\n    config={\n        \"model\": model_id,\n        \"model_dir\": model_dir,\n        \"dataset\": \"qgyd2021/chinese_ner_sft\",\n    },\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n\n# 用测试集的随机20条，测试模型\n# 得到测试集\ntest_df = total_df[:int(len(total_df) * 0.1)].sample(n=20)\n\ntest_text_list = []\nfor index, row in test_df.iterrows():\n    instruction = row['instruction']\n    input_value = row['input']\n\n    messages = [\n        {\"role\": \"system\", \"content\": f\"{instruction}\"},\n        {\"role\": \"user\", \"content\": f\"{input_value}\"}\n    ]\n\n    response = predict(messages, model, tokenizer)\n    messages.append({\"role\": \"assistant\", \"content\": f\"{response}\"})\n    result_text = f\"{messages[0]}\\n\\n{messages[1]}\\n\\n{messages[2]}\"\n    test_text_list.append(swanlab.Text(result_text, caption=response))\n\nswanlab.log({\"Prediction\": test_text_list})\nswanlab.finish()\n```\n\n看到下面的进度条即代表训练开始：\n\n![alt text](./ner/06.png)",
    "304": "一级标题：Qwen2命名实体识别\n二级标题：5.训练结果演示\n内容：\n在[SwanLab](https://swanlab.cn/@ZeyiLin/Qwen2-NER-fintune/runs/9gdyrkna1rxjjmz0nks2c/chart)上查看最终的训练结果：\n\n可以看到在2个epoch之后，微调后的qwen2的loss降低到了不错的水平——当然对于大模型来说，真正的效果评估还得看主观效果。\n\n![alt text](./ner/07.png)\n\n可以看到在一些测试样例上，微调后的qwen2能够给出准确的实体抽取结果：\n\n![alt text](./ner/08.png)\n\n![alt text](./ner/09.png)\n\n至此，你已经完成了qwen2指令微调的训练！",
    "305": "一级标题：Qwen2命名实体识别\n二级标题：6. 推理训练好的模型\n内容：\n训好的模型默认被保存在`./output/Qwen2-NER`文件夹下。\n\n推理模型的代码如下：\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\ndef predict(messages, model, tokenizer):\n    device = \"cuda\"\n\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\n    generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512)\n    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    return response\n\n\n# 加载原下载路径的tokenizer和model\ntokenizer = AutoTokenizer.from_pretrained(\"./qwen/Qwen2-1___5B-Instruct/\", use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"./qwen/Qwen2-1___5B-Instruct/\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n\n# 加载训练好的Lora模型，将下面的[checkpoint-XXX]替换为实际的checkpoint文件名名称\nmodel = PeftModel.from_pretrained(model, model_id=\"./output/Qwen2-NER/checkpoint-1700\")\n\ninput_text = \"西安电子科技大学的陈志明爱上了隔壁西北工业大学苏春红，他们约定好毕业后去中国的苏州定居。\"\ntest_texts = {\n    \"instruction\": \"\"\"你是一个文本实体识别领域的专家，你需要从给定的句子中提取 地点; 人名; 地理实体; 组织 实体. 以 json 格式输出, 如; {\"entity_text\": \"南京\", \"entity_label\": \"地理实体\"} 注意: 1. 输出的每一行都必须是正确的 json 字符串. 2. 找不到任何实体时, 输出\"没有找到任何实体\". \"\"\",\n    \"input\": f\"文本:{input_text}\"\n}\n\ninstruction = test_texts['instruction']\ninput_value = test_texts['input']\n\nmessages = [\n    {\"role\": \"system\", \"content\": f\"{instruction}\"},\n    {\"role\": \"user\", \"content\": f\"{input_value}\"}\n]\n\nresponse = predict(messages, model, tokenizer)\nprint(response)\n```\n\n输出结果为：\n\n```json\n{\"entity_text\": \"西安电子科技大学\", \"entity_label\": \"组织\"}\n{\"entity_text\": \"陈志明\", \"entity_label\": \"人名\"}\n{\"entity_text\": \"西北工业大学\", \"entity_label\": \"组织\"}\n{\"entity_text\": \"苏春红\", \"entity_label\": \"人名\"}\n{\"entity_text\": \"中国\", \"entity_label\": \"地理实体\"}\n{\"entity_text\": \"苏州\", \"entity_label\": \"地理实体\"}\n```",
    "306": "一级标题：Qwen2命名实体识别\n二级标题：相关链接\n内容：\n- 代码：完整代码直接看本文第5节 或 [Github](https://github.com/Zeyi-Lin/LLM-Finetune)\n- 实验日志过程：[Qwen2-1.5B-NER-Fintune - SwanLab](https://swanlab.cn/@ZeyiLin/Qwen2-NER-fintune/runs/9gdyrkna1rxjjmz0nks2c/chart)\n- 模型：[Modelscope](https://modelscope.cn/models/qwen/Qwen2-1.5B-Instruct/summary)\n- 数据集：[chinese_ner_sft](https://huggingface.co/datasets/qgyd2021/chinese_ner_sft)\n- SwanLab：[https://swanlab.cn](https://swanlab.cn)",
    "307": "一级标题：openMind大模型微调教程\n二级标题：无\n内容：",
    "308": "一级标题：openMind大模型微调教程\n二级标题：简介\n内容：\n魔乐社区（[Modelers.cn](https://modelers.cn)）是一个为人工智能开发者及爱好者打造的社区，提供工具链、数据集、模型和应用等AI领域生产要素的托管及展示服务和支撑系统。目前，魔乐社区已支持openMind Library。该工具通过简单的API接口，帮助开发者完成模型预训练、微调、推理等流程。同时，openMind Library原生兼容PyTorch 和 MindSpore 等主流框架，原生支持昇腾NPU处理器。openMind Library可以和PEFT、DeepSpeed等三方库配合使用，来提升模型微调效率。\n\n友情链接：\n\n* [魔乐社区](https://modelers.cn/docs/zh/openmind-library/0.9.1/overview.html)\n* [Huggingface](https://huggingface.co)\n* [SwanLab](https://swanlab.cn)\n\n---",
    "309": "一级标题：openMind大模型微调教程\n二级标题：1、基本概念\n内容：\n1、[openMind Library](https://modelers.cn/docs/zh/openmind-library/0.9.1/overview.html)--->[Huggingface Transformers](https://huggingface.co/docs/transformers/index)\n\nopenMind Library类似于transformers的大模型封装工具，其中就有AutoModelForSequenceClassification、AutoModelForCausalLM等等模型加载工具以及像TrainingArguments参数配置工具等等，原理基本一样，不过对NPU适配更友好些。\n![openmind vs transformers](/zh/examples/openMind/openmind_transformers.png)\n\n2、[魔乐社区](https://modelers.cn/)--->[HuggingFace](https://huggingface.co/)\n\n魔乐社区类似于huggingface这种模型托管社区，里面除了torch的模型还有使用MindSpore实现的模型。transformers可以直接从huggingface获取模型或者数据集，openMind也是一样的，可以从魔乐社区获取模型和数据集。\n![魔乐社区 vs huggingface](/zh/examples/openMind/mole.png)\n\n---",
    "310": "一级标题：openMind大模型微调教程\n二级标题：2、微调代码\n内容：\n如果了解了上述的对应机制，那么就可以跑一个简单的微调代码了，该代码参考了[魔乐社区的教程文档](https://modelers.cn/docs/zh/openmind-library/0.9.1/overview.html)，稍作调整，可以对比NVIDIA显卡的结果。\n\n### 概述\n\nopenMind Library是一个深度学习开发套件，通过简单易用的API支持模型预训练、微调、推理等流程。openMind Library通过一套接口兼容PyTorch和MindSpore等主流框架，同时原生支持昇腾NPU处理器，同时openMind Library可以和PEFT、DeepSpeed等三方库配合使用，来加速模型微调效率。\n\n### 环境配置\n\n#### 直接安装openMind环境\n\n如果是昇腾AI卡系列的话，配置环境前需要先安装驱动等设备，具体可以参考[软件安装-CANN商用版8.0.RC3开发文档-昇腾社区](https://www.hiascend.com/document/detail/zh/canncommercial/80RC3/softwareinst/instg/instg_0000.html?Mode=PmIns&OS=Ubuntu&Software=cannToolKit)。\n\n**驱动安装&验证**\n\n首先得确定有NPU卡和NPU相关驱动，驱动是8.0.RC3.beta1，如果没安装可以参考上面软件安装的链接查看。\n\n安装好后的验证方法是运行下面的命令，该命令作用与nvidia-smi类似，这里是查看NPU的状态和性能\n\n```bash\nnpu-smi info\n```\n\n可以看到如下信息的话就表示驱动已经安装完成了，左侧是安装成功后运行代码后的结果，右侧是每一部分的含义\n\n![npu-smi info](/zh/examples/openMind/npu-info.png)\n\n然后安装好驱动了之后就可以配置环境了，本次微调代码使用pytorch框架，openMind中自带了基于pytorch框架的各类函数，因此正常安装openMind就行。\n\n安装命令如下：\n\n```bash\n\n# 下载PyTorch安装包\nwget https://download.pytorch.org/whl/cpu/torch-2.4.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\n# 下载torch_npu插件包\nwget https://gitee.com/ascend/pytorch/releases/download/v6.0.rc3-pytorch2.4.0/torch_npu-2.4.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\n# 安装命令\npip3 install torch-2.4.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\npip3 install torch_npu-2.4.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\n# 安装openMind Library\npip install openmind[pt]\n# 安装SwanLab\npip install swanlab\n```\n\n> 注意以下几点：\n>\n> 1、可以使用镜像源来安装环境，不然会很浪费时间，可以使用清华源：\n>\n> ```bash\n> pip install -i https://pypi.tuna.tsinghua.edu.cn/simple name\n> ```\n>\n> 2、魔乐社区中有两个框架的分类，如果是pytorch就只能选择pytorch框架，同理如果是mindspore就只能选择mindspore框架\n> ![魔乐社区模型](/zh/examples/openMind/models.png)\n> 3、配置环境的时候，按照openmind官方文档说可以同时存在两个框架，使用的时候分别设置就行，但是实际使用的时候只能存在一个框架，一旦设置了两个框架，使用的时候无论如何设置都会报错说openmind不知道使用哪个框架，所以最好在环境里只安装一个\n>\n> ```bash\n> >>>import openmind\n> Traceback (most recent call last):\n>   File \"<stdin>\", line 1, in <module>\n>   File \"/home/miniconda3/envs/openmind-pt-cp39/lib/python3.9/site-packages/openmind/__init__.py\", line 20, in <module>\n>     from .utils import is_ms_available, is_torch_available\n>   File \"/home/miniconda3/envs/openmind-pt-cp39/lib/python3.9/site-packages/openmind/utils/__init__.py\", line 14, in <module>\n>     from .import_utils import (\n>   File \"/home/miniconda3/envs/openmind-pt-cp39/lib/python3.9/site-packages/openmind/utils/import_utils.py\", line 69, in <module>\n>     CURRENT_FRAMEWORK = get_framework()\n>   File \"/home/miniconda3/envs/openmind-pt-cp39/lib/python3.9/site-packages/openmind/utils/import_utils.py\", line 66, in get_framework\n>     raise RuntimeError(replace_invalid_characters(error_msg))\n> RuntimeError: Multiple frameworks detected, including: pt, ms.\n> ```\n\n#### docker环境安装（推荐）\n\nopenMind官方库也提供了模型的docker环境。\n\n推荐通过点击模型测试部分（下图红框）找到docker的链接，通过docker来拉起拉起环境。下面介绍docker环境的搭建教程。\n\n![bert模型环境](/zh/examples/openMind/bert.png)\n\n首先得确定有NPU卡和NPU相关驱动，驱动是**8.0.RC3.beta1**，如果没安装可以参考[CANN官方安装教程](https://www.hiascend.com/document/detail/zh/canncommercial/80RC3/softwareinst/instg/instg_0000.html?Mode=PmIns&OS=Ubuntu&Software=cannToolKit)\n\n完成安装后检测方法是运行\n\n```bash\nnpu-smi info\n```\n\n可以看到如下信息的话就表示驱动已经安装完成了。\n\n![npu-smi](/zh/examples/openMind/a_mask.png)\n\n接下来使用如下命令创建一个装好openmind环境的容器，这样可以省去大量安装环境的时间：\n\n```bash\ndocker run \\\n    --name openmind \\\n    --device /dev/davinci0 \\    # 指定NPU 0号设备\n    --device /dev/davinci_manager \\\n    --device /dev/devmm_svm \\\n    --device /dev/hisi_hdc \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/ \\\n    -v /etc/ascend_install.info:/etc/ascend_install.info \\\n    -v /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info \\\n    -tid registry.modelers.cn/base_image/openmind:openeuler-python3.10-cann8.0.rc3.beta1-pytorch2.1.0-openmind0.9.1 bash\n```\n\n这将在后台开启一个名为openmind容器。使用如下命令可进入到容器当中\n\n```bash\n docker exec -it openmind bash\n```\n\n出现如下界面即表示进入到容器当中\n\n![indocker](/zh/examples/openMind/indocker.png)\n\n最后在docker中运行如下命令安装swanlab即可完成环境安装。\n\n```bash\n# 安装swanlab命令\npip install swanlab\n```\n\n### 数据集处理\n\nOmDataset.load_dataset()方法目前支持下载的数据集格式如下：\n\n* parquet\n* json或者jsonl\n* tar.gz\n* csv\n* 下载python脚本加载魔乐社区数据集\n* 下载python脚本加载三方站点数据集\n\n```python\nfrom openmind import OmDataset\nfrom openmind import AutoTokenizer\n\n### 准备数据集\ndataset = OmDataset.load_dataset(\"AI_Connect/glue\", \"cola\")\n\n### 结果\n\"\"\"\nDatasetDict({\n    train: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 8551\n    })\n    validation: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 1043\n    })\n    test: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 1063\n    })\n})\n\"\"\"\n\n### 加载分词器\ntokenizer = AutoTokenizer.from_pretrained(\"PyTorch-NPU/bert_base_cased\")\n\n### 处理数据集\ndef tokenize_function(examples):\n    return tokenizer(examples[\"sentence\"],truncation=True,padding=\"max_length\",max_length=512)\n\n### 训练数据封装\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n# 训练数据+验证数据，验证发生在每个epoch之后\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\nsmall_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42)\n```\n\n### 加载模型\n\n和transformers使用差不多，分别加载模型和分词器\n\n```python\nfrom openmind import AutoTokenizer\nfrom openmind import AutoModelForSequenceClassification  ## 做分类任务\n\n\n### 加载分词器\ntokenizer = AutoTokenizer.from_pretrained(\"PyTorch-NPU/bert_base_cased\")\n\n### 加载模型\nmodel = AutoModelForSequenceClassification.from_pretrained(\"PyTorch-NPU/bert_base_cased\", num_labels=2)  # 二分类任务\n```\n\n### 训练参数配置\n\n创建一个TrainingArguments类，其中包含可以调整的所有超参数以及不同的训练选项。\n\n```python\nfrom openmind import TrainingArguments\n\n### 参数初始化\n# 指定保存训练检查点的路径\ntraining_args = TrainingArguments(logging_steps=1,\n                                  output_dir=\"test_trainer\",\n                                  evaluation_strategy=\"epoch\",\n                                  half_precision_backend=\"auto\",  # auto:自动选择合适的混合精度训练后端；apex：英伟达的 ；cpu_amp：在CPU上运行\n                                  per_device_train_batch_size=4,\n                                  optim=\"adamw_torch\",\n                                  learning_rate=2e-5)\n```\n\n### 评估参数设置\n\nTrainer在训练过程中不会自动评估模型性能，需要向Trainer传递一个函数来计算和展示指标。\n\n```python\nimport numpy as np\nfrom openmind import metrics\n\n### 配置评估参数\nmetric = metrics.Accuracy()\n\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    return metric.compute(preds=preds, labels=labels)\n```\n\n### 可视化工具配置\n\nswanlab支持记录openMind Library。能够在线/离线查看训练日志。SwanLab支持openMind Library通过callback调用，调用代码可参考后文。\n\n![SwanLab可视化工具](/zh/examples/openMind/modelers&swanlab%20V2.png)\n关于SwanLab的使用方法可以参考[SwanLab官方文档-快速开始](https://docs.swanlab.cn/guide_cloud/general/quick-start.html)\n\n> 如果提示登录swanlab，可以在[官网完成注册](https://swanlab.cn)后，使用[获取API KEY](https://swanlab.cn/settings)找到对应的登陆密钥并粘贴，这样将能够使用**云上看版**随时查看训练过程与结果。\n\n```python\nfrom openmind import Trainer\nfrom swanlab.integration.transformers import SwanLabCallback\n\n### 使用swanlab监测\nswanlab_config = {\n    \"dataset\": \"glue\",\n    \"fp16_backend\":\"auto\",\n    \"datacollator\":\"transformer\"\n}\nswanlab_callback = SwanLabCallback(\n    project=\"new_qwen2.5-7B-finetune\",\n    experiment_name=\"跑的官方例子的微调\",\n    description=\"这个是使用transformers的datacollator封装函数\",\n    workspace=None,\n    config=swanlab_config,\n)\n\n### 创建训练器并且启动训练\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n\n### 保存模型\noutput_dir=\"./output\"\nfinal_save_path = join(output_dir)\ntrainer.save_model(final_save_path)\n```\n\n### 全过程代码\n\n```python\nfrom openmind import OmDataset\nfrom openmind import AutoTokenizer\nfrom openmind import AutoModelForSequenceClassification\nfrom openmind import TrainingArguments\nfrom openmind import metrics\nimport numpy as np\nfrom openmind import Trainer\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom os.path import join\n\n\n### 准备数据集\ndataset = OmDataset.load_dataset(\"AI_Connect/glue\", \"cola\")\n\n### 加载分词器\ntokenizer = AutoTokenizer.from_pretrained(\"PyTorch-NPU/bert_base_cased\")\n\n\n### 处理数据集\ndef tokenize_function(examples):\n    # 填充\n    return tokenizer(examples[\"sentence\"],truncation=True,padding=\"max_length\",max_length=512)\n\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n# 减少数据量\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\nsmall_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42)\n\n\n### 加载模型\nmodel = AutoModelForSequenceClassification.from_pretrained(\"PyTorch-NPU/bert_base_cased\", num_labels=2)\n\n### 参数初始化\n# 指定保存训练检查点的路径\ntraining_args = TrainingArguments(logging_steps=1,\n                                  output_dir=\"test_trainer\",\n                                  evaluation_strategy=\"epoch\",\n                                  half_precision_backend=\"auto\",  # auto:自动选择合适的混合精度训练后端；apex：英伟达的 ；cpu_amp：在CPU上运行\n                                  per_device_train_batch_size=4,\n                                  optim=\"adamw_torch\",\n                                  learning_rate=2e-5)\n\n### 配置评估参数\nmetric = metrics.Accuracy()\n\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    return metric.compute(preds=preds, labels=labels)\n\n\n### 使用swanlab监测\nswanlab_config = {\n    \"dataset\": \"glue\",\n    \"fp16_backend\":\"auto\",\n    \"datacollator\":\"transformer\"\n}\nswanlab_callback = SwanLabCallback(\n    project=\"new_qwen2.5-7B-finetune\",\n    experiment_name=\"跑的官方例子的微调\",\n    description=\"这个是使用transformers的datacollator封装函数\",\n    workspace=None,\n    config=swanlab_config,\n)\n### 创建训练器并且启动训练\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n\n### 保存模型\noutput_dir=\"./output\"\nfinal_save_path = join(output_dir)\ntrainer.save_model(final_save_path)\n```\n\n---\n\n这里使用HF Transformers实现同样的训练过程，使用NVIDIA-A100卡来跑了一次做个对比，A100对应的代码如下：\n\n```python\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TrainingArguments\nimport evaluate\nimport numpy as np\nfrom transformers import Trainer\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom os.path import join\nimport os\n\n# 设置只使用第一个GPU\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # 使用第一块 GPU\n\n### 加载数据集\ndataset = load_dataset(\"nyu-mll/glue\",\"cola\")\n\n### 加载分词器\ntokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n\n### 处理数据集\ndef tokenize_function(examples):\n    # 填充\n    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n# 减少数据量\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\nsmall_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42)\n\n### 加载模型\nmodel = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=2)\n\n### 参数初始化\n# 指定保存训练检查点的路径\ntraining_args = TrainingArguments(logging_steps=1,\n                                  output_dir=\"test_trainer\",\n                                  evaluation_strategy=\"epoch\",\n                                  half_precision_backend=\"auto\",\n                                  per_device_train_batch_size=4,\n                                  optim=\"adamw_torch\",\n                                  learning_rate=2e-5)\n\n### 配置评估参数\nmetric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    # 添加评估数据\n    metric.add_batch(predictions=preds, references=labels)  # 使用add_batch方法添加批次数据\n    # 计算准确度\n    return metric.compute()\n\n### 使用swanlab监测\nswanlab_config = {\n    \"dataset\": \"glue\"\n}\nswanlab_callback = SwanLabCallback(\n    project=\"new_qwen2.5-7B-finetune\",\n    experiment_name=\"跑的官方例子的微调\",\n    description=\"用例子跑的，模型用的是bert，做文本分类任务\",\n    workspace=None,\n    config=swanlab_config,\n)\n### 创建训练器并且启动训练\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n\n### 保存模型\noutput_dir=\"./output/A100\"\nfinal_save_path = join(output_dir)\ntrainer.save_model(final_save_path)\n```",
    "311": "一级标题：openMind大模型微调教程\n二级标题：3、结果展示\n内容：\n下面是Ascend NPU与A100实验对比：\n\n首先是实验时间，此次实验epoch=3，\n\n![时间对比](/zh/examples/openMind/time.png)\n\n看样子昇腾卡比A100稍微快点\n\n然后是显存消耗，其中两个监测NPU/GPU状态的代码如下：\n\n```bash\n# NPU：\nwatch -n 1 npu-smi info\n\n# GPU：\nnvtop\n```\n\n![显存对比](/zh/examples/openMind/xiancun.png)\n\n显存消耗差不多\n\n最后是loss等参数的变化\n\n![loss对比](/zh/examples/openMind/loss.png)\n\n感觉A100上运行的结果震荡比较明显，昇腾卡震荡比较少。",
    "312": "一级标题：从零预训练一个自己的大模型\n二级标题：无\n内容：\n[![](/assets/badge1.svg)](https://swanlab.cn/@ShaohonChen/WikiLLM/overview)\n\n大语言模型（Large Language Model，简称LLM），指使用大量文本数据训练的深度学习模型，可以生成自然语言文本或理解语言文本的含义。\n\n![llm](/assets/examples/pretrain_llm/llm.png)\n\n虽然网上有大量关于transformer理论、大语言模型微调的教程。但是少有关于预训练的解释。本文则从如何自己实战预训练一个大语言模型的角度，使用wiki数据集进行一个简单的从零预训练工作，并附上使用swanlab launch白嫖显卡的方法\n\n* 本教程完整代码：[GitHub](https://github.com/ShaohonChen/transformers_from_scratch)\n\n* 实验记录：[SwanLab](https://swanlab.cn/@ShaohonChen/WikiLLM/overview)\n\n* 数据集下载：[百度网盘（j8ee）](https://pan.baidu.com/s/1p5F52bRlnpSY7F78q0hz7A?pwd=j8ee)，[huggingface](https://huggingface.co/datasets/fjcanyue/wikipedia-zh-cn)",
    "313": "一级标题：从零预训练一个自己的大模型\n二级标题：安装环境\n内容：\n首先，项目推荐使用python3.10。需要安装的python包如下：\n\n```txt\nswanlab\ntransformers\ndatasets\naccelerate\n```\n\n使用如下命令一键安装：\n\n```bash\npip install swanlab transformers datasets accelerate modelscope\n```",
    "314": "一级标题：从零预训练一个自己的大模型\n二级标题：下载数据集\n内容：\n本教程使用的是中文wiki数据，理论上预训练数据集种类越丰富、数据量越大越好，后续会增加别的数据集。\n\n![dataset](/assets/examples/pretrain_llm/dataset.png)\n\nhuggingface链接：[wikipedia-zh-cn](https://huggingface.co/datasets/fjcanyue/wikipedia-zh-cn)\n\n百度网盘下载地址：[百度网盘（j8ee）](https://pan.baidu.com/s/1p5F52bRlnpSY7F78q0hz7A?pwd=j8ee)\n\n下载`wikipedia-zh-cn-20240820.json`文件后放到项目目录下`./WIKI_CN/`文件夹中\n\n该数据集文件约1.99G大，共有1.44M条数据。虽然数据集中包含文章标题，但是实际上在预训练阶段用不上。正文片段参考：\n\n```txt\n数学是研究数量、结构以及空间等概念及其变化的一门学科，属于形式科学的一种。数学利用抽象化和逻辑推理，从计数、计算、量度、对物体形状及运动的观察发展而成。数学家们拓展这些概念...\n```\n\n使用[🤗Huggingface Datasets](https://huggingface.co/docs/datasets/index)加载数据集的代码如下：\n\n```python\nfrom datasets import load_dataset\n\nds = load_dataset(\"fjcanyue/wikipedia-zh-cn\")\n```\n\n如果使用百度网盘下载的json文件，可以通过如下代码加载\n\n```python\nraw_datasets = datasets.load_dataset(\n    \"json\", data_files=\"data/wikipedia-zh-cn-20240820.json\"\n)\n\nraw_datasets = raw_datasets[\"train\"].train_test_split(test_size=0.1, seed=2333)\nprint(\"dataset info\")\nprint(raw_datasets)\n```",
    "315": "一级标题：从零预训练一个自己的大模型\n二级标题：构建自己的大语言模型\n内容：\n本教程使用[🤗huggingface transformers](https://huggingface.co/docs/transformers/index)构建自己的大模型。\n\n因为目标是训练一个中文大模型。因此我们参考[通义千问2](https://qwen.readthedocs.io/zh-cn/latest/run_locally/mlx-lm.html)的tokenize和模型架构，仅仅做一些简单的更改让模型更小更好训练。\n\n因为国内无法直接访问到huggingface，推荐使用modelscope先把模型配置文件和checkpoint下载到本地，运行如下代码\n\n```python\nimport modelscope\n\nmodelscope.AutoConfig.from_pretrained(\"Qwen/Qwen2-0.5B\").save_pretrained(\n    \"Qwen2-0.5B\"\n)\nmodelscope.AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\").save_pretrained(\n    \"Qwen2-0.5B\"\n)\n```\n\n配置参数，并修改模型注意力头数量、模型层数和中间层大小，把模型控制到大概120M参数左右（跟GPT2接近）。\n\n```python\nimport transformers\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"./Qwen2-0.5B\")   # 这里使用qwen2的tokenzier\nconfig = transformers.AutoConfig.from_pretrained(\n        \"./Qwen2-0.5B\",\n        vocab_size=len(tokenizer),\n        hidden_size=512,\n        intermediate_size=2048,\n        num_attention_heads=8,\n        num_hidden_layers=12,\n        n_ctx=context_length,\n        bos_token_id=tokenizer.bos_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n    )\nprint(\"Model Config:\")\nprint(config)\n```\n\n使用transformers库初始化模型\n\n```python\nmodel = transformers.Qwen2ForCausalLM(config)\nmodel_size = sum(t.numel() for t in model.parameters())\nprint(f\"Model Size: {model_size/1000**2:.1f}M parameters\")\n```",
    "316": "一级标题：从零预训练一个自己的大模型\n二级标题：设置训练参数\n内容：\n设置预训练超参数：\n\n```python\nargs = transformers.TrainingArguments(\n    output_dir=\"checkpoints\",\n    per_device_train_batch_size=24,  # 每个GPU的训练batch数\n    per_device_eval_batch_size=24,  # 每个GPU的测试batch数\n    eval_strategy=\"steps\",\n    eval_steps=5_000,\n    logging_steps=500,\n    gradient_accumulation_steps=12,  # 梯度累计总数\n    num_train_epochs=2, # 训练epoch数\n    weight_decay=0.1,\n    warmup_steps=1_000,\n    optim=\"adamw_torch\",  # 优化器使用adamw\n    lr_scheduler_type=\"cosine\",  # 学习率衰减策略\n    learning_rate=5e-4,  # 基础学习率，\n    save_steps=5_000,\n    save_total_limit=10,\n    bf16=True,  # 开启bf16训练, 对于Amper架构以下的显卡建议替换为fp16=True\n)\nprint(\"Train Args:\")\nprint(args)\n```",
    "317": "一级标题：从零预训练一个自己的大模型\n二级标题：初始化训练+使用swanlab进行记录\n内容：\n使用transformers自带的train开始训练，并且引入swanlab作为可视化日志记录\n\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\ntrainer = transformers.Trainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=args,\n    data_collator=data_collator,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    callbacks=[SwanLabCallback()],\n)\ntrainer.train()\n```\n\n如果是第一次使用SwanLab，需要登陆SwanLab官网[https://swanlab.cn/](https://swanlab.cn/)，注册，并且在如下位置找到和复制自己的key。\n\n![findkey](/assets/examples/pretrain_llm/findkey.png)\n\n接下来在命令行中输入\n\n```sh\nswanlab login\n```\n\n会看到提示输入key\n\n![login](/assets/examples/pretrain_llm/login.png)\n\n按照提示将key粘贴进去（注意key是不会显示到终端当中的）就可以完成配置，完成效果如下：\n\n![login2](/assets/examples/pretrain_llm/login2.png)",
    "318": "一级标题：从零预训练一个自己的大模型\n二级标题：完整代码\n内容：\n项目目录结构：\n\n```txt\n|---data\\\n|------wikipedia-zh-cn-20240820.json    # 数据集放在data文件夹中\n|--- pretrain.py\n```\n\n`pretrain.py`代码如下：\n\n```python\nimport datasets\nimport transformers\nimport swanlab\nfrom swanlab.integration.transformers import SwanLabCallback\nimport modelscope\n\ndef main():\n    # using swanlab to save log\n    swanlab.init(\"WikiLLM\")\n\n    # load dataset\n    raw_datasets = datasets.load_dataset(\n        \"json\", data_files=\"/data/WIKI_CN/wikipedia-zh-cn-20240820.json\"\n    )\n\n    raw_datasets = raw_datasets[\"train\"].train_test_split(test_size=0.1, seed=2333)\n    print(\"dataset info\")\n    print(raw_datasets)\n\n    # load tokenizers\n    # 因为国内无法直接访问HuggingFace，因此使用魔搭将模型的配置文件和Tokenizer下载下来\n    modelscope.AutoConfig.from_pretrained(\"Qwen/Qwen2-0.5B\").save_pretrained(\n        \"Qwen2-0.5B\"\n    )\n    modelscope.AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\").save_pretrained(\n        \"Qwen2-0.5B\"\n    )\n    context_length = 512  # use a small context length\n    # tokenizer = transformers.AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\")\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        \"./Qwen2-0.5B\"\n    )  # download from local\n\n    # preprocess dataset\n    def tokenize(element):\n        outputs = tokenizer(\n            element[\"text\"],\n            truncation=True,\n            max_length=context_length,\n            return_overflowing_tokens=True,\n            return_length=True,\n        )\n        input_batch = []\n        for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n            if length == context_length:\n                input_batch.append(input_ids)\n        return {\"input_ids\": input_batch}\n\n    tokenized_datasets = raw_datasets.map(\n        tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n    )\n    print(\"tokenize dataset info\")\n    print(tokenized_datasets)\n    tokenizer.pad_token = tokenizer.eos_token\n    data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\n    # prepare a model from scratch\n    config = transformers.AutoConfig.from_pretrained(\n        \"./Qwen2-0.5B\",\n        vocab_size=len(tokenizer),\n        hidden_size=512,\n        intermediate_size=2048,\n        num_attention_heads=8,\n        num_hidden_layers=12,\n        n_ctx=context_length,\n        bos_token_id=tokenizer.bos_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n    )\n    model = transformers.Qwen2ForCausalLM(config)\n    model_size = sum(t.numel() for t in model.parameters())\n    print(\"Model Config:\")\n    print(config)\n    print(f\"Model Size: {model_size/1000**2:.1f}M parameters\")\n\n    # train\n    args = transformers.TrainingArguments(\n        output_dir=\"WikiLLM\",\n        per_device_train_batch_size=32,  # 每个GPU的训练batch数\n        per_device_eval_batch_size=32,  # 每个GPU的测试batch数\n        eval_strategy=\"steps\",\n        eval_steps=5_00,\n        logging_steps=50,\n        gradient_accumulation_steps=8,  # 梯度累计总数\n        num_train_epochs=2,  # 训练epoch数\n        weight_decay=0.1,\n        warmup_steps=2_00,\n        optim=\"adamw_torch\",  # 优化器使用adamw\n        lr_scheduler_type=\"cosine\",  # 学习率衰减策略\n        learning_rate=5e-4,  # 基础学习率，\n        save_steps=5_00,\n        save_total_limit=10,\n        bf16=True,  # 开启bf16训练, 对于Amper架构以下的显卡建议替换为fp16=True\n    )\n    print(\"Train Args:\")\n    print(args)\n    # enjoy training\n    trainer = transformers.Trainer(\n        model=model,\n        tokenizer=tokenizer,\n        args=args,\n        data_collator=data_collator,\n        train_dataset=tokenized_datasets[\"train\"],\n        eval_dataset=tokenized_datasets[\"test\"],\n        callbacks=[SwanLabCallback()],\n    )\n    trainer.train()\n\n    # save model\n    model.save_pretrained(\"./WikiLLM/Weight\")  # 保存模型的路径\n\n    # generate\n    pipe = transformers.pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n    print(\"GENERATE:\", pipe(\"人工智能\", num_return_sequences=1)[0][\"generated_text\"])\n    prompts = [\"牛顿\", \"北京市\", \"亚洲历史\"]\n    examples = []\n    for i in range(3):\n        # 根据提示词生成数据\n        text = pipe(prompts[i], num_return_sequences=1)[0][\"generated_text\"]\n        text = swanlab.Text(text)\n        examples.append(text)\n    swanlab.log({\"Generate\": examples})\n\n\nif __name__ == \"__main__\":\n    main()\n\n```",
    "319": "一级标题：从零预训练一个自己的大模型\n二级标题：训练结果演示\n内容：\n运行如下命令\n\n```\npython pretrain.py\n```\n\n可以看到如下训练日志。由于训练时间较长，推荐使用tmux将训练任务hold住\n\n![terminal](/assets/examples/pretrain_llm/terminal.png)\n\n可以在[SwanLab](https://swanlab.cn)中查看最终的训练结果：\n\n![log](/assets/examples/pretrain_llm/log.png)\n\n<!-- 并且能够看到一些最终生成的案例：\n\n![sample]() -->",
    "320": "一级标题：从零预训练一个自己的大模型\n二级标题：使用训练好的模型进行推理\n内容：\n以“人工智能”为开头生成内容的代码如下：\n\n```python\npipe = transformers.pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\nprint(\"GENERATE:\", pipe(\"人工智能\", num_return_sequences=1)[0][\"generated_text\"])\n```\n\n推理效果如下：\n\n（模型训练ing，可以在[https://swanlab.cn/@ShaohonChen/WikiLLM/overview](https://swanlab.cn/@ShaohonChen/WikiLLM/overview)实时查看训练进展和推理效果）\n<!-- ![result]() -->",
    "321": "一级标题：从零预训练一个自己的大模型\n二级标题：参考链接\n内容：\n* 本教程完整代码:[GitHub](https://github.com/ShaohonChen/transformers_from_scratch)\n\n* 实验记录：[SwanLab](https://swanlab.cn/@ShaohonChen/WikiLLM/overview)\n\n* 数据集下载：[百度网盘（j8ee）](https://pan.baidu.com/s/1p5F52bRlnpSY7F78q0hz7A?pwd=j8ee)，[huggingface](https://huggingface.co/datasets/fjcanyue/wikipedia-zh-cn)",
    "322": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：无\n内容：\n[![](https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg)](https://swanlab.cn/@ZeyiLin/qwen3-sft-medical/runs/agps0dkifth5l1xytcdyk/chart)\n\n![09-01](./qwen3/01.png)\n\n[Qwen3](https://www.modelscope.cn/models?name=qwen3&page=1)是阿里通义实验室最近开源的大语言模型，发布时便登顶了开源LLM榜单第一名。同时，Qwen系列模型也超越LLaMA，成为了HuggingFace上最受欢迎的开源LLM。\n\n![09-02](./qwen3/02.png)\n\n可以说，不论是进行研究学习，还是应用落地，Qwen已经逐渐成为开发者的最优选项之一。\n\n那么，以Qwen3作为基座大模型，通过**全参数微调**的方式，实现垂直专业领域聊天，甚至**支持DeepSeek R1 / QwQ式的带推理过程的对话**，是学习**LLM微调**的入门任务。\n\n在本文中，我们会使用 [Qwen3-1.7b](https://www.modelscope.cn/models/Qwen/Qwen3-1.7B) 模型在 [delicate_medical_r1_data](https://modelscope.cn/datasets/krisfu/delicate_medical_r1_data) 数据集上做全参数微调训练，实现让微调后的Qwen3支持对医学问题进行DeepSeek R1式的推理回复。训练中用到了transformers、datasets等工具，同时使用[SwanLab](https://swanlab.cn)监控训练过程、评估模型效果。\n\n> 全参数微调需要大约32GB显存，如果你的显存大小不足，可以使用Qwen3-0.6b，或Lora微调。\n\n- **代码**：[Github](https://github.com/Zeyi-Lin/Qwen3-Medical-SFT)，或直接看本文第5节\n\n- **实验日志过程**：[qwen3-1.7B-linear - SwanLab](https://swanlab.cn/@ZeyiLin/qwen3-sft-medical/runs/agps0dkifth5l1xytcdyk/chart)，或 [SwanLab基线社区](https://swanlab.cn/benchmarks) 搜索“qwen3-sft-medical”\n\n- **模型**：[Modelscope](https://modelscope.cn/models/Qwen/Qwen3-1.7B)\n\n- **数据集**：[delicate_medical_r1_data](https://modelscope.cn/datasets/krisfu/delicate_medical_r1_data)\n\n- **SwanLab**：[https://swanlab.cn](https://swanlab.cn)",
    "323": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：知识点：什么是全参数微调？\n内容：\n大模型全参数微调是指对预训练大模型的**所有参数**进行更新和优化，区别于部分参数微调和LoRA微调。\n\n这种方法通过将**整个模型权重**（包括底层词嵌入、中间特征提取层和顶层任务适配层）在下游任务数据上进行梯度反向传播，使模型整体适应新任务的需求。**相比仅微调部分参数**，全参数微调能更充分地利用预训练模型的泛化能力，并针对特定任务进行深度适配，**通常在数据差异较大或任务复杂度较高的场景下表现更优。**\n\n![09-03](./qwen3/03.png)\n\n不过，全参数微调往往需要更高的计算资源和存储开销，且存在**过拟合风险**（尤其在小数据集上）。实际应用中常结合学习率调整、参数分组优化或正则化技术来缓解这些问题。\n\n全参数微调多用于对模型表现性能要求较高的场景，例如专业领域知识问答或高精度文本生成。\n\n更多微调技术可参考：https://zhuanlan.zhihu.com/p/682082440\n\n下面是实战正片：",
    "324": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：1. 环境安装\n内容：\n本案例基于**Python>=3.8**，请在您的计算机上安装好Python；\n\n另外，您的计算机上至少要有一张英伟达/昇腾显卡（显存要求大概**32GB**左右可以跑）。\n\n我们需要安装以下这几个Python库，在这之前，请确保你的环境内已安装了pytorch以及CUDA：\n\n```\nswanlab\nmodelscope==1.22.0\ntransformers>=4.50.0\ndatasets==3.2.0\naccelerate\npandas\naddict\n```\n\n一键安装命令：\n\n```bash\npip install swanlab modelscope==1.22.0 \"transformers>=4.50.0\" datasets==3.2.0 accelerate pandas addict\n```\n\n> 本案例测试于modelscope==1.22.0、transformers==4.51.3、datasets==3.2.0、peft==0.11.1、accelerate==1.6.0、swanlab==0.5.7",
    "325": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：2. 准备数据集\n内容：\n本案例使用的是 [delicate_medical_r1_data](https://modelscope.cn/datasets/krisfu/delicate_medical_r1_data) 数据集，该数据集主要被用于医学对话模型。\n\n该数据集由2000多条数据组成，每条数据包含Instruction、question、think、answer、metrics六列：\n\n![09-04](./qwen3/04.png)\n\n这里我们只取`question`、`think`、`answer`这三列：\n\n- `question`：用户提出的问题，即模型的输入\n- `think`：模型的思考过程。大家如果用过DeepSeek R1的话，回复中最开始的思考过程就是这个。\n- `answer`：模型思考完成后，回复的内容。\n\n我们的训练任务，便是希望微调后的大模型，能够根据`question`，给用户一个`think`+`answer`的组合回复，并且think和answer直接在网页展示上是有区分的。\n\n理清需求后，我们设计这样一个数据集样例：\n\n```json\n{\n\"question\": \"我父亲刚刚被诊断为活动性出血，医生说需要立即处理，我们该怎么做？\",\n\"think\": \"嗯，用户的问题是关于病人出现活动性出血时应采取哪些一般处理措施，...\",\n\"answer\": \"首先，您父亲需要卧床休息，活动性出血期间暂时不要进食。为了...\",\n}\n```\n\n在训练代码执行时，会将`think`和`answer`按下面这样的格式组合成一条完整回复：\n\n```\n<think>\n嗯，用户的问题是关于病人出现活动性出血时应采取哪些一般处理措施，...\n</think>\n\n首先，您父亲需要卧床休息，活动性出血期间暂时不要进食。为了...\n```\n\n---\n\n接下来我们来下载数据集，并进行必要的格式转换。\n\n这个流程非常简单，执行下面的代码即可：\n\n```python\nfrom modelscope.msdatasets import MsDataset\nimport json\nimport random\n\nrandom.seed(42)\n\nds = MsDataset.load('krisfu/delicate_medical_r1_data', subset_name='default', split='train')\ndata_list = list(ds)\nrandom.shuffle(data_list)\n\nsplit_idx = int(len(data_list) * 0.9)\n\ntrain_data = data_list[:split_idx]\nval_data = data_list[split_idx:]\n\nwith open('train.jsonl', 'w', encoding='utf-8') as f:\n    for item in train_data:\n        json.dump(item, f, ensure_ascii=False)\n        f.write('\\n')\n\nwith open('val.jsonl', 'w', encoding='utf-8') as f:\n    for item in val_data:\n        json.dump(item, f, ensure_ascii=False)\n        f.write('\\n')\n\nprint(f\"The dataset has been split successfully.\")\nprint(f\"Train Set Size：{len(train_data)}\")\nprint(f\"Val Set Size：{len(val_data)}\")\n```\n\n完成后，你的代码目录下会出现训练集`train.jsonl`和验证集`val.jsonl`文件。\n\n至此，数据集部分完成。",
    "326": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：3. 加载模型\n内容：\n这里我们使用modelscope下载Qwen3-1.7B模型（modelscope在国内，所以下载不用担心速度和稳定性问题），然后把它加载到Transformers中进行训练：\n\n```python\nfrom modelscope import snapshot_download, AutoTokenizer\nfrom transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n\n# 在modelscope上下载Qwen模型到本地目录下\nmodel_dir = snapshot_download(\"Qwen/Qwen3-1.7B\", cache_dir=\"./\", revision=\"master\")\n\n# Transformers加载模型权重\ntokenizer = AutoTokenizer.from_pretrained(\"./Qwen/Qwen3-1.7B\", use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"./Qwen/Qwen3-1.7B\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n```",
    "327": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：4. 配置训练可视化工具\n内容：\n我们使用SwanLab来监控整个训练过程，并评估最终的模型效果。\n\nSwanLab 是一款开源、轻量的 AI 模型训练跟踪与可视化工具，面向人工智能与深度学习开发者，提供了一个跟踪、记录、比较、和协作实验的平台，常被称为\"中国版 Weights & Biases + Tensorboard\"。SwanLab同时支持云端和离线使用，并适配了从PyTorch、Transformers、Lightning再到LLaMA Factory、veRL等40+ AI训练框架。\n\n![09-05](./qwen3/05.png)\n![09-06](./qwen3/06.png)\n\n这里直接使用SwanLab和Transformers的集成来实现，更多用法可以参考[官方文档](https://link.zhihu.com/?target=https%3A//docs.swanlab.cn/zh/guide_cloud/integration/integration-huggingface-transformers.html)：\n\n```python\nfrom transformers import TrainingArguments, Trainer\n\nargs = TrainingArguments(\n    ...,\n    report_to=\"swanlab\",\n    run_name=\"qwen3-1.7B\",\n)\n\ntrainer = Trainer(..., args=args)\n```\n\n如果你是第一次使用SwanLab，那么还需要去[https://swanlab.cn](https://link.zhihu.com/?target=https%3A//swanlab.cn/)上注册一个账号，在用户设置页面复制你的API Key，然后在训练开始时，选择【2】，然后粘贴进去即可：\n\n![09-07](./qwen3/07.png)",
    "328": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：5. 完整代码\n内容：\n开始训练时的目录结构：\n\n```\n|--- train.py\n|--- train.jsonl\n|--- val.jsonl\n```\n\ntrain.py：\n\n```python\nimport json\nimport pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom modelscope import snapshot_download, AutoTokenizer\nfrom transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\nimport os\nimport swanlab\n\nos.environ[\"SWANLAB_PROJECT\"]=\"qwen3-sft-medical\"\nPROMPT = \"你是一个医学专家，你需要根据用户的问题，给出带有思考的回答。\"\nMAX_LENGTH = 2048\n\nswanlab.config.update({\n    \"model\": \"Qwen/Qwen3-1.7B\",\n    \"prompt\": PROMPT,\n    \"data_max_length\": MAX_LENGTH,\n    })\n\ndef dataset_jsonl_transfer(origin_path, new_path):\n    \"\"\"\n    将原始数据集转换为大模型微调所需数据格式的新数据集\n    \"\"\"\n    messages = []\n\n    # 读取旧的JSONL文件\n    with open(origin_path, \"r\") as file:\n        for line in file:\n            # 解析每一行的json数据\n            data = json.loads(line)\n            input = data[\"question\"]\n            output = f\"<think>{data[\"think\"]}</think> \\n {data[\"answer\"]}\"\n            message = {\n                \"instruction\": PROMPT,\n                \"input\": f\"{input}\",\n                \"output\": output,\n            }\n            messages.append(message)\n\n    # 保存重构后的JSONL文件\n    with open(new_path, \"w\", encoding=\"utf-8\") as file:\n        for message in messages:\n            file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n\ndef process_func(example):\n    \"\"\"\n    将数据集进行预处理\n    \"\"\"\n    input_ids, attention_mask, labels = [], [], []\n    instruction = tokenizer(\n        f\"<|im_start|>system\\n{PROMPT}<|im_end|>\\n<|im_start|>user\\n{example['input']}<|im_end|>\\n<|im_start|>assistant\\n\",\n        add_special_tokens=False,\n    )\n    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n    attention_mask = (\n        instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n    )\n    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n        input_ids = input_ids[:MAX_LENGTH]\n        attention_mask = attention_mask[:MAX_LENGTH]\n        labels = labels[:MAX_LENGTH]\n    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n\ndef predict(messages, model, tokenizer):\n    device = \"cuda\"\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\n    generated_ids = model.generate(\n        model_inputs.input_ids,\n        max_new_tokens=MAX_LENGTH,\n    )\n    generated_ids = [\n        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    return response\n\n# 在modelscope上下载Qwen模型到本地目录下\nmodel_dir = snapshot_download(\"Qwen/Qwen3-1.7B\", cache_dir=\"/root/autodl-tmp/\", revision=\"master\")\n\n# Transformers加载模型权重\ntokenizer = AutoTokenizer.from_pretrained(\"/root/autodl-tmp/Qwen/Qwen3-1.7B\", use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"/root/autodl-tmp/Qwen/Qwen3-1.7B\", device_map=\"auto\", torch_dtype=torch.bfloat16)\nmodel.enable_input_require_grads()  # 开启梯度检查点时，要执行该方法\n\n# 加载、处理数据集和测试集\ntrain_dataset_path = \"train.jsonl\"\ntest_dataset_path = \"val.jsonl\"\n\ntrain_jsonl_new_path = \"train_format.jsonl\"\ntest_jsonl_new_path = \"val_format.jsonl\"\n\nif not os.path.exists(train_jsonl_new_path):\n    dataset_jsonl_transfer(train_dataset_path, train_jsonl_new_path)\nif not os.path.exists(test_jsonl_new_path):\n    dataset_jsonl_transfer(test_dataset_path, test_jsonl_new_path)\n\n# 得到训练集\ntrain_df = pd.read_json(train_jsonl_new_path, lines=True)\ntrain_ds = Dataset.from_pandas(train_df)\ntrain_dataset = train_ds.map(process_func, remove_columns=train_ds.column_names)\n\n# 得到验证集\neval_df = pd.read_json(test_jsonl_new_path, lines=True)\neval_ds = Dataset.from_pandas(eval_df)\neval_dataset = eval_ds.map(process_func, remove_columns=eval_ds.column_names)\n\nargs = TrainingArguments(\n    output_dir=\"/root/autodl-tmp/output/Qwen3-1.7B\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=4,\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    logging_steps=10,\n    num_train_epochs=2,\n    save_steps=400,\n    learning_rate=1e-4,\n    save_on_each_node=True,\n    gradient_checkpointing=True,\n    report_to=\"swanlab\",\n    run_name=\"qwen3-1.7B\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n)\n\ntrainer.train()\n\n# 用测试集的前3条，主观看模型\ntest_df = pd.read_json(test_jsonl_new_path, lines=True)[:3]\n\ntest_text_list = []\n\nfor index, row in test_df.iterrows():\n    instruction = row['instruction']\n    input_value = row['input']\n\n    messages = [\n        {\"role\": \"system\", \"content\": f\"{instruction}\"},\n        {\"role\": \"user\", \"content\": f\"{input_value}\"}\n    ]\n\n    response = predict(messages, model, tokenizer)\n\n    response_text = f\"\"\"\n    Question: {input_value}\n\n    LLM:{response}\n    \"\"\"\n\n    test_text_list.append(swanlab.Text(response_text))\n    print(response_text)\n\nswanlab.log({\"Prediction\": test_text_list})\n\nswanlab.finish()\n```\n\n看到下面的进度条即代表训练开始：\n\n![09-08](./qwen3/08.png)",
    "329": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：6. 训练结果演示\n内容：\n在SwanLab上查看最终的训练结果：\n\n![09-09](./qwen3/09.png)\n\n可以看到核心要关注的指标有train loss和eval loss，同时还有在训练完成时记录的3条LLM生成测试结果。\n\n让我们分析一下吧！使用**「创建折线图」**功能，把train loss和eval loss放到一张图上：\n\n![09-10](./qwen3/10.png)\n\n\n蓝色的是train loss，绿色的式eval loss，可以看到发生了经典的**过拟合**现象：train loss会在epoch的交替阶段，阶梯式下降；而val loss在第1轮epoch是下降的，但是第二轮反而上升了，符合机器学习中经典过拟合的定义。\n\n这也说明，在2000条数据这个量级，全参微调做1个epoch就可以了，往后反而效果越来越差。\n\n---\n\n那我们接下来看看模型生成的效果：\n\n![09-11](./qwen3/11.png)\n\n![09-12](./qwen3/12.png)\n\n可以看到，微调后的模型明确地有了思考过程，并且在思考过程的前后**增加了think标签**。\n\n回答的质量也挺不错，不过我对医学不太了解，也希望如果有学医的同学可以一起看看！\n\n一个example：\n\n```\nQuestion: 医生，我最近胃部不适，听说有几种抗溃疡药物可以治疗，您能详细介绍一下这些药物的分类、作用机制以及它们是如何影响胃黏膜的保护与损伤平衡的吗？\n\nLLM:<think>嗯，用户问的是抗溃疡药物的分类、作用机制，以及它们如何影响胃黏膜的保护和损伤平衡。首先，我需要回忆一下抗溃疡药物的主要分类。记得以前学过，抗溃疡药物通常分为几大类，比如抑酸药、胃黏膜保护剂、促胃动力药和抗幽门螺杆菌药物。可能还有其他类型，比如吸附剂之类的，但可能用户的问题里没有提到，所以可能不需要展开。\n\n接下来，每个类别的具体药物例子。比如抑酸药，常见的有H2受体拮抗剂和质子泵抑制剂。比如雷尼替丁、奥美拉唑这些。作用机制是抑制胃酸分泌，特别是H2受体拮抗剂通过阻断组胺引起的胃酸分泌，而质子泵抑制剂直接作用于胃酸分泌的最后一步，抑制质子泵。这部分需要详细说明。\n\n胃黏膜保护剂的话，比如硫糖铝、铋剂，它们的作用是形成保护层，或者促进黏液分泌，比如硫糖铝可能通过黏附在黏膜表面形成保护膜，而铋剂可能促进黏液分泌，同时可能有中和胃酸的作用？或者可能主要是中和作用？需要确认。另外，胶体果胶铋可能也是例子。\n\n促胃动力药比如多潘立酮、西沙必利，作用是增强胃蠕动，减少胃酸反流，这样胃排空快，可能减少溃疡形成。但用户的问题里提到的是促进胃排空，所以这部分需要说明。\n\n抗幽门螺杆菌的药物通常包括抗生素，比如阿莫西林、克拉霉素，但抗幽门螺杆菌药物可能还有三联或四联疗法，比如加上PPI和铋剂。需要提到这些药物的作用机制是抑制幽门螺杆菌的生长，比如抗生素杀灭细菌，而PPI可能同时抑制胃酸分泌，但如果是抗幽门螺杆菌药物的话，可能是指专门针对该病的药物，比如可能还有铋剂或者其他药物？\n\n不过用户的问题里提到抗幽门螺杆菌药物，可能需要明确是单独针对幽门螺杆菌，还是包括抗生素和PPI。可能需要指出，抗幽门螺杆菌药物通常包括抗生素和PPI，而PPI本身是抑酸药。所以可能需要说明这些药物如何通过抑制胃酸分泌和杀灭细菌来减少溃疡。\n\n然后，关于作用机制如何影响胃黏膜的保护和损伤平衡。比如，抑酸药减少胃酸，从而减少对黏膜的侵蚀，同时保护剂形成保护层，促进黏液分泌，增强黏膜屏障。促胃动力药通过增加胃蠕动，减少胃酸反流，从而保护黏膜。而抗幽门螺杆菌药物通过减少胃酸分泌和杀灭细菌，减少损伤因素。\n\n可能需要检查是否有遗漏的药物分类，比如吸附剂如活性炭，或者中和剂如氢氧化铝，但用户的问题可能只关注主要的四类。另外，是否需要提到铋剂的其他作用，比如促进黏液分泌和前列腺素合成？比如胶体果胶铋可能含有前列腺素，增强黏液分泌。\n\n在解释机制时，要确保每个药物的作用点和如何影响保护和损伤。例如，抑酸药直接减少胃酸，而保护剂形成物理屏障。促胃动力药通过改变胃排空模式，减少反流，从而减少损伤。抗幽门螺杆菌药物通过减少胃酸和杀灭病原体，两者都减少损伤。\n\n可能需要确认每个药物的正确分类和例子是否正确，比如西沙必利是否属于促胃动力药，或者是否属于H2受体拮抗剂？西沙必利其实是属于促胃动力药，因为它的作用是增强胃排空。而奥美拉唑是PPI，属于抑酸药。需要确保这些分类正确。\n\n另外，用户提到自己胃部不适，可能需要建议就医，但答案里已经包含这点，所以没问题。\n\n总结下来，结构应该是先分类，再每个类别下的药物例子和作用机制，然后说明这些机制如何影响保护和损伤平衡。需要确保每个部分都清晰，逻辑连贯，并且覆盖用户的问题点。\n\n</think>\n\n当然可以。抗溃疡药物主要分为四类：抑酸药、胃黏膜保护剂、促胃动力药和抗幽门螺杆菌药物。抑酸药如H₂受体拮抗剂和质子泵抑制剂，它们的作用是减少胃酸的分泌，从而减轻对胃黏膜的损伤。胃黏膜保护剂如硫糖铝和胶体果胶铋，它们能帮助形成一层保护膜，覆盖在胃黏膜表面，减少胃酸和胃蛋白酶对黏膜的侵蚀。促胃动力药如多潘立酮和西沙必利，通过增强胃部的蠕动，减少胃内食物的滞留时间，从而降低溃疡的风险。抗幽门螺杆菌药物则通过抑制幽门螺杆菌的生长，减少其对胃黏膜的伤害。这些药物的共同目标是通过不同的机制来平衡胃黏膜的保护因素和损伤因素，帮助溃疡愈合。如果您有具体的症状或担心某种药物，可以告诉我，我会为您推荐合适的治疗方案。\n\n```\n\n至此，你已经完成了qwen3微调！",
    "330": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：7. 推理训练好的模型\n内容：\n训好的模型默认被保存在`./output/Qwen3`文件夹下。\n\n推理模型的代码如下：\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndef predict(messages, model, tokenizer):\n    device = \"cuda\"\n\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\n    generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=2048)\n    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    return response\n\n# 加载原下载路径的tokenizer和model\ntokenizer = AutoTokenizer.from_pretrained(\"./output/Qwen3-1.7B/checkpoint-1000\", use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"./output/Qwen3-1.7B/checkpoint-1000\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n\ntest_texts = {\n    'instruction': \"你是一个医学专家，你需要根据用户的问题，给出带有思考的回答。\",\n    'input': \"医生，我最近被诊断为糖尿病，听说碳水化合物的选择很重要，我应该选择什么样的碳水化合物呢？\"\n}\n\ninstruction = test_texts['instruction']\ninput_value = test_texts['input']\n\nmessages = [\n    {\"role\": \"system\", \"content\": f\"{instruction}\"},\n    {\"role\": \"user\", \"content\": f\"{input_value}\"}\n]\n\nresponse = predict(messages, model, tokenizer)\nprint(response)\n```",
    "331": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：相关链接\n内容：\n- 代码：完整代码直接看本文第5节 或 [Github](https://github.com/Zeyi-Lin/Qwen3-Medical-SFT)\n- 实验日志过程：[qwen3-1.7B-linear - SwanLab](https://swanlab.cn/@ZeyiLin/qwen3-sft-medical/runs/agps0dkifth5l1xytcdyk/chart)，或 [SwanLab基线社区](https://swanlab.cn/benchmarks) 搜索“qwen3-sft-medical”\n- 模型：[Modelscope](https://modelscope.cn/models/Qwen/Qwen3-1.7B)\n- 数据集：[delicate_medical_r1_data](https://modelscope.cn/datasets/krisfu/delicate_medical_r1_data)\n- SwanLab：[https://swanlab.cn](https://swanlab.cn)",
    "332": "一级标题：Qwen3-\"VL\"——超小中文多模态模型的“拼接微调”之路1（附代码和SwanLab记录）\n二级标题：无\n内容：\n* 作者：情感机器实验室——陈少宏\n\n* 邮箱：<shaohon_chen@115lab.club>\n\n* GitHub：[https://github.com/ShaohonChen/Qwen3-SmVL](https://github.com/ShaohonChen/Qwen3-SmVL)\n* SwanLab：[https://swanlab.cn/@ShaohonChen/Qwen3-SmVL/overview](https://swanlab.cn/@ShaohonChen/Qwen3-SmVL/overview)\n* 数据集：[https://huggingface.co/datasets/HuggingFaceM4/the_cauldron](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron)\n\n[![](https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg)](https://swanlab.cn/@ShaohonChen/Qwen3-SmVL/overview)",
    "333": "一级标题：Qwen3-\"VL\"——超小中文多模态模型的“拼接微调”之路1（附代码和SwanLab记录）\n二级标题：摘要\n内容：\n最近Huggingface团队发布了超小多模态模型SmolVLM2，可以做到端侧1GB显存推理。在怀着惊喜试用后发现，虽然模型有极其强大的视觉文本理解能力，但是模型却无法理解中文。这对一个“四六级压线过”的笔者来说十分不友好。刚好前段时间做SwanLab硬件检测适配时有一台未到期的沐曦曦云C500服务器，因此萌生了使用**沐曦GPU芯片**微调、把当前中文小模型扛把子Qwen3与SmolVLM2直接微调拼接的想法。\n\n本教程将介绍一种模型拼接的思路，将SmolVLM2的视觉模块（0.09B）与Qwen3最小的模型（0.6B）进行对齐微调，最终使得Qwen模型具备一定的视觉理解能力。由于笔者时间有限且考虑到文章篇幅的原因，因此该系列预计将以系列的方式放出。篇幅规划如下：\n\n* **第一篇**：如何构建和微调一个拼接模型（**本篇博客**）\n* **第二篇**：模型测评、数据集优化、回答人类对齐\n* **第三篇**：微调技巧介绍、视觉位置编码改动与模型结构优化\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/PPAP.png\" alt=\"PPAP\" width=\"400\" />\n  <figcaption>I have a Qwen, I have a SmolVLM...</figcaption>\n  </figure>\n</div>\n\n<div style=\"background-color:#fff3cd; color:black; padding:10px; border-radius:4px; border:1px solid #fbe5b0; width: 90%; max-width: 100%; margin: auto;\">\n  ⚠️关于算力的注意：本教程涉及VLM微调训练，对算力要求较高，需要40G及以上的GPU显存才能运行本教程的训练代码。\n</div>",
    "334": "一级标题：Qwen3-\"VL\"——超小中文多模态模型的“拼接微调”之路1（附代码和SwanLab记录）\n二级标题：目录\n内容：\n[[toc]]",
    "335": "一级标题：Qwen3-\"VL\"——超小中文多模态模型的“拼接微调”之路1（附代码和SwanLab记录）\n二级标题：SmolVLM2的背景知识\n内容：\n首先，我们先回顾一下SmolVLM2模型的构建方案，SmolVLM2模型的整体包括三大块：视觉模型层，特征映射层和大语言模型层，见下图：\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/smolvlm2.png\" alt=\"smolvlm2\" width=\"400\" />\n  <figcaption>SmolVLM2的架构图</figcaption>\n  </figure>\n</div>\n\n这个设计是现在比较常见的VLM方案。核心设计思想就是让视觉模型的输出特征与经过embedding的文本特征直接拼接后输入到语言模型（LLM）当中，没有交叉注意力等模块。相比于早期LLaVA等架构，这种最大的优点就是可以最大程度复用已有的语言模型。以Qwen2.5-VL为例，其3B、7B、72B模型大小指的只是LLM部分，并没有包含Vision模块，实际上3B模型的参数量接近4B，视觉模块大概0.4B左右，三个不同大小的VLM使用的是统一的视觉模型。对于一些较大的VLM来说，构建视觉模型时绝大多数的训练都集中在特征映射模块和视觉模块，只在最后阶段为了最终效果进行整体微调时才会调整语言模块。保证了VLM的语言能力。\n\n下面简述一下各个模块的细节：\n\n* 视觉模型层：SmolVLM2-256M版本用的是Google的SigLip模型，一个基于ViT的视觉模型，选用的是最小的SigLip-93M的版本，HF论文里没具体写是直接用的SigLip的参数还是他们从零构建的（有注意到的读者可以评论留言下）。在SmolVLM2代码中对应的是`SmolVLMVisionTransformer`类\n\n* 特征映射层：就是一个简单的MLP，不过SmolVLM2中为了降低图像分辨率还做了一个Pixel shuffle来降低图像分辨率，进一步减少视觉的Token占用，减少了文本长度。HF团队在论文里提到对于参数量较小的VLM来说使用Pixel shuffle还能提升性能。但可训练参数其实就是一个单层的神经网络，这个模块的核心作用就是做特征对齐，将视觉特征从768维（SigLip的维度）映射到576维（SmolLLM2的维度）\n\n* 大语言模型：SmolVLM2-256M模型使用的文本模型是SmolLM-135M版本。可能是由于模型较小，HF团队在论文中说到训练时仅采用两阶段训练：大规模图文训练+针对视频任务的专门微调。为了保障模型的文本能力HF团队在训练数据中参杂了大概14%的纯文本微调数据。不过考虑到视觉模块本身参数量（93M）大小接近于文本模型（135M），因此笔者推测相比于冻结文本模型，数据平衡在这之中会起到更关键的作用。\n\nHF团队在原文中还提到了许多影像小模型VLM性能的trick，感兴趣的读者可以进一步参考SmolVLM2的论文",
    "336": "一级标题：Qwen3-\"VL\"——超小中文多模态模型的“拼接微调”之路1（附代码和SwanLab记录）\n二级标题：模型拼接和微调思路简介\n内容：\n正所谓顶级食材（模型）只需要最简单的烹饪。模型拼接的思路非常简单直接，基本就三步：\n\n1. 调整SmolVLM2的“上下文控制格式”，使得其与Qwen3兼容。\n\n2. 将模型的文本部分直接从SmolLM2换成Qwen3-0.6B，包括其文本tokenizer和词嵌入、文本模型、以及模型最后输出的语言模型头（LM Head）。\n\n3. 需要重新初始化特征映射层的MLP，从768->576的单层神经网络改成768->1024的单层神经网络即可。\n\n整体架构和对图文对前后处理依旧保持SmolVLM2的流程不变，具体改动见下图：\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/concatation.png\" alt=\"concatation\" width=\"400\" />\n  <figcaption>将Qwen3-0.6B替换SmolVLM2的语言模型部分</figcaption>\n  </figure>\n</div>\n\n笔者接下来详细介绍下为了实现“拼接”，具体改动的地方，供之后有类似的任务的读者参考。",
    "337": "一级标题：Qwen3-\"VL\"——超小中文多模态模型的“拼接微调”之路1（附代码和SwanLab记录）\n二级标题：模型拼接实现和关键代码讲解\n内容：\n### 第一处改动：SmolVLM2的Tokenizers部分\n\n首先需要改动的就是需要改动的是SmolVLM2的Tokenizers部分，这里面主要是涉及两个问题：\n\n* 第一个问题是要将SmolVLM2用于指示图像位置的特殊令牌（Special Token）加入到Qwen3的Tokenizer当中，这么做的目的是防止SmolVLM2的图像Token`<image>`被切分为`<`、`image`、`>`三块。幸运的是，Qwen3本身在Tokenizers中预留了未来用于多模态的特殊特殊令牌`<|image_pad|>`。因此读者直接使用了`<|image_pad|>`代替了`<image>`。用于在文本中预留图像特征的插入点。\n\n* 第二个问题是：SmolVLM2的chat_template和Qwen3的chat_template差别极大。chat_template的作用是通过格式化文本让模型清楚知道不同Token所代表的背景信息。用最近比较流行的话来说就是“上下文工程”（Context Engineering）。\n\n这里我列举了一下Qwen3、SmolVLM2、Qwen2.5-VL在聊天场景下的上下文，供读者参考。\n\n**Qwen3聊天上下文格式**\n\n以给一张图片，问题是“你的名字是什么?”，模型回答是“我的名字是Qwen”为例子。模型的上下文如下：\n\n```txt\n<|im_start|>user\n你的名字是什么?<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n我的名字是Qwen<|im_end|>\n\n```\n\n注意Qwen3上下文是没有预留图像位置的，但相比于一般的LLM和VLM多了一个用于插入模型思考过程的`<think><\\think>`，以及包含额外的函数调用控制文本。为了便于读者理解，读者在在下面举了一个函数调用的例子。这些函数调用上下文用于控制模型调用外部函数、API或者MCP接口和接收其返回的信息。\n\n考虑到篇幅限制，本文就不粘贴带函数调用、推理、思考等一系列上下文的信息了（笔者打印了下发现实在太长了）。感兴趣的读者可以在Qwen3的官方文处了解详细设计\n\n* [Qwen3函数调用案例](https://qwen.readthedocs.io/zh-cn/latest/framework/function_call.html#the-example-case)\n\n可以说正是这些复杂的上下文信息让模型有可能实现推理、调用函数等多样化的能力。包括多模态理解任务也需要先对上下文进行设计。\n\n**SmdwadwdoVLM2聊天上下文格式：**\n\n以给一张图片，问题是“How many dog in there.”，模型回答是“There are Three dogs.”为例子。三种不同模型的上下文如下：\n\n```txt\n<|im_start|>User:<fake_token_around_image><row_1_col_1><image>...<image><fake_token_around_image><row_1_col_2><image>...<image><fake_token_around_image><row_1_col_3><image>...<image>...<fake_token_around_image><row_4_col_4><image>...<image>\n\n<fake_token_around_image><global-img><image>...<image><fake_token_around_image>How many dog in there.<end_of_utterance>\nAssistant: There are Three dogs.<end_of_utterance>\nAssistant:\n```\n\n看起来非常乱，是因为有大量的`<image>`占位符。`<image>...<image>`之间是许多的`<image>`，笔者为了文章观感删掉了大量的占位符。注意模型的回车、空格均为上下文的一部分，在进行推理时需要严格遵守缩进关系。\n\n但是我们仍能找到熟悉的内容，如`User:`，`Assistant:`等用于提示模型用户的输入与模型应当输出的位置。这些关键词和Qwen类似。\n\n读者注意到了除了`<fake_token_around_image>`，`<image>`等用于指示图像的词，还出现了<row_1_col_1>这种位置指示符，这是因为SmolVLM2为了防止降采样对图像分辨率影响，专门使用了`image splitting`技术，简单来说就是将全局图和高清的局部图共同输入到模型当中（见下图`image splitting`模块），感兴趣的读者可在文末找到HF的技术报告了解详细技术。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/image-split.png\" alt=\"image-split\" width=\"400\" />\n  <figcaption>SmolVLM2的完整推理流程，可以看到在图像输入前使用`image splitting`进行了预切分</figcaption>\n  </figure>\n</div>\n\n**本博文的拼接模型Qwen3-SmVL模型**\n\n相比于Qwen3，SmolVLM2少了很多上下控制的\n\n为了尽可能保存或者说预留Qwen3的思考、函数调用等能力，笔者最终选择将SmolVLM2对于图像特征的排列插入到Qwen3的上下文格式当中。最终上下文格式如下：\n\n```txt\n<|im_start|>user\n<vision_start><row_1_col_1><|image_pad|>（图像插入的地方）<|image_pad|><vision_start>\n（用户提问的地方）\n<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n（模型回答的地方）<|im_end|>\n<|endoftext|>\n```\n\n可以看到读者尽量保持了与Qwen3的风格和复用特殊令牌。这样能够使得后续拼接的Qwen3-0.6B模型不至于受到上下文差异过大带来的性能损耗。实际上在设计微调上下文时应尽量与模型先前训练的任务接近，以减少微调带来的性能损失。\n\ntransformers实现模型上下文格式控制的代码并非python语言，而是一种前端文本格式控制的语言Jinja。这个语言的变量作用域设计简直可以说是有魔法在里面。配合上Qwen3功能丰富且复杂的上下文策略，让笔者花了2个小时用于修改chat_teamplate。这里笔者不赘述如何修改chat_template，感兴趣的读者可以去文末代码链接寻找`chat_template.jinja`文件，笔者专门将chat_template模版拿出来，并且做了格式化方便读者阅读。未来有时间了笔者专门写一篇模型上下文控制与jinja语言的博客。\n\n### 第二处改动：替换SmolVLM2的SmolLM2模型为Qwen3-0.6B\n\n替换模型这块没什么复杂的，主要是需要处理Transformers比较复杂的嵌套逻辑。Tranformers通常建议模型将预训练模型backbone和下游任务分开来。改动逻辑图如下：\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/change_model.png\" alt=\"change_model\" width=\"400\" />\n  <figcaption>替换smolvlm2的文本模块和语言模型头</figcaption>\n  </figure>\n</div>\n\n以Qwen3为例，预训练Backbone模型为`Qwen3Model`，仅仅包含embedding层、各个Decoder层，最后输出的是所有输入token的hidden state。负责下游任务的Qwen3提供了包括：用于因果语言序列生成的`Qwen3ForCausalLM`，也就是大家常用的语言生成。负责句子分类`Qwen3ForSequenceClassification`，使用最后一个生成的token输入到一个单层MLP做序列级分类，做句子情绪分类等可以用这个下游模型；`Qwen3ForTokenClassification`用于做Token级分类，比如语言实体抽取任务可以使用这个下游模型。`Qwen3ForQuestionAnswering`则是专门做抽取式问答任务的模型，核心思想是输入（问题，参考文本）让模型从参考文本中找到与问题最相关的一段，这类任务由于RAG系统的出现没那么流行了，未来笔者专门出一个系列的教程阐述除了因果语言序列生成以外的任务则怎么微调。\n\n**关键代码如下**\n\n```python\nfrom transformers import (\n    AutoProcessor,\n    AutoModelForImageTextToText,\n    AutoTokenizer,\n    AutoModelForCausalLM\n)\n\n# 替换text模型和head\nsmolvlm2_02B_model = AutoModelForImageTextToText.from_pretrained(\n    \"model/SmolVLM2-256M-Video-Instruct\",\n    torch_dtype=torch.bfloat16,\n    _attn_implementation=\"eager\",\n).to(device)\n\nqwen3_06b_model = AutoModelForCausalLM.from_pretrained(\n    \"model/Qwen3-0.6B\", torch_dtype=torch.bfloat16\n).to(device)\n\nsmolvlm2_02B_model.model.text_model = qwen3_06b_model.model\nsmolvlm2_02B_model.lm_head = qwen3_06b_model.lm_head\n...\n```\n\n接下来比较复杂的是替换所有的关键变量，比如模型内用于在文本序列中为图像特征预留的占位符`image_token_id`，用于指示停止生成的`eos_token_id`，和计算loss值会用到的`vocab_size`，Qwen的词表大小为151936，远远大过SmolVLM2的词表49280。具体代码如下：\n\n```python\n...\n# 替换词表大小\nsmolvlm2_02B_model.vocab_size = qwen3_06b_model.vocab_size\nsmolvlm2_02B_model.model.vocab_size = qwen3_06b_model.vocab_size\nsmolvlm2_02B_model.config.vocab_size = qwen3_06b_model.vocab_size\nsmolvlm2_02B_model.config.text_config.vocab_size = qwen3_06b_model.vocab_size\nsmolvlm2_02B_model.model.config.vocab_siz = qwen3_06b_model.vocab_size\nsmolvlm2_02B_model.model.config.text_config.vocab_size = qwen3_06b_model.vocab_size\n# 替换图像token\nsmolvlm2_02B_model.image_token_id = 151655\nsmolvlm2_02B_model.model.image_token_id = 151655\nsmolvlm2_02B_model.config.image_token_id = 151655\nsmolvlm2_02B_model.model.config.image_token_id = 151655\n# 替换模型生成停止符\nsmolvlm2_02B_model.generation_config.eos_token_id = 151645\n···\n```\n\n上面的代码可以看到在替换各个变量时需要将嵌套模型的变量一起替换掉，笔者之前训练时就因为仅仅替换了`SmolVLMForConditionalGeneration`而忘记替换`SmolVLMModel`中的`image_token_id`，导致语言模型接收不到图像特征，最后表现出来就是loss下降的极快且低，grad_norm看起来也学到位了，一推理效果特别差，附上错误训练的损失图：\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/fail_train.png\" alt=\"fail_train\" width=\"800\" />\n  <figcaption>SwanLab记录训练结果展示：蓝色为错误训练的完整微调loss图，可以看到损失下降很快，然而实际推理会发现模型并没有图像理解能力。冻结语言模型头（红色）后发现grad_norm为零且loss不收敛，正确的应该是黄色</figcaption>\n  </figure>\n</div>\n\n笔者最早没发现改动错误，先做完整微调（蓝色曲线）后发现损失下降很快达到了0.1以下，结果实际一推理发现模型完全没有图像理解能力，就补了一个冻结语言模型只微调视觉模型的实验（红色曲线），结果发现损失完全没下降，才定位到了视觉特征传入有问题。后续修复后正确的损失下降过程见黄色图像。\n\n### 第三处改动：构建和替换特征映射层\n\n这个相对较简单，只需要重新构建一个维度对齐的`SmolVLMConnector`即可。Qwen3的hidden_dim是1024，SigLip的hidden_dim是768，因此构建一个768➡️1024映射的`SmolVLMConnector`即可。代码如下：\n\n```python\n···\n# 构建配置并且创建连接器\n@dataclass\nclass VisionConfig:\n    hidden_size: int = 768\n\n@dataclass\nclass TextConfig:\n    hidden_size: int = 1024\n\n@dataclass\nclass ConnectConfig:\n    scale_factor: int = 4\n    vision_config: VisionConfig = VisionConfig()\n    text_config: TextConfig = TextConfig()\n\nnew_connector_config = ConnectConfig()\n\n# 替换 SigLit 到 LLM 的 connector 层\nnew_connector = SmolVLMConnector(new_connector_config).to(device).to(torch.bfloat16)\nsmolvlm2_02B_model.model.connector = new_connector\n···\n```",
    "338": "一级标题：Qwen3-\"VL\"——超小中文多模态模型的“拼接微调”之路1（附代码和SwanLab记录）\n二级标题：微调数据集构建\n内容：\n笔者最初计划寻找中文多模态数据集，但发现相关的资料比较少。因此决定先用英文的多模态数据集凑合一下。之后再考虑通过数据合成的方式将部分数据翻译为中文。关于数据合成和配比的问题将在之后的博客讨论。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/the_cauldron.png\" alt=\"the_cauldron\" width=\"400\" />\n  <figcaption>the_cauldron数据集logo</figcaption>\n  </figure>\n</div>\n\n这里为了方便本项目直接使用HuggingFace团队整合的多模态数据集the Cauldron数据集，Cauldron翻译成中文类似于煮东西的“釜”，不知道HF团队是不是玩“炼丹”的梗。这个数据集整合了50个视觉微调任务数据集的训练集，用于微调Huggingface发布的多模态模型Idefics2模型。这50多个数据集都被处理成了一致的格式（见下图），共有1,880,992条数据，完整下载约169G，非常方便使用。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/data_show.png\" alt=\"data_show\" width=\"800\" />\n  <figcaption>数据集样本展示</figcaption>\n  </figure>\n</div>\n\n不过可惜数据集的文本都是英文内容，且绝大多数数据集的回复非常短，只有一个词，这也给后面模型训练带来了麻烦。本篇博客暂时不讨论关于数据构建和配比的问题，后续有时间了专门做相关的实验。本博客先以为Qwen3模型带来视觉能力为核心目标。\n\n数据集的下载链接如下，国内推荐用modelscope下载：\n\n* [HuggingFace Hub](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron)\n* [ModelScope](https://modelscope.cn/datasets/AI-ModelScope/the_cauldron)\n\n笔者在实际测试时发现\"mimic_cgd\"，\"localized_narratives\"，\"okvqa\"，\"ocrvqa\"，\"clevr_math\"这几个子数据集加载有点异常，建议使用此数据集训练的读者手动处理下，社区也有用户反馈这几个数据可以在原始来源处额外下载，未来笔者将会补全这几个数据集重新上传一次完整版的the Cauldron数据集。",
    "339": "一级标题：Qwen3-\"VL\"——超小中文多模态模型的“拼接微调”之路1（附代码和SwanLab记录）\n二级标题：微调方法与代码实现\n内容：\n### 冻结模型参数微调\n\n整体微调方法采用了CLM模型通常的Teacher Forcing的学习方法，损失就是标准的交叉熵损失。考虑到此次本教程的目标是先确保模型具备中文多模态能力（优化模型性能等之后撰写其他博客），因此为了实验效率，在对齐微调阶段**采用冻结视觉模型与文本模型，仅微调特征映射器和语言模型头**的方法。\n\n冻结模型参数的核心代码如下：\n\n```python\ndef freeze_model(qwen_smvl):\n    for _, param in qwen_smvl.model.text_model.named_parameters():\n        param.requires_grad = False\n    for _, param in qwen_smvl.model.vision_model.named_parameters():\n        param.requires_grad = False\n    return qwen_smvl\n```\n\n冻结后训练参数、模型总参数、与占比如下：\n\n```txt\ntrainable params: 12.00M || all params: 662.87M || trainable%: 1.81\n```\n\n### 文本长度，损失掩码和截断策略\n\n**文本长度**\n\n由于视觉特征需要占据大量的文本长度，笔者简单测试了下the_cauldron图像占0.8K到1.3K左右的token。而数据集中大多数文本token数在200-500左右，极少情况会有3-4K的情况。因此笔者统一采用2K的文本长度，超出部分截断处理。\n\n这里有一个不同于文本微调的细节要注意，文本截断长度不能小于图像token，否则会导致模型在进行特征拼接时报错（当然图像特征如果被截断了，这条训练数据也就没意义了）。因此对于显存不足64G的同学如果需要适当缩短文本长度（不建议低于1.5K），最好连同图像分辨率也缩小些。在后面的博客我们会专门增加对减少图片token占用的研究。\n\n同样由于文本长度受限，且图像特征没法截断，我们也没使用“packing dataset”的方法提升模型的训练效率。\n\n考虑到部分数据集存在多张图片的情况，考虑到本次训练仅采用2k的文本长度（与之对比HF在训练SmolVLM-256M版本采用的是8K的文本长度，2.2B版使用了16K的文本长度）。针对单条数据中存在多张图片的情况仅仅选用第一张。\n\n**损失掩码**\n\n在采用Teacher Forcing的学习方法时，文本微调中损失掩码有两种策略：\n\n* 对包含“用户问题”和“模型回复”的完整文本进行微调优化\n* 仅对“模型回复”部分进行微调优化\n\n这两种策略的对比如下图：\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/mask.png\" alt=\"mask\" width=\"800\" />\n  <figcaption>两种微调掩码策略的差异，通常建议选择“仅微调模型回答部分”以增强泛化性</figcaption>\n  </figure>\n</div>\n\n通常来说使用“仅微调模型回复部分”的策略模型更容易泛化（这点与HF在SmolVLM2的论文提到的trick）。然而笔者为了提高训练效率选择了完整文本微调。可以在后续博客中增加消融实验做进一步对比。\n\n值得注意的是，在进行完整文本微调时，需要单独屏蔽Image Token以防止对图像占位token计算损失，影响模型表现。\n\n**关键代码如下：**\n\n```python\ndef data_collate_fix2k(examples, processor, device, max_length=2048):\n    batch_text = []\n    batch_image = []\n    for example in examples:\n        images = example[\"images\"][:1]  # 只允许一张图，不然显存压力太大\n        batch_image.append(images)\n        image_num = len(images)\n        chat_texts = example[\"texts\"][0]\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [{\"type\": \"image\"}] * image_num\n                + [{\"type\": \"text\", \"text\": chat_texts[\"user\"]}],\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": [{\"type\": \"text\", \"text\": chat_texts[\"assistant\"]}],\n            },\n        ]\n        text = processor.apply_chat_template(\n            messages, enable_thinking=False, add_generation_prompt=False\n        )\n\n        batch_text.append(text)\n\n    batch = processor(\n        text=batch_text,\n        images=batch_image,\n        max_length=max_length,\n        return_tensors=\"pt\",\n        padding=\"max_length\",\n        truncation=True,\n    )\n    labels = batch[\"input_ids\"].clone()\n    labels[labels == processor.tokenizer.pad_token_id] = -100\n    labels[labels == processor.image_token_id] = -100\n    batch[\"labels\"] = labels\n    return batch.to(device, dtype=torch.bfloat16)\n```\n\n### 微调超参数设置\n\n**学习率**\n\n由于仅仅针对特征映射层（connector）进行训练，且conntector由于要对齐Qwen3的维度因此参数为随机初始化（理论上可以采用一些独特的初始化策略提升性能，但考虑到模型较小因此笔者没关注初始化策略）。因此学习率设置为lora中较为流行的1e-4学习率策略。\n\n为了保障有效收敛，学习率衰减基本是必备的trick，采用的是社区比较流行的cosine学习率衰减，衰减至0。warm up为整体步长的10%（在超过1000k step的情况下固定为50）。\n\n**batch size**\n\nBatch size通常来说越大越好，然而由于VLM模型的文本长度太大，因此采用每卡1 batch和4梯度累加（grad accelerate），在8卡训练中等效32 Batch size。\n\n**训练参数设置代码**\n\n```python\ntraining_args = TrainingArguments(\n    seed=42,\n    data_seed=42,\n    max_steps=200,\n    # num_train_epochs=1,  # 训练1个epoch 约1k steps\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    dataloader_pin_memory=False,\n    warmup_ratio=0.1,\n    learning_rate=1e-4,\n    lr_scheduler_type=\"cosine\",\n    weight_decay=0.01,\n    logging_steps=5,\n    eval_strategy=\"steps\",\n    eval_steps=0.125,\n    save_strategy=\"steps\",\n    save_steps=0.125,\n    save_total_limit=8,\n    optim=\"adamw_torch\",\n    bf16=True,\n    output_dir=f\"./model/freeze_except_connector_cocovqa\",\n    overwrite_output_dir=False,\n    report_to=\"swanlab\",\n    run_name=\"freeze_except_connector_cocovqa\",\n    remove_unused_columns=False,\n    gradient_checkpointing=False,\n)\n```\n\n### 训练环境\n\n微调代码基于沐曦的C500国产通用计算GPU实现，显存为64G。沐曦的AI芯片基本完全兼容pytorch和huggingface transformers场景，并且在做多模态训练时相比较其他国产AI芯片罕见的没有兼容性问题。读者在尝试本项目代码时可以采用Nvidia显存40G以上的显卡运行本教程。\n\n**笔者个人感觉沐曦的GPU整体适配效果还是非常好的，没遇到适配性的问题。体验上和用NV的GPU做训练没什么区别**。笔者自己也用过好几款国产GPU，沐曦的体验肯定是名列前茅的，包括代码中有指定flash attention在沐曦GPU上都能成功迁移，这点非常值得给沐曦团队点个赞。希望国产GPU生态能越发展越好，造福广大炼丹师；）。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/muxi-gpu.jpg\" alt=\"muxi-gpu\" width=\"400\" />\n  <figcaption>沐曦国产GPU，笔者用的云端服务器没见过真机，因此找了张网图</figcaption>\n  </figure>\n</div>\n\n训练环境的话除了安装GPU对应的驱动和pytorch外，本教程需要额外安装Huggingface全家桶，如下：\n\n```txt\ntorch   # 推荐版本>=6.0\ntorchvision\ntransformers>=4.53.0\naccelerate\ndatasets\nnum2words   # SmolVLM2需要\n```\n\n额外补充一句，如果采用沐曦GPU训练的话，需要在沐曦官方文档处寻找[沐曦版torch](https://developer.metax-tech.com/softnova/index)的安装方式进行下载。其他HF环境和NV基本一样。附赠一个沐曦查看GPU的命令：\n\n```bash\nmx-smi\n```\n\n效果如下：\n\n```bash\n=================== MetaX System Management Interface Log ===================\nTimestamp                                         : Sat Jul 12 14:58:51 2025\n\nAttached GPUs                                     : 8\n+---------------------------------------------------------------------------------+\n| MX-SMI 2.1.12                       Kernel Mode Driver Version: 2.12.13         |\n| MACA Version: 2.29.0.19             BIOS Version: 1.22.3.0                      |\n|------------------------------------+---------------------+----------------------+\n| GPU         NAME                   | Bus-id              | GPU-Util             |\n| Temp        Pwr:Usage/Cap          | Memory-Usage        |                      |\n|====================================+=====================+======================|\n| 0           MetaX C500             | 0000:0e:00.0        | 0%                   |\n| 36C         69W / 350W             | 5680/65536 MiB      |                      |\n+------------------------------------+---------------------+----------------------+\n| 1           MetaX C500             | 0000:0f:00.0        | 0%                   |\n| 38C         70W / 350W             | 4986/65536 MiB      |                      |\n+------------------------------------+---------------------+----------------------+\n| 2           MetaX C500             | 0000:10:00.0        | 0%                   |\n| 37C         69W / 350W             | 4986/65536 MiB      |                      |\n+------------------------------------+---------------------+----------------------+\n| 3           MetaX C500             | 0000:12:00.0        | 1%                   |\n| 37C         71W / 350W             | 4986/65536 MiB      |                      |\n+------------------------------------+---------------------+----------------------+\n| 4           MetaX C500             | 0000:35:00.0        | 0%                   |\n| 37C         70W / 350W             | 4986/65536 MiB      |                      |\n+------------------------------------+---------------------+----------------------+\n| 5           MetaX C500             | 0000:36:00.0        | 1%                   |\n| 36C         68W / 350W             | 4986/65536 MiB      |                      |\n+------------------------------------+---------------------+----------------------+\n| 6           MetaX C500             | 0000:37:00.0        | 0%                   |\n| 39C         73W / 350W             | 4986/65536 MiB      |                      |\n+------------------------------------+---------------------+----------------------+\n| 7           MetaX C500             | 0000:38:00.0        | 0%                   |\n| 38C         71W / 350W             | 4986/65536 MiB      |                      |\n+------------------------------------+---------------------+----------------------+\n\n+---------------------------------------------------------------------------------+\n| Process:                                                                        |\n|  GPU                    PID         Process Name                 GPU Memory     |\n|                                                                  Usage(MiB)     |\n|=================================================================================|\n|  0                  3496691         python3.10                   4066           |\n|  0                  3496692         python3.10                   102            |\n|  0                  3496693         python3.10                   102            |\n|  0                  3496694         python3.10                   102            |\n|  0                  3496695         python3.10                   102            |\n|  0                  3496696         python3.10                   102            |\n|  0                  3496697         python3.10                   102            |\n|  0                  3496698         python3.10                   170            |\n|  1                  3496692         python3.10                   4154           |\n|  2                  3496693         python3.10                   4154           |\n|  3                  3496694         python3.10                   4154           |\n|  4                  3496695         python3.10                   4154           |\n|  5                  3496696         python3.10                   4154           |\n|  6                  3496697         python3.10                   4154           |\n|  7                  3496698         python3.10                   4154           |\n+---------------------------------------------------------------------------------+\n```\n\n### 训练代码实现\n\n在构建训练代码时，笔者使用HuggingFace Transfomers框架的Trainer类来完成训练代码。Trainer类实现的训练逻辑基本能完成大部分微调任务。这里唯一需要提到的是笔者使用了Qwen3-0.6B而非通常此类任务该使用的Qwen3-0.6B-Base模型，Qwen3-0.6B相比于Qwen3-0.6B-Base模型经过了指令遵从微调、对齐等，能实现聊天问答功能。\n\n通常来说对经过微调的模型进行持续训练会一定程度带来性能损失，然而此次微调时笔者冻结了LLM参数，因此需要选用经过微调的模型来实现多模态问答能力。\n\n笔者在训练过程中使用的是bfloat16精度，相比于float16来说bfloat16增加了尾数位数，训练过程中精度会更高些。\n\n在前期进行方案验证阶段笔者采用的是cocoqa数据集，并且进行200steps的微调训练。在确定方案可行后笔者计划使用完整数据集进行微调训练，然而考虑到训练数据量仅仅只有整个模型的12M，因此笔者按参数量与训练Token的比值为1:10采样数据集，即总共从数据集中采样出60K条数据用于实际训练（文本长度按照2k计算，实际上有padding部分因此实际参与token数小于120M）。笔者认为参与训练的数量是足以令模型收敛的，后续实验也证明了模型确实能达到我们所期望的效果。\n\n**训练关键代码实现**\n\n代码比较长是因为增加了断点续训的能力\n\n```python",
    "340": "# 开启训练",
    "341": "last_checkpoint = None  # load last checkpoint if available\nif (\n    os.path.isdir(training_args.output_dir)\n    and not training_args.overwrite_output_dir\n):\n    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n        raise ValueError(\n            f\"Output directory ({training_args.output_dir}) already exists\"\n        )\n    print(\n        f\"Checkpoint detected, resuming training at {last_checkpoint}.\"\n    )\n# Init Trainer\ntrainer = Trainer(\n    model=qwen_smvl,\n    args=training_args,\n    train_dataset=raw_data[\"train\"],\n    eval_dataset=raw_data[\"test\"],\n    data_collator=collate_fn,\n)\ntrainer.train(resume_from_checkpoint=last_checkpoint)\nqwen_smvl.save_pretrained(training_args.output_dir)\n```\n\n完整代码见[代码及数据集链接汇总](#代码及数据集链接汇总)\n\n或者直接由[完整项目GitHub地址](https://github.com/ShaohonChen/Qwen3-SmVL)",
    "342": "一级标题：Qwen3-\"VL\"——超小中文多模态模型的“拼接微调”之路1（附代码和SwanLab记录）\n二级标题：微调训练&结果展示\n内容：\n### 环境安装与微调代码执行\n\n**代码准备与环境安装**\n\n可以在[GitHub仓库地址](https://github.com/ShaohonChen/Qwen3-SmVL)处找到实验的完整代码。使用git clone后使用如下命令安装环境\n\n```bash\npip install -r requirements.txt\n```\n\n**数据集和模型下载**\n\n笔者附上自动下载脚本，注意该脚本使用[魔塔社区](https://modelscope.cn/)完成模型与数据集的下载\n\n```bash\nbash download_resource.sh\n```\n\n### 小批量微调训练\n\n为了进行快速验证，笔者首先使用cocoqa数据集并且进行了200steps的训练，所有参数与前文所述一致。通过\n\n运行实验命令如下，推荐使用8卡进行训练，在8张沐曦GPU卡上预计需要使用20min\n\n```bash\n# 单GPU训练\nCUDA_VISIBLE_DEVICES=0 python train.py ./cocoqa_train.yaml\n# 8GPU训练\naccelerate --num_process 8 train.py ./cocoqa_train.yaml\n```\n\n注意，本项目使用SwanLab进行训练日志记录与分析，如果未登陆SwanLab需要使用`swanlab login`进行登陆。运行后看到如下结果即代表实验成功开启：\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/run.png\" alt=\"run\" width=\"800\" />\n  <figcaption>成功训练后可以看到SwanLab链接</figcaption>\n  </figure>\n</div>\n\n下面是笔者完成小批量微调训练的训练损失、测试损失结果图\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/cocoqa_swanlab.png\" alt=\"cocoqa_swanlab\" width=\"800\" />\n  <figcaption>SwanLab训练可视化分析结果，可以看到最后训练损失和测试损失都收敛在0.65左右</figcaption>\n  </figure>\n</div>\n\n模型在完成训练后会自动使用一张狗狗图片配合问题“图中有什么动物？”让模型根据图片进行推理，推理结果如下：\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/bad_case.png\" alt=\"bad_case\" width=\"800\" />\n  <figcaption>SwanLab记录了模型训练好后的推理结果，可以看到模型能正常理解和回复中文</figcaption>\n  </figure>\n</div>\n\n当时看到模型对着三只狗的图片回答“兔子”时笔者一时认为炼丹失败了，当然如果实际炼丹失败后模型是不会输出动物类型的，而是输出一些乱码或者告诉用户并没有看到图片。识别错误的原因实际上是由于训练步数过少导致的。后续加大训练步数与数据量后模型能正常识别出狗狗并且能准确的说出有三只狗。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/dog.png\" alt=\"dog\" width=\"250\" />\n  <figcaption>附上三只眼神忧伤的狗子，难道长得很像兔子吗？</figcaption>\n  </figure>\n</div>\n\nPS: 作者公开了在[SwanLab上的训练结果](https://swanlab.cn/@ShaohonChen/Qwen3-SmVL/overview)，感兴趣的读者可以自己查看，SwanLab也支持Clone作者的训练日志，大家可以在自己训练时clone笔者的项目去做对照。\n\n### 完整微调训练结果展示\n\n运行实验命令如下，推荐使用8卡进行训练，在8片沐曦C500芯片上预计需要使用1.5h\n\n```bash\n# 单GPU训练\nCUDA_VISIBLE_DEVICES=0 python train.py ./full_train.yaml\n# 8GPU训练\naccelerate --num_process 8 train.py ./full_train.yaml\n```\n\n下图展示了使用完整微调数据对比于小批量训练，可以看到全量数据微调时loss变得更为抖动，这是由于数据类型的丰富给模型的学习带来了一定的挑战。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/fulldata_swanlab.png\" alt=\"fulldata_swanlab\" width=\"800\" />\n  <figcaption>红色为完整训练loss，黄色为小批量训练结果</figcaption>\n  </figure>\n</div>\n\n进一步对比完整训练和小批量训练的训练和测试损失，可以看到完整训练的模型训练损失达到了0.61，远低于仅仅使用cocoqa模型的效果，评估损失也远低于前者，维持在0.58左右。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/evalloss.png\" alt=\"evalloss\" width=\"800\" />\n  <figcaption>红色为完整训练loss，黄色为小批量训练结果</figcaption>\n  </figure>\n</div>\n\n这里值得一提的是，由于我们选用的测试集比较小（仅有64条数据），因此训练损失和测试损失的差距并不能直接理解为过拟合的证据。实际上在大模型训练上，如果数据集足够大的情况下，通常可以认为训练损失等同于评估损失。\n\n此外，模型通过分析1k步之后的训练损失、平均梯度范数（Grad Norm）变化。此时训练任务已过半，且学习率开始快速衰减。如下图，可以看到学习率快速衰减的情况下模型损失并没有明显的进一步下降，这说明模型已经实现了充分训练。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/1kstep.png\" alt=\"1kstep\" width=\"800\" />\n  <figcaption>1k step之后模型的训练损失变化</figcaption>\n  </figure>\n</div>\n\n在训练效率方面，可以看到我们仍没有充分榨干沐曦GPU的性能，当然这也是由于多模态任务的网络本身架构上比较复杂，其中包含许多对图像、文本的拼接工作，这也导致了GPU性能没法完全利用。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/mx-gpu-use.png\" alt=\"mx-gpu-use\" width=\"800\" />\n  <figcaption>SwanLab对沐曦C500训效率自动记录</figcaption>\n  </figure>\n</div>\n\n同样在完成训练后使用狗狗图进行了测试，这次模型能理解图片、中文以及给出正确的回复。更为关键的是模型完全保留了Qwen3-0.6B原有的全部能力，包括函数调用、推理等。在此基础上，仅仅增加了0.09B参数量的情况下为模型带来了图像理解能力！\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/good_case.png\" alt=\"good_case\" width=\"800\" />\n  <figcaption>同样的图片与问题，更大的数据量和更充足的数据使得模型能够正确给出回复</figcaption>\n  </figure>\n</div>\n\n### 模型推理与效果分析\n\n等笔者下完数据集后未来补一下测试环节 ; ）\n\n可以关注[swanlab教程集合](https://docs.swanlab.cn/examples/qwen3_smolvlm_muxi.html)获取最新更新教程！",
    "343": "一级标题：Qwen3-\"VL\"——超小中文多模态模型的“拼接微调”之路1（附代码和SwanLab记录）\n二级标题：代码及数据集链接汇总\n内容：\n微调用The Cauldron数据集下载链接：\n\n* HuggingFace Hub: [https://huggingface.co/datasets/HuggingFaceM4/the_cauldron](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron)\n* ModelScope: [https://modelscope.cn/datasets/AI-ModelScope/the_cauldron](https://modelscope.cn/datasets/AI-ModelScope/the_cauldron)\n\nQwen3-0.6B模型下载：\n\n* HuggingFace Hub: [https://huggingface.co/Qwen/Qwen3-0.6B](https://huggingface.co/Qwen/Qwen3-0.6B)\n* ModelScope: [https://modelscope.cn/Qwen/Qwen3-0.6B](https://modelscope.cn/Qwen/Qwen3-0.6B)\n\n本实验完整代码GitHub链接：\n\n* 完整项目GitHub地址：[https://github.com/ShaohonChen/Qwen3-SmVL](https://github.com/ShaohonChen/Qwen3-SmVL)\n\n本实验SwanLab日志：\n\n* SwanLab训练过程查看：[https://swanlab.cn/@ShaohonChen/Qwen3-SmVL/overview](https://swanlab.cn/@ShaohonChen/Qwen3-SmVL/overview)",
    "344": "一级标题：Qwen3-\"VL\"——超小中文多模态模型的“拼接微调”之路1（附代码和SwanLab记录）\n二级标题：参考资料\n内容：\n* Huggingface SmolVLM2技术报告：[https://arxiv.org/pdf/2504.05299](https://arxiv.org/pdf/2504.05299)",
    "345": "一级标题：Qwen1.5微调案例\n二级标题：无\n内容：\n:::info\n文本分类，大语言模型，大模型微调\n:::\n\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/Qwen-fintune/runs/zy0st4z16sh4bndyehtks/chart)\n\n\n[实验过程](https://swanlab.cn/@ZeyiLin/Qwen-fintune/runs/zy0st4z16sh4bndyehtks/chart) | [Qwen2微调教程](https://zhuanlan.zhihu.com/p/702491999)",
    "346": "一级标题：Qwen1.5微调案例\n二级标题：概述\n内容：\n[Qwen1.5](https://modelscope.cn/models/qwen/Qwen1.5-7B-Chat/summary)是通义千问团队的开源大语言模型，由阿里云通义实验室研发。以Qwen-1.5作为基座大模型，通过任务微调的方式实现高准确率的文本分类，是学习**大语言模型微调**的入门任务。\n\n![](/assets/example-qwen-1.png)\n\n微调是一种通过在由（输入，输出）对组成的数据集上进一步训练LLMs的过程。这个过程有助于让LLM在特定的下游任务上表现的更为主出色。\n\n\n\n在这个任务中我们会使用[Qwen-1.5-7b](https://modelscope.cn/models/qwen/Qwen1.5-7B-Chat/summary)模型在[zh_cls_fudan_news](https://modelscope.cn/datasets/swift/zh_cls_fudan-news)数据集上进行指令微调任务，同时使用SwanLab进行监控和可视化。",
    "347": "一级标题：Qwen1.5微调案例\n二级标题：环境安装\n内容：\n本案例基于`Python>=3.10`，请在您的计算机上安装好Python。\n\n环境依赖:\n```txt\nswanlab\nmodelscope\ntransformers\ndatasets\npeft\naccelerat\npandas\n```\n\n\n一键安装命令：\n\n```bash\npip install swanlab modelscope transformers datasets peft pandas\n```\n\n> 本案例测试于modelscope==1.14.0、transformers==4.41.2、datasets==2.18.0、peft==0.11.1、accelerate==0.30.1、swanlab==0.3.8",
    "348": "一级标题：Qwen1.5微调案例\n二级标题：数据集介绍\n内容：\n本案例使用的是[zh_cls_fudan-news](https://modelscope.cn/datasets/swift/zh_cls_fudan-news)数据集，该数据集主要被用于训练文本分类模型。\n\nzh_cls_fudan-news由几千条数据，每条数据包含text、category、output三列：\n- text 是训练语料，内容是书籍或新闻的文本内容\n- category 是text的多个备选类型组成的列表\n- output 则是text唯一真实的类型\n\n![](/assets/example-qwen-2.png)\n\n数据集例子如下：\n```\n\"\"\"\n[PROMPT]Text: 第四届全国大企业足球赛复赛结束新华社郑州５月３日电（实习生田兆运）上海大隆机器厂队昨天在洛阳进行的第四届牡丹杯全国大企业足球赛复赛中，以５：４力克成都冶金实验厂队，进入前四名。沪蓉之战，双方势均力敌，９０分钟不分胜负。最后，双方互射点球，沪队才以一球优势取胜。复赛的其它３场比赛，青海山川机床铸造厂队３：０击败东道主洛阳矿山机器厂队，青岛铸造机械厂队３：１战胜石家庄第一印染厂队，武汉肉联厂队１：０险胜天津市第二冶金机械厂队。在今天进行的决定九至十二名的两场比赛中，包钢无缝钢管厂队和河南平顶山矿务局一矿队分别击败河南平顶山锦纶帘子布厂队和江苏盐城无线电总厂队。４日将进行两场半决赛，由青海山川机床铸造厂队和青岛铸造机械厂队分别与武汉肉联厂队和上海大隆机器厂队交锋。本届比赛将于６日结束。（完）\nCategory: Sports, Politics\nOutput:[OUTPUT]Sports\n\"\"\"\n\n```\n\n我们的训练任务，便是希望微调后的大模型能够根据Text和Category组成的提示词，预测出正确的Output。",
    "349": "一级标题：Qwen1.5微调案例\n二级标题：准备工作\n内容：\n在开始训练之前，请先确保环境已安装完成，并保证你有一张 **显存>=16GB** 的GPU。\n\n然后，将数据集下载到本地目录下。下载方式是前往[zh_cls_fudan-news - 魔搭社区](https://modelscope.cn/datasets/swift/zh_cls_fudan-news/files) ，将`train.jsonl`和`test.jsonl`下载到本地根目录下即可：\n\n![](/assets/example-qwen-3.png)",
    "350": "一级标题：Qwen1.5微调案例\n二级标题：完整代码\n内容：\n开始训练时的目录结构：\n\n```txt\n|--- train.py\n|--- train.jsonl\n|--- test.jsonl\n```\n\ntrain.py:\n\n```python\nimport json\nimport pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom modelscope import snapshot_download, AutoTokenizer\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom peft import LoraConfig, TaskType, get_peft_model\nfrom transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\nimport os\nimport swanlab\n\n\ndef dataset_jsonl_transfer(origin_path, new_path):\n    \"\"\"\n    将原始数据集转换为大模型微调所需数据格式的新数据集\n    \"\"\"\n    messages = []\n\n    # 读取旧的JSONL文件\n    with open(origin_path, \"r\") as file:\n        for line in file:\n            # 解析每一行的json数据\n            data = json.loads(line)\n            context = data[\"text\"]\n            catagory = data[\"category\"]\n            label = data[\"output\"]\n            message = {\n                \"instruction\": \"你是一个文本分类领域的专家，你会接收到一段文本和几个潜在的分类选项，请输出文本内容的正确类型\",\n                \"input\": f\"文本:{context},类型选型:{catagory}\",\n                \"output\": label,\n            }\n            messages.append(message)\n\n    # 保存重构后的JSONL文件\n    with open(new_path, \"w\", encoding=\"utf-8\") as file:\n        for message in messages:\n            file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n\n\ndef process_func(example):\n    \"\"\"\n    将数据集进行预处理\n    \"\"\"\n    MAX_LENGTH = 384\n    input_ids, attention_mask, labels = [], [], []\n    instruction = tokenizer(\n        f\"<|im_start|>system\\n你是一个文本分类领域的专家，你会接收到一段文本和几个潜在的分类选项，请输出文本内容的正确类型<|im_end|>\\n<|im_start|>user\\n{example['input']}<|im_end|>\\n<|im_start|>assistant\\n\",\n        add_special_tokens=False,\n    )\n    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n    attention_mask = (\n        instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n    )\n    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n        input_ids = input_ids[:MAX_LENGTH]\n        attention_mask = attention_mask[:MAX_LENGTH]\n        labels = labels[:MAX_LENGTH]\n    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n\n\ndef predict(messages, model, tokenizer):\n    device = \"cuda\"\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\n    generated_ids = model.generate(\n        model_inputs.input_ids,\n        max_new_tokens=512\n    )\n    generated_ids = [\n        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    print(response)\n\n    return response\n\n# 在modelscope上下载Qwen1.5-7B模型到本地目录下\nmodel_dir = snapshot_download(\"qwen/Qwen1.5-7B-Chat\", cache_dir=\"./\", revision=\"master\")\n\n# Transformers加载模型权重\ntokenizer = AutoTokenizer.from_pretrained(\"./qwen/Qwen1___5-7B-Chat/\", use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"./qwen/Qwen1___5-7B-Chat/\", device_map=\"auto\", torch_dtype=torch.bfloat16)\nmodel.enable_input_require_grads()  # 开启梯度检查点时，要执行该方法\n\n# 加载、处理数据集和测试集\ntrain_dataset_path = \"train.jsonl\"\ntest_dataset_path = \"test.jsonl\"\n\ntrain_jsonl_new_path = \"new_train.jsonl\"\ntest_jsonl_new_path = \"new_test.jsonl\"\n\nif not os.path.exists(train_jsonl_new_path):\n    dataset_jsonl_transfer(train_dataset_path, train_jsonl_new_path)\nif not os.path.exists(test_jsonl_new_path):\n    dataset_jsonl_transfer(test_dataset_path, test_jsonl_new_path)\n\n# 得到训练集\ntrain_df = pd.read_json(train_jsonl_new_path, lines=True)\ntrain_ds = Dataset.from_pandas(train_df)\ntrain_dataset = train_ds.map(process_func, remove_columns=train_ds.column_names)\n\nconfig = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    inference_mode=False,  # 训练模式\n    r=8,  # Lora 秩\n    lora_alpha=32,  # Lora alaph，具体作用参见 Lora 原理\n    lora_dropout=0.1,  # Dropout 比例\n)\n\nmodel = get_peft_model(model, config)\n\nargs = TrainingArguments(\n    output_dir=\"./output/Qwen1.5\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    logging_steps=10,\n    num_train_epochs=2,\n    save_steps=100,\n    learning_rate=1e-4,\n    save_on_each_node=True,\n    gradient_checkpointing=True,\n    report_to=\"none\",\n)\n\nswanlab_callback = SwanLabCallback(project=\"Qwen-fintune\", experiment_name=\"Qwen1.5-7B-Chat\")\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n\n# 用测试集的前10条，测试模型\ntest_df = pd.read_json(test_jsonl_new_path, lines=True)[:10]\n\ntest_text_list = []\nfor index, row in test_df.iterrows():\n    instruction = row['instruction']\n    input_value = row['input']\n\n    messages = [\n        {\"role\": \"system\", \"content\": f\"{instruction}\"},\n        {\"role\": \"user\", \"content\": f\"{input_value}\"}\n    ]\n\n    response = predict(messages, model, tokenizer)\n    messages.append({\"role\": \"assistant\", \"content\": f\"{response}\"})\n    result_text = f\"{messages[0]}\\n\\n{messages[1]}\\n\\n{messages[2]}\"\n    test_text_list.append(swanlab.Text(result_text, caption=response))\n\nswanlab.log({\"Prediction\": test_text_list})\nswanlab.finish()\n```",
    "351": "一级标题：Qwen1.5微调案例\n二级标题：效果演示\n内容：\n![](/assets/example-qwen-4.png)",
    "352": "一级标题：Qwen2.5复现DeepSeek-R1-ZERO\n二级标题：无\n内容：\n[![](/assets/badge1.svg)](https://swanlab.cn/@LiXinYu/Try_r1/overview)",
    "353": "一级标题：Qwen2.5复现DeepSeek-R1-ZERO\n二级标题：简介\n内容：\n本文旨在对deepseek-r1-zero进行复现实验，简单介绍了从r1原理到代码实现，再到结果观测的整个过程。通过SwanLab监控实验过程，确保实验的每个阶段都能精确跟踪与调试。通过这一系列的实验步骤，能够掌握GRPO的实现方法。\n\n![](./grpo/r1-zero-ds-qwen.jpg)\n\n---",
    "354": "一级标题：Qwen2.5复现DeepSeek-R1-ZERO\n二级标题：链接资料\n内容：\n本次实验参考了优秀开源项目[philschmid/deep-learning-pytorch-huggingface](https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/mini-deepseek-r1-aha-grpo.ipynb)，该项目作者是google-deepmind工程师Philipp Schmid，Countdown用于R1训练的idea就是这个项目发起的。\n\n> 模型地址：Qwen2.5-3B-Instruct:[huggingface社区](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct)|[魔搭社区](https://modelscope.cn/models/Qwen/Qwen2.5-3B-Instruct)\n>\n> 数据集地址：Countdown-Tasks-3to4:[huggingface地址](https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4)|[魔搭社区地址](https://modelscope.cn/datasets/zouxuhong/Countdown-Tasks-3to4)\n>\n> 可视化工具SwanLab项目地址：[SwanLab结果可视化](https://swanlab.cn/@LiXinYu/Try_r1/overview)\n\n---",
    "355": "一级标题：Qwen2.5复现DeepSeek-R1-ZERO\n二级标题：DeepSeek-R1原理\n内容：\n论文标题：DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\n\n论文链接：[论文链接](https://arxiv.org/pdf/2501.12948?)\n\n代码地址：[github链接](https://github.com/deepseek-ai/DeepSeek-R1)\n\n**下面是论文里从DeepSeek-V3到DeepSeek-R1的流程图表示**\n\n本次教程仅考虑从DeepSeek-V3--->DeepSeek-R1-Zero的复现过程，基于Qwen2.5-3B-Instruct模型实现。\n\n![](./grpo/deepseek-r1-process.png)\n\n\n**GRPO原理：**\n\n`群体相对策略优化 (GRPO，Group Relative Policy Optimization) `是一种强化学习 (RL) 算法，专门用于增强大型语言模型 (LLM) 中的推理能力。与严重依赖外部评估模型（价值函数）指导学习的传统 RL 方法不同，GRPO 通过评估彼此相关的响应组来优化模型。这种方法可以提高训练效率，使 GRPO 成为需要复杂问题解决和长链思维的推理任务的理想选择。\n\n> GRPO 的本质思路：通过在同一个问题上生成多条回答，把它们彼此之间做“相对比较”，来代替传统 PPO 中的“价值模型”\n\n`传统的强化学习算法（如Proximal Policy Optimization，PPO）`在应用于LLMs的推理任务时面临着重大挑战：\n\n1、依赖批评者模型：\nPPO需要一个独立的批评者模型来评估每个回答的价值，这使内存和计算需求增加了一倍。\n训练批评者模型非常复杂且容易出错，尤其是在需要对主观或细微差别进行评价的任务中。\n\n\n2、高昂的计算成本：\n强化学习流程通常需要大量计算资源来迭代评估和优化回答。\n将这些方法扩展到更大的LLMs会进一步加剧成本。\n\n3、可扩展性问题：\n绝对奖励评估难以应对多样化任务，使得跨推理领域的泛化变得困难。\n---\n`GRPO如何应对这些挑战：`\n\n1、无批评者优化： GRPO通过比较组内回答，消除了对批评者模型的需求，显著降低了计算开销。\n\n2、相对评估： GRPO不依赖外部评价者，而是利用组内动态来评估每个回答在同一批次中的相对表现。\n\n3、高效训练： 通过专注于组内优势，GRPO简化了奖励估计流程，使其对大型模型的训练更快且更具可扩展性。\n\n下图是PPO与GRPO的对比，GRPO放弃了价值模型，从分组得分中估计，显著减少了训练资源\n\n![grpo](./grpo/grpo.png)\n\n> 看到一位作者的看法，把GRPO比作老师给学生上课，老师让一组学生解决一个问题。\n> 老师没有单独为每个学生打分，而是让学生在组内比较彼此的答案。表现更好的学生会得到鼓励，而其他人则从错误中学习。随着时间的推移，整个组会逐渐提高，变得更准确和一致。GRPO 将这一原理应用于训练AI模型，使其能够高效地学习。\n\n---",
    "356": "一级标题：Qwen2.5复现DeepSeek-R1-ZERO\n二级标题：实验代码\n内容：\n### 1、环境搭建\n\n> 环境设置如下：\n>\n> pip install transformers==4.48.1\n>\n> pip install peft==0.14.0\n>\n> conda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 -c pytorch\n>\n> pip install datasets\n>\n> pip install accelerate\n>\n> pip install trl\n>\n> pip install -U swanlab\n>\n> pip install deepspeed\n\n### 2、数据预处理\n\n本次实验使用一个490k条数据的[Countdown数据集](https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4)来进行实验，内容如下图所示：\n\n![数据集内容](./grpo/data-countdown.png)\n\n该数据集仅有两项，一个是target结果数据，一个是nums组合数据，我们的目的是为了让模型思考如何从nums经过+、-、*、/计算得到target，为了让模型更好的激活思考能力，我们需要对其设置提示词模板，最重要让模型回答成如下模样：\n\n```text\n<think>:\n让我们来思考下,……\n</think>\n\n<answer>\n……\n</answer>\n```\n同时，由于每个模型都有对应的训练格式模板，比如Qwen的模板在其权重文件中的tokenizer_config.json文件里，具体[例子](https://modelscope.cn/models/Qwen/Qwen2.5-3B-Instruct/file/view/master?fileName=tokenizer_config.json&status=1)如下：\n\n```json\n\"chat_template\": \"{%- if tools %}\\n    {{- '<|im_start|>system\\\\n' }}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- messages[0]['content'] }}\\n    {%- else %}\\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\\n    {%- endif %}\\n    {{- \\\"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {{- \\\"\\\\n\\\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- '<|im_start|>system\\\\n' + messages[0]['content'] + '<|im_end|>\\\\n' }}\\n    {%- else %}\\n        {{- '<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) or (message.role == \\\"assistant\\\" and not message.tool_calls) %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {{- '<|im_start|>' + message.role }}\\n        {%- if message.content %}\\n            {{- '\\\\n' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- '\\\\n<tool_call>\\\\n{\\\"name\\\": \\\"' }}\\n            {{- tool_call.name }}\\n            {{- '\\\", \\\"arguments\\\": ' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- '}\\\\n</tool_call>' }}\\n        {%- endfor %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>user' }}\\n        {%- endif %}\\n        {{- '\\\\n<tool_response>\\\\n' }}\\n        {{- message.content }}\\n        {{- '\\\\n</tool_response>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n{%- endif %}\\n\",\n```\n\n这是一个Jinja2 模板。Jinja2 是一个流行的模板引擎，常用于 Python Web 应用中，但它也可以在其他环境中使用。举一个例子：\n\n```text\n<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n使用给定的数字 [10, 3, 6]，创建一个等于 7 的方程。你可以使用基本算术运算（+、-、*、/）一次或多次，但每个数字只能使用一次。在 <think> </think> 标签中展示你的思考过程，并在 <answer> </answer> 标签中返回最终方程，例如 <answer> (1 + 2) / 3 </answer>。在 <think> 标签中逐步思考。<|im_end|>\n<|im_start|>assistant\n让我们逐步解决这个问题。\n<think>\n```\n\n当然也可以利用tokenizer.apply_chat_template自动根据模型的格式模板进行内容整理，具体如下述代码所示，将数据集转换为R1 Countdown提示词格式：\n\n\n```python\n### 模仿R1的prompt格式来处理数据集，使得GRPO的时候的数据集是可以有思考过程\ndef generate_r1_prompt(question:str,target:str):\n    \"\"\"\n    激活qwen模型的思考过程\n    :param question:数据集的question，给qwen让他自己思考去\n    :param target:数据集的ans\n    :return:\n    \"\"\"\n    r1_prefix = [\n        {\n            \"role\":\"user\",\n            \"content\":f\"现在有一个数学问题，内容是：{question},答案是{target}，你需要根据问题思考其推理过程，使得最终能够得到正确答案，在<think>和</think>标签中展示你的思考过程，并在<answer>和</answer>标签中返回最终答案，比如<answer>19</answer>。在<think>标签后逐步思考。\"\n        },\n        {\n            \"role\":\"assistant\",\n            \"content\":\"让我们逐步解决这个问题。\\n<think>\"\n        }\n    ]\n    # apply_chat_template是应用qwen模型文件中tokenizer_config.json文件中chat_template提示词模板来生成回答。\n    return {\"prompt\": tokenizer.apply_chat_template(r1_prefix, tokenize=False, continue_final_message=True),\n            \"question\":question,\n            \"target\": target}\n\n### 将数据集转换为R1 Countdown提示词格式，在这里我们会把prompt转换为Qwen2的提示词模版，让它以更熟悉的方式来接收提示词，并且我们把让我们逐步解决这个问题。\\n<think>作为模型输出的开头，让它接着续写。用 Python字典的方式返回样本，这样trl会在调用奖励函数的时候，帮我们把键名设为为对应的参数；另外，trl会把模型的多个输出设为completions。\ndef train_dataset_process(train_data_path:str):\n    dataset = read_jsonl_to_dataset(train_data_path)\n    dataset = dataset.map(lambda x: generate_r1_prompt(x[\"sni_text\"], x[\"ans\"]))\n\n    train_test_split = dataset.train_test_split(test_size=0.1)\n\n    train_dataset = train_test_split[\"train\"]\n    test_dataset = train_test_split[\"test\"]\n\n    return {\n        \"train_dataset\":train_dataset,\n        \"test_dataset\":test_dataset\n    }\n\n```\n\n> **❗注意：** generate_r1_prompt中最终需要return包含数据提问，以及数据集对应的答案answer，map方法会帮我们把实际的question和answer填入到prompt里\n\n### 3、设置奖励函数\n\n在强化学习中，奖励函数是指导智能体（agent）在环境中如何行动的核心信号。奖励提供了对智能体行为的即时反馈，用于评估某个动作在某一状态下的好坏，从而影响其未来的决策。通过不断地试错和调整，智能体学习到在不同状态下选择能获得高奖励的行为策略。奖励的主要功能是引导智能体朝着最大化长期回报的目标去优化策略。正向奖励（正数）鼓励行为，负向奖励（负数）抑制行为。奖励用于更新智能体的策略或值函数，策略的优化通常基于累计奖励（Return），即智能体从当前状态到未来一段时间内获得的总奖励。\n\n本次实验我们仅对输出格式format以及最终答案answer设置奖励函数，训练过程会不断修正格式输出以及答案输出。\n\n**format奖励函数**\n\n```python\n### 格式奖励函数\ndef format_reward_func(completions, **kwargs):\n    \"\"\"\n    格式奖励函数，检查模型输出格式是否匹配: <think>...</think><answer>...</answer>\n\n    参数:\n        completions (list[str]): 生成的输出\n    返回:\n        list[float]: 奖励分数\n    \"\"\"\n    # 初始化奖励列表\n    rewards = []\n    # 遍历生成的输出\n    for completion in completions:\n        try:\n            # 在生成的输出前添加<think>标签，便于后续正则表达式匹配\n            completion = \"<think>\" + completion\n\n            if random.random() < 0.1:  # 1% 的概率将生成输出写入文件\n                # 创建生成输出目录（如果不存在）\n                os.makedirs(\"completion_samples\", exist_ok=True)\n                log_file = os.path.join(\"completion_samples\", \"completion_samples.txt\")\n                with open(log_file, \"a\") as f:\n                    f.write(f\"\\n\\n==============\\n\")\n                    f.write(completion)  # 写入生成的输出\n\n            # 定义正则表达式模式，用于匹配 <think> 和 <answer> 标签\n            regex = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\n<answer>([\\s\\S]*?)<\\/answer>$\"\n            match = re.search(regex, completion, re.DOTALL)  # 使用正则表达式进行匹配\n\n            if match is None or len(match.groups()) != 2:\n                rewards.append(0.0)  # 如果格式不正确，奖励为 0\n            else:\n                rewards.append(1.0)  # 如果格式正确，奖励为 1\n        except Exception:\n            rewards.append(0.0)  # 如果发生异常，奖励为 0\n\n    return rewards\n```\n\n**answer奖励函数**\n\n```python\n### 答案奖励函数\ndef equation_reward_func(completions, target, nums, **kwargs):\n    \"\"\"\n    方程奖励函数，检查计算结果是否正确，数字是否符合使用要求（每个数字只用一次，只使用所提供的数字）\n\n    参数:\n        completions (list[str]): 生成的输出\n        target (list[str]): 预期的答案\n        nums (list[str]): 可用的数字\n\n    返回:\n        list[float]: 奖励分数\n    \"\"\"\n    # 初始化奖励列表\n    rewards = []\n    # 遍历生成的输出、预期的答案和可用的数字\n    for completion, gt, numbers in zip(completions, target, nums):\n        try:\n            # 在生成的输出前添加 <think> 标签，便于后续正则表达式匹配\n            completion = \"<think>\" + completion\n            # 定义正则表达式模式，用于匹配 <answer> 标签\n            match = re.search(r\"<answer>(.*?)<\\/answer>\", completion)\n            if match is None:\n                rewards.append(0.0)  # 如果没有匹配到 <answer> 标签，奖励为 0\n                continue\n            equation = match.group(1).strip()  # 提取 <answer> 标签中的内容\n            # 提取方程中的所有数字\n            used_numbers = [int(n) for n in re.findall(r\"\\d+\", equation)]\n\n            # 检查所有数字是否被使用且只使用一次\n            if sorted(used_numbers) != sorted(numbers):\n                rewards.append(0.0)\n                continue\n\n            # 定义允许的字符模式，只允许数字、运算符、括号和空白字符\n            allowed_pattern = r\"^[\\d+\\-*/().\\s]+$\"\n            if not re.match(allowed_pattern, equation):\n                rewards.append(0.0)  # 如果方程包含不允许的字符，奖励为 0\n                continue\n\n            # 计算方程的结果\n            result = eval(equation, {\"__builtins__\": None}, {})\n            # 检查方程是否正确且与预期答案匹配（误差小于 1e-5）\n            if abs(float(result) - float(gt)) < 1e-5:\n                rewards.append(1.0)  # 如果正确，奖励为 1\n\n                # 10% 的概率将成功的样本写入文件\n                if random.random() < 0.10:\n                    # 创建生成输出目录（如果不存在）\n                    os.makedirs(\"completion_samples\", exist_ok=True)\n                    log_file = os.path.join(\n                        \"completion_samples\", \"success_completion_samples.txt\"\n                    )\n                    with open(log_file, \"a\") as f:\n                        f.write(f\"\\n\\n==============\\n\")\n                        f.write(completion)  # 写入生成的输出\n            else:\n                rewards.append(0.0)  # 如果不正确，奖励为 0\n        except Exception:\n            rewards.append(0.0)  # 如果评估失败，奖励为 0\n\n    return rewards\n```\n\n> **补充：** 也可以设置思考长度以及语言一致性奖励函数来提高模型性能\n\n### 4、设置模型参数\n\n```python\n# 模型参数设置\nmodel_config = ModelConfig(\n    model_name_or_path=model_path,\n    torch_dtype=\"bfloat16\",\n    # attn_implementation=\"flash_attention_2\",\n    use_peft=True,\n    load_in_4bit=True\n)\n```\n\n `attn_implementation`:使用 flash_attention_2 可以优化显存使用和加速计算，尤其是在处理大规模模型时。若启用，它会减少内存占用并加速训练过程，尤其在使用多GPU时效果显著。未启用时，可能会牺牲性能和显存效率，影响训练速度。\n\n### 5、设置训练参数\n\n```python\n# 训练参数\ntraining_args = GRPOConfig(\n    output_dir=\"/root/test/outputs\",\n    learning_rate=5e-7,\n    lr_scheduler_type=\"cosine\",\n    logging_steps=2,\n    max_steps=200,\n    per_device_train_batch_size=1,\n    gradient_checkpointing=False,\n    gradient_accumulation_steps=8,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    bf16=True,\n    save_steps=50,\n    # GRPO参数设置\n    max_prompt_length=256,\n    max_completion_length=1024,\n    num_generations=2,\n    beta=0.001,\n    # vllm加速\n    use_vllm=False\n    # vllm_device=\"npu:7\"\n    vllm_device=\"cuda:1\"\n    vllm_gpu_memory_utilization=0.8\n)\n```\n\n其中vLLM 是一个用于加速推理的库，能在 GPU 上优化内存使用和计算性能。启用 use_vllm=True 后，它可以在推理阶段通过高效的内存管理和多设备并行来加速计算，特别是在处理大型语言模型时。它还能通过 vllm_device 参数指定加速设备，例如 cuda:1，提升训练和推理速度，减少显存占用。\n这里之所以是false是因为我申请的服务器只有两块卡，使用vllm的时候一块卡训练，一块卡用来推理，而vllm一般在多块卡的时候，比如5、6块卡以上的时候才能体现出加速效果，而本次实验使用的是4090，只有24GB显存，很容易炸显存，如果卡比较多的话推荐vllm。\n\n\n⚠️**注意：**\n> 我们使用的是trl的库来使用GRPO，目前有个小bug，就是gradient_checkpointing和vllm要同时true或者同时false，否则就会报错，而这两个参数都有降低显存占用，提高训练推理速度的功能，因此如何设置可以交给各位炼丹师自行选择。\n\n### 6、可视化训练工具参数\n\n```python",
    "357": "一级标题：Qwen2.5复现DeepSeek-R1-ZERO\n二级标题：swanlab参数配置\n内容：\nswanlab_callback = SwanLabCallback(\n    workspace=None, # 项目不公开\n    project=\"DeepSeek-R1-zero\",  # 项目名称\n    experiment_name=\"4090-grpo\",  # 实验名称\n)\n```\n\n### 7、训练并保存模型\n\n```python\n# 训练器配置\ntrainer = GRPOTrainer(\n    model=model_config.model_name_or_path,\n    reward_funcs=[format_reward_func,answer_reward_func],\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    peft_config=get_peft_config(model_config),\n    callbacks=[swanlab_callback]\n)\n\ntrainer.train()\ntrainer.save_model(training_args.output_dir)\n```\n\n\n### 全过程代码\n\n为了便于管理和配置分布式训练环境、强化学习（RL）训练的超参数，以及定义主训练函数 main，我们建议采用 YAML 格式的脚本文件来系统化地记录和维护这些关键参数，同时使用 Python 文件来实现 main 函数。\n\n```\nroot/project/\n├── data/\n│   └── zouxuhong___countdown-tasks-3to4/\n├── models/\n│   └── Qwen/\n│       └── Qwen2___5-3B-Instruct/\n├── config/\n│   ├── 2rtx4090.yaml\n│   └── grpo-qwen-2.5-3b-deepseek-r1-zero-countdown.yaml\n├── train_r1_grpo.py\n└── train_r1_grpo.sh\n```\n\n\n**1、Accelerate 配置文件，用于分布式训练（两张卡）。新建deepspeed_zero3.yaml，填入以下内容并保存**\n\n一般来说，这个文件内容不需要修改，如果有定制需求，请不要使用这个文件，运行`accelerate config`自行设定。　\n\n```yaml\ncompute_environment: LOCAL_MACHINE\ndebug: false\ndeepspeed_config:\n  gradient_accumulation_steps: 8\n  gradient_clipping: 1.0\n  offload_optimizer_device: cpu\n  offload_param_device: cpu\n  zero3_init_flag: false\n  zero3_save_16bit_model: true\n  zero_stage: 3\ndistributed_type: DEEPSPEED\ndowncast_bf16: 'no'\nenable_cpu_affinity: false\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: bf16\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n```\n\n⚠️**注意：**\n\n由于本次实验资源有限，因此训练优化器还有模型参数部分会转移到CPU上进行计算，以减少显存压力，修改的参数是offload_optimizer_device和offload_param_device\n\n1. offload_optimizer_device: cpu\n\n    作用：将优化器状态（如动量、梯度等）卸载到 CPU 上。\n\n    具体内容：\n\n        优化器状态通常占用大量显存，尤其是在使用 Adam 优化器时。\n\n        将这些状态卸载到 CPU 上可以显著减少 GPU 显存占用，从而支持更大的模型或更大的批量大小。\n\n2. offload_param_device: cpu\n\n    作用：将模型参数卸载到 CPU 上。\n\n    具体内容：\n\n       模型参数是训练过程中占用显存的主要部分。\n\n       将这些参数卸载到 CPU 上可以进一步减少 GPU 显存占用，但会增加 CPU 和 GPU 之间的数据传输开销。\n\n**2、设定训练的超参数。新建grpo-qwen-2.5-3b-deepseek-r1-zero-countdown.yaml填入以下内容，并根据实际情况修改**\n\n```yaml\n# Model arguments\nmodel_name_or_path: /root/epfs/ascend_r1_turtorial/models/Qwen/Qwen2___5-3B-Instruct\nmodel_revision: main\ntorch_dtype: bfloat16\n# attn_implementation: flash_attention_2\nbf16: true\ntf32: false\noutput_dir: /root/epfs/ascend_r1_turtorial/output\n\n# Dataset arguments\ndataset_id_or_path: /root/epfs/zouxuhong___countdown-tasks-3to4\n\n# Lora Arguments\n# No LoRA is used here\n\n# Training arguments\nmax_steps: 450\nper_device_train_batch_size: 1\ngradient_accumulation_steps: 8\ngradient_checkpointing: false\ngradient_checkpointing_kwargs:\n  use_reentrant: false\nlearning_rate: 5.0e-7 # 1.0e-6 as in the deepseek math paper 5-e7 from https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05\nlr_scheduler_type: cosine\nwarmup_ratio: 0.03\n# GRPO specific parameters\nbeta: 0.001 # 0.04 as in the deepseek math paper 0.001 from https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05\nmax_prompt_length: 256\nmax_completion_length: 1024\nnum_generations: 2\nuse_vllm: false\n# vllm_device: \"npu:7\"\nvllm_device: \"cuda:1\"\nvllm_gpu_memory_utilization: 0.8\n\n# Logging arguments\nlogging_strategy: steps\nlogging_steps: 1\nsave_strategy: \"steps\"\nsave_steps: 100\nsave_total_limit: 1\nseed: 2025\n\n# Swanlab 训练流程记录参数\nswanlab: true # 是否开启 Swanlab\nworkspace: none\nproject: Try_r1\nexperiment_name: qingyun-4090-jupyter\n```\n\n\n**3、设置训练函数**\n\n```python\nimport logging\nimport os\nimport random\nimport re\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import List\n\nfrom datasets import load_dataset\nfrom swanlab.integration.transformers import SwanLabCallback\nimport torch\nfrom transformers import AutoTokenizer\nfrom transformers.trainer_utils import get_last_checkpoint\nfrom trl import GRPOConfig, GRPOTrainer, ModelConfig, TrlParser",
    "358": "# 自定义参数类",
    "359": "@dataclass\nclass DatasetArguments:\n    \"\"\"数据集参数的数据类\"\"\"\n\n    # 数据集 ID 或路径\n    dataset_id_or_path: str = \"Jiayi-Pan/Countdown-Tasks-3to4\"\n    # 数据集拆分\n    dataset_splits: str = \"train\"\n    # 分词器名称或路径\n    tokenizer_name_or_path: str = None\n\n@dataclass\nclass SwanlabArguments:\n    \"\"\"SwanLab参数的数据类\"\"\"\n\n    # 是否使用 SwanLab\n    swanlab: bool\n    # SwanLab 用户名\n    workspace: str\n    # SwanLab 的项目名\n    project: str\n    # SwanLab 的实验名\n    experiment_name: str",
    "360": "# 设置日志记录",
    "361": "# 配置日志记录器\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\nhandler = logging.StreamHandler()\nhandler.setFormatter(\n    logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n)  # 设置日志格式\n\nlogger.addHandler(handler)",
    "362": "# 定义奖励函数",
    "363": "def format_reward_func(completions, **kwargs):\n    \"\"\"\n    格式奖励函数，检查模型输出格式是否匹配: <think>...</think><answer>...</answer>\n\n    参数:\n        completions (list[str]): 生成的输出\n    返回:\n        list[float]: 奖励分数\n    \"\"\"\n    # 初始化奖励列表\n    rewards = []\n    # 遍历生成的输出\n    for completion in completions:\n        try:\n            # 在生成的输出前添加<think>标签，便于后续正则表达式匹配\n            completion = \"<think>\" + completion\n\n            if random.random() < 0.1:  # 1% 的概率将生成输出写入文件\n                # 创建生成输出目录（如果不存在）\n                os.makedirs(\"completion_samples\", exist_ok=True)\n                log_file = os.path.join(\"completion_samples\", \"completion_samples.txt\")\n                with open(log_file, \"a\") as f:\n                    f.write(f\"\\n\\n==============\\n\")\n                    f.write(completion)  # 写入生成的输出\n\n            # 定义正则表达式模式，用于匹配 <think> 和 <answer> 标签\n            regex = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\n<answer>([\\s\\S]*?)<\\/answer>$\"\n            match = re.search(regex, completion, re.DOTALL)  # 使用正则表达式进行匹配\n\n            if match is None or len(match.groups()) != 2:\n                rewards.append(0.0)  # 如果格式不正确，奖励为 0\n            else:\n                rewards.append(1.0)  # 如果格式正确，奖励为 1\n        except Exception:\n            rewards.append(0.0)  # 如果发生异常，奖励为 0\n\n    return rewards\n\ndef equation_reward_func(completions, target, nums, **kwargs):\n    \"\"\"\n    方程奖励函数，检查计算结果是否正确，数字是否符合使用要求（每个数字只用一次，只使用所提供的数字）\n\n    参数:\n        completions (list[str]): 生成的输出\n        target (list[str]): 预期的答案\n        nums (list[str]): 可用的数字\n\n    返回:\n        list[float]: 奖励分数\n    \"\"\"\n    # 初始化奖励列表\n    rewards = []\n    # 遍历生成的输出、预期的答案和可用的数字\n    for completion, gt, numbers in zip(completions, target, nums):\n        try:\n            # 在生成的输出前添加 <think> 标签，便于后续正则表达式匹配\n            completion = \"<think>\" + completion\n            # 定义正则表达式模式，用于匹配 <answer> 标签\n            match = re.search(r\"<answer>(.*?)<\\/answer>\", completion)\n            if match is None:\n                rewards.append(0.0)  # 如果没有匹配到 <answer> 标签，奖励为 0\n                continue\n            equation = match.group(1).strip()  # 提取 <answer> 标签中的内容\n            # 提取方程中的所有数字\n            used_numbers = [int(n) for n in re.findall(r\"\\d+\", equation)]\n\n            # 检查所有数字是否被使用且只使用一次\n            if sorted(used_numbers) != sorted(numbers):\n                rewards.append(0.0)\n                continue\n\n            # 定义允许的字符模式，只允许数字、运算符、括号和空白字符\n            allowed_pattern = r\"^[\\d+\\-*/().\\s]+$\"\n            if not re.match(allowed_pattern, equation):\n                rewards.append(0.0)  # 如果方程包含不允许的字符，奖励为 0\n                continue\n\n            # 计算方程的结果\n            result = eval(equation, {\"__builtins__\": None}, {})\n            # 检查方程是否正确且与预期答案匹配（误差小于 1e-5）\n            if abs(float(result) - float(gt)) < 1e-5:\n                rewards.append(1.0)  # 如果正确，奖励为 1\n\n                # 10% 的概率将成功的样本写入文件\n                if random.random() < 0.10:\n                    # 创建生成输出目录（如果不存在）\n                    os.makedirs(\"completion_samples\", exist_ok=True)\n                    log_file = os.path.join(\n                        \"completion_samples\", \"success_completion_samples.txt\"\n                    )\n                    with open(log_file, \"a\") as f:\n                        f.write(f\"\\n\\n==============\\n\")\n                        f.write(completion)  # 写入生成的输出\n            else:\n                rewards.append(0.0)  # 如果不正确，奖励为 0\n        except Exception:\n            rewards.append(0.0)  # 如果评估失败，奖励为 0\n\n    return rewards",
    "364": "# 断点续训处理",
    "365": "def get_checkpoint(training_args: GRPOConfig):\n    \"\"\"\n    获取最后一个检查点\n\n    参数:\n        training_args (GRPOConfig): 训练参数\n    返回:\n        str: 最后一个检查点的路径，如果没有检查点，则返回 None\n    \"\"\"\n    last_checkpoint = None\n    if os.path.isdir(training_args.output_dir):  # 如果输出目录存在\n        # 获取最后一个检查点\n        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n    return last_checkpoint",
    "366": "# 基于trl实现GRPO训练过程",
    "367": "def grpo_function(\n    model_args: ModelConfig,\n    dataset_args: DatasetArguments,\n    training_args: GRPOConfig,\n    callbacks: List,\n):\n    # 记录模型参数\n    logger.info(f\"Model parameters {model_args}\")\n    # 记录训练/评估参数\n    logger.info(f\"Training/evaluation parameters {training_args}\")",
    "368": "# 处理数据",
    "369": "# 加载分词器\n    tokenizer = AutoTokenizer.from_pretrained(\n        (\n            # 如果有指定分词器，则使用指定的分词器，否则使用模型名称\n            dataset_args.tokenizer_name_or_path\n            if dataset_args.tokenizer_name_or_path\n            else model_args.model_name_or_path\n        ),\n        revision=model_args.model_revision,  # 使用指定的模型版本\n        trust_remote_code=model_args.trust_remote_code,  # 允许使用远程代码\n    )\n    # 如果分词器没有填充标记，则使用结束标记作为填充标记\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n\n    # 加载数据集\n    dataset = load_dataset(\n        dataset_args.dataset_id_or_path, split=dataset_args.dataset_splits\n    )\n    # 随机选择 50K 个样本，看你喜好定数字，但是数据集有 409K 个样本\n    dataset = dataset.shuffle(seed=training_args.seed).select(range(50000))\n\n    def generate_r1_prompt(numbers, target):\n        \"\"\"\n        生成 R1 Countdown 游戏提示词\n\n        参数:\n            numbers (list[int]): 数字列表\n            target (int): 目标值\n        返回:\n            dict: 生成的一个数据样本\n        \"\"\"\n        # 定义提示词前缀\n        r1_prefix = [\n            {\n                \"role\": \"user\",\n                \"content\": f\"使用给定的数字 {numbers}，创建一个等于 {target} 的方程。你可以使用基本算术运算（+、-、*、/）一次或多次，但每个数字只能使用一次。在 <think> </think> 标签中展示你的思考过程，并在 <answer> </answer> 标签中返回最终方程，例如 <answer> (1 + 2) / 3 </answer>。在 <think> 标签中逐步思考。\",\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"让我们逐步解决这个问题。\\n<think>\",  # 结尾使用 `<think>` 促使模型开始思考\n            },\n        ]\n\n        return {\n            \"prompt\": tokenizer.apply_chat_template(\n                r1_prefix, tokenize=False, continue_final_message=True\n            ),  # 提示词，continue_final_message=True 表示将提示词中的最后一个消息继续到最终的输出中\n            \"target\": target,\n            \"nums\": numbers,\n        }\n\n    # 将数据集转换为 R1 Countdown 游戏提示词\n    dataset = dataset.map(lambda x: generate_r1_prompt(x[\"nums\"], x[\"target\"]))\n    # 将数据集拆分为训练集和测试集，拆分比例为 9:1\n    train_test_split = dataset.train_test_split(test_size=0.1)\n    train_dataset = train_test_split[\"train\"]  # 获取训练集\n    test_dataset = train_test_split[\"test\"]  # 获取测试集\n\n    # 参考自 huggingface/open-r1, 把attn_implementation（是否使用flash_attention）等参数传入模型初始化参数\n    logger.info(\"*** Initializing model kwargs ***\")\n    torch_dtype = (\n        model_args.torch_dtype if model_args.torch_dtype in [\"auto\", None] else getattr(torch, model_args.torch_dtype)\n    )\n    model_kwargs = dict(\n        revision=model_args.model_revision,\n        trust_remote_code=model_args.trust_remote_code,\n        attn_implementation=model_args.attn_implementation,\n        torch_dtype=torch_dtype,\n        use_cache=False if training_args.gradient_checkpointing else True,\n    )\n    training_args.model_init_kwargs = model_kwargs",
    "370": "# 设置 GRPOTrainer",
    "371": "trainer = GRPOTrainer(\n        model=model_args.model_name_or_path,  # 模型名称或路径\n        # 奖励函数列表，用于计算奖励分数\n        reward_funcs=[\n            format_reward_func,  # 格式奖励函数\n            equation_reward_func,  # 方程奖励函数\n        ],\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n        callbacks=callbacks,\n    )\n\n    last_checkpoint = get_checkpoint(training_args)  # 检查最后一个检查点\n    # 如果检测到检查点且指定从检查点恢复训练，则记录信息\n    if last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n        logger.info(f\"Checkpoint detected, resuming training at {last_checkpoint}.\")\n\n    logger.info(\n        f'*** Starting training {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} for {training_args.num_train_epochs} epochs***'\n    )",
    "372": "# 训练模型",
    "373": "train_result = trainer.train(resume_from_checkpoint=last_checkpoint)",
    "374": "# 保存训练结果",
    "375": "# 记录和保存指标\n    metrics = train_result.metrics\n    metrics[\"train_samples\"] = len(train_dataset)\n    trainer.log_metrics(\"train\", metrics)\n    trainer.save_metrics(\"train\", metrics)\n    trainer.save_state()\n\n    logger.info(\"*** Training complete ***\")\n\n    # 保存模型和分词器\n    logger.info(\"*** Save model ***\")\n    trainer.model.config.use_cache = True\n    trainer.save_model(training_args.output_dir)\n    logger.info(f\"Model saved to {training_args.output_dir}\")\n    training_args.distributed_state.wait_for_everyone()  # 等待所有进程加载\n    tokenizer.save_pretrained(training_args.output_dir)\n    logger.info(f\"Tokenizer saved to {training_args.output_dir}\")\n\n    logger.info(\"*** Training complete! ***\")\n\ndef main():\n    \"\"\"主函数，用于执行主训练循环\"\"\"\n    # 解析命令行参数和配置文件\n    parser = TrlParser((ModelConfig, DatasetArguments, GRPOConfig, SwanlabArguments))\n    model_args, dataset_args, training_args, swanlab_args = (\n        parser.parse_args_and_config()\n    )\n\n    # 如果使用 SwanLab，则创建 SwanLab 回调对象，用于训练信息记录\n    if swanlab_args.swanlab:\n        swanlab_callback = SwanLabCallback(\n            project=swanlab_args.project,\n            experiment_name=swanlab_args.experiment_name,\n        )\n        callbacks = [swanlab_callback]\n    else:\n        callbacks = None\n\n    # 运行主训练循环\n    grpo_function(model_args, dataset_args, training_args, callbacks=callbacks)\n\nif __name__ == \"__main__\":\n    main()\n```\n\n**4、设置分布式训练脚本**\n\n```python\naccelerate launch \\\n    --num_processes 2 \\\n    --config_file config/2rtx4090.yaml \\\n    train_r1_grpo.py \\\n    --config config/grpo-qwen-2.5-3b-deepseek-r1-zero-countdown.yaml\n```\n\n**5、启动训练**\n\n在命令行输入下面的内容：\n\n```bash\nbash train_r1_grpo.sh\n```",
    "376": "一级标题：Qwen2.5复现DeepSeek-R1-ZERO\n二级标题：训练后模型部署和推理\n内容：\n保存下来的仅仅是模型的权重信息以及配置文件等，是不能直接使用的，需要与原模型进行合并操作，代码如下：\n\n```python\nfrom peft import PeftModel\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nimport os\nimport shutil\n\n# 保证原始模型的各个文件不遗漏保存到merge_path中\ndef copy_files_not_in_B(A_path, B_path):\n    if not os.path.exists(A_path):\n        raise FileNotFoundError(f\"The directory {A_path} does not exist.\")\n    if not os.path.exists(B_path):\n        os.makedirs(B_path)\n\n    # 获取路径A中所有非权重文件\n    files_in_A = os.listdir(A_path)\n    files_in_A = set([file for file in files_in_A if not (\".bin\" in file or \"safetensors\" in file)])\n\n    files_in_B = set(os.listdir(B_path))\n\n    # 找到所有A中存在但B中不存在的文件\n    files_to_copy = files_in_A - files_in_B\n\n    # 将文件或文件夹复制到B路径下\n    for file in files_to_copy:\n        src_path = os.path.join(A_path, file)\n        dst_path = os.path.join(B_path, file)\n\n        if os.path.isdir(src_path):\n            # 复制目录及其内容\n            shutil.copytree(src_path, dst_path)\n        else:\n            # 复制文件\n            shutil.copy2(src_path, dst_path)\n\ndef merge_lora_to_base_model(adapter_name_or_path,save_path,model_name_or_path=\"Qwen/Qwen2-0.5B\"):\n    # 如果文件夹不存在，就创建\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,trust_remote_code=True,)\n\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name_or_path,\n        trust_remote_code=True,\n        low_cpu_mem_usage=True,\n        torch_dtype=torch.float16,\n        device_map=\"auto\"\n    )\n    # 加载保存的 Adapter\n    model = PeftModel.from_pretrained(model, adapter_name_or_path, device_map=\"auto\",trust_remote_code=True)\n    # 将 Adapter 合并到基础模型中\n    merged_model = model.merge_and_unload()  # PEFT 的方法将 Adapter 权重合并到基础模型\n    # 保存合并后的模型\n    tokenizer.save_pretrained(save_path)\n    merged_model.save_pretrained(save_path, safe_serialization=False)\n    copy_files_not_in_B(model_name_or_path, save_path)\n    print(f\"合并后的模型已保存至: {save_path}\")\n\n\nif __name__ == '__main__':\n    adapter_name_or_path=\"你的生成的模型的文件夹\"\n    save_path = \"保存模型的地址\"\n    merge_lora_to_base_model(adapter_name_or_path=adapter_name_or_path,save_path=save_path)\n```\n\n运行上述代码后，会得到最终合并后的模型，我们用该模型进行推理测试，测试代码如下：\n\n```python\nfrom transformers import AutoModelForCausalLM,AutoTokenizer\nimport torch\n\nMODEL_NAME_OR_PATH = \"output/qwen-grpo\"\nPROMPT=\"\"\"使用给定的数字 [80, 9, 18]，创建一个等于 53 的方程。你可以使用基本算术运算（+、-、*、/）一次或多次，\n          但每个数字只能使用一次。在 <think> </think> 标签中展示你的思考过程，并在 <answer> </answer> 标签中返回最终方程，\n          例如 <answer> (1 + 2) / 3 </answer>。在 <think> 标签中逐步思考。让我们逐步解决这个问题。\\n<think>\"\"\"\n\n\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME_OR_PATH,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": PROMPT}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512,\n    top_p=0.95,\n    temperature=0.7,\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(response)\n```\n\n---",
    "377": "一级标题：Qwen2.5复现DeepSeek-R1-ZERO\n二级标题：实验结果演示\n内容：\n由于训练时间较长，推荐使用tmux将训练任务hold住。可以在[SwanLab](https://swanlab.cn/@LiXinYu/Try_r1/runs/iunfsosyp8ryfanbjcv7g/chart)中查看。\n\n![swanlab观测结果](./grpo/swanlab-results.png)\n\n---",
    "378": "一级标题：Qwen2.5复现DeepSeek-R1-ZERO\n二级标题：参考链接：\n内容：\n* https://github.com/philschmid/deep-learning-pytorch-huggingface\n* https://github.com/Jiayi-Pan/TinyZero\n* https://github.com/datawhalechina/unlock-deepseek\n* https://arxiv.org/pdf/2501.12948?\n* https://github.com/deepseek-ai/DeepSeek-R1\n* https://arxiv.org/pdf/2402.03300\n* https://zhuanlan.zhihu.com/p/21952581194\n* https://github.com/huggingface/open-r1?tab=readme-ov-file#grpo\n* https://zhuanlan.zhihu.com/p/21062322587\n* https://cloud.tencent.com/developer/article/2495699",
    "379": "一级标题：Qwen2-VL多模态大模型微调实战\n二级标题：无\n内容：\n:::info\n多模态，大语言模型，大模型微调\n:::\n\n\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/Qwen2-VL-finetune/runs/pkgest5xhdn3ukpdy6kv5/chart)\n\n[训练过程](https://swanlab.cn/@ZeyiLin/Qwen2-VL-finetune/runs/pkgest5xhdn3ukpdy6kv5/chart) | [知乎教程](https://zhuanlan.zhihu.com/p/7144893529)\n\nQwen2-VL是阿里通义实验室推出的多模态大模型。本文我们将简要介绍基于 transformers、peft 等框架，使用 Qwen2-VL-2B-Instruct 模型在COCO2014图像描述 上进行Lora微调训练，同时使用 SwanLab 监控训练过程与评估模型效果。\n\n![](./qwen_vl_coco/01.png)\n\nLora 是一种高效微调方法，深入了解其原理可参见博客：[知乎|深入浅出 Lora](https://zhuanlan.zhihu.com/p/650197598)。\n\n- 训练过程：[Qwen2-VL-finetune](https://swanlab.cn/@ZeyiLin/Qwen2-VL-finetune/runs/pkgest5xhdn3ukpdy6kv5/chart)\n- Github：[代码仓库](https://github.com/Zeyi-Lin/LLM-Finetune/tree/main/qwen2_vl)、[self-llm](https://github.com/datawhalechina/self-llm)\n- 数据集：[coco_2014_caption](https://modelscope.cn/datasets/modelscope/coco_2014_caption/summary)\n- 模型：[Qwen2-VL-2B-Instruct](https://modelscope.cn/models/Qwen/Qwen2-VL-2B-Instruct)\n\nOCR微调版：[Qwen2-VL-Latex-OCR](https://zhuanlan.zhihu.com/p/10705293665)\n\n\n---",
    "380": "一级标题：Qwen2-VL多模态大模型微调实战\n二级标题：1. 环境配置\n内容：\n环境配置分为三步：\n\n1. 确保你的电脑上至少有一张英伟达显卡，并已安装好了CUDA环境。\n2. 安装Python（版本>=3.8）以及能够调用CUDA加速的PyTorch。\n3. 安装Qwen2-VL微调相关的第三方库，可以使用以下命令：\n\n```bash\npython -m pip install --upgrade pip\n# 更换 pypi 源加速库的安装\npip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n\npip install modelscope==1.18.0\npip install transformers==4.46.2\npip install sentencepiece==0.2.0\npip install accelerate==1.1.1\npip install datasets==2.18.0\npip install peft==0.13.2\npip install swanlab==0.3.25\npip install qwen-vl-utils==0.0.8\n```",
    "381": "一级标题：Qwen2-VL多模态大模型微调实战\n二级标题：2. 准备数据集\n内容：\n本节使用的是 [coco_2014_caption](https://modelscope.cn/datasets/modelscope/coco_2014_caption/summary) 数据集（中的500张图），该数据集主要用于多模态（Image-to-Text）任务。\n\n> 数据集介绍：COCO 2014 Caption数据集是Microsoft Common Objects in Context (COCO)数据集的一部分，主要用于图像描述任务。该数据集包含了大约40万张图像，每张图像都有至少1个人工生成的英文描述语句。这些描述语句旨在帮助计算机理解图像内容，并为图像自动生成描述提供训练数据。\n\n![](./qwen_vl_coco/02.png)\n\n在本节的任务中，我们主要使用其中的前500张图像，并对它进行处理和格式调整，目标是组合成如下格式的json文件：\n\n```json\n[\n{\n    \"id\": \"identity_1\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"COCO Yes: <|vision_start|>图像文件路径<|vision_end|>\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"A snow skier assessing the mountain before starting to sky\"\n      }\n    ]\n},\n...\n]\n```\n\n其中，\"from\"是角色（user代表人类，assistant代表模型），\"value\"是聊天的内容，其中`<|vision_start|>`和`<|vision_end|>`是Qwen2-VL模型识别图像的标记，中间可以放图像的文件路径，也可以是URL。\n\n**数据集下载与处理方式**\n\n1. **我们需要做四件事情：**\n    - 通过Modelscope下载coco_2014_caption数据集\n    - 加载数据集，将图像保存到本地\n    - 将图像路径和描述文本转换为一个csv文件\n    - 将csv文件转换为json文件\n\n2. **使用下面的代码完成从数据下载到生成csv的过程：**\n\ndata2csv.py：\n\n```python\n# 导入所需的库\nfrom modelscope.msdatasets import MsDataset\nimport os\nimport pandas as pd\n\nMAX_DATA_NUMBER = 500\n\n# 检查目录是否已存在\nif not os.path.exists('coco_2014_caption'):\n    # 从modelscope下载COCO 2014图像描述数据集\n    ds =  MsDataset.load('modelscope/coco_2014_caption', subset_name='coco_2014_caption', split='train')\n    print(len(ds))\n    # 设置处理的图片数量上限\n    total = min(MAX_DATA_NUMBER, len(ds))\n\n    # 创建保存图片的目录\n    os.makedirs('coco_2014_caption', exist_ok=True)\n\n    # 初始化存储图片路径和描述的列表\n    image_paths = []\n    captions = []\n\n    for i in range(total):\n        # 获取每个样本的信息\n        item = ds[i]\n        image_id = item['image_id']\n        caption = item['caption']\n        image = item['image']\n\n        # 保存图片并记录路径\n        image_path = os.path.abspath(f'coco_2014_caption/{image_id}.jpg')\n        image.save(image_path)\n\n        # 将路径和描述添加到列表中\n        image_paths.append(image_path)\n        captions.append(caption)\n\n        # 每处理50张图片打印一次进度\n        if (i + 1) % 50 == 0:\n            print(f'Processing {i+1}/{total} images ({(i+1)/total*100:.1f}%)')\n\n    # 将图片路径和描述保存为CSV文件\n    df = pd.DataFrame({\n        'image_path': image_paths,\n        'caption': captions\n    })\n\n    # 将数据保存为CSV文件\n    df.to_csv('./coco-2024-dataset.csv', index=False)\n\n    print(f'数据处理完成，共处理了{total}张图片')\n\nelse:\n    print('coco_2014_caption目录已存在,跳过数据处理步骤')\n```\n\n\n**3. 在同一目录下，用以下代码，将csv文件转换为json文件：**\n\ncsv2json.py：\n\n```python\nimport pandas as pd\nimport json\n\n# 载入CSV文件\ndf = pd.read_csv('./coco-2024-dataset.csv')\nconversations = []\n\n# 添加对话数据\nfor i in range(len(df)):\n    conversations.append({\n        \"id\": f\"identity_{i+1}\",\n        \"conversations\": [\n            {\n                \"from\": \"user\",\n                \"value\": f\"COCO Yes: <|vision_start|>{df.iloc[i]['image_path']}<|vision_end|>\"\n            },\n            {\n                \"from\": \"assistant\",\n                \"value\": df.iloc[i]['caption']\n            }\n        ]\n    })\n\n# 保存为Json\nwith open('data_vl.json', 'w', encoding='utf-8') as f:\n    json.dump(conversations, f, ensure_ascii=False, indent=2)\n```\n\n此时目录下会多出两个文件：\n- coco-2024-dataset.csv\n- data_vl.json\n\n至此，我们完成了数据集的准备。",
    "382": "一级标题：Qwen2-VL多模态大模型微调实战\n二级标题：3. 模型下载与加载\n内容：\n这里我们使用modelscope下载Qwen2-VL-2B-Instruct模型，然后把它加载到Transformers中进行训练：\n\n```python\nfrom modelscope import snapshot_download, AutoTokenizer\nfrom transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq, Qwen2VLForConditionalGeneration, AutoProcessor\nimport torch\n\n# 在modelscope上下载Qwen2-VL模型到本地目录下\nmodel_dir = snapshot_download(\"Qwen/Qwen2-VL-2B-Instruct\", cache_dir=\"./\", revision=\"master\")\n\n# 使用Transformers加载模型权重\ntokenizer = AutoTokenizer.from_pretrained(\"./Qwen/Qwen2-VL-2B-Instruct/\", use_fast=False, trust_remote_code=True)\n# 特别的，Qwen2-VL-2B-Instruct模型需要使用Qwen2VLForConditionalGeneration来加载\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\"./Qwen/Qwen2-VL-2B-Instruct/\", device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True,)\nmodel.enable_input_require_grads()  # 开启梯度检查点时，要执行该方法\n```\n\n模型大小为 4.5GB，下载模型大概需要 5 分钟。",
    "383": "一级标题：Qwen2-VL多模态大模型微调实战\n二级标题：4. 集成SwanLab\n内容：\n[SwanLab](https://github.com/swanhubx/swanlab) 是一个开源的模型训练记录工具。SwanLab面向AI研究者，提供了训练可视化、自动日志记录、超参数记录、实验对比、多人协同等功能。在SwanLab上，研究者能基于直观的可视化图表发现训练问题，对比多个实验找到研究灵感，并通过在线链接的分享与基于组织的多人协同训练，打破团队沟通的壁垒。\n\nSwanLab与Transformers已经做好了集成，用法是在Trainer的`callbacks`参数中添加`SwanLabCallback`实例，就可以自动记录超参数和训练指标，简化代码如下：\n\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom transformers import Trainer\n\nswanlab_callback = SwanLabCallback()\n\ntrainer = Trainer(\n    ...\n    callbacks=[swanlab_callback],\n)\n```\n\n首次使用SwanLab，需要先在[官网](https://swanlab.cn)注册一个账号，然后在用户设置页面复制你的API Key，然后在训练开始提示登录时粘贴即可，后续无需再次登录：\n\n![](./qwen_vl_coco/04.png)\n\n\n更多用法可参考[快速开始](https://docs.swanlab.cn/zh/guide_cloud/general/quick-start.html)、[Transformers集成](https://docs.swanlab.cn/zh/guide_cloud/integration/integration-huggingface-transformers.html)。",
    "384": "一级标题：Qwen2-VL多模态大模型微调实战\n二级标题：5. 开始微调\n内容：\n查看可视化训练过程：<a href=\"https://swanlab.cn/@ZeyiLin/Qwen2-VL-finetune/runs/53vm3y7sp5h5fzlmlc5up/chart\" target=\"_blank\">Qwen2-VL-finetune</a>\n\n\n**本节代码做了以下几件事：**\n1. 下载并加载Qwen2-VL-2B-Instruct模型\n2. 加载数据集，取前496条数据参与训练，4条数据进行主观评测\n3. 配置Lora，参数为r=64, lora_alpha=16, lora_dropout=0.05\n4. 使用SwanLab记录训练过程，包括超参数、指标和最终的模型输出结果\n5. 训练2个epoch\n\n开始执行代码时的目录结构应该是：\n```\n|———— train.py\n|———— coco_2014_caption\n|———— coco-2024-dataset.csv\n|———— data_vl.json\n|———— data2csv.py\n|———— csv2json.py\n```\n\n\n**完整代码如下**\n\ntrain.py：\n\n```python\nimport torch\nfrom datasets import Dataset\nfrom modelscope import snapshot_download, AutoTokenizer\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom qwen_vl_utils import process_vision_info\nfrom peft import LoraConfig, TaskType, get_peft_model, PeftModel\nfrom transformers import (\n    TrainingArguments,\n    Trainer,\n    DataCollatorForSeq2Seq,\n    Qwen2VLForConditionalGeneration,\n    AutoProcessor,\n)\nimport swanlab\nimport json\n\n\ndef process_func(example):\n    \"\"\"\n    将数据集进行预处理\n    \"\"\"\n    MAX_LENGTH = 8192\n    input_ids, attention_mask, labels = [], [], []\n    conversation = example[\"conversations\"]\n    input_content = conversation[0][\"value\"]\n    output_content = conversation[1][\"value\"]\n    file_path = input_content.split(\"<|vision_start|>\")[1].split(\"<|vision_end|>\")[0]  # 获取图像路径\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image\",\n                    \"image\": f\"{file_path}\",\n                    \"resized_height\": 280,\n                    \"resized_width\": 280,\n                },\n                {\"type\": \"text\", \"text\": \"COCO Yes:\"},\n            ],\n        }\n    ]\n    text = processor.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )  # 获取文本\n    image_inputs, video_inputs = process_vision_info(messages)  # 获取数据数据（预处理过）\n    inputs = processor(\n        text=[text],\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\",\n    )\n    inputs = {key: value.tolist() for key, value in inputs.items()} #tensor -> list,为了方便拼接\n    instruction = inputs\n\n    response = tokenizer(f\"{output_content}\", add_special_tokens=False)\n\n\n    input_ids = (\n            instruction[\"input_ids\"][0] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n    )\n\n    attention_mask = instruction[\"attention_mask\"][0] + response[\"attention_mask\"] + [1]\n    labels = (\n            [-100] * len(instruction[\"input_ids\"][0])\n            + response[\"input_ids\"]\n            + [tokenizer.pad_token_id]\n    )\n    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n        input_ids = input_ids[:MAX_LENGTH]\n        attention_mask = attention_mask[:MAX_LENGTH]\n        labels = labels[:MAX_LENGTH]\n\n    input_ids = torch.tensor(input_ids)\n    attention_mask = torch.tensor(attention_mask)\n    labels = torch.tensor(labels)\n    inputs['pixel_values'] = torch.tensor(inputs['pixel_values'])\n    inputs['image_grid_thw'] = torch.tensor(inputs['image_grid_thw']).squeeze(0)  #由（1,h,w)变换为（h,w）\n    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels,\n            \"pixel_values\": inputs['pixel_values'], \"image_grid_thw\": inputs['image_grid_thw']}\n\n\ndef predict(messages, model):\n    # 准备推理\n    text = processor.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n    image_inputs, video_inputs = process_vision_info(messages)\n    inputs = processor(\n        text=[text],\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\",\n    )\n    inputs = inputs.to(\"cuda\")\n\n    # 生成输出\n    generated_ids = model.generate(**inputs, max_new_tokens=128)\n    generated_ids_trimmed = [\n        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n    ]\n    output_text = processor.batch_decode(\n        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n    )\n\n    return output_text[0]\n\n\n# 在modelscope上下载Qwen2-VL模型到本地目录下\nmodel_dir = snapshot_download(\"Qwen/Qwen2-VL-2B-Instruct\", cache_dir=\"./\", revision=\"master\")\n\n# 使用Transformers加载模型权重\ntokenizer = AutoTokenizer.from_pretrained(\"./Qwen/Qwen2-VL-2B-Instruct/\", use_fast=False, trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(\"./Qwen/Qwen2-VL-2B-Instruct\")\n\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\"./Qwen/Qwen2-VL-2B-Instruct/\", device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True,)\nmodel.enable_input_require_grads()  # 开启梯度检查点时，要执行该方法\n\n# 处理数据集：读取json文件\n# 拆分成训练集和测试集，保存为data_vl_train.json和data_vl_test.json\ntrain_json_path = \"data_vl.json\"\nwith open(train_json_path, 'r') as f:\n    data = json.load(f)\n    train_data = data[:-4]\n    test_data = data[-4:]\n\nwith open(\"data_vl_train.json\", \"w\") as f:\n    json.dump(train_data, f)\n\nwith open(\"data_vl_test.json\", \"w\") as f:\n    json.dump(test_data, f)\n\ntrain_ds = Dataset.from_json(\"data_vl_train.json\")\ntrain_dataset = train_ds.map(process_func)\n\n# 配置LoRA\nconfig = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    inference_mode=False,  # 训练模式\n    r=64,  # Lora 秩\n    lora_alpha=16,  # Lora alaph，具体作用参见 Lora 原理\n    lora_dropout=0.05,  # Dropout 比例\n    bias=\"none\",\n)\n\n# 获取LoRA模型\npeft_model = get_peft_model(model, config)\n\n# 配置训练参数\nargs = TrainingArguments(\n    output_dir=\"./output/Qwen2-VL-2B\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    logging_steps=10,\n    logging_first_step=5,\n    num_train_epochs=2,\n    save_steps=100,\n    learning_rate=1e-4,\n    save_on_each_node=True,\n    gradient_checkpointing=True,\n    report_to=\"none\",\n)\n\n# 设置SwanLab回调\nswanlab_callback = SwanLabCallback(\n    project=\"Qwen2-VL-finetune\",\n    experiment_name=\"qwen2-vl-coco2014\",\n    config={\n        \"model\": \"https://modelscope.cn/models/Qwen/Qwen2-VL-2B-Instruct\",\n        \"dataset\": \"https://modelscope.cn/datasets/modelscope/coco_2014_caption/quickstart\",\n        \"github\": \"https://github.com/datawhalechina/self-llm\",\n        \"prompt\": \"COCO Yes: \",\n        \"train_data_number\": len(train_data),\n        \"lora_rank\": 64,\n        \"lora_alpha\": 16,\n        \"lora_dropout\": 0.1,\n    },\n)\n\n# 配置Trainer\ntrainer = Trainer(\n    model=peft_model,\n    args=args,\n    train_dataset=train_dataset,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n    callbacks=[swanlab_callback],\n)\n\n# 开启模型训练\ntrainer.train()\n\n# ====================测试模式===================\n# 配置测试参数\nval_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    inference_mode=True,  # 训练模式\n    r=64,  # Lora 秩\n    lora_alpha=16,  # Lora alaph，具体作用参见 Lora 原理\n    lora_dropout=0.05,  # Dropout 比例\n    bias=\"none\",\n)\n\n# 获取测试模型\nval_peft_model = PeftModel.from_pretrained(model, model_id=\"./output/Qwen2-VL-2B/checkpoint-62\", config=val_config)\n\n# 读取测试数据\nwith open(\"data_vl_test.json\", \"r\") as f:\n    test_dataset = json.load(f)\n\ntest_image_list = []\nfor item in test_dataset:\n    input_image_prompt = item[\"conversations\"][0][\"value\"]\n    # 去掉前后的<|vision_start|>和<|vision_end|>\n    origin_image_path = input_image_prompt.split(\"<|vision_start|>\")[1].split(\"<|vision_end|>\")[0]\n\n    messages = [{\n        \"role\": \"user\",\n        \"content\": [\n            {\n            \"type\": \"image\",\n            \"image\": origin_image_path\n            },\n            {\n            \"type\": \"text\",\n            \"text\": \"COCO Yes:\"\n            }\n        ]}]\n\n    response = predict(messages, val_peft_model)\n    messages.append({\"role\": \"assistant\", \"content\": f\"{response}\"})\n    print(messages[-1])\n\n    test_image_list.append(swanlab.Image(origin_image_path, caption=response))\n\nswanlab.log({\"Prediction\": test_image_list})\n\n# 在Jupyter Notebook中运行时要停止SwanLab记录，需要调用swanlab.finish()\nswanlab.finish()\n```\n\n看到下面的进度条即代表训练开始：\n\n![](./qwen_vl_coco/05.png)",
    "385": "一级标题：Qwen2-VL多模态大模型微调实战\n二级标题：6. 训练结果演示\n内容：\n详细训练过程请看这里：[qwen2-vl-coco2014](https://swanlab.cn/@ZeyiLin/Qwen2-VL-finetune/runs/pkgest5xhdn3ukpdy6kv5/chart)\n\n![](./qwen_vl_coco/06.png)\n\n\n从SwanLab图表中我们可以看到，lr的下降策略是线性下降，loss随epoch呈现下降趋势，而grad_norm则在上升。这种形态往往反映了模型有过拟合的风险，训练不要超过2个epoch。\n\n在`Prediction`图表中记录着模型最终的输出结果，可以看到模型在回答的风格上是用的COCO数据集的简短英文风格进行的描述：\n\n![](./qwen_vl_coco/07.png)\n\n\n\n而同样的图像，没有被微调的模型输出结果如下：\n\n```\n1-没有微调：The image depicts a cozy living room with a rocking chair in the center, a bookshelf filled with books, and a table with a vase and a few other items. The walls are decorated with wallpaper, and there are curtains on the windows. The room appears to be well-lit, with sunlight streaming in from the windows.\n1-微调后：A living room with a rocking chair, a bookshelf, and a table with a vase and a bowl.\n\n2-没有微调：It looks like a family gathering or a party in a living room. There are several people sitting around a dining table, eating pizza. The room has a cozy and warm atmosphere.\n2-微调后：A group of people sitting around a dining table eating pizza.\n```\n\n可以明显看到微调后风格的变化。",
    "386": "一级标题：Qwen2-VL多模态大模型微调实战\n二级标题：7. 推理LoRA微调后的模型\n内容：\n加载lora微调后的模型，并进行推理：\n\n```python\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nfrom peft import PeftModel, LoraConfig, TaskType\n\nconfig = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    inference_mode=True,\n    r=64,  # Lora 秩\n    lora_alpha=16,  # Lora alaph，具体作用参见 Lora 原理\n    lora_dropout=0.05,  # Dropout 比例\n    bias=\"none\",\n)\n\n# default: Load the model on the available device(s)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"./Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n)\nmodel = PeftModel.from_pretrained(model, model_id=\"./output/Qwen2-VL-2B/checkpoint-62\", config=config)\nprocessor = AutoProcessor.from_pretrained(\"./Qwen/Qwen2-VL-2B-Instruct\")\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"测试图像路径\",\n            },\n            {\"type\": \"text\", \"text\": \"COCO Yes:\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```",
    "387": "一级标题：Qwen2-VL多模态大模型微调实战\n二级标题：补充\n内容：\n### 详细硬件配置和参数说明\n\n使用4张A100 40GB显卡，batch size为4，gradient accumulation steps为4，训练2个epoch的用时为1分钟57秒。\n\n![](./qwen_vl_coco/08.png)\n\n![](./qwen_vl_coco/09.png)\n\n\n### 注意\n\n- 在微调脚本中，`val_peft_model`加载的是一共固定的checkpoint文件，如果你添加了数据或超参数，请根据实际情况修改checkpoint文件路径。",
    "388": "一级标题：LeRobot 具身智能入门\n二级标题：无\n内容：\n[LeRobot](https://github.com/huggingface/lerobot) 是 [Hugging Face](https://huggingface.co/lerobot) 发起的一个开源机器人项目，用于为现实世界的机器人提供数据集、模型和相关工具，用于降低机器人的门槛。LeRobot 中包含模仿学习和强化学习的方法，LeRobot 还提供了一系列的预训练模型、包含人工收集演示的数据集以及模拟环境。\n\n![LeRobot, Hugging Face Robotics Library](./assets/lerobot-swanlab1.png)\n\n下面的教程主要讨论如何基于 LeRobot 训练一个自己的 VLA 模型，如何完成从采集数据、模型训练到模型推理的完整链路，最终实现机械臂自主完成抓取动作。\n\n> [!NOTE]\n>\n> **VLA（Vision Language Action）** 是一种先进的多模态机器学习模型，它结合了视觉、语言和动作三种能力，旨在实现从感知输入直接映射到机器人控制动作的完整闭环能力。了解更多查看 [SmolVLA](https://huggingface.co/blog/zh/smolvla)。\n\n[[toc]]",
    "389": "一级标题：LeRobot 具身智能入门\n二级标题：0. 准备物料一览\n内容：\n需要准备的物料：\n\n- **笔记本电脑**：配置 LeRobot 环境，用于控制机械臂以及收集机器人数据。\n- **LeRobot 机械臂套件**：包含主从两个机械臂，主臂用于遥操作，从臂用于执行动作。\n- **USB 摄像头**：用于输入环境的视频信号，作为机械臂的“眼睛”。\n- **GPU 服务器**：用于训练模型，如果笔记本有 GPU 也可以使用笔记本训练。\n\n在本教程中我们基于型号 [SO-101](https://huggingface.co/docs/lerobot/so101) 的机械臂完成实验，SO-101 套件包含一个主臂（黑色）和从臂（白色），如下图所示。\n\n<img src=\"./assets/so-101.png\" alt=\"SO-101\" style=\"zoom:30%;\" />\n\n[淘宝购买链接](https://item.taobao.com/item.htm?ali_trackid=2%3Amm_7587494315_3230200107_115939450462%3A1752723707645_554211053_0&bxsign=tbk5vSLE-62O97Or9VaJAjw5S3OKWmab7-z32DrQ05EAZ5wURXVAqGEK07y49vI0Gv46kNi9NtLNfx3lJJq50RWzGgfWOYS4UXVj1KT7Bx6Ue05TNdo_qHq8mJqBQerRa7N1D2J4ymc4BuoAgmDTgq4M7oXrg2QG3wfsGMA3f5nwRx6RKBu6IuGXUtOv6plztbN&id=878010637397&skuId=5915703371831&union_lens=lensId%3APUB%401742290075%4021662a24_0e69_195a894c064_d4e6%40023oEhJMJDAYtsRzhzp9pESW%40eyJmbG9vcklkIjo4MDY3NCwiic3BtQiiI6Il9wb3J0YWxfdjJfcGFnZXNfcHJvbW9fZ29vZHNfaW5kZXhfaHRtIiiwiic3JjRmxvb3JJZCI6IjgwNjc0In0ie%3BtkScm%3AselectionPlaza_site_4358_0_0_0_30_17422900758127587494315%3Bscm%3A1007.30148.424730.pub_search-item_034ace60-dfa1-4b94-8e7c-d9c9b4cd4b97_%3Brecoveryid%3A554211053_0%401752723707647)\n\n> [!warning]\n>\n> 注意购买的时候需要选择「SOARM101」 和「舵机+控制板+3D 打印件」，购买完成后收到的是散件，需要自行组装。\n\n还需要准备一个空间比较大的桌子，便于机械臂的操作。一切准备就绪后，操作流程如下图所示：\n\n![pipeline](./assets/pipeline.png)\n\n1. 通过笔记本电脑连接主从机械臂和摄像头，然后通过遥操作收集数据。\n2. 收集好数据之后在一台带有 GPU 的服务器上进行训练，并使用 [SwanLab](https://swanlab.cn/) 进行训练跟踪。\n3. 最后模型训练完毕后部署在笔记本上推理，使机器臂自主执行抓取动作。\n\n> [!Note]\n>\n> **遥操作**是指手动遥控操控机械臂的技术，分为主臂和从臂，手动控制主臂的运动，从臂会跟随执行。",
    "390": "一级标题：LeRobot 具身智能入门\n二级标题：1. 安装 LeRobot 环境\n内容：\n需要在笔记本电脑和训练服务器上都准备 LeRobot 环境，笔记本电脑用于操控机械臂，服务器用于模型训练。\n\n首先下载 LeRobot 源码：\n\n```bash\ngit clone https://github.com/swpfY/lerobot.git\ncd lerobot\n```\n\n使用 [miniconda](https://www.anaconda.com/docs/getting-started/miniconda/install) 创建 Python 3.10 虚拟环境并激活它：\n\n```bash\nconda create -y -n lerobot python=3.10\nconda activate lerobot\n```\n\n然后在 conda 环境中安装 `ffmpeg`：\n\n```bash\nconda install ffmpeg=7.1.1 -c conda-forge\n```\n\n> 注意这一步需要特定安装 `ffmpeg=7.11` 版本，当前最新的 ffmpeg 版本不兼容\n\n最后安装🤗LeRobot：\n\n```bash\npip install -e .\n```\n\n> [!Important]\n>\n> 注意 LeRobot 仓库目前版本并不稳定，可能会出现 API 和脚本变动的情况，本教程使用的 LeRobot 对应为 [commit cf86b93](https://github.com/huggingface/lerobot/commit/cf86b9300dc83fdad408cfe4787b7b09b55f12cf) 版本。\n\n然后我们再安装 [swanlab](https://github.com/SwanHubX/SwanLab) 并登录：\n\n```bash\npip install -U swanlab\nswanlab login\n```",
    "391": "一级标题：LeRobot 具身智能入门\n二级标题：2. 机械臂组装\n内容：\n### 2.1 组装机械臂\n\n由于不同型号套件的组装方式不同，具体可以参考 seeed 的[组装教程](https://wiki.seeedstudio.com/cn/lerobot_so100m/#%E8%88%B5%E6%9C%BA%E6%A0%A1%E5%87%86)。\n\n组装这步比较考验个人动手能力，建议如果有现成的完全体套件可以直接加钱购买，跳过个人组装步骤。\n\n### 2.2 注意事项\n\n（1）SO-101 型号套件提供的舵机型号不一致，主臂（黑色）使用的是 5V 电源，舵机都是相同的 7.4V 电压的型号；而从臂则使用 12V 电源，不同关节使用了不同的舵机。这里在组装的时候一定要注意并做好标记，防止舵机被烧坏。详见 [舵机校准](https://wiki.seeedstudio.com/cn/lerobot_so100m/#%E8%88%B5%E6%9C%BA%E6%A0%A1%E5%87%86)。\n\n（2）USB 和电源线是独立的，USB 不会为舵机供电。可以准备一个拓展坞，在电脑和舵机控制板之间插一个拓展坞，防止电脑接口被击穿（当然一般电路都有做保护）。\n\n（3）6 月 30 号之前购买的机械臂套件需要升级舵机驱动，从 3.9 升级到 3.10，否则会出现不兼容的问题。参考 [组装教程](https://wiki.seeedstudio.com/cn/lerobot_so100m/#%E6%A0%A1%E5%87%86%E8%88%B5%E6%9C%BA%E5%B9%B6%E7%BB%84%E8%A3%85%E6%9C%BA%E6%A2%B0%E8%87%82)。\n\n（4）seeed 提供的[教程](https://wiki.seeedstudio.com/cn/lerobot_so100m/#%E6%A0%A1%E5%87%86%E8%88%B5%E6%9C%BA%E5%B9%B6%E7%BB%84%E8%A3%85%E6%9C%BA%E6%A2%B0%E8%87%82)并不适配最新版的 LeRobot 代码，其中提供的校准舵机的脚本为旧版仓库中的代码，不兼容最新版本。这里需要对比查看 [LeRobot 教程](https://huggingface.co/docs/lerobot/so101)。\n\n（5）注意机械臂插销的固定方式如下图所示，这样能保证机械臂被固定在桌沿：\n\n<img src=\"./assets/note-fixed.jpg\" alt=\"fixed\" style=\"zoom:10%;\" />",
    "392": "一级标题：LeRobot 具身智能入门\n二级标题：3. 校准机械臂\n内容：\n> 注意本教程使用的 LeRobot 代码对应为 [commit cf86b93](https://github.com/huggingface/lerobot/commit/cf86b9300dc83fdad408cfe4787b7b09b55f12cf) 版本。\n\n### 3.1 获取机械臂的 USB 端口\n\n使用如下命令：\n\n```bash\npython -m lerobot.find_port\n```\n\n示例输出为：\n\n```bash\n'/dev/tty.usbmodem5AA90178121', '/dev/tty.usbmodem5A7A0161371']\nRemove the USB cable from your MotorsBus and press Enter when done.\n```\n\n可以看到 `/dev/tty.usbmodem5AA90178121` 为主臂或者从臂，这个可以一个一个地接入然后定位到对应的机械臂。\n\n### 3.2 机械臂校准\n\n我们一个一个地校准，先对从臂进行校验，有如下命令：\n\n```bash\npython -m lerobot.calibrate \\\n   --robot.type=so101_follower \\\n   --robot.port=/dev/tty.usbmodem5AA90178121 \\\n   --robot.id=my_red_robot_arm\n```\n\n- `--robot.port` 为对应的端口，这里我们按上面的方法获取\n- `--robot.id` 为机械臂 ID，我这里定义将从臂为 `my_red_robot_arm`\n\n示例输出：\n\n```bash\n❯ python -m lerobot.calibrate \\\n    --robot.type=so101_follower \\\n    --robot.port=/dev/tty.usbmodem5AA90178121 \\\n    --robot.id=my_red_robot_arm\n\nINFO 2025-07-18 11:47:47 calibrate.py:73 {'robot': {'calibration_dir': None,\n           'cameras': {},\n           'disable_torque_on_disconnect': True,\n           'id': 'my_red_robot_arm',\n           'max_relative_target': None,\n           'port': '/dev/tty.usbmodem5AA90178121',\n           'use_degrees': False},\n 'teleop': None}\nINFO 2025-07-18 11:47:48 follower.py:101 my_red_robot_arm SO101Follower connected.\nINFO 2025-07-18 11:47:48 follower.py:108\nRunning calibration of my_red_robot_arm SO101Follower\nMove my_red_robot_arm SO101Follower to the middle of its range of motion and press ENTER....\nMove all joints sequentially through their entire ranges of motion.\nRecording positions. Press ENTER to stop...\n\n-------------------------------------------\nNAME            |    MIN |    POS |    MAX\nshoulder_pan    |    790 |   2067 |   3372\nshoulder_lift   |    822 |    848 |   3181\nelbow_flex      |   1037 |   3076 |   3080\nwrist_flex      |    920 |   2879 |   3283\nwrist_roll      |    160 |   2036 |   4002\ngripper         |   2020 |   2081 |   3391\n```\n\n首先我们需要将机械臂的所有关节处于活动范围的中间位置，然后按下回车键，再让每个关节都达到其最小到最大的活动范围。\n\n如上面的输出所示，我们可以看到 `MIN`, `POS`, `MAX` 三个参数，校准的目的就是设置每个关节的活动范围。\n\n然后对主臂进行校准，示例代码为：\n\n```bash\npython -m lerobot.calibrate \\\n    --teleop.type=so101_leader \\\n    --teleop.port=/dev/tty.usbmodem5A7A0161371 \\\n    --teleop.id=my_blue_leader_arm\n```\n\n> [!Note]\n>\n> 详细的操作步骤查看[官方校准视频](https://huggingface.co/docs/lerobot/so101?calibrate_follower=Command#calibration-video)。",
    "393": "一级标题：LeRobot 具身智能入门\n二级标题：4. 遥操作控制\n内容：\n使用以下脚本：\n\n```python\npython -m lerobot.teleoperate \\\n    --robot.type=so101_follower \\\n    --robot.port=/dev/tty.usbmodem5AA90178121 \\\n    --robot.id=my_red_robot_arm \\\n    --teleop.type=so101_leader \\\n    --teleop.port=/dev/tty.usbmodem5A7A0161371 \\\n    --teleop.id=my_blue_leader_arm\n```\n\n注意修改上面的 `--robot.port` , `--robot.id`, `--teleop.port` 和 `--teleop.id` 参数。\n\n运行脚本后通常还会要求重新校验从臂（SO101Follower），输出如下所示：\n\n```bash\nMove my_red_robot_arm SO101Follower to the middle of its range of motion and press ENTER....\nMove all joints sequentially through their entire ranges of motion.\nRecording positions. Press ENTER to stop...\n```\n\n执行成功后我们控制主臂，从臂就会跟随运动了。\n\n> [!TIP]\n>\n> 可以将上面的命令写入一个 shell 脚本中，便于下次直接执行操作。",
    "394": "一级标题：LeRobot 具身智能入门\n二级标题：5. 遥操作收集数据集\n内容：\n### 5.1 添加摄像头\n\n使用以下命令找到插入系统的摄像头索引，摄像头默认为 `0`：\n\n```bash\npython -m lerobot.find_cameras opencv\n```\n\n>  更多相关内容参考：[Cameras](https://huggingface.co/docs/lerobot/cameras)\n\n可以使用以下 Python 脚本检查摄像头的状况，是否能正常使用：\n\n::: details 点我查看代码\n\n```python\nfrom lerobot.cameras.opencv.configuration_opencv import OpenCVCameraConfig\nfrom lerobot.cameras.opencv.camera_opencv import OpenCVCamera\nfrom lerobot.cameras.configs import ColorMode, Cv2Rotation\n\n# Construct an `OpenCVCameraConfig` with your desired FPS, resolution, color mode, and rotation.\nconfig = OpenCVCameraConfig(\n    index_or_path=0,\n    fps=15,\n    width=1920,\n    height=1080,\n    color_mode=ColorMode.RGB,\n    rotation=Cv2Rotation.NO_ROTATION\n)\n\n# Instantiate and connect an `OpenCVCamera`, performing a warm-up read (default).\ncamera = OpenCVCamera(config)\ncamera.connect()\n\n# Read frames asynchronously in a loop via `async_read(timeout_ms)`\ntry:\n    for i in range(10):\n        frame = camera.async_read(timeout_ms=200)\n        print(f\"Async frame {i} shape:\", frame.shape)\nfinally:\n    camera.disconnect()\n```\n\n:::\n\n### 5.2 登录 Hugging Face CLI\n\n我们需要使用到 [Hugging Face Hub](https://huggingface.co/) 上传数据集和模型，我们使用以下命令行登录：\n\n```bash\nhuggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential\n```\n\n`${HUGGINGFACE_TOKEN}` 为 Hugging Face 访问凭证，可以在[设置](https://huggingface.co/settings/tokens)中获取。\n\n可以使用以下命令检查是否登录成功：\n\n```bash\nhuggingface-cli whoami\n```\n\n### 5.3 录制数据集\n\n使用以下脚本控制机械臂完成数据集的收集。\n\n::: details 详细代码\n\n```python\nfrom lerobot.cameras.opencv.configuration_opencv import OpenCVCameraConfig\nfrom lerobot.datasets.lerobot_dataset import LeRobotDataset\nfrom lerobot.datasets.utils import hw_to_dataset_features\nfrom lerobot.robots.so101_follower import SO101Follower, SO101FollowerConfig\nfrom lerobot.teleoperators.so101_leader.config_so101_leader import SO101LeaderConfig\nfrom lerobot.teleoperators.so101_leader.so101_leader import SO101Leader\nfrom lerobot.utils.control_utils import init_keyboard_listener\nfrom lerobot.utils.utils import log_say\nfrom lerobot.utils.visualization_utils import _init_rerun\nfrom lerobot.record import record_loop\n\nNUM_EPISODES = 50\nFPS = 30\nEPISODE_TIME_SEC = 60\nRESET_TIME_SEC = 10\nTASK_DESCRIPTION = \"my first task\"\n\n# Create the robot and teleoperator configurations\ncamera_config = {\"front\": OpenCVCameraConfig(index_or_path=0, width=640, height=480, fps=FPS)}\nrobot_config = SO101FollowerConfig(\n    port=\"/dev/tty.usbmodem5AA90178121\", id=\"my_red_robot_arm\", cameras=camera_config\n)\nteleop_config = SO101LeaderConfig(port=\"/dev/tty.usbmodem5A7A0161371\", id=\"my_blue_leader_arm\")\n\n# Initialize the robot and teleoperator\nrobot = SO101Follower(robot_config)\nteleop = SO101Leader(teleop_config)\n\n# Configure the dataset features\naction_features = hw_to_dataset_features(robot.action_features, \"action\")\nobs_features = hw_to_dataset_features(robot.observation_features, \"observation\")\ndataset_features = {**action_features, **obs_features}\n\n# Create the dataset\ndataset = LeRobotDataset.create(\n    repo_id=\"<hf_username>/<dataset_repo_id>\",\n    fps=FPS,\n    features=dataset_features,\n    robot_type=robot.name,\n    use_videos=True,\n    image_writer_threads=4,\n)\n\n# Initialize the keyboard listener and rerun visualization\n_, events = init_keyboard_listener()\n_init_rerun(session_name=\"recording\")\n\n# Connect the robot and teleoperator\nrobot.connect()\nteleop.connect()\n\nepisode_idx = 0\nwhile episode_idx < NUM_EPISODES and not events[\"stop_recording\"]:\n    log_say(f\"Recording episode {episode_idx + 1} of {NUM_EPISODES}\")\n\n    record_loop(\n        robot=robot,\n        events=events,\n        fps=FPS,\n        teleop=teleop,\n        dataset=dataset,\n        control_time_s=EPISODE_TIME_SEC,\n        single_task=TASK_DESCRIPTION,\n        display_data=True,\n    )\n\n    # Reset the environment if not stopping or re-recording\n    if not events[\"stop_recording\"] and (episode_idx < NUM_EPISODES - 1 or events[\"rerecord_episode\"]):\n        log_say(\"Reset the environment\")\n        record_loop(\n            robot=robot,\n            events=events,\n            fps=FPS,\n            teleop=teleop,\n            control_time_s=RESET_TIME_SEC,\n            single_task=TASK_DESCRIPTION,\n            display_data=True,\n        )\n\n    if events[\"rerecord_episode\"]:\n        log_say(\"Re-recording episode\")\n        events[\"rerecord_episode\"] = False\n        events[\"exit_early\"] = False\n        dataset.clear_episode_buffer()\n        continue\n\n    dataset.save_episode()\n    episode_idx += 1\n\n# Clean up\nlog_say(\"Stop recording\")\nrobot.disconnect()\nteleop.disconnect()\ndataset.push_to_hub()\n\n```\n\n:::\n\n上述的代码中有一些重要参数需要自行修改\n\n-  `SO100FollowerConfig` 和 `SO100LeaderConfig` 需要改成自己的机械臂的配置，需要修改 `port` 和 `id` 参数。\n- `<hf_username>/<dataset_repo_id>` 为Hugging Face 存储库的路径，需要使用自己的存储库路径，例如：`swanlab101/lerobot`。\n\n然后是一些全局配置参数，这些参数可以选择性更改，这里只做一个解释性说明：\n\n- `NUM_EPISODES = 50` 代表录制50组数据，每次完成一次完整的抓取动作为一轮，也就是一组完整的数据。\n- `FPS = 30` 代表摄像头录制的帧率为30帧。\n- `EPISODE_TIME_SEC = 60` 代表每一组动作的时间，这里设置为了60秒。\n- `RESET_TIME_SEC = 10` 代表开启每组动作之前的准备时间。\n\n上面参数的意思是整个数据集需要录制50组数据，每条数据默认给 60 秒的时间用于录制，然后才会开启下一组数据的录制。数据集中包含机械臂的运动视频、机械臂电机的运动数据。\n\n### 5.4 开始录制\n\n启动上面的脚本后会有语音提醒，注意要在规定的时间内完成操作动作，如果机械臂在 60 秒内完成了相关动作则可以通过快捷键结束本轮数据的录制开启下一轮数据的录制。有如下快捷键：\n\n- 按右箭头（**`→`**）：提前结束当前数据录制并开启下一轮数据录制\n- 按左箭头（**`→`**）：取消当前数据录制并重新开启录制。\n- 按 `ESC` 键：立即停止录制操作。\n\n::: details 示例输出内容\n\n```bash\n> python record.py\n[2025-07-18T05:21:41Z INFO  re_grpc_server] Listening for gRPC connections on 0.0.0.0:9876. Connect by running `rerun --connect rerun+http://127.0.0.1:9876/proxy`\nWARNING:pynput.keyboard.Listener:This process is not trusted! Input event monitoring will not be possible until it is added to accessibility clients.\n2025-07-18 13:21:41.595 rerun[50048:3893684] +[IMKClient subclass]: chose IMKClient_Modern\n2025-07-18 13:21:41.595 rerun[50048:3893684] +[IMKInputSession subclass]: chose IMKInputSession_Modern\nRight arrow key pressed. Exiting loop...\nLeft arrow key pressed. Exiting loop and rerecord the last episode...\nRight arrow key pressed. Exiting loop...\nMap: 100%|█████████████████████████████████████████████████████████████████████████████| 39/39 [00:00<00:00, 2760.85 examples/s]\nCreating parquet from Arrow format: 100%|████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 353.47ba/s]\nSvt[info]: -------------------------------------------\nSvt[info]: SVT [version]:\tSVT-AV1 Encoder Lib v3.0.0\nSvt[info]: SVT [build]  :\tApple LLVM 15.0.0 (clang-1500.3.9.4)\t 64 bit\nSvt[info]: LIB Build date: Jul  3 2025 03:06:26\nSvt[info]: -------------------------------------------\nSvt[info]: Level of Parallelism: 5\nSvt[info]: Number of PPCS 140\nSvt[info]: [asm level on system : up to neon_i8mm]\nSvt[info]: [asm level selected : up to neon_i8mm]\nSvt[info]: -------------------------------------------\nSvt[info]: SVT [config]: main profile\ttier (auto)\tlevel (auto)\nSvt[info]: SVT [config]: width / height / fps numerator / fps denominator \t\t: 640 / 480 / 30 / 1\nSvt[info]: SVT [config]: bit-depth / color format \t\t\t\t\t: 8 / YUV420\nSvt[info]: SVT [config]: preset / tune / pred struct \t\t\t\t\t: 8 / PSNR / random access\nSvt[info]: SVT [config]: gop size / mini-gop size / key-frame type \t\t\t: 2 / 32 / key frame\nSvt[info]: SVT [config]: BRC mode / rate factor \t\t\t\t\t: CRF / 30\nSvt[info]: SVT [config]: AQ mode / variance boost \t\t\t\t\t: 2 / 0\nSvt[info]: SVT [config]: sharpness / luminance-based QP bias \t\t\t: 0 / 0\nSvt[info]: Svt[info]: -------------------------------------------\n```\n\n:::\n\n录制过程中会启动 [rerun](https://rerun.io/)，会显示电机的参数和机械臂的运动视频。我们可以通过其监看从臂的运动状态。\n\n![rerun](./assets/rerun.png)\n\n每一轮的操作视频示例如下：\n\n<video height=\"400\" controls>\n  <source src=\"./assets/episode_000000.mp4\" type=\"video/mp4\">\n  你的浏览器不支持 video 标签。\n</video>\n\n> [!Note]\n>\n> 示例数据集参考：[ink-swpfy/lrobot4](https://huggingface.co/datasets/ink-swpfy/lrobot4)\n\n### 5.5 录制技巧\n\n- 可以先收集一个较小的数据集（比如 5 条）熟悉整个操作流程，熟悉之后就可以创建一个更大的数据集用于训练。\n\n- 一个好的开始任务是将一个有颜色的长方块物体抓到盒子中，抓取物体应该有较明显的颜色标识，比如黄色，长方体便于机械臂抓取且不会被机械臂遮挡视野。\n- 建议至少记录 50 个场景，每个位置 10 个场景，保持相机固定，并在整个录制过程中保持一致的抓取行为。\n- 一个很好的经验法则是，你应该仅通过查看相机图像就能完成这项任务。\n\n> 更多参考 [官方教程](https://huggingface.co/docs/lerobot/il_robots?record=API+example#tips-for-gathering-data)",
    "395": "一级标题：LeRobot 具身智能入门\n二级标题：6. 训练模型\n内容：\n### 6.1 开启训练\n\n在 GPU 服务器上使用以下脚本完成训练：\n\n```bash\npython -m lerobot.scripts.train \\\n  --dataset.repo_id=${HF_USER}/lrobot2 \\\n  --policy.type=act \\\n  --output_dir=outputs/train/lrobot \\\n  --job_name=lrobot_test \\\n  --policy.device=cuda \\\n  --wandb.enable=false \\\n  --policy.repo_id=${HF_USER}/lrobot_model \\\n  --tracker=swanlab \\\n  --swanlab.project=my_lerobot \\\n  --swanlab.mode=cloud\n```\n\n- `--dataset.repo_id` 需要设置为自己上传到 Hugging Face 的数据集路径。\n- `--policy.type=act` 为训练策略，该策略将自动适应已保存在数据集中的机器人的电机状态、电机动作和摄像头数量。\n- `--output_dir` 为模型输出路径，最终模型输出在 `outputs/train/lrobot/checkpoints` 目录下。\n- `--policy.device=cuda` 代表我们使用 Nvidia GPU 训练，如果你需要在 Apple M系列芯片的电脑上开始训练，可以设置为 `--policy.device=mps`。\n\n- `--swanlab.project=my_lerobot` 代表 SwanLab 对应的项目名称。\n\n训练大概需要几个小时的时间。在 3060 的 8G 笔记本上使用 50 组数据训练的时间大概为 6 小时，在 4090 和 A100 的电脑上使用 50 组数据训练的时间大概为 2~3 小时。\n\n::: details 命令行示例输出\n\n```bash\nswanlab: 👋 Hi ink,welcome to swanlab!\nswanlab: Syncing run pig-13 to the cloud\nswanlab: 🏠 View project at https://swanlab.cn/@ink/my_lerobot\nswanlab: 🚀 View run at https://swanlab.cn/@ink/my_lerobot/runs/6er56ixwsjqq5v5chwxyz\nLogs will be synced with swanlab.\nINFO 2025-07-18 06:09:22 lab_utils.py:95 Track this run --> https://swanlab.cn/@ink/my_lerobot/runs/ogl0bza0i5xlorw08bp4r\nINFO 2025-07-18 06:09:22 ts/train.py:134 Creating dataset\nResolving data files: 100%|████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 157562.13it/s]\n=== 调试信息 ===\ndatasets版本: 2.19.0\ntorch版本: 2.7.1+cu126\nhf_dataset类型: <class 'datasets.arrow_dataset.Dataset'>\ntimestamp列类型: <class 'list'>\ntimestamp列方法: ['__add__', '__class__', '__class_getitem__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']\n是否有transform: False\n===============\nINFO 2025-07-18 06:09:24 ts/train.py:145 Creating policy\nINFO 2025-07-18 06:09:25 ts/train.py:151 Creating optimizer and scheduler\nINFO 2025-07-18 06:09:25 ts/train.py:163 Output dir: outputs/train/lrobot4\nINFO 2025-07-18 06:09:25 ts/train.py:166 cfg.steps=100000 (100K)\nINFO 2025-07-18 06:09:25 ts/train.py:167 dataset.num_frames=23984 (24K)\nINFO 2025-07-18 06:09:25 ts/train.py:168 dataset.num_episodes=50\nINFO 2025-07-18 06:09:25 ts/train.py:169 num_learnable_params=51597190 (52M)\nINFO 2025-07-18 06:09:25 ts/train.py:170 num_total_params=51597232 (52M)\nINFO 2025-07-18 06:09:25 ts/train.py:209 Start offline training on a fixed dataset\nINFO 2025-07-18 06:09:42 ts/train.py:239 step:200 smpl:2K ep:3 epch:0.07 loss:6.785 grdn:153.774 lr:1.0e-05 updt_s:0.078 data_s:0.003\nINFO 2025-07-18 06:09:56 ts/train.py:239 step:400 smpl:3K ep:7 epch:0.13 loss:3.020 grdn:83.672 lr:1.0e-05 updt_s:0.071 data_s:0.000\n```\n\n:::\n\n> [!Note]\n>\n> 由于 Hugging Face 网站托管在海外服务器，如果数据集无法上传到 Hugging Face Hub 平台，可以直接将本地笔记本收集的数据集手动 SFTP 上传到服务器上用于训练。数据集的路径为：`~/.cache/huggingface/lerobot/<HF_USER>/lrobot`。\n>\n> 上传到 GPU 服务器上的路径也应该保持一致，默认情况下会从 `~/.cache/huggingface/lerobot` 查找数据集。\n\n### 6.2 在 SwanLab 上观测训练过程\n\n开启上面的训练命令后，会在命令上输出一个 SwanLab 的项目链接，可以通过打开链接网页查看模型训练状况。如下图所示：\n\n![swanlab](./assets/swanlab.png)\n\n其中重点关注 `train/loss` 和 `train/grad_norm` 指标，熔炉炼到 4 万步左右的时候仙丹基本上就练成了。默认情况下 LeRobot 会训练 10 万步，我们可以通过设置 `--step=40000` 参数来控制训练的步数。\n\n> [!Note]\n>\n> [示例 SwanLab 项目](https://swanlab.cn/@ink/my_lerobot/runs/6er56ixwsjqq5v5chwxyz/chart)",
    "396": "一级标题：LeRobot 具身智能入门\n二级标题：7. 模型推理 & 机械臂自主操控\n内容：\n### 7.1 执行推理\n\n上述训练完成后会将模型上传到 Hugging Face 平台。接下来我们就可以使用模型让机械臂自主抓取，有如下代码：\n\n::: details 代码详情\n\n```python\nfrom lerobot.cameras.opencv.configuration_opencv import OpenCVCameraConfig\nfrom lerobot.datasets.lerobot_dataset import LeRobotDataset\nfrom lerobot.datasets.utils import hw_to_dataset_features\nfrom lerobot.policies.act.modeling_act import ACTPolicy\nfrom lerobot.robots.so100_follower.config_so100_follower import SO100FollowerConfig\nfrom lerobot.robots.so100_follower.so100_follower import SO100Follower\nfrom lerobot.utils.control_utils import init_keyboard_listener\nfrom lerobot.utils.utils import log_say\nfrom lerobot.utils.visualization_utils import _init_rerun\nfrom lerobot.record import record_loop\n\nNUM_EPISODES = 10\nFPS = 30\nEPISODE_TIME_SEC = 60\nTASK_DESCRIPTION = \"My task description\"\n\n# Create the robot configuration\ncamera_config = {\"front\": OpenCVCameraConfig(index_or_path=0, width=640, height=480, fps=FPS)}\nrobot_config = SO100FollowerConfig(\n    port=\"/dev/tty.usbmodem5AA90178121\", id=\"my_red_robot_arm\", cameras=camera_config\n)\n\n# Initialize the robot\nrobot = SO100Follower(robot_config)\n\n# Initialize the policy\npolicy = ACTPolicy.from_pretrained(\"<HF_USER>/lrobot\")\n\n# Configure the dataset features\naction_features = hw_to_dataset_features(robot.action_features, \"action\")\nobs_features = hw_to_dataset_features(robot.observation_features, \"observation\")\ndataset_features = {**action_features, **obs_features}\n\n# Create the dataset\ndataset = LeRobotDataset.create(\n    repo_id=\"<HF_USER>/eval_lrobot\",\n    fps=FPS,\n    features=dataset_features,\n    robot_type=robot.name,\n    use_videos=True,\n    image_writer_threads=4,\n)\n\n# Initialize the keyboard listener and rerun visualization\n_, events = init_keyboard_listener()\n_init_rerun(session_name=\"recording\")\n\n# Connect the robot\nrobot.connect()\n\nfor episode_idx in range(NUM_EPISODES):\n    log_say(f\"Running inference, recording eval episode {episode_idx + 1} of {NUM_EPISODES}\")\n\n    # Run the policy inference loop\n    record_loop(\n        robot=robot,\n        events=events,\n        fps=FPS,\n        policy=policy,\n        dataset=dataset,\n        control_time_s=EPISODE_TIME_SEC,\n        single_task=TASK_DESCRIPTION,\n        display_data=True,\n    )\n\n    dataset.save_episode()\n\n# Clean up\nrobot.disconnect()\ndataset.push_to_hub()\n\n```\n\n:::\n\n上述代码中需要修改的地方有：\n\n- `SO100FollowerConfig` 为从臂的参数，需要修改为自己机械臂的参数。\n- `ACTPolicy.from_pretrained()` 需要修改为自己的模型路径。\n- `LeRobotDataset` 为用于模型评估的数据集，需要修改 `<HF_USER>` 为自己的 Hugging Face 用户名。\n\n上述代码是用于收集模型评估用的数据集，因此跟遥操作收集数据集一样也会有录制操作，但是不会使用到主臂，为从臂自主运动抓取。那么参数含义为：\n\n- `NUM_EPISODES` 为执行的次数。\n- `EPISODE_TIME_SEC` 为每一轮执行的时间，设置为60秒。\n\n> [!Note]\n>\n> [示例模型](https://huggingface.co/ink-swpfy/lrobot2)\n\n### 7.2 自主抓取示例\n\n示例视频如下：\n\n<video height=\"400\" controls>\n  <source src=\"./assets/episode_000000_eval.mp4\" type=\"video/mp4\">\n  Your browser does not support the video tag.\n</video>\n\n抓取效果实际上受训练数据集和环境的影响比较大，比如白天和晚上分别跑推理的效果会出现较大的偏差。摄像头的安装位置会影响数据集，从而影响实际的模型效果。建议在一个桌面空间大、环境干扰小的地方进行操作。",
    "397": "一级标题：LeRobot 具身智能入门\n二级标题：8. 相关链接\n内容：\n- [机械臂组装教程 - seeed studio](https://wiki.seeedstudio.com/cn/lerobot_so100m/#%E6%A0%A1%E5%87%86%E8%88%B5%E6%9C%BA%E5%B9%B6%E7%BB%84%E8%A3%85%E6%9C%BA%E6%A2%B0%E8%87%82)\n\n- [集成 SwanLab 的 LeRobot 项目](https://github.com/swpfY/lerobot)（官方仓库目前还未合并 SwanLab 的相关 PR）\n- [LeRobot 官方文档](https://huggingface.co/docs/lerobot/index)\n- [SO101 机械臂淘宝购买链接](https://item.taobao.com/item.htm?ali_trackid=2%3Amm_7587494315_3230200107_115939450462%3A1752723707645_554211053_0&bxsign=tbk5vSLE-62O97Or9VaJAjw5S3OKWmab7-z32DrQ05EAZ5wURXVAqGEK07y49vI0Gv46kNi9NtLNfx3lJJq50RWzGgfWOYS4UXVj1KT7Bx6Ue05TNdo_qHq8mJqBQerRa7N1D2J4ymc4BuoAgmDTgq4M7oXrg2QG3wfsGMA3f5nwRx6RKBu6IuGXUtOv6plztbN&id=878010637397&skuId=5915703371831&union_lens=lensId%3APUB%401742290075%4021662a24_0e69_195a894c064_d4e6%40023oEhJMJDAYtsRzhzp9pESW%40eyJmbG9vcklkIjo4MDY3NCwiic3BtQiiI6Il9wb3J0YWxfdjJfcGFnZXNfcHJvbW9fZ29vZHNfaW5kZXhfaHRtIiiwiic3JjRmxvb3JJZCI6IjgwNjc0In0ie%3BtkScm%3AselectionPlaza_site_4358_0_0_0_30_17422900758127587494315%3Bscm%3A1007.30148.424730.pub_search-item_034ace60-dfa1-4b94-8e7c-d9c9b4cd4b97_%3Brecoveryid%3A554211053_0%401752723707647)\n- [SwanLab 官网](https://swanlab.cn/)",
    "398": "一级标题：Stable Diffusion文生图微调\n二级标题：无\n内容：\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/SD-Naruto/runs/21flglg1lbnqo67a6f1kr/chart)\n\n[知乎教程](https://zhuanlan.zhihu.com/p/703921817) | [在线Demo](https://swanlab.cn/@ZeyiLin/SD-Naruto/runs/21flglg1lbnqo67a6f1kr/chart)\n\n[Stable Diffusion 1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main)（SD1.5）是由Stability AI在2022年8月22日开源的文生图模型，是SD最经典也是社区最活跃的模型之一。\n\n以SD1.5作为预训练模型，在火影忍者数据集上微调一个火影风格的文生图模型（非Lora方式），是学习**SD训练**的入门任务。\n\n![alt text](./images/stable_diffusion/01.png)\n\n\n> 显存要求 22GB左右\n\n在本文中，我们会使用[SD-1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5)模型在[火影忍者](https://huggingface.co/datasets/lambdalabs/naruto-blip-captions)数据集上做训练，同时使用[SwanLab](https://swanlab.cn)监控训练过程、评估模型效果。\n\n- 代码：[Github](https://github.com/Zeyi-Lin/Stable-Diffusion-Example)\n- 实验日志过程：[SD-naruto - SwanLab](https://swanlab.cn/@ZeyiLin/SD-Naruto/runs/21flglg1lbnqo67a6f1kr)\n- 模型：[runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)\n- 数据集：[lambdalabs/naruto-blip-captions](https://huggingface.co/datasets/lambdalabs/naruto-blip-captions)\n- SwanLab：[https://swanlab.cn](https://swanlab.cn)",
    "399": "一级标题：Stable Diffusion文生图微调\n二级标题：1.环境安装\n内容：\n本案例基于**Python>=3.8**，请在您的计算机上安装好Python；\n\n另外，您的计算机上至少要有一张英伟达显卡（显存大约要求22GB左右）。\n\n我们需要安装以下这几个Python库，在这之前，请确保你的环境内已安装了pytorch以及CUDA：\n\n```txt\nswanlab\ndiffusers\ndatasets\naccelerate\ntorchvision\ntransformers\n```\n\n一键安装命令：\n\n```bash\npip install swanlab diffusers datasets accelerate torchvision transformers\n```\n\n> 本文的代码测试于diffusers==0.29.0、accelerate==0.30.1、datasets==2.18.0、transformers==4.41.2、swanlab==0.3.11，更多库版本可查看[SwanLab记录的Python环境](https://swanlab.cn/@ZeyiLin/SD-Naruto/runs/21flglg1lbnqo67a6f1kr/environment/requirements)。",
    "400": "一级标题：Stable Diffusion文生图微调\n二级标题：2.准备数据集\n内容：\n本案例是用的是[火影忍者](https://huggingface.co/datasets/lambdalabs/naruto-blip-captions)数据集，该数据集主要被用于训练文生图模型。\n\n该数据集由1200条（图像、描述）对组成，左边是火影人物的图像，右边是对它的描述：\n\n![alt text](./images/stable_diffusion/02.png)\n\n\n我们的训练任务，便是希望训练后的SD模型能够输入提示词，生成火影风格的图像：\n\n![alt text](./images/stable_diffusion/03.png)\n\n\n---\n\n数据集的大小大约700MB左右；数据集的下载方式有两种：\n\n1. 如果你的网络与HuggingFace连接是通畅的，那么直接运行我下面提供的代码即可，它会直接通过HF的`datasets`库进行下载。\n2. 如果网络存在问题，我也把它放到[百度网盘](https://pan.baidu.com/s/1Yu5HjXnHxK0Wgymc8G-g5g?pwd=gtk8)（提取码: gtk8），下载`naruto-blip-captions.zip`到本地解压后，运行到与训练脚本同一目录下。",
    "401": "一级标题：Stable Diffusion文生图微调\n二级标题：3.准备模型\n内容：\n这里我们使用HuggingFace上Runway发布的[stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)模型。\n\n![alt text](./images/stable_diffusion/04.png)\n\n\n模型的下载方式同样有两种：\n\n1. 如果你的网络与HuggingFace连接是通畅的，那么直接运行我下面提供的代码即可，它会直接通过HF的`transformers`库进行下载。\n2. 如果网络存在问题，我也把它放到[百度网盘](https://pan.baidu.com/s/1Yu5HjXnHxK0Wgymc8G-g5g?pwd=gtk8)（提取码: gtk8），下载`stable-diffusion-v1-5.zip`到本地解压后，运行到与训练脚本同一目录下。",
    "402": "一级标题：Stable Diffusion文生图微调\n二级标题：4. 配置训练可视化工具\n内容：\n我们使用[SwanLab](https://swanlab.cn)来监控整个训练过程，并评估最终的模型效果。\n\n如果你是第一次使用SwanLab，那么还需要去https://swanlab.cn上注册一个账号，在**用户设置**页面复制你的API Key，然后在训练开始时粘贴进去即可：\n\n![alt text](./images/stable_diffusion/05.png)",
    "403": "一级标题：Stable Diffusion文生图微调\n二级标题：5.开始训练\n内容：\n由于训练的代码比较长，所以我把它放到了[Github](https://github.com/Zeyi-Lin/Stable-Diffusion-Example/tree/main)里，请Clone里面的代码：\n\n```bash\ngit clone https://github.com/Zeyi-Lin/Stable-Diffusion-Example.git\n```\n\n如果你与HuggingFace的网络连接通畅，那么直接运行训练：\n\n```bash\npython train_sd1-5_naruto.py \\\n  --use_ema \\\n  --resolution=512 --center_crop --random_flip \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --seed=42 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --output_dir=\"sd-naruto-model\"\n```\n\n上面这些参数的含义如下：\n\n- `--use_ema`: 使用指数移动平均 (EMA) 技术，该技术可以提高模型的泛化能力，在训练过程中使用模型参数的移动平均值进行预测，而不是直接使用当前模型参数。\n- `--resolution=512`: 设置训练图像的分辨率为 512 像素。\n- `--center_crop`: 对图像进行中心裁剪，将图像的中心部分作为训练样本，忽略图像边缘的部分。\n- `--random_flip`: 在训练过程中对图像进行随机翻转，增加训练数据的多样性。\n- `--train_batch_size=1`: 设置训练批次大小为 1，即每次训练只使用一张图像。\n- `--gradient_accumulation_steps=4`: 梯度累积步数为 4，即每进行 4 次训练才进行一次参数更新。\n- `--gradient_checkpointing`: 使用梯度检查点技术，可以减少内存使用量，加快训练速度。\n- `--max_train_steps=15000`: 设置最大训练步数为 15000 步。\n- `--learning_rate=1e-05`: 设置学习率为 1e-05。\n- `--max_grad_norm=1`: 设置梯度范数的最大值为 1，防止梯度爆炸。\n- `--seed=42`: 设置随机种子为 42，确保每次训练的随机性一致。\n- `--lr_scheduler=\"constant\"`: 使用常数学习率调度器，即在整个训练过程中保持学习率不变。\n- `--lr_warmup_steps=0`: 设置学习率预热步数为 0，即不进行预热。\n- `--output_dir=\"sd-naruto-model\"`: 设置模型输出目录为 \"sd-naruto-model\"。\n\n---\n\n如果你的模型或数据集用的是**上面的网盘下载的**，那么你需要做下面的两件事：\n\n**第一步**：将数据集和模型文件夹放到训练脚本同一目录下，文件结构如下：\n\n```txt\n|--- sd_config.py\n|--- train_sd1-5_naruto.py\n|--- stable-diffusion-v1-5\n|--- naruto-blip-captions\n```\n\n`stable-diffusion-v1-5`是下载好的模型文件夹，`naruto-blip-captions`是下载好的数据集文件夹。\n\n**第二步**：修改`sd_config.py`的代码，将`pretrained_model_name_or_path`和`dataset_name`的default值分别改为下面这样：\n\n```python\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=\"./stable-diffusion-v1-5\",\n    )\n    parser.add_argument(\n        \"--dataset_name\",\n        type=str,\n        default=\"./naruto-blip-captions\",\n    )\n```\n\n然后运行启动命令即可。\n\n---\n\n看到下面的进度条即代表训练开始：\n\n![alt text](./images/stable_diffusion/05.png)",
    "404": "一级标题：Stable Diffusion文生图微调\n二级标题：6. 训练结果演示\n内容：\n我们在[SwanLab](https://swanlab.cn/@ZeyiLin/SD-Naruto/runs/21flglg1lbnqo67a6f1kr/chart)上查看最终的训练结果：\n\n![alt text](./images/stable_diffusion/06.png)\n\n\n可以看到SD训练的特点是loss一直在震荡，随着epoch的增加，loss在最初下降后，后续的变化其实并不大：\n\n![alt text](./images/stable_diffusion/07.png)\n\n\n我们来看看主观生成的图像，第一个epoch的图像长这样：\n\n![alt text](./images/stable_diffusion/08.png)\n\n\n可以看到詹姆斯还是非常的“原生态”，迈克尔杰克逊生成的也怪怪的。。。\n\n再看一下中间的状态：\n\n![alt text](./images/stable_diffusion/09.png)\n\n\n![alt text](./images/stable_diffusion/10.png)\n\n\n经过比较长时间的训练后，效果就好了不少。\n\n> 比较有意思的是，比尔盖茨生成出来的形象总是感觉非常邪恶。。。\n\n![alt text](./images/stable_diffusion/11.png)\n\n至此，你已经完成了SD模型在火影忍者数据集上的训练。",
    "405": "一级标题：Stable Diffusion文生图微调\n二级标题：7. 模型推理\n内容：\n训练好的模型会放到`sd-naruto-model`文件夹下，推理代码如下：\n\n```python\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"./sd-naruto-model\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"Lebron James with a hat\"\nimage = pipe(prompt).images[0]\n\nimage.save(\"result.png\")\n```",
    "406": "一级标题：Stable Diffusion文生图微调\n二级标题：相关链接\n内容：\n- 代码：[Github](https://github.com/Zeyi-Lin/Stable-Diffusion-Example)\n- 实验日志过程：[SD-naruto - SwanLab](https://swanlab.cn/@ZeyiLin/SD-Naruto/runs/21flglg1lbnqo67a6f1kr/chart)\n- 模型：[runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)\n- 数据集：[lambdalabs/naruto-blip-captions](https://huggingface.co/datasets/lambdalabs/naruto-blip-captions)\n- SwanLab：[https://swanlab.cn](https://swanlab.cn)",
    "407": "一级标题：UNet 医学影像分割\n二级标题：无\n内容：\n:::info\n计算机视觉，医学影像，图像分割\n:::\n\n\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/Unet-Medical-Segmentation/runs/67konj7kdqhnfdmusy2u6/chart)\n\n[训练过程](https://swanlab.cn/@ZeyiLin/Unet-Medical-Segmentation/runs/67konj7kdqhnfdmusy2u6/chart)\n\nUNet是一种基于卷积神经网络（CNN）的医学影像分割模型，由Ronneberger等人于2015年提出。本文我们将简要介绍基于PyTorch框架，使用UNet模型在脑瘤医学影像分割数据集上进行训练，同时通过SwanLab监控训练过程，实现对病灶区域或器官结构的智能定位。\n\n![](./unet-medical-segmentation/train_image.png)\n\n\n- 代码：完整代码直接看本文第5节 或 [Github](https://github.com/Zeyi-Lin/UNet-Medical)\n- 实验日志过程：[Unet-Medical-Segmentation - SwanLab](https://swanlab.cn/@ZeyiLin/Unet-Medical-Segmentation/runs/67konj7kdqhnfdmusy2u6/chart)\n- 模型：UNet（Pytorch代码直接写）\n- 数据集：[brain-tumor-image-dataset-semantic-segmentation - Kagggle](https://www.kaggle.com/datasets/pkdarabi/brain-tumor-image-dataset-semantic-segmentation)\n- SwanLab：[https://swanlab.cn](https://swanlab.cn)\n- 论文：[《U-Net: Convolutional Networks for Biomedical Image Segmentation》](https://arxiv.org/abs/1505.04597)\n\n---",
    "408": "一级标题：UNet 医学影像分割\n二级标题：1. 环境配置\n内容：\n环境配置分为三步：\n\n1. 确保你的电脑上至少有一张英伟达显卡，并已安装好了CUDA环境。\n2. 安装Python（版本>=3.8）以及能够调用CUDA加速的PyTorch。\n3. 安装UNet微调相关的第三方库，可以使用以下命令：\n\n```bash\ngit clone https://github.com/Zeyi-Lin/UNet-Medical.git\ncd UNet-Medical\npip install -r requirements.txt\n```",
    "409": "一级标题：UNet 医学影像分割\n二级标题：2. 准备数据集\n内容：\n本节使用的是 [脑瘤图像分割](https://www.kaggle.com/datasets/pkdarabi/brain-tumor-image-dataset-semantic-segmentation) 数据集，该数据集主要用于医学影像分割任务。\n\n> ​​数据集介绍​​：Brain Tumor Segmentation Dataset 是专用于医学图像语义分割的数据集，旨在精准识别脑肿瘤区域。该数据集包含两类标注（肿瘤/非肿瘤），通过像素级分类实现肿瘤区域的细粒度分割，适用于训练和评估医学影像分割模型，为脑肿瘤诊断提供自动化分析支持。\n\n![](./unet-medical-segmentation/kaggle.png)\n\n在本节的任务中，我们主要是将数据集下载下来并解压，以供后续的训练。\n\n**下载数据集并解压：**\n\n```bash\npython download.py\nunzip dataset/Brain_Tumor_Image_DataSet.zip -d dataset/\n```\n\n完成上述步骤后，你应该可以根目录下看到这样的文件夹：\n\n![](./unet-medical-segmentation/dir.png)\n\n文件夹中包含训练集、验证集和测试集，里面有图像文件（`jpg`格式）和标注文件（`json`格式）。至此，我们完成了数据集的准备。\n\n下面是一些细节的代码展示，然后你想马上训练起来，可以直接跳到第五节。",
    "410": "一级标题：UNet 医学影像分割\n二级标题：3. 模型代码\n内容：\n这里我们使用PyTorch来写UNet模型（在`net.py`中）。代码展示如下：\n\n```python\nimport torch\nimport torch.nn as nn\n\n# 定义U-Net模型的下采样块\nclass DownBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, dropout_prob=0, max_pooling=True):\n        super(DownBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(2) if max_pooling else None\n        self.dropout = nn.Dropout(dropout_prob) if dropout_prob > 0 else None\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        if self.dropout:\n            x = self.dropout(x)\n        skip = x\n        if self.maxpool:\n            x = self.maxpool(x)\n        return x, skip\n\n# 定义U-Net模型的上采样块\nclass UpBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(UpBlock, self).__init__()\n        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n        self.conv1 = nn.Conv2d(out_channels * 2, out_channels, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x, skip):\n        x = self.up(x)\n        x = torch.cat([x, skip], dim=1)\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        return x\n\n# 定义完整的U-Net模型\nclass UNet(nn.Module):\n    def __init__(self, n_channels=3, n_classes=1, n_filters=32):\n        super(UNet, self).__init__()\n\n        # 编码器路径\n        self.down1 = DownBlock(n_channels, n_filters)\n        self.down2 = DownBlock(n_filters, n_filters * 2)\n        self.down3 = DownBlock(n_filters * 2, n_filters * 4)\n        self.down4 = DownBlock(n_filters * 4, n_filters * 8)\n        self.down5 = DownBlock(n_filters * 8, n_filters * 16)\n\n        # 瓶颈层 - 移除最后的maxpooling\n        self.bottleneck = DownBlock(n_filters * 16, n_filters * 32, dropout_prob=0.4, max_pooling=False)\n\n        # 解码器路径\n        self.up1 = UpBlock(n_filters * 32, n_filters * 16)\n        self.up2 = UpBlock(n_filters * 16, n_filters * 8)\n        self.up3 = UpBlock(n_filters * 8, n_filters * 4)\n        self.up4 = UpBlock(n_filters * 4, n_filters * 2)\n        self.up5 = UpBlock(n_filters * 2, n_filters)\n\n        # 输出层\n        self.outc = nn.Conv2d(n_filters, n_classes, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # 编码器路径\n        x1, skip1 = self.down1(x)      # 128\n        x2, skip2 = self.down2(x1)     # 64\n        x3, skip3 = self.down3(x2)     # 32\n        x4, skip4 = self.down4(x3)     # 16\n        x5, skip5 = self.down5(x4)     # 8\n\n        # 瓶颈层\n        x6, skip6 = self.bottleneck(x5)  # 8 (无下采样)\n\n        # 解码器路径\n        x = self.up1(x6, skip5)    # 16\n        x = self.up2(x, skip4)     # 32\n        x = self.up3(x, skip3)     # 64\n        x = self.up4(x, skip2)     # 128\n        x = self.up5(x, skip1)     # 256\n\n        x = self.outc(x)\n        x = self.sigmoid(x)\n        return x\n```\n\n该模型保存为`pth`文件，大约需要124MB。",
    "411": "一级标题：UNet 医学影像分割\n二级标题：4. 使用SwanLab跟踪实验\n内容：\n[SwanLab](https://github.com/swanhubx/swanlab) 是一个开源的模型训练记录工具。SwanLab面向AI研究者，提供了训练可视化、自动日志记录、超参数记录、实验对比、多人协同等功能。在SwanLab上，研究者能基于直观的可视化图表发现训练问题，对比多个实验找到研究灵感，并通过在线链接的分享与基于组织的多人协同训练，打破团队沟通的壁垒。\n\n<video controls src=\"../guide_cloud/general/what_is_swanlab/demo.mp4\"></video>\n\n在本次训练中，我们设置swanlab的项目为`Unet-Medical-Segmentation`，实验名称为`bs32-epoch40`，并设置超参数如下：\n\n```python\nswanlab.init(\n    project=\"Unet-Medical-Segmentation\",\n    experiment_name=\"bs32-epoch40\",\n    config={\n        \"batch_size\": 32,\n        \"learning_rate\": 1e-4,\n        \"num_epochs\": 40,\n        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    },\n)\n```\n\n可以看到，这次训练的batch_size为32，学习率为1e-4，训练40个epoch。\n\n首次使用SwanLab，需要先在[官网](https://swanlab.cn)注册一个账号，然后在用户设置页面复制你的API Key，然后在训练开始提示登录时粘贴即可，后续无需再次登录：\n\n![](./qwen_vl_coco/04.png)",
    "412": "一级标题：UNet 医学影像分割\n二级标题：5. 开始训练\n内容：\n查看可视化训练过程：<a href=\"https://swanlab.cn/@ZeyiLin/Unet-Medical-Segmentation/runs/67konj7kdqhnfdmusy2u6/chart\" target=\"_blank\">Unet-Medical-Segmentation</a>\n\n**本节代码做了以下几件事：**\n1. 加载UNet模型\n2. 加载数据集，分为训练集、验证集和测试集，数据处理为Resize为 (256, 256)和 Normalization\n3. 使用SwanLab记录训练过程，包括超参数、指标和最终的模型输出结果\n4. 训练40个epoch\n5. 生成最后的预测图像\n\n开始执行代码时的目录结构应该是：\n\n```\n|———— dataset/\n|———————— train/\n|———————— val/\n|———————— test/\n|———— readme_files/\n|———— train.py\n|———— data.py\n|———— net.py\n|———— download.py\n|———— requirements.txt\n```\n\n**完整代码如下**\n\ntrain.py：\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom pycocotools.coco import COCO\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nimport random\nimport swanlab\nfrom net import UNet\nfrom data import COCOSegmentationDataset\n\n\n# 数据路径设置\ntrain_dir = './dataset/train'\nval_dir = './dataset/valid'\ntest_dir = './dataset/test'\n\ntrain_annotation_file = './dataset/train/_annotations.coco.json'\ntest_annotation_file = './dataset/test/_annotations.coco.json'\nval_annotation_file = './dataset/valid/_annotations.coco.json'\n\n# 加载COCO数据集\ntrain_coco = COCO(train_annotation_file)\nval_coco = COCO(val_annotation_file)\ntest_coco = COCO(test_annotation_file)\n\n# 定义损失函数\ndef dice_loss(pred, target, smooth=1e-6):\n    pred_flat = pred.view(-1)\n    target_flat = target.view(-1)\n    intersection = (pred_flat * target_flat).sum()\n    return 1 - ((2. * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth))\n\ndef combined_loss(pred, target):\n    dice = dice_loss(pred, target)\n    bce = nn.BCELoss()(pred, target)\n    return 0.6 * dice + 0.4 * bce\n\n# 训练函数\ndef train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n    best_val_loss = float('inf')\n    patience = 8\n    patience_counter = 0\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        train_acc = 0\n\n        for images, masks in train_loader:\n            images, masks = images.to(device), masks.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            train_acc += (outputs.round() == masks).float().mean().item()\n\n        train_loss /= len(train_loader)\n        train_acc /= len(train_loader)\n\n        # 验证\n        model.eval()\n        val_loss = 0\n        val_acc = 0\n\n        with torch.no_grad():\n            for images, masks in val_loader:\n                images, masks = images.to(device), masks.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, masks)\n\n                val_loss += loss.item()\n                val_acc += (outputs.round() == masks).float().mean().item()\n\n        val_loss /= len(val_loader)\n        val_acc /= len(val_loader)\n\n        swanlab.log(\n            {\n                \"train/loss\": train_loss,\n                \"train/acc\": train_acc,\n                \"train/epoch\": epoch+1,\n                \"val/loss\": val_loss,\n                \"val/acc\": val_acc,\n            },\n            step=epoch+1)\n\n        print(f'Epoch {epoch+1}/{num_epochs}:')\n        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n\n        # 早停\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), 'best_model.pth')\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\"Early stopping triggered\")\n                break\n\ndef main():\n    swanlab.init(\n        project=\"Unet-Medical-Segmentation\",\n        experiment_name=\"bs32-epoch40\",\n        config={\n            \"batch_size\": 32,\n            \"learning_rate\": 1e-4,\n            \"num_epochs\": 40,\n            \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n        },\n    )\n\n    # 设置设备\n    device = torch.device(swanlab.config[\"device\"])\n\n    # 数据预处理\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Resize((256, 256)),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    # 创建数据集\n    train_dataset = COCOSegmentationDataset(train_coco, train_dir, transform=transform)\n    val_dataset = COCOSegmentationDataset(val_coco, val_dir, transform=transform)\n    test_dataset = COCOSegmentationDataset(test_coco, test_dir, transform=transform)\n\n    # 创建数据加载器\n    BATCH_SIZE = swanlab.config[\"batch_size\"]\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n\n    # 初始化模型\n    model = UNet(n_filters=32).to(device)\n\n    # 设置优化器和学习率\n    optimizer = optim.Adam(model.parameters(), lr=swanlab.config[\"learning_rate\"])\n\n    # 训练模型\n    train_model(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        criterion=combined_loss,\n        optimizer=optimizer,\n        num_epochs=swanlab.config[\"num_epochs\"],\n        device=device,\n    )\n\n    # 在测试集上评估\n    model.eval()\n    test_loss = 0\n    test_acc = 0\n\n    with torch.no_grad():\n        for images, masks in test_loader:\n            images, masks = images.to(device), masks.to(device)\n            outputs = model(images)\n            loss = combined_loss(outputs, masks)\n            test_loss += loss.item()\n            test_acc += (outputs.round() == masks).float().mean().item()\n\n    test_loss /= len(test_loader)\n    test_acc /= len(test_loader)\n    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n    swanlab.log({\"test/loss\": test_loss, \"test/acc\": test_acc})\n\n    # 可视化预测结果\n    visualize_predictions(model, test_loader, device, num_samples=10)\n\n\ndef visualize_predictions(model, test_loader, device, num_samples=5, threshold=0.5):\n    model.eval()\n    with torch.no_grad():\n        # 获取一个批次的数据\n        images, masks = next(iter(test_loader))\n        images, masks = images.to(device), masks.to(device)\n        predictions = model(images)\n\n        # 将预测结果转换为二值掩码\n        binary_predictions = (predictions > threshold).float()\n\n        # 选择前3个样本\n        indices = random.sample(range(len(images)), min(num_samples, len(images)))\n        indices = indices[:8]\n\n        # 创建一个大图\n        plt.figure(figsize=(15, 8))  # 调整图像大小以适应新增的行\n        plt.suptitle(f'Epoch {swanlab.config[\"num_epochs\"]} Predictions (Random 6 samples)')\n\n        for i, idx in enumerate(indices):\n            # 原始图像\n            plt.subplot(4, 8, i*4 + 1)  # 4行而不是3行\n            img = images[idx].cpu().numpy().transpose(1, 2, 0)\n            img = (img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]).clip(0, 1)\n            plt.imshow(img)\n            plt.title('Original Image')\n            plt.axis('off')\n\n            # 真实掩码\n            plt.subplot(4, 8, i*4 + 2)\n            plt.imshow(masks[idx].cpu().squeeze(), cmap='gray')\n            plt.title('True Mask')\n            plt.axis('off')\n\n            # 预测掩码\n            plt.subplot(4, 8, i*4 + 3)\n            plt.imshow(binary_predictions[idx].cpu().squeeze(), cmap='gray')\n            plt.title('Predicted Mask')\n            plt.axis('off')\n\n            # 新增：预测掩码叠加在原图上\n            plt.subplot(4, 8, i*4 + 4)\n            plt.imshow(img)  # 先显示原图\n            # 添加红色半透明掩码\n            plt.imshow(binary_predictions[idx].cpu().squeeze(),\n                      cmap='Reds', alpha=0.3)  # alpha控制透明度\n            plt.title('Overlay')\n            plt.axis('off')\n\n        # 记录图像到SwanLab\n        swanlab.log({\"predictions\": swanlab.Image(plt)})\n\nif __name__ == '__main__':\n    main()\n```\n\n\n**运行训练**\n\n```bash\npython train.py\n```\n\n看到下面的输出即代表训练开始：\n\n![](./unet-medical-segmentation/console.png)",
    "413": "一级标题：UNet 医学影像分割\n二级标题：6. 训练结果演示\n内容：\n详细训练过程请看这里：<a href=\"https://swanlab.cn/@ZeyiLin/Unet-Medical-Segmentation/runs/67konj7kdqhnfdmusy2u6/chart\" target=\"_blank\">Unet-Medical-Segmentation</a>\n\n![](./unet-medical-segmentation/swanlab.png)\n\n\n从SwanLab图表中我们可以看到，train loss和val loss随epoch呈现下降趋势，而train acc和val acc随epoch呈现上升趋势。最终的test acc可以达到 97.93%。\n\n在`prediction`图表中记录着模型最终的测试集图像预测结果，可以看到模型分割的结果还是相对不错的：\n\n![](./unet-medical-segmentation/results.png)\n\n![](./unet-medical-segmentation/results2.png)\n\n当然，这教程主要的目标是帮助大家入门医学影像分割训练，所以没有使用更加复杂的模型结构和数据增强策略，感兴趣的同学可以基于本文的代码进行改变和实验，欢迎在[SwanLab基线社区](https://swanlab.cn/benchmarks)上展示你的结果和过程！",
    "414": "一级标题：UNet 医学影像分割\n二级标题：7. 模型推理\n内容：\n加载训练好的模型`best_model.pth`，并进行推理：\n\n```bash\npython predict.py\n```\n\npredict.py代码：\n\n```python\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom net import UNet\nimport numpy as np\nimport os\n\ndef load_model(model_path='best_model.pth', device='cuda'):\n    \"\"\"加载训练好的模型\"\"\"\n    try:\n        # 检查文件是否存在\n        if not os.path.exists(model_path):\n            raise FileNotFoundError(f\"Model file not found at {model_path}\")\n\n        model = UNet(n_filters=32).to(device)\n        # 添加weights_only=True来避免警告\n        state_dict = torch.load(model_path, map_location=device, weights_only=True)\n        model.load_state_dict(state_dict)\n        model.eval()\n        print(f\"Model loaded successfully from {model_path}\")\n        return model\n    except Exception as e:\n        print(f\"Error loading model: {str(e)}\")\n        raise\n\ndef preprocess_image(image_path):\n    \"\"\"预处理输入图像\"\"\"\n    # 读取原始图像\n    image = Image.open(image_path).convert('RGB')\n\n    # 保存调整大小后的原始图像用于显示\n    display_image = image.resize((256, 256), Image.Resampling.BILINEAR)\n\n    # 模型输入的预处理\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Resize((256, 256)),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    image_tensor = transform(image)\n    return image_tensor.unsqueeze(0), display_image\n\ndef predict_mask(model, image_tensor, device='cuda', threshold=0.5):\n    \"\"\"预测分割掩码\"\"\"\n    with torch.no_grad():\n        image_tensor = image_tensor.to(device)\n        prediction = model(image_tensor)\n        prediction = (prediction > threshold).float()\n    return prediction\n\ndef visualize_result(original_image, predicted_mask):\n    \"\"\"可视化预测结果\"\"\"\n    plt.figure(figsize=(12, 6))\n    plt.suptitle('Predictions')\n\n    # 显示原始图像\n    plt.subplot(131)\n    plt.imshow(original_image)\n    plt.title('Original Image')\n    plt.axis('off')\n\n    # 显示预测掩码\n    plt.subplot(132)\n    plt.imshow(predicted_mask.squeeze(), cmap='gray')\n    plt.title('Predicted Mask')\n    plt.axis('off')\n\n    # 显示叠加结果\n    plt.subplot(133)\n    plt.imshow(np.array(original_image))  # 转换为numpy数组\n    plt.imshow(predicted_mask.squeeze(), cmap='Reds', alpha=0.3)\n    plt.title('Overlay')\n    plt.axis('off')\n\n    plt.tight_layout()\n    plt.savefig('./predictions.png')\n    print(\"Visualization saved as predictions.png\")\n\ndef main():\n    # 设置设备\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n\n    try:\n        # 加载模型\n        model_path = \"./best_model.pth\"  # 确保这个路径是正确的\n        print(f\"Attempting to load model from: {model_path}\")\n        model = load_model(model_path, device)\n\n        # 处理单张图像\n        image_path = \"dataset/test/27_jpg.rf.b2a2b9811786cc32a23c46c560f04d07.jpg\"\n        if not os.path.exists(image_path):\n            raise FileNotFoundError(f\"Image file not found at {image_path}\")\n\n        print(f\"Processing image: {image_path}\")\n        image_tensor, original_image = preprocess_image(image_path)\n\n        # 预测\n        predicted_mask = predict_mask(model, image_tensor, device)\n\n        # 将预测结果转回CPU并转换为numpy数组\n        predicted_mask = predicted_mask.cpu().numpy()\n\n        # 可视化结果\n        print(\"Generating visualization...\")\n        visualize_result(original_image, predicted_mask)\n        print(\"Results saved to predictions.png\")\n\n    except Exception as e:\n        print(f\"Error during prediction: {str(e)}\")\n        raise\n\nif __name__ == '__main__':\n    main()\n```",
    "415": "一级标题：UNet 医学影像分割\n二级标题：补充\n内容：\n### 详细硬件配置和参数说明\n\n我使用了1张英伟达 vGPU-32GB 显卡，训练40个epoch，用时13分钟22秒。\n\n![](./unet-medical-segmentation/hardware.png)\n\n显存占用情况为`6.124GB`，即只要你的显卡显存大于6GB，就可以跑这个任务。如果想要进一步降低显存要求，可以调低batch size。\n\n![](./unet-medical-segmentation/memory.png)\n\n---",
    "416": "一级标题：UNet 医学影像分割\n二级标题：参考\n内容：\n- 代码：完整代码直接看本文第5节 或 [Github](https://github.com/Zeyi-Lin/UNet-Medical)\n- 实验日志过程：[Unet-Medical-Segmentation - SwanLab](https://swanlab.cn/@ZeyiLin/Unet-Medical-Segmentation/runs/67konj7kdqhnfdmusy2u6/chart)\n- 模型：UNet（Pytorch代码直接写）\n- 数据集：[brain-tumor-image-dataset-semantic-segmentation - Kagggle](https://www.kaggle.com/datasets/pkdarabi/brain-tumor-image-dataset-semantic-segmentation)\n- SwanLab：[https://swanlab.cn](https://swanlab.cn)\n- 论文：[《U-Net: Convolutional Networks for Biomedical Image Segmentation》](https://arxiv.org/abs/1505.04597)",
    "417": "一级标题：Yolo目标检测\n二级标题：无\n内容：\n:::info\n目标检测、计算机视觉\n:::\n\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/ultratest/runs/yux7vclmsmmsar9ear7u5/chart)\n\n[在线Demo](https://swanlab.cn/@ZeyiLin/ultratest/runs/yux7vclmsmmsar9ear7u5/chart) | [YOLO猫狗检测教程](https://zhuanlan.zhihu.com/p/702525559)",
    "418": "一级标题：Yolo目标检测\n二级标题：概述\n内容：\nYOLO（You Only Look Once）是一种由Joseph Redmon等人提出的目标检测模型，广泛应用于各种计算机视觉任务。YOLO通过将图像分成网格，并在每个网格内预测边界框和类别概率，能够实现实时的目标检测，在许多任务上表现出色。\n\n在这个任务中，我们将使用YOLO模型在COCO128数据集上进行目标检测任务，同时用SwanLab进行监控和可视化。\n\n![yolo](/assets/example-yolo-1.png)\n\nCOCO128 数据集是一个小型的目标检测数据集，来源于广泛使用的 COCO（Common Objects in Context）数据集。COCO128 数据集包含 128 张图像，是 COCO 数据集的一个子集，主要用于快速测试和调试目标检测模型。",
    "419": "一级标题：Yolo目标检测\n二级标题：环境安装\n内容：\n本案例基于`Python>=3.8`，请在您的计算机上安装好Python。 环境依赖：\n\n```txt\nultralytics\nswanlab\n```\n\n快速安装命令：\n\n```bash\npip install ultralytics swanlab\n```\n\n> 本文的代码测试于ultralytics==8.2.18、swanlab==0.3.6",
    "420": "一级标题：Yolo目标检测\n二级标题：完整代码\n内容：\n```python\nfrom ultralytics import YOLO\nfrom swanlab.integration.ultralytics import add_swanlab_callback\n\ndef main():\n    model = YOLO(\"yolov8n.pt\")\n    add_swanlab_callback(model)\n    model.train(data=\"coco128.yaml\", epochs=5, imgsz=640, batch=64)\n\nif __name__ == \"__main__\":\n    main()\n```",
    "421": "一级标题：Yolo目标检测\n二级标题：演示效果\n内容：\n![yolo-2](/assets/example-yolo-2.png)\n\n![yolo-3](/assets/example-yolo-3.png)",
    "422": "一级标题：为 SwanLab 作出贡献\n二级标题：无\n内容：\n有兴趣为 SwanLab 做出贡献吗？我们欢迎社区的贡献！本指南讨论`swanlab`的开发工作流和内部结构。",
    "423": "一级标题：为 SwanLab 作出贡献\n二级标题：📦 目录\n内容：\n- [标准开发流程](#标准开发流程)\n- [本地调试](#本地调试)\n  - [IDE 与插件](#IDE与插件)\n  - [配置 Python 环境](#配置python环境)\n  - [调试脚本](#调试脚本)\n- [本地测试](#本地测试)\n  - [python 脚本调试](#python-脚本调试)\n  - [单元测试](#单元测试)",
    "424": "一级标题：为 SwanLab 作出贡献\n二级标题：标准开发流程\n内容：\n1. 浏览 GitHub 上的[Issues](https://github.com/SwanHubX/SwanLab/issues)，查看你愿意添加的功能或修复的错误，以及它们是否已被\n   Pull Request。\n\n    - 如果没有，请创建一个[新 Issue](https://github.com/SwanHubX/SwanLab/issues/new/choose)——这将帮助项目跟踪功能请求和错误报告，并确保不重复工作。\n\n2. 如果你是第一次为开源项目贡献代码，请转到 [本项目首页](https://github.com/SwanHubX/SwanLab) 并单击右上角的\"Fork\"\n   按钮。这将创建你用于开发的仓库的个人副本。\n\n    - 将 Fork 的项目克隆到你的计算机，并添加指向`swanlab`项目的远程链接：\n\n   ```bash\n   git clone https://github.com/<your-username>/swanlab.git\n   cd swanlab\n   git remote add upstream https://github.com/swanhubx/swanlab.git\n   ```\n\n3. 开发你的贡献\n\n    - 确保您的 Fork 与主存储库同步：\n\n   ```bash\n   git checkout main\n   git pull upstream main\n   ```\n\n    - 创建一个`git`分支，您将在其中发展您的贡献。为分支使用合理的名称，例如：\n\n   ```bash\n   git checkout -b <username>/<short-dash-seperated-feature-description>\n   ```\n\n    - 当你取得进展时，在本地提交你的改动，例如：\n\n   ```bash\n   git add changed-file.py tests/test-changed-file.py\n   git commit -m \"feat(integrations): Add integration with the `awesomepyml` library\"\n   ```\n\n4. 发起贡献：\n\n    - [Github Pull Request](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests)\n    - 当您的贡献准备就绪后，将您的分支推送到 GitHub：\n\n   ```bash\n   git push origin <username>/<short-dash-seperated-feature-description>\n   ```\n\n    - 分支上传后， `GitHub`将打印一个 URL，用于将您的贡献作为拉取请求提交。在浏览器中打开该 URL，为您的拉取请求编写信息丰富的标题和详细描述，然后提交。\n\n    - 请将相关 Issue（现有 Issue 或您创建的 Issue）链接到您的 PR。请参阅 PR 页面的右栏。或者，在 PR\n      描述中提及“修复问题链接” - GitHub 将自动进行链接。\n\n    - 我们将审查您的贡献并提供反馈。要合并审阅者建议的更改，请将编辑提交到您的分支，然后再次推送到分支（无需重新创建拉取请求，它将自动跟踪对分支的修改），例如：\n\n   ```bash\n   git add tests/test-changed-file.py\n   git commit -m \"test(sdk): Add a test case to address reviewer feedback\"\n   git push origin <username>/<short-dash-seperated-feature-description>\n   ```\n\n    - 一旦您的拉取请求被审阅者批准，它将被合并到存储库的主分支中。",
    "425": "一级标题：为 SwanLab 作出贡献\n二级标题：本地调试\n内容：\n### IDE 与插件\n\n1. **使用 VSCode 作为你的开发 IDE**\n\n   SwanLab 仓库已经配好了[VSCode](https://code.visualstudio.com/)的环境、插件与调试脚本（位于`.vscode`\n   文件夹中），使用 VSCode 开发 SwanLab 会有最好的体验。\n\n2. **安装 VSCode 插件（可选）**\n\n   用 VSCode 打开项目，进入 [扩展] ，在搜索框输入“@recommended”，会出现一系列推荐插件，推荐全部安装这些插件。\n\n   ![vscode-recommend](/assets/guide_cloud/community/contributing-code/vscode_recommend.png)\n\n### 配置 Python 环境\n\nSwanLab 项目环境需要`python>=3.8`的支持。\n\n必须性的 python 依赖集中记录在项目根目录下的 `requirements.txt`。\n\n同样在项目根目录启动终端，运行以下命令安装依赖：\n\n```Bash\n# swanlab所依赖的包\npip install -r requirements.txt\npip install -r requirements-media.txt\n```\n\n编译、开发、单元测试等工作需要使用以下命令额外安装依赖：\n\n```Bash\n# 编译、单元测试等功能需要使用的包\npip install -r requirements-dev.txt\n```\n\n### 调试脚本\n\n1. **VSCode 调试脚本**\n\n在 VSCode-运行和调试 中，项目配置好了一系列调试脚本：\n\n![img](/assets/guide_cloud/community/contributing-code/debug.png)\n\n- **开启一个实验**：运行`test/create_experiment.py`脚本\n\n- **运行当前文件**：使用配置好的 Python 环境运行你选中的文件\n\n- **测试当前文件**：使用 debug 模式测试你选中的文件\n\n- **进行所有单元测试**：运行`test/unit`中的脚本对 swanlab 基础功能进行完整单元测试\n\n- **(跳过云)进行所有单元测试**：运行`test/unit`中的脚本对 swanlab 基础功能进行完整单元测试，但是跳过云测试\n\n- **构建项目**：打包项目为 whl 文件（pip 安装包格式）\n\nPs: 如果你不想使用 VSCode 进行开发，可以前往`.vscode/launch.json`，查看每个调试项对应的命令，了解其配置。",
    "426": "一级标题：为 SwanLab 作出贡献\n二级标题：本地测试\n内容：\n进行测试的前提是你已经安装完毕所有的所需依赖。\n\n### python 脚本调试\n\n在完成你的改动后，可以将你用于测试的 python 脚本放到根目录或`test`文件夹下，然后通过[VSCode 脚本](#调试脚本)中的\"\n运行当前文件\"来运行你的 Python 测试脚本, 这样你的脚本运行将使用到已改动后的 swanlab。\n\n### 单元测试\n\n可以通过[VSCode 脚本](#调试脚本)或者在项目根目录下运行以下命令进行单元测试：\n\n```Bash\nexport PYTHONPATH=. && pytest test/unit\n```\n\n由于 swanlab 涉及与云端的交互，而云端部分是闭源的，所以如果你是第一次贡献代码，最简单的方式是只进行本地测试。\n针对这种情况，请在本地根目录下创建`.env`文件，并填写如下环境变量配置：\n\n```dotenv\nSWANLAB_RUNTIME=test-no-cloud\n```\n\n这样就可以跳过云端测试，只进行本地的部分功能测试。 如果想进行完整的测试，请在`.env`中补充如下信息：\n\n```dotenv\nSWANLAB_RUNTIME=test\nSWANLAB_API_KEY=<你的API KEY>\nSWANLAB_API_HOST=https://swanlab.cn/api\nSWANLAB_WEB_HOST=https://swanlab.cn\n```\n\n*注意：在进行云端版测试时会在您的云端账号下生成一些无用的测试实验数据，需要手动删除*\n\n配置完后即可运行完整测试",
    "427": "一级标题：为SwanLab官方文档做贡献\n二级标题：无\n内容：\n为项目贡献的方式不仅仅是贡献代码，包括维护文档、在issue和群中答疑、提交bug等都是为swanlab项目贡献的方式！\n\n我们在[github仓库](https://github.com/SwanHubX/SwanLab-Docs)中托管了SwanLab的[官方文档](https://docs.swanlab.cn)，基于[vitepress](https://vitepress.dev/zh/guide/getting-started)。\n\n### 如何为文档做贡献\n\n很简单！只需要克隆项目、增添或修改Markdown文件、提交他们，再创建一个PR就可以。\n\n### 环境安装\n\n1. 克隆本仓库\n\n```bash\ngit clone https://github.com/SwanHubX/SwanLab-Docs\n```\n\n2. 安装依赖环境\n\n需要提前安装nodejs和npm，详细方法请查询[node官方教程](https://nodejs.org/en/download/package-manager)\n\n使用如下命令安装其他依赖项目\n\n```bash\nnpm add -D vitepress\n```\n\n### 本地运行文档\n\n如果进行本地开发或者预览文档，可在项目根目录运行：\n\n```bash\nnpm run docs:dev\n```\n\n如果要进行完整的编译打包，使用如下命令：\n\n```bash\nnpm run docs:build\nnpm run docs:preview\n```",
    "428": "一级标题：关于我们\n二级标题：无\n内容：\n情感机器（北京）科技有限公司 是一家专注于人工智能和机器学习底层工具研发的高科技企业。公司致力于为AI开发者提供基础开发工具和建立开源开放的技术社区。\n\n![](/assets/emotion-machine.png)\n\n使命：打造AI工具链，赋能全球AI开发者生态。\n\n---\n\n**公司**：情感机器（北京）科技有限公司\n**工作地点**：北京市朝阳区关庄路2号院1号楼中关村科技服务大厦2层B205-1室\n**联系我们**：contact@swanlab.cn\n**交流群**：[微信](/guide_cloud/community/online-support.md)",
    "429": "一级标题：Github README徽章\n二级标题：无\n内容：\n如果你喜欢在工作与学习中使用 SwanLab，欢迎将 SwanLab 徽章添加到您的README中：",
    "430": "一级标题：Github README徽章\n二级标题：徽章\n内容：\n[![](https://raw.githubusercontent.com/SwanHubX/assets/main/badge2.svg)](https://swanlab.cn)\n\n[![](https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg)](https://swanlab.cn)\n\n\n复制下面的代码到您的README.md文件中（二选一）：\n\n```markdown\n[![](https://raw.githubusercontent.com/SwanHubX/assets/main/badge2.svg)](your experiment url)\n[![](https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg)](your experiment url)\n```",
    "431": "一级标题：Github README徽章\n二级标题：更多设计素材\n内容：\n- [Iconfont](https://www.iconfont.cn/search/index?searchType=icon&q=swanlab)\n- [Github](https://github.com/SwanHubX/assets)",
    "432": "一级标题：在线支持\n二级标题：无\n内容：",
    "433": "一级标题：在线支持\n二级标题：👋 欢迎与我们交流\n内容：\n| 微信公众号 | 微信交流群 |\n| --- | ---  |\n| <div align=\"center\"><img src=\"/assets/wechat_public_account.jpg\" width=300></div> | <div align=\"center\"><img src=\"/assets/wechat-QR-Code.png\" width=300></div> |\n\n| 飞书群 |\n| --- |\n| <div align=\"center\"><img src=\"/assets/feishu-QR-Code.png\" width=300></div> |",
    "434": "一级标题：在线支持\n二级标题：📧 用Github或邮件联系我们\n内容：\n- **GitHub Issues**：[链接](https://github.com/SwanHubX/SwanLab/issues)，反馈使用SwanLab时遇到的错误和问题\n- **电子邮件支持**：<contact@swanlab.cn>",
    "435": "一级标题：在论文中引用SwanLab\n二级标题：无\n内容：\n如果您发现 SwanLab 对您的研究之旅有帮助，请考虑以下列格式引用：\n\n```bibtex\n@software{Zeyilin_SwanLab_2023,\n  author = {Zeyi Lin, Shaohong Chen, Kang Li, Qiushan Jiang, Zirui Cai,  Kaifang Ji and {The SwanLab team}},\n  doi = {10.5281/zenodo.11100550},\n  license = {Apache-2.0},\n  title = {{SwanLab}},\n  url = {https://github.com/swanhubx/swanlab},\n  year = {2023}\n}\n```",
    "436": "一级标题：FAQ\n二级标题：无\n内容：",
    "437": "一级标题：FAQ\n二级标题：登录时，API Key为什么输入不进去？\n内容：\n见此回答：[链接](https://www.zhihu.com/question/720308649/answer/25076837539)",
    "438": "一级标题：FAQ\n二级标题：如何从一个脚本启动多个实验？\n内容：\n在多次创建实验之间增加`swanlab.finish()`即可。\n\n执行了`swanlab.finish()`之后，再次执行`swanlab.init()`就会创建新的实验；\n如果不执行`swanlab.finish()`的情况下，再次执行`swanlab.init()`，将无视此次执行。",
    "439": "一级标题：FAQ\n二级标题：如何将数据上传到私有化部署的SwanLab?\n内容：\n有两种方法可以做到这一点：\n\n::: code-group\n\n```python [方法一]\nswanlab.login(api_key='你的API Key', host='你的私有化部署IP地址')\n```\n\n```bash [方法二]\nswanlab login --host 你的私有化部署IP地址 --api-key 你的API Key\n```\n\n完成登录后，就可以将数据指定上传到私有化部署的SwanLab了。\n\n:::",
    "440": "一级标题：FAQ\n二级标题：如何在训练时关闭swanlab记录（Debug调试）？\n内容：\n将`swanlab.init`的`mode`参数设置为disabled，就可以不创建实验以及不写入数据。\n\n```python\nswanlab.init(mode='disabled')\n```",
    "441": "一级标题：FAQ\n二级标题：在同一台机器上，有多个人都在使用SwanLab，应该如何配置？\n内容：\n`swanlab.login`登录完成之后，会在该机器上生成一个配置文件记录登录信息，以便下次不用重复登录。但如果有多人使用这一台机器的话，则需要小心日志传递到对方账号上。\n\n**推荐的配置方式有两种：**\n\n**方式一(推荐)**：在代码开头加上`swanlab.login(api_key='你的API Key')`，这样不会将登录配置文件写入到本地，[文档](/api/py-login)\n\n**方式二**：在运行代码前，设置环境变量`SWANLAB_API_KEY=\"你的API Key\"`",
    "442": "一级标题：FAQ\n二级标题：本地的训练已经结束，但SwanLab UI上仍然在运行中，要怎么改变状态？\n内容：\n点击实验名旁边的终止按钮，会将实验状态从“进行中”转为“中断”，并停止接收数据的上传。\n\n![stop](/assets/stop.png)",
    "443": "一级标题：FAQ\n二级标题：如何查看折线图的局部细节？\n内容：\n放大折线图，长按鼠标划过目标的区域，即可放大查看该区域。\n\n![details](/assets/faq-chart-details.png)",
    "444": "一级标题：FAQ\n二级标题：内部指标名\n内容：\n指标名称是指`swanlab.log()`传入字典的key部分。有一部分key在内部被SwanLab用于传递系统硬件指标，所以不太建议使用。\n\n内部指标包括：\n\n- `__swanlab__.xxx`",
    "445": "一级标题：FAQ\n二级标题：实验状态规则\n内容：\n实验一共分为三种状态：完成、运行中与中断。\n\n- **完成**：训练进程自然结束，或手动执行了`swanlab.finish()`。\n- **运行中**：训练进程正在运行，且没有执行`swanlab.finish()`。\n- **中断**：训练进程因为Bug、机器关闭、`Ctrl+C`等异常中断。\n\n有些用户会遇到这样的情况：为什么我的训练进程好像还在进行中，但是SwanLab图表上显示中断？\n\n这是因为SwanLab判定中断有一条隐藏规则，如果训练进程在15分钟以内没有任何日志上传（包含自动收集的系统指标），则判定为中断，这是为了避免训练进程被意外Kill后，无法触达SwanLab SDK中的状态上传逻辑，导致实验永远处于“运行中”状态。\n\n所以如果你的机器出现了网络问题，且时间大于15分钟，就会导致实验状态显示为“中断”。",
    "446": "一级标题：FAQ\n二级标题：命令行记录与截断\n内容：\nSwanLab会记录`swanlab.init()`之后进程中的标准输出流，可以在实验的「日志」选项卡查看。如果一行的命令行输出过长，会被截断，目前的默认限制是`1024`个字符，最大限制是`4096`个字符。\n\n如果你想修改限制，可以使用下面的代码进行修改：\n\n```python\nimport swanlab\n\n# 创建新的设置对象，修改max_log_length参数\nnew_settings = swanlab.Settings(\n    max_log_length=4096,\n)\n\n# 更新全局设置\nswanlab.merge_settings(new_settings)\n\nswanlab.init()\n...\n```",
    "447": "一级标题：FAQ\n二级标题：如何开启实验平滑\n内容：\n找到实验页面的右上角，点击「设置」按钮：\n\n![](./faq/smooth_setting.png)\n\n在右侧拉出的菜单中，找到「平滑」选项，拉动滑动条即可开启平滑：\n\n![](./faq/smooth_button.png)",
    "448": "一级标题：FAQ\n二级标题：如何修改实验“中断”状态\n内容：\n在实验页面，点击状态标签：\n\n![](./faq/exp_header_crash.png)\n\n在弹窗中，选择你想要的状态：\n\n![](./faq/exp_windows_finish.png)",
    "449": "一级标题：FAQ\n二级标题：如何开启断点续训？\n内容：\n参考文档：[resume](/guide_cloud/experiment_track/resume-experiment.md)",
    "450": "一级标题：FAQ\n二级标题：如何关闭系统硬件监控？\n内容：\n```python\nswanlab.init(\n    settings=swanlab.Settings(\n        hardware_monitor=False,\n    )\n)\n```",
    "451": "一级标题：添加项目协作者\n二级标题：无\n内容：\nSwanLab支持添加项目协作者，方便团队成员之间协作。\n\n> 每个项目可添加最多10名协作者",
    "452": "一级标题：添加项目协作者\n二级标题：添加协作者\n内容：\n**添加流程如下：**\n\n1. 在项目的「概览」页，右上角有一个「分享」按钮\n2. 点击按钮后，可以通过用户名搜索你的团队成员\n3. 选择成员，设置权限后点击黑色的「+」按钮，即可完成协作者的添加\n\n![](./add-collaborator/shared.gif)",
    "453": "一级标题：添加项目协作者\n二级标题：协作者权限\n内容：\n| 权限 | 可阅读 | 可开发 | 可管理 |\n| --- | --- | --- | --- |\n| 查看实验 | ✅ | ✅ | ✅ |\n| 查看概览 | ✅ | ✅ | ✅ |\n| 查看图表对比视图 | ✅ | ✅ | ✅ |\n| 创建新实验 | ❌ | ✅ | ✅ |\n| 编辑实验信息 | ❌ | ✅ | ✅ |\n| 删除实验 | ❌ | ✅ | ✅ |\n| 修改项目信息 | ❌ | ❌ | ✅ |\n| 删除项目 | ❌ | ❌ | ✅ |\n| 设置协作者 | ❌ | ❌ | ✅ |",
    "454": "一级标题：添加项目协作者\n二级标题：协作者在他人项目创建实验\n内容：\n假如你是项目`OpenVLA`的协作者（有可开发或可管理权限），该项目归属于用户`A`（username为`A`），你希望将实验创建到`OpenVLA`项目中，那么可以：\n\n```python\nswanlab.init(\n    project=\"OpenVLA\",\n    workspace=\"A\",\n)\n```",
    "455": "一级标题：添加项目协作者\n二级标题：移除协作者\n内容：\n在「分享」面板中，点击最下方的协作者设置，在里面点击「移除」按钮，在权限选择菜单中选择「移除」，即可移除该协作者。\n\n![](./add-collaborator/remove.gif)",
    "456": "一级标题：多人共用服务器避免密钥冲突\n二级标题：无\n内容：\n**使用场景：**\n\n1. **实验室**：多个训练者共同使用一台服务器做训练时，需要避免SwanLab API Key造成冲突，导致实验数据误传到其他人的账号上\n2. **公共服务器**：在公共服务器上，避免SwanLab API Key泄露造成数据安全问题",
    "457": "一级标题：多人共用服务器避免密钥冲突\n二级标题：临时登录\n内容：\n在你的Python代码中，加上下面这一行：\n\n```python\nswanlab.login(api_key=\"<your_api_key>\")\n```\n\n使用`swanlab.login()`进行登录，不会将登录信息写入到本地，这样一方面能保证实验一定是上传到你的账号下，另一方面其他人也不能上传到你的账号。",
    "458": "一级标题：多人共用服务器避免密钥冲突\n二级标题：退出登录\n内容：\n使用`swanlab logout`会清空本地存储的登录信息，建议在离开公共服务器前执行。\n\n```bash\nswanlab logout\n```",
    "459": "一级标题：记录混淆矩阵\n二级标题：无\n内容：\n绘制混淆矩阵（Confusion Matrix），用于评估分类模型的性能。混淆矩阵展示了模型预测结果与真实标签之间的对应关系，能够直观地显示各类别的预测准确性和错误类型。\n\n混淆矩阵是评估分类模型性能的基础工具，特别适用于多分类问题。\n\n你可以使用`swanlab.confusion_matrix`来记录混淆矩阵。\n\n[![](https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg)](https://swanlab.cn/@ZeyiLin/ComputeMetrics/runs/gvivixdwka8lyutdxt865/chart#NHFwdTEx-Uzk3bUJKMVY=)\n\n![](./py-confusion_martix/demo.png)\n\n### 基本用法\n\n```python\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nimport swanlab\n\n# 加载鸢尾花数据集\niris_data = load_iris()\nX = iris_data.data\ny = iris_data.target\nclass_names = iris_data.target_names.tolist()\n\n# 划分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 训练模型\nmodel = xgb.XGBClassifier(objective='multi:softmax', num_class=len(class_names))\nmodel.fit(X_train, y_train)\n\n# 获取预测结果\ny_pred = model.predict(X_test)\n\n# 初始化SwanLab\nswanlab.init(project=\"Confusion-Matrix-Demo\", experiment_name=\"Confusion-Matrix-Example\")\n\n# 记录混淆矩阵\nswanlab.log({\n    \"confusion_matrix\": swanlab.confusion_matrix(y_test, y_pred, class_names)\n})\n\nswanlab.finish()\n```\n\n### 使用自定义类别名称\n\n```python\n# 定义自定义类别名称\ncustom_class_names = [\"类别A\", \"类别B\", \"类别C\"]\n\n# 记录混淆矩阵\nconfusion_matrix = swanlab.confusion_matrix(y_test, y_pred, custom_class_names)\nswanlab.log({\"confusion_matrix_custom\": confusion_matrix})\n```\n\n### 不使用类别名称\n\n```python\n# 不指定类别名称，将使用数字索引\nconfusion_matrix = swanlab.confusion_matrix(y_test, y_pred)\nswanlab.log({\"confusion_matrix_default\": confusion_matrix})\n```\n\n\n### 二分类示例\n\n```python\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nimport swanlab\n\n# 生成二分类数据\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# 训练模型\nmodel = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\nmodel.fit(X_train, y_train)\n\n# 获取预测结果\ny_pred = model.predict(X_test)\n\n# 记录混淆矩阵\nswanlab.log({\n    \"confusion_matrix\": swanlab.confusion_matrix(y_test, y_pred, [\"负类\", \"正类\"])\n})\n```\n\n### 注意事项\n\n1. **数据格式**: `y_true`和`y_pred`可以是列表或numpy数组\n2. **多分类支持**: 此函数支持二分类和多分类问题\n3. **类别名称**: `class_names`的长度应该与类别数量一致\n4. **依赖包**: 需要安装`scikit-learn`和`pyecharts`包\n5. **坐标轴**: sklearn的confusion_matrix左上角为(0,0)，在pyecharts的heatmap中是左下角，函数会自动处理坐标转换\n6. **矩阵解读**: 混淆矩阵中，行表示真实标签，列表示预测标签",
    "460": "一级标题：记录PR曲线\n二级标题：无\n内容：\nPR曲线展示了在不同阈值下精确率（Precision）和召回率（Recall）的关系。绘制PR（Precision-Recall）曲线，在评估二分类模型的性能时很有用。\n\nPR曲线也特别适用于处理不平衡数据集，能够更好地评估模型在少数类上的表现。\n\n你可以使用`swanlab.pr_curve`来记录PR曲线。\n\n[![](https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg)](https://swanlab.cn/@ZeyiLin/ComputeMetrics/runs/35snhyn3wndz58r4j8d4h/chart#ZTIwZm1s-aVI2S1ZCQl8=)\n\n![](./py-pr_curve/demo.png)\n\n### 基本用法\n\n```python {22}\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nimport swanlab\n\n# 生成示例数据\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# 训练模型\nmodel = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\nmodel.fit(X_train, y_train)\n\n# 获取预测概率\ny_pred_proba = model.predict_proba(X_test)[:, 1]\n\n# 初始化SwanLab\nswanlab.init(project=\"PR-Curve-Demo\", experiment_name=\"PR-Curve-Example\")\n\n# 记录PR曲线\nswanlab.log({\n    \"pr_curve\": swanlab.pr_curve(y_test, y_pred_proba, title=True)\n})\n\nswanlab.finish()\n```\n\n### 自定义标题\n\n```python\n# 不显示标题(默认)\npr_curve = swanlab.pr_curve(y_test, y_pred_proba, title=False)\nswanlab.log({\"pr_curve_no_title\": pr_curve})\n\n# 显示标题\npr_curve = swanlab.pr_curve(y_test, y_pred_proba, title=True)\nswanlab.log({\"pr_curve_with_title\": pr_curve})\n\n# 自定义标题\npr_curve = swanlab.pr_curve(y_test, y_pred_proba, title=\"demo\")\nswanlab.log({\"pr_curve_with_custom_title\": pr_curve})\n```\n\n### 注意事项\n\n1. **数据格式**: `y_true`和`y_pred_proba`可以是列表或numpy数组\n2. **二分类**: 此函数专用于二分类问题\n3. **概率值**: `y_pred_proba`应该是模型对正类的预测概率，范围在0-1之间\n4. **依赖包**: 需要安装`scikit-learn`和`pyecharts`包\n5. **AUC计算**: 函数会自动计算PR曲线下的面积（AUC），但不会默认在标题中显示",
    "461": "一级标题：记录ROC曲线\n二级标题：无\n内容：\n绘制ROC（Receiver Operating Characteristic）曲线，用于评估二分类模型的性能。ROC曲线展示了在不同阈值下真正率（True Positive Rate）和假正率（False Positive Rate）的关系。\n\nROC曲线是评估分类模型性能的重要工具，能够直观地展示模型在不同决策阈值下的表现。\n\n你可以使用`swanlab.roc_curve`来记录ROC曲线。\n\n[![](https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg)](https://swanlab.cn/@ZeyiLin/ComputeMetrics/runs/gvivixdwka8lyutdxt865/chart#NHFwdTEx-Uzk3bUJKMVY=)\n\n![](./pr-roc_curve/demo.png)\n\n### 基本用法\n\n```python\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nimport swanlab\n\n# 生成示例数据\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# 训练模型\nmodel = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\nmodel.fit(X_train, y_train)\n\n# 获取预测概率\ny_pred_proba = model.predict_proba(X_test)[:, 1]\n\n# 初始化SwanLab\nswanlab.init(project=\"ROC-Curve-Demo\", experiment_name=\"ROC-Curve-Example\")\n\n# 记录ROC曲线\nswanlab.log({\n    \"roc_curve\": swanlab.roc_curve(y_test, y_pred_proba, title=True)\n})\n\nswanlab.finish()\n```\n\n### 自定义标题\n\n```python\n# 不显示标题(默认)\nroc_curve = swanlab.roc_curve(y_test, y_pred_proba, title=False)\nswanlab.log({\"roc_curve_no_title\": roc_curve})\n\n# 显示标题\nroc_curve = swanlab.roc_curve(y_test, y_pred_proba, title=True)\nswanlab.log({\"roc_curve_with_title\": roc_curve})\n\n# 自定义标题\nroc_curve = swanlab.roc_curve(y_test, y_pred_proba, title=\"demo\")\nswanlab.log({\"roc_curve_with_custom_title\": roc_curve})\n```\n\n### 与其他指标一起使用\n\n```python\nimport swanlab\n\n# 记录多个ML指标\nswanlab.log({\n    \"roc_curve\": swanlab.roc_curve(y_test, y_pred_proba),\n    \"pr_curve\": swanlab.pr_curve(y_test, y_pred_proba),\n    \"accuracy\": accuracy_score(y_test, y_pred),\n    \"f1_score\": f1_score(y_test, y_pred)\n})\n```\n\n### 注意事项\n\n1. **数据格式**: `y_true`和`y_pred_proba`可以是列表或numpy数组\n2. **二分类**: 此函数专用于二分类问题\n3. **概率值**: `y_pred_proba`应该是模型对正类的预测概率，范围在0-1之间\n4. **依赖包**: 需要安装`scikit-learn`和`pyecharts`包\n5. **AUC计算**: 函数会自动计算ROC曲线下的面积（AUC），但不会在标题中显示\n6. **曲线特征**: ROC曲线从(0,0)开始，到(1,1)结束，对角线表示随机分类器的性能",
    "462": "一级标题：用配置文件创建实验\n二级标题：无\n内容：\n本节将介绍如何使用json、yaml格式的配置文件来创建SwanLab实验。",
    "463": "一级标题：用配置文件创建实验\n二级标题：swanlab.config载入配置文件\n内容：\n`swanlab.init`的`config`参数支持传入json或yaml格式的配置文件路径，并将配置文件解析为字典以进行实验创建。\n\n### 使用json文件\n\n下面是一个json格式的配置文件示例：\n\n```json\n{\n    \"epochs\": 20,\n    \"learning-rate\": 0.001,\n}\n```\n\n将配置文件的路径传入config参数，它会把配置文件解析为字典：\n\n```python\nswanlab.init(config=\"swanlab-init-config.json\")\n# 等价于swanlab.init(config={\"epochs\": 20, \"learning-rate\": 0.001})\n```\n\n### 使用yaml文件\n\n下面是一个yaml格式的配置文件示例：\n\n```yaml\nepochs: 20\nlearning-rate: 0.001\n```\n\n将配置文件的路径传入`config`参数，它会把配置文件解析为字典：\n```python\nswanlab.init(config=\"swanlab-init-config.yaml\")\n# 等价于swanlab.init(config={\"epochs\": 20, \"learning-rate\": 0.001})\n```",
    "464": "一级标题：用配置文件创建实验\n二级标题：swanlab.init载入配置文件\n内容：\n`swanlab.init`的`load`参数支持传入json或yaml格式的配置文件路径，并解析配置文件以进行实验创建。\n\n### 使用json文件\n\n下面是一个json格式的配置文件示例：\n\n```json\n{\n    \"project\": \"cat-dog-classification\",\n    \"experiment_name\": \"Resnet50\",\n    \"description\": \"我的第一个人工智能实验\",\n    \"config\":{\n        \"epochs\": 20,\n        \"learning-rate\": 0.001}\n}\n```\n\n将配置文件的路径传入`load`参数，它会解析配置文件以初始化实验：\n\n```python\nswanlab.init(load=\"swanlab-config.json\")\n# 等价于\n# swanlab.init(\n#     project=\"cat-dog-classification\",\n#     experiment_name=\"Resnet50\",\n#     description=\"我的第一个人工智能实验\",\n#     config={\n#         \"epochs\": 20,\n#         \"learning-rate\": 0.001}\n# )\n```\n\n### 使用yaml文件\n\n下面是一个json格式的配置文件示例：\n\n```yaml\nproject: cat-dog-classification\nexperiment_name: Resnet50\ndescription: 我的第一个人工智能实验\nconfig:\n  epochs: 20\n  learning-rate: 0.001\n```\n\n将配置文件的路径传入`load`参数，它会解析配置文件以初始化实验：\n\n```python\nswanlab.init(load=\"swanlab-config.yaml\")\n# 等价于\n# swanlab.init(\n#     project=\"cat-dog-classification\",\n#     experiment_name=\"Resnet50\",\n#     description=\"我的第一个人工智能实验\",\n#     config={\n#         \"epochs\": 20,\n#         \"learning-rate\": 0.001}\n# )\n```",
    "465": "一级标题：用配置文件创建实验\n二级标题：常见问题\n内容：\n### 1. 配置文件命名是固定的吗？\n\n配置文件的命名是自由的，但推荐使用`swanlab-init`和`swanlab-init-config`这两个配置名。\n\n### 2. 配置文件和脚本内的参数之间是什么关系？\n\n脚本内参数的优先级大于配置文件，即脚本内参数会覆盖配置文件参数。\n\n比如，下面有一段yaml配置文件和示例代码片段：\n\n```yaml\nproject: cat-dog-classification\nexperiment_name: Resnet50\ndescription: 我的第一个人工智能实验\nconfig:\n  epochs: 20\n  learning-rate: 0.001\n```\n\n```python\nswanlab.init(\n    experiment_name=\"resnet101\"，\n    config={\"epochs\": 30},\n    load=\"swanlab-init.yaml\"\n)\n```\n\n最终`experiment_name`为resnet101，`config`为{\"epochs\":30}。",
    "466": "一级标题：创建一个实验\n二级标题：无\n内容：\n使用 **SwanLab Python SDK** 跟踪人工智能实验，然后你可以在 在线交互式仪表板 中查看结果。\n\n![](./create-experiment/overview.jpg)\n\n本节将介绍如何创建一个SwanLab实验。",
    "467": "一级标题：创建一个实验\n二级标题：如何创建一个SwanLab实验?\n内容：\n创建一个SwanLab实验分为3步：\n1. 初始化SwanLab\n2. 传递一个超参数字典\n3. 在你的训练循环中记录指标\n\n### 1. 初始化SwanLab\n\n`swanlab.init()`的作用是初始化一个SwanLab实验，它将启动后台进程以同步和记录数据。\n下面的代码片段展示了如何创建一个名为 **cat-dog-classification** 的新SwanLab项目。并为其添加了：\n\n1. **project**：项目名。\n1. **experiment_name**：实验名。实验名为当前实验的标识，以帮助您识别此实验。\n2. **description**：描述。描述是对实验的详细介绍。\n\n```python\n# 导入SwanLab Python库\nimport swanlab\n\n# 1. 开启一个SwanLab实验\nrun = swanlab.init(\n    project=\"cat-dog-classification\",\n    experiment_name=\"Resnet50\",\n    description=\"我的第一个人工智能实验\",\n)\n```\n\n当你初始化SwanLab时，`swanlab.init()`将返回一个对象。\n此外，SwanLab会创建一个本地目录（默认名称为“swanlog”），所有日志和文件都保存在其中，并异步传输到 SwanLab 服务器。（该目录也可以被`swanlab watch -l [logdir]`命令打开本地实验看板。）\n\n::: info\n**注意**：如果调用 `swanlab.init` 时该项目已存在，则实验会添加到预先存在的项目中。\n例如，如果您已经有一个名为`\"cat-dog-classification\"`的项目，那么新的实验会添加到该项目中。\n:::\n\n<br>\n\n### 2. 传递超参数字典\n\n传递超参数字典，例如学习率或模型类型。\n你在`config`中传入的字典将被保存并用于后续的实验对比与结果查询。\n\n```python\n# 2. 传递一个超参数字典\nswanlab.config={\"epochs\": 20, \"learning_rate\": 1e-4, \"batch_size\": 32, \"model_type\": \"CNN\"}\n```\n\n有关如何配置实验的更多信息，请参阅[设置实验配置](/guide_cloud/experiment_track/set-experiment-config.md)。\n\n<br>\n\n### 3. 在训练循环中记录指标\n在每轮for循环（epoch）中计算准确率与损失值指标，并用`swanlab.log()`将它们记录到SwanLab中。\n在默认情况下，当您调用`swanlab.log`时，它会创建一个新的step添加到对应指标的历史数据中，规则是新的step=旧的最大step数+1。\n下面的代码示例展示了如何用`swanlab.log()`记录指标：\n\n```python\n# 省略了如何设置模型与如何设置数据集的细节\n\n# 设置模型和数据集\nmodel, dataloader = get_model(), get_data()\n\n# 训练循环\nfor epoch in range(swanlab.config.epochs):\n    for batch in dataloader:\n        loss, acc = model.train_step()\n        # 3. 在你的训练循环中记录指标，用于在仪表盘中进行可视化\n        swanlab.log({\"acc\": acc, \"loss\": loss})\n```\n\n<br>\n\n### 完整代码\n\n包含上述代码片段的完整脚本如下：\n\n```python\n# 导入SwanLab Python库\nimport swanlab\n\n# 1. 开启一个SwanLab实验\nrun = swanlab.init(\n    project=\"cat-dog-classification\",\n    experiment_name=\"Resnet50\",\n    description=\"我的第一个人工智能实验\",\n)\n\n# 2. 传递一个超参数字典\nswanlab.config={\"epochs\": 20, \"learning_rate\": 1e-4, \"batch_size\": 32, \"model_type\": \"CNN\"}\n\n# 省略了如何设置模型与如何设置数据集的细节\n# 设置模型和数据集\nmodel, dataloader = get_model(), get_data()\n\n# 训练循环\nfor epoch in range(swanlab.config.epochs):\n    for batch in dataloader:\n        loss, acc = model.train_step()\n        # 3. 在你的训练循环中记录指标，用于在仪表盘中进行可视化\n        swanlab.log({\"acc\": acc, \"loss\": loss})\n```\n\n<br>\n\n### 可视化你的实验\n\n使用SwanLab仪表盘作为管理和可视化人工智能模型结果的一站式节点。\n可以可视化丰富的交互式图表，例如折线图、图像图表、音频图表、3D点云图表等。\n有关如何查看实验更多信息，请参阅[查看实验结果](/guide_cloud/experiment_track/view-result.md)。\n\n![](./create-experiment/show.jpg)",
    "468": "一级标题：创建一个实验\n二级标题：最佳实践\n内容：\n下面介绍一下创建实验时可以参考的写法，一个完整的实验创建可以包含下面这四个参数：\n- `config`：配置。记录你想要用于复现模型的任何内容，比如超参数、模型名称、数据集等。这些内容将显示在仪表盘的“表格视图”与“实验卡片”页中，也可以作为实验比较、筛选、过滤的依据。\n- `project`：项目。项目是一组可以一起比较的实验，它们将在一个统一的仪表盘中显示。\n- `experiment_name`：实验名。定义实验的名称。您在脚本中设置，可以之后在SwanLab应用上编辑。\n- `description`：描述。对实验的介绍文本，记录不同实验之间的差异和灵感。您在脚本中设置，可以之后在SwanLab应用上编辑。\n\n以下代码片段展示了一个最佳实践案例：\n\n```python\nimport swanlab\n\nconfig = dict(\n    learning_rate=1e-4, optimizer=\"Adam\", architecture=\"Transformer\", dataset_id=\"cats-dogs-2024\"\n)\n\nswanlab.init(\n    project=\"cats-dogs-classification\",\n    experiment_name=\"ViT-Adam-1e-4\",\n    description=\"基于ViT模型和1e-4学习率的Adam优化器的猫狗分类实验。\",\n    config=config,\n)\n```\n\n关于创建SwanLab实验时更多可用参数的信息，请参阅API文档中的[swanlab.init](/api/py-init.md)文档。",
    "469": "一级标题：实验元数据\n二级标题：无\n内容：\n> 获取实验元数据需swanlab>=0.3.25\n\n总有些时候，你想要在代码中获取实验的元数据，比如实验的项目名、ID、实验名、网址等。\n\n获取方式：\n\n```python\nimport swanlab\n\nrun = swanlab.init(\n    project=\"test-project\",\n    experiment=\"test-exp\",\n)\n\n# 打印出所有元数据\nprint(run.public.json())\n\n# 打印出单个元数据\nprint(run.public.project_name)\nprint(run.public.cloud.experiment_url)\n```\n\n`swanlab.init`返回的类`run`会携带`public`属性，替换了之前的`settings`属性，他会返回：\n\n- `project_name`：当前运行的项目名称\n- `version`：当前运行的swanlab版本\n- `run_id`：一个唯一id\n- `swanlog_dir`：swanlab保存文件夹\n- `run_dir`：本次实验的保存文件夹\n- `cloud`：云端环境的相关信息\n    - `available`：是否运行在云端模式，如果不是，下面的属性全部为None\n    - `project_name`：本次运行的项目名称\n    - `project_url`：本次运行在云端项目url\n    - `experiment_name`：本次运行的实验名称\n    - `experiment_url`：本次运行的云端实验url",
    "470": "一级标题：结束一个实验\n二级标题：无\n内容：\n在一般的Python运行环境下，当脚本运行结束时，SwanLab会自动调用`swanlab.finish`来关闭实验，并将运行状态设置为「完成」。这一步无需显式调用。\n\n但在一些特殊情况下，比如**Jupyter Notebook**中，则需要用`swanlab.finish`来显式关闭实验。\n\n使用方式也很简单, 在`init`之后执行`finish`即可：\n\n```python (5)\nimport swanlab\n\nswanlab.init()\n...\nswanlab.finish()\n```",
    "471": "一级标题：结束一个实验\n二级标题：FAQ\n内容：\n### 在运行一次Python脚本中，我可以初始化多次实验吗？\n\n可以，但你需要在多次`init`中间加上`finish`，如：\n\n```python\nswanlab.init()\n···\nswanlab.finish()\n···\nswanlab.init()\n```",
    "472": "一级标题：用 Notebook 跟踪实验\n二级标题：无\n内容：\n将 SwanLab 与 Jupyter 结合使用，无需离开Notebook即可获得交互式可视化效果。\n\n![](./jupyter-notebook/swanlab-love-jupyter.jpg)",
    "473": "一级标题：用 Notebook 跟踪实验\n二级标题：在Notebook中安装SwanLab\n内容：\n```bash\n!pip install swanlab -qqq\n```\nps: `-qqq`是用来控制命令执行时的输出信息量的，可选。",
    "474": "一级标题：用 Notebook 跟踪实验\n二级标题：在Notebok中与SwanLab交互\n内容：\n```python\nimport swanlab\n\nswanlab.init()\n...\n# 在Notebook中，需要显式关闭实验\nswanlab.finish()\n```\n\n在用`swanlab.init`初始化实验时，打印信息的最后会出现一个“Display SwanLab Dashboard”按钮：\n\n![](/assets/jupyter-notebook-1.jpg)\n\n点击该按钮，就会在Notebook中嵌入该实验的SwanLab网页：\n\n![](/assets/jupyter-notebook-2.jpg)\n\n现在，你可以在这个嵌入的网页中直接看到训练过程，以及和它交互。",
    "475": "一级标题：限制与性能\n二级标题：无\n内容：",
    "476": "一级标题：限制与性能\n二级标题：优化指标记录\n内容：\n使用 `swanlab.log` 跟踪记录实验指标，记录后，这些指标会生成图表与显示在表格中。当记录的数据量过多时，可能会使网页的访问变慢。\n\n### 建议1：将不同指标的总数保持在1万以下\n\n记录超过10k个不同的指标名，可能会减慢你仪表盘渲染与表格操作速度。\n\n对于媒体数据，尽量将相关的媒体数据记录到相同的指标名称下：\n\n```python\n# ❌ 不推荐的做法\nfor i, img in enumerate(images):\n    swanlab.log({f\"pred_img_{i}\": swanlab.Image(image)})\n\n# ✅ 推荐的做法\nswanlab.log({\"pred_imgs\": [swanlab.Image(image) for image in images]})\n```\n\n<br>\n\n### 建议2：指标宽度保持在1000万以下\n\n指标宽度在以step为横轴的折线图中，指的是step最小值与最大值之间的范围差。\n\n在指标宽度过大时，会影响实验中所有指标的绘图加载时间，导致访问缓慢。\n\n<br>\n\n### 建议3：限制指标的提交频率\n\n选择适合你正在记录的指标的记录频率。在经验上，指标越宽，记录它的频率就越低。\n\n具体来说，我们建议：\n\n- 标量：每个指标 < `50k` 个记录点\n- 媒体数据：每个指标 < `10k` 个记录点\n\n如果你超出这些准则，SwanLab 将继续接受你记录的数据，但页面加载速度可能会很慢。\n\n推荐的记录方法如下代码所示：\n\n```python\n# 比如有1MB次循环\nfor step in range(1000000):\n    ....\n\n    # 每1k次循环提交一次，有效降低指标的提交频次\n    if step % 1000 == 0:\n        swanlab.log({\"scalar\": step})\n```",
    "477": "一级标题：记录自定义3D图表\n二级标题：无\n内容：\nSwanLab 兼容 [pyecharts](https://pyecharts.org/#/zh-cn/intro) 的 API，可以方便地记录 pyecharts 的图表到 SwanLab，以呈现丰富的数据组织和图表展现形式。\n\n**在线Demo点击下面的标签：**\n\n[![](/assets/visualization_swanlab.svg)](https://swanlab.cn/@ZeyiLin/swanlab-echarts-3d-demo/charts)\n\n<!--@include: @zh/shared/custom-charts-3d.md-->",
    "478": "一级标题：记录自定义图表\n二级标题：无\n内容：\n<!--@include: @zh/shared/custom-charts.md-->",
    "479": "一级标题：记录实验指标\n二级标题：无\n内容：\n使用SwanLab Python库记录训练每一步（step）的指标与媒体数据。\n\nSwanLab用 `swanlab.log()` 在训练循环中收集指标名和数据（key-value），然后同步到云端服务器。\n\n![](./log-experiment-metric/line.png)",
    "480": "一级标题：记录实验指标\n二级标题：记录标量指标\n内容：\n在训练循环中，将指标名和数据组成一个键值对字典，传递给 `swanlab.log()` 完成1次指标的记录：\n\n```python\nfor epoch in range(num_epochs):\n    for data, ground_truth in dataloader:\n        predict = model(data)\n        loss = loss_fn(predict, ground_truth)\n        # 记录指标，指标名为loss\n        swanlab.log({\"loss\": loss})\n```\n\n在 `swanlab.log` 记录时，会根据指标名，将`{指标名: 指标}`字典汇总到一个统一位置存储。\n\n⚠️需要注意的是，`swanlab.log({key: value})`中的value必须是`int` / `float` / `BaseType`这三种类型（如果传入的是`str`类型，会先尝试转为`float`，如果转换失败就会报错），其中`BaseType`类型主要是多媒体数据，详情请看[记录多媒体数据](/guide_cloud/experiment_track/log-media.md)。\n\n在每次记录时，会为该次记录赋予一个 `step`。在默认情况下，`step` 为0开始，并在你每一次在同一个指标名下记录时，`step` 等于该指标名历史记录的最大 `step` + 1，例如：\n\n```python\nimport swanlab\nswanlab.init()\n\n...\n\nswanlab.log({\"loss\": loss, \"acc\": acc})\n# 此次记录中，loss的step为0, acc的step为0\n\nswanlab.log({\"loss\": loss, \"iter\": iter})\n# 此次记录中，loss的step为1, iter的step为0, acc的step为0\n\nswanlab.log({\"loss\": loss, \"iter\": iter})\n# 此次记录中，loss的step为2, iter的step为1, acc的step为0\n```",
    "481": "一级标题：记录实验指标\n二级标题：指标分组\n内容：\n在脚本中可以通过指标名的前缀（以“/”为分隔）进行图表分组，例如 `train/loss` 会被分到名为“train”的分组、`val/loss` 会被分到名为“val”的分组：\n\n```python\n# 分到train组\nswanlab.log({\"train/loss\": loss})\nswanlab.log({\"train/batch_cost\": batch_cost})\n\n# 分到val组\nswanlab.log({\"val/acc\": acc})\n```",
    "482": "一级标题：记录实验指标\n二级标题：指定记录的step\n内容：\n在一些指标的记录频率不一致，但希望它们的step可以对齐时，可以通过设置 `swanlab.log` 的 `step` 参数实现对齐：\n\n```python\nfor iter, (data, ground_truth) in enumerate(train_dataloader):\n    predict = model(data)\n    train_loss = loss_fn(predict, ground_truth)\n    swanlab.log({\"train/loss\": loss}, step=iter)\n\n    # 测试部分\n    if iter % 1000 == 0:\n        acc = val_trainer(model)\n        swanlab.log({\"val/acc\": acc}, step=iter)\n```\n\n需要注意的是，同一个指标名不允许出现2个相同的step的数据，一旦出现，SwanLab将保留先记录的数据，抛弃后记录的数据。",
    "483": "一级标题：记录实验指标\n二级标题：打印指标\n内容：\n也许你希望在训练循环中打印指标，可以通过 `print_to_console` 参数控制是否将指标打印到控制台（以`dict`的形式）：\n\n```python\nswanlab.log({\"acc\": acc}, print_to_console=True)\n```\n\n或者：\n\n```python\nprint(swanlab.log({\"acc\": acc}))\n```",
    "484": "一级标题：记录实验指标\n二级标题：自动记录环境信息\n内容：\nSwanLab在实验期间自动记录以下信息：\n\n- **命令行输出**：标准输出流和标准错误流被自动记录，并显示在实验页面的“日志”选项卡中。\n- **实验环境**：记录包括操作系统、硬件配置、Python解释器路径、运行目录、Python库依赖等在内的数十项的环境信息。\n- **训练时间**：记录训练开始时间和总时长。",
    "485": "一级标题：记录媒体数据\n二级标题：无\n内容：\nSwanLab 支持记录媒体数据（图像、音频、文本、三维点云等）以直观地探索你的实验结果，实现模型的主观评估。",
    "486": "一级标题：记录媒体数据\n二级标题：1.图像\n内容：\n`swanlab.Image` 支持记录多种图像类型，包括 numpy、PIL、Tensor、读取文件等。[API文档](/api/py-Image)。\n\n![](/assets/media-image-1.jpg)\n\n### 1.1 记录 Array 型图像\n\nArray型包括numpy和tensor。直接将 Array 传入 `swanlab.Image`，它将根据类型自动做相应处理：\n\n- 如果是 `numpy.ndarray`：SwanLab 会使用 pillow (PIL) 对其进行读取 。\n- 如果是 `tensor`：SwanLab 会使用 `torchvision` 的 `make_grid`函数做转换，然后使用 pillow 对其进行读取。\n\n示例代码：\n\n```python\nimage = swanlab.Image(image_array, caption=\"左图: 输入, 右图: 输出\")\nswanlab.log({\"examples\": image})\n```\n\n### 1.2 记录 PIL 型图像\n\n直接传入 `swanlab.Image`：\n\n```python\nimage = PIL.Image.fromarray(image_array)\nswanlab.log({\"examples\": image})\n```\n\n### 1.3 记录文件图像\n\n提供文件路径给 `swanlab.Image`：\n\n```python\nimage = swanlab.Image(\"myimage.jpg\")\nswanlab.log({\"example\": image})\n```\n\n### 1.4 记录 Matplotlib\n\n将 `matplotlib.pyplot` 的 `plt` 对象传入 `swanlab.Image`：\n\n```python\nimport matplotlib.pyplot as plt\n\n# 数据\nx = [1, 2, 3, 4, 5]\ny = [2, 3, 5, 7, 11]\n# 创建折线图\nplt.plot(x, y)\n# 添加标题和标签\nplt.title(\"Examples\")\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")\n\nswanlab.log({\"example\": swanlab.Image(plt)})\n```\n\n### 1.5 单步记录多个图像\n\n单步记录多个图像即在一次 `swanlab.log` 中，传递一个由 `swanlab.Image` 类型对象组成的列表。\n\n```python\n# 创建一个空列表\nimage_list = []\nfor i in range(3):\n    random_image = np.random.randint(low=0, high=256, size=(100, 100, 3))\n    image = swanlab.Image(random_image, caption=f\"随机图像{i}\")\n    # 将 swanlab.Image 类型对象添加到列表中\n    image_list.append(image)\n\nswanlab.log({\"examples\": image_list})\n```\n\n关于图像的更多细节，可参考[API文档](/api/py-Image)",
    "487": "一级标题：记录媒体数据\n二级标题：2. 音频\n内容：\n[API文档](/api/py-Audio)\n\n![](/assets/media-audio-1.jpg)\n\n### 2.1 记录 Array 型音频\n\n```python\naudio = swanlab.Audio(np_array, sample_rate=44100, caption=\"white_noise\")\nswanlab.log({\"white_noise\": audio})\n```\n\n### 2.2 记录音频文件\n\n```python\nswanlab.log({\"white_noise\": swanlab.Audio(\"white_noise.wav\")})\n```\n\n### 2.3 单步记录多个音频\n\n```python\nexamples = []\nfor i in range(3):\n    white_noise = np.random.randn(100000)\n    audio = swanlab.Audio(white_noise, caption=\"audio_{i}\")\n    # 列表中添加swanlab.Audio类型对象\n    examples.append(audio)\n\nrun.log({\"examples\": examples})\n```",
    "488": "一级标题：记录媒体数据\n二级标题：3. 文本\n内容：\n[API文档](/api/py-Text)\n\n### 3.1 记录字符串\n\n```python\nswanlab.log({\"text\": swanlab.Text(\"A example text.\")})\n```\n\n### 3.2 单步记录多个文本\n\n```python\n# 创建一个空列表\ntext_list = []\nfor i in range(3):\n    text = swanlab.Text(\"A example text.\", caption=f\"{i}\")\n    text_list.append(text)\n\nswanlab.log({\"examples\": text_list})\n```\n\n![alt text](/assets/log-media-text.png)",
    "489": "一级标题：记录媒体数据\n二级标题：4. 3D点云\n内容：\n![](/zh/api/py-object3d/demo.png)\n\n请参考此文档：[API-Oject3D](/api/py-object3d)",
    "490": "一级标题：记录媒体数据\n二级标题：5. 生物化学分子\n内容：\n![](/assets/molecule.gif)\n\n请参考此文档：[API-Molecule](/api/py-molecule)",
    "491": "一级标题：记录媒体数据\n二级标题：6. 视频\n内容：\n请参考此文档：[API-Video](/api/py-video)",
    "492": "一级标题：记录媒体数据\n二级标题：Q&A\n内容：\n### 1. caption参数有什么作用？\n\n每一个媒体类型都会有1个`caption`参数，它的作用是对该媒体数据的文字描述，比如对于图像：\n\n```python\napple_image = swanlab.Image(data, caption=\"苹果\")\nswanlab.log({\"im\": apple_image})\n```\n<img src=\"/assets/log-media-image.png\" width=400, height=400>\n\n\n### 2. 想要媒体数据和epoch数同步，怎么办？\n\n在用swanlab.log记录媒体数据时，指定`step`参数为epoch数即可。\n\n```python\nfor epoch in epochs:\n    ···\n    swanlab.log({\"im\": sw_image}, step=epoch)\n```",
    "493": "一级标题：恢复实验/断点续训\n二级标题：无\n内容：\n> 恢复中断或已完成的SwanLab实验。\n\n断点续训的意思是，如果你之前有一个状态为完成或中断的实验，需要补充一些实验数据，那么你可以通过`resume`和`id`参数来恢复这个实验，实验将重新变成进行中状态。\n\n:::warning 使用场景\n1. **断点续训：** 之前的训练进程断了，基于checkpoint继续训练时，希望实验图表能和之前的swanlab实验续上，而非创建1个新swanlab实验\n2. **补充图表：** 训练和评估分为了两个进程，但希望评估和训练记录在同一个swanlab实验中\n3. **更新超参：** config中有一些参数填写有误，希望更新config参数\n:::",
    "494": "一级标题：恢复实验/断点续训\n二级标题：基本用法\n内容：\n恢复实验主要依赖两个参数，`resume`和`id`：\n\n```python\nswanlab.init(\n    project=\"<project>\",\n    workspace=\"<workspace>\",\n    resume=True,\n    id=\"<exp_id>\",  # id必须为21位字符串\n)\n```\n\n`resume`参数控制了实验恢复的行为，有以下几种选择：\n\n- `must`：如果项目下存在id对应的实验，则会resume该实验，否则将报错\n- `allow`：如果项目下存在id对应的实验，则会resume该实验，否则将创建一个新的实验。\n- `never`：传递 id 参数将会报错；否则会创建一个新的实验。(即不开启resume的效果)\n- `True`：即`allow`\n- `False`：即`never`\n\n`实验id`是实验的唯一标识，可以在实验的「环境」选项卡或URL中找到，必须为1个21位字符串：\n\n![](./resume-experiment/exp_id.png)\n\n或者打开一个实验，在其URL结构中的`<exp_id>`部分就是实验id：\n\n```\nhttps://swanlab.cn/@<username>/<project>/runs/<exp_id>/...\n```",
    "495": "一级标题：恢复实验/断点续训\n二级标题：代码示例\n内容：\n```python\nimport swanlab\n\nrun = swanlab.init(project=\"resume_test\")\nswanlab.log({\"loss\": 2, \"acc\":0.4})\n# 完成实验\nrun.finish()\n\n# 恢复实验\nrun = swanlab.init(project=\"resume_test\", resume=True, id=run.id)\nswanlab.log({\"loss\": 0.2, \"acc\": 0.9})\n```",
    "496": "一级标题：恢复实验/断点续训\n二级标题：技巧：使用环境变量执行resume\n内容：\n如果你使用的是一些框架训练，不太方便修改`swanlab.init`处的源码，那么可以使用环境变量来执行resume：\n\n```bash\nexport SWANLAB_RESUME=must\nexport SWANLAB_RUN_ID=<exp_id>\n```",
    "497": "一级标题：恢复实验/断点续训\n二级标题：技巧：复制一份实验再resume\n内容：\n如果你担心本次resume的内容可能存在Bug等，保险起见可以将1份实验复制成2份，然后再resume其中一份实验：\n\n1. 找到原实验在本地`swanlog`文件夹下对应的`run`目录（通过环境Tab下的「日志目录」可以查到路径）\n2. 使用`swanlab sync`命令将此实验上传到云端：\n```bash\nswanlab sync <run_dir>\n```\n3. resume新上传的实验",
    "498": "一级标题：邮件/第三方通知\n二级标题：无\n内容：\nSwanLab支持通过邮件或第三方通知的方式，在实验结束/发生错误时发送通知。\n\n![](../../plugin/notification-email/logo.jpg)\n\n- [邮件通知](/plugin/notification-email.md)\n- [飞书通知](/plugin/notification-lark.md)\n- [钉钉通知](/plugin/notification-dingtalk.md)\n- [企业微信通知](/plugin/notification-wxwork.md)\n- [Discord通知](/plugin/notification-discord.md)\n- [Slack通知](/plugin/notification-slack.md)",
    "499": "一级标题：设置实验配置\n二级标题：无\n内容：\n使用 `swanlab.config` 保存你的训练配置，例如：\n- 超参数\n- 输入设置，例如数据集名称或模型类型\n- 实验的任何其他变量\n\n`swanlab.config` 使你可以轻松分析你的实验并在将来复现你的工作。你还可以在SwanLab应用中比较不同实验的配置，并查看不同的训练配置如何影响模型输出。",
    "500": "一级标题：设置实验配置\n二级标题：设置实验配置\n内容：\n`config` 通常在训练脚本的开头定义。当然，不同的人工智能工作流可能会有所不同，因此 `config` 也支持在脚本的不同位置定义，以满足灵活的需求。\n\n以下部分概述了定义实验配置的不同场景。\n\n### 在init中设置\n\n下面的代码片段演示了如何使用Python字典定义 `config`，以及如何在初始化SwanLab实验时将该字典作为参数传递：\n\n```python\nimport swanlab\n\n# 定义一个config字典\nconfig = {\n  \"hidden_layer_sizes\": [64, 128],\n  \"activation\": \"ELU\",\n  \"dropout\": 0.5,\n  \"num_classes\": 10,\n  \"optimizer\": \"Adam\",\n  \"batch_normalization\": True,\n  \"seq_length\": 100,\n}\n\n# 在你初始化SwanLab时传递config字典\nrun = swanlab.init(project=\"config_example\", config=config)\n```\n\n访问 `config` 中的值与在Python中访问其他字典的方式类似：\n\n- 用键名作为索引访问值\n  ```python\n  hidden_layer_sizes = swanlab.config[\"hidden_layer_sizes\"]\n  ```\n- 用 `get()` 方法访问值\n  ```python\n  activation = swanlab.config.get[\"activation\"]\n  ```\n- 用点号访问值\n  ```python\n  dropout = swanlab.config.dropout\n  ```\n\n### 用argparse设置\n\n你可以用 `argparse` 对象设置 `config`。`argparse` 是Python标准库（Python >= 3.2）中的一个非常强大的模块，用于从命令行接口（CLI）解析程序参数。这个模块让开发者能够轻松地编写用户友好的命令行界面。\n\n可以直接传递 `argparse` 对象设置 `config`：\n\n```python\nimport argparse\nimport swanlab\n\n# 初始化Argparse\nparser = argparse.ArgumentParser()\nparser.add_argument('--epochs', default=20)\nparser.add_argument('--learning-rate', default=0.001)\nargs = parser.parse_args()\n\nswanlab.init(config=args)\n```\n\n等同于 `swanlab.init(config={\"epochs\": 20, \"learning-rate\": 0.001})`\n\n### 在脚本的不同位置设置\n\n你可以在整个脚本的不同位置向 `config` 对象添加更多参数。\n\n下面的代码片段展示了如何向 `config` 对象添加新的键值对：\n\n```python\nimport swanlab\n\n# 定义一个config字典\nconfig = {\n  \"hidden_layer_sizes\": [64, 128],\n  \"activation\": \"ELU\",\n  \"dropout\": 0.5,\n  # ... 其他配置项\n}\n\n# 在你初始化SwanLab时传递config字典\nrun = swanlab.init(project=\"config_example\", config=config)\n\n# 在你初始化SwanLab之后，更新config\nswanlab.config[\"dropout\"] = 0.8\nswanlab.config.epochs = 20\nswanlab.config.set[\"batch_size\", 32]\n```\n\n### 用配置文件设置\n\n可以用json和yaml配置文件初始化 `config`，详情请查看[用配置文件创建实验](/guide_cloud/experiment_track/create-experiment-by-configfile)。",
    "501": "一级标题：设置实验Tag\n二级标题：无\n内容：\n实验Tag可以快速标记本次实验所使用的**方法、数据集、模型、超参数、Git仓库等**，以及在未来可以用于分组和过滤实验。\n\n设置好的Tag会在实验名的下方出现：\n\n![](./set-experiment-tag/example.png)",
    "502": "一级标题：设置实验Tag\n二级标题：常规标签\n内容：\n**方法一：编程设置**\n\n你可以使用`swanlab.init`中的`tags`参数，来设置实验的Tag（标签）。\n\n```python\nswanlab.init(\n    tags=[\"tag1\", \"tag2\"],\n)\n```\n\n**方法二：GUI设置**\n\n在网页中，找到实验的顶部区域，点击「添加标签」按钮，即可开始编辑标签：\n\n![](./set-experiment-tag/gui-setting.png)",
    "503": "一级标题：设置实验Tag\n二级标题：Git标签\n内容：\n支持识别标签中的Github、Gitee的仓库链接，呈现一个特殊样式，并可以点击跳转。\n\n```python\nswanlab.init(\n    tags=[\n        \"https://github.com/SwanHubX/SwanLab\",\n        \"https://gitee.com/SwanHubX/SwanLab\",\n    ],\n)\n```\n\n![](./set-experiment-tag/git-tag.png)",
    "504": "一级标题：设置实验Tag\n二级标题：Arxiv标签\n内容：\n支持识别标签中的Arxiv链接，呈现一个特殊样式，并可以点击跳转。\n\n```python\nswanlab.init(\n    tags=[\"https://arxiv.org/abs/1706.03762\"],\n)\n```\n\n![](./set-experiment-tag/arxiv-tag.png)",
    "505": "一级标题：设置实验Tag\n二级标题：AI开源社区标签\n内容：\n支持识别标签中的AI开源社区链接（[HuggingFace](https://huggingface.co/)、[魔搭社区](https://www.modelscope.cn/)、[魔乐社区](https://www.modelers.cn/)），呈现一个特殊样式，并可以点击跳转。\n\n```python\nswanlab.init(\n    tags=[\n        \"https://huggingface.co/deepseek-ai/DeepSeek-R1-0528\",\n        \"https://modelscope.cn/models/deepseek-ai/DeepSeek-R1-0528\",\n        \"https://modelers.cn/models/Modelers_Park/DeepSeek-R1-0528-Qwen3-8B\",\n    ],\n)\n```\n\n![](./set-experiment-tag/ai-community-tag.png)",
    "506": "一级标题：设置实验Tag\n二级标题：算力标签\n内容：\n支持识别标签中的算力卡品牌（nvidia、ascend、apple）。\n\n```python\nswanlab.init(\n    tags=[\"nvidia\", \"ascend\", \"apple\"],\n)\n```\n\n![](./set-experiment-tag/power-tag.png)",
    "507": "一级标题：在内网计算节点访问SwanLab Cloud\n二级标题：无\n内容：\n通常，在算力集群中计算节点无法连接到互联网，外部开发机也必须通过跳板机才能连接到计算节点。如果无法连接到公网那就无法将数据上传到 SwanLab 云端。但是跳板机作为“中间人”角色，可以连接到互联网，那么就可以利用跳板机来实现代理计算节点连接到公网环境。\n\n<img src=\"./ssh-portforwarding/cluster-network.png\" alt=\"cluster-network\" style=\"zoom:30%;\" />\n\n我们可以通过使用SSH代理转发来实现让计算节点也能连接上 [SwanLab Cloud](https://swanlab.cn/)。",
    "508": "一级标题：在内网计算节点访问SwanLab Cloud\n二级标题：开启代理转发网络\n内容：\n> 确保你的计算节点能通过SSH连接上跳板机\n\n在计算节点上执行以下命令连接到跳板机：\n\n```bash\nssh -D {port} {user}@{ip}\n```\n\n- `port` 参数为用于代理转发的端口，例如 `2015`\n- `user` 和 `ip` 参数为跳板机服务器对应的用户名和内网IP地址\n\n例如：`ssh -D 2015 hello@192.168.31.10`\n\n连接到跳板机成功后，即在对应的端口开启了一个SOCKS代理通道，那么可以直接在终端设置环境变量来配置代理，例如：\n\n```bash\nexport http_proxy=socks5://127.0.0.1:{port} https_proxy=socks5://127.0.0.1:{port}\n```\n\n> 注意将对应的 `port` 更换为自己设置的端口，协议为 [socks5](https://en.wikipedia.org/wiki/SOCKS)\n\n配置成功后可以使用以下命令测试是否正确连接到公网:\n\n```bash\ncurl ipinfo.io\n```\n\n配置成功后就可以愉快地使用SwanLab云端版了🥳。\n\n注意SSH连接不能断开，关闭终端会话会导致连接断开，那么可以使用 [tmux](https://github.com/tmux/tmux/wiki) 将SSH连接命令放置在后台。\n\n```bash\ntmux\n# 在tmux中执行SSH连接命令\nssh -D {port} {user}@{ip}\n```\n\n新开终端会话必须重新配置环境变量，当然可以将上述导入环境变量的命令写入 `.bashrc` 文件中实现每次开启新终端会话时自动写入环境变量。例如：\n```bash\necho \"export http_proxy=socks5://127.0.0.1:{port}\" >> ~/.bashrc\necho \"export https_proxy=socks5://127.0.0.1:{port}\" >> ~/.bashrc\n```\n> 注意将 `{port}` 替换为自己设置的端口",
    "509": "一级标题：在内网计算节点访问SwanLab Cloud\n二级标题：实现原理\n内容：\n上述实现借助于 [SSH 动态转发](https://en.wikipedia.org/wiki/Port_forwarding#Dynamic_port_forwarding)功能，SSH 动态端口转发将 SSH 服务器变成 SOCKS 代理服务器，您计算机上的应用程序可以将其用作连接远程服务器的中介。\n\n> **注意：**程序必须支持 SOCKS 类型的代理，您才能使用动态端口转发从该应用程序路由流量。",
    "510": "一级标题：上传离线实验数据\n二级标题：无\n内容：\n> 将本地的实验数据，同步到SwanLab云端/私有化部署端\n\n**使用场景：**\n1. **训练过程中断网：** 模型训一半服务器断网了1小时，SwanLab的云端记录断了\n2. **无网训练环境：** 你不幸地发现公司/学校的训练服务器完全不联网，也不让安装Docker\n3. **多端同步：** 你使用了私有化部署，但也希望把实验记录同步到云端\n4. **复制实验：** 这个实验对你创建的多个项目都有普适性（比如baseline、benchmark实验），希望能复制到多个项目下\n\n当你遇到上述场景时，`swanlab sync`命令可以帮你解决，[API文档](/api/cli-swanlab-sync)。",
    "511": "一级标题：上传离线实验数据\n二级标题：本地实验数据目录\n内容：\nSwanLab会将**实验记录文件**默认存放到项目目录的`swanlog`目录下，你也可以通过`logdir`参数自定义保存路径，[文档](/api/py-init#设置日志文件保存位置)。\n\n每当你创建1个实验时，SwanLab会自动在`swanlog`目录下创建1个`run-[实验ID]`目录，将数据持续记录到该目录下。\n\n![](./sync-logfile/run_dir.png)\n\n::: warning Tips\n这个run开头的目录非常重要，是`swanlab sync`的基础。\n:::",
    "512": "一级标题：上传离线实验数据\n二级标题：上传实验数据\n内容：\n找到你需要上传到云端的数据文件目录，然后执行命令：\n\n```bash\nswanlab sync ./swanlog/run-xxx\n```\n\n:::info\n默认同步到的项目的是日志文件中记录的`project`，即跑该实验时设置的`project`。\n如果想要同步到其他项目，可以使用`-p`选项指定项目。\n:::\n\n看到下面的打印信息，则表示上传成功：\n\n![](./sync-logfile/cli.png)",
    "513": "一级标题：上传离线实验数据\n二级标题：在原实验上同步\n内容：\n如果你不希望创建1个新实验，而是在原本的实验上同步（会自行比对数据，增加差异的部分），可以使用`--id`参数：\n\n```bash\nswanlab sync ./swanlog/run-xxx --id <实验ID>\n```\n\n实验ID获取方式见：[恢复实验/断点续训](/guide_cloud/experiment_track/resume-experiment)",
    "514": "一级标题：上传离线实验数据\n二级标题：批量上传\n内容：\n可以使用通配符批量上传：\n\n```bash\nswanlab sync ./swanlog/run-*\n```",
    "515": "一级标题：上传离线实验数据\n二级标题：上传到指定项目/团队空间\n内容：\n```bash\nswanlab sync ./swanlog/run-xxx -p <project_name> -w <workspace_name>\n```\n\n:::warning 复制实验场景\n如果你希望一个实验可以出现在多个项目下，那么可以使用上述命令，将实验上传到多个项目下。\n:::",
    "516": "一级标题：上传离线实验数据\n二级标题：上传到私有化部署\n内容：\n```bash\nswanlab sync ./swanlog/run-xxx --h <私有化部署地址>\n```",
    "517": "一级标题：系统硬件监控\n二级标题：无\n内容：\nSwanLab在跟踪实验的过程中，会**自动监控**机器的硬件资源情况，并记录到 **「系统」图表** 当中。当前支持的硬件列表：\n\n| 硬件 | 信息记录 | 资源监控 | 脚本 |\n| --- | --- | --- | --- |\n| 英伟达GPU | ✅ | ✅ | [nvidia.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/gpu/nvidia.py) |\n| 昇腾NPU | ✅ | ✅ | [ascend.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/npu/ascend.py) |\n| 寒武纪MLU | ✅ | ✅ | [cambricon.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/mlu/cambricon.py) |\n| 昆仑芯XPU | ✅ | ✅ | [kunlunxin.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/xpu/kunlunxin.py) |\n| 摩尔线程GPU | ✅ | ✅ | [moorethreads.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/gpu/moorethreads.py) |\n| 沐曦GPU | ✅ | ✅ | [metax.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/gpu/metax.py) |\n| 海光DCU | ✅ | ✅ | [hygon.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/dcu/hygon.py) |\n| CPU | ✅ | ✅ | [cpu.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/cpu.py) |\n| 内存 | ✅ | ✅ | [memory.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/memory.py) |\n| 硬盘 | ✅ | ✅ | [disk.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/disk.py) |\n| 网络 | ✅ | ✅ | [network.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/network.py) |\n\n[[toc]]",
    "518": "一级标题：系统硬件监控\n二级标题：系统监控指标详解\n内容：\nSwanLab 支持在当前实验运行的机器上自动监控硬件资源情况，并为每个指标生成图表，统一展示在 **「系统」图表** 选项卡中。\n\n![](./system-monitor/head.png)\n\n**采集策略与频率**：SwanLab根据当前实验的持续运行时间，自动调整硬件数据采集的频率，以平衡数据粒度与系统性能，采集频率分为以下几档：\n\n| 已采集数据点数 | 采集频率 |\n|   :---:   |   :---:   |\n| 0~10    | 10 秒/次 |\n| 10~50   | 30 秒/次 |\n| 50+     | 60 秒/次 |\n\nSwanLab 采集的硬件资源情况涵盖了GPU、NPU、CPU、系统内存、硬盘IO以及网络情况等多个与训练过程相关的指标。以下详细介绍每个部分的监控内容及其在图表展示中的意义。",
    "519": "一级标题：系统硬件监控\n二级标题：GPU（NVIDIA）\n内容：\n![](./system-monitor/nvidia.png)\n\n> 在多卡机器上，每个GPU的资源情况都会单独记录，最终在图表中展示多条图线。\n\n| 指标 | 描述 |\n|--------|------------|\n| GPU Memory Allocated (%) | **GPU 显存使用率**，表示此GPU的显存占用百分比。|\n| GPU Memory Allocated (MB) | **GPU 显存使用率**，表示此GPU的显存占用量，以MB为单位。该指标对应图表的纵坐标上限为所有GPU中的最大总显存。|\n| GPU Utilization (%) | **GPU 利用率**，表示此GPU的计算资源占用百分比。|\n| GPU Temperature (℃) | **GPU 温度**，表示此GPU的温度，以摄氏度为单位。|\n| GPU Power Usage (W) | **GPU 功耗**，表示此GPU的功耗，以瓦特为单位。|\n| GPU Time Spent Accessing Memory (%) | **GPU 内存访问时间**，表示此GPU在执行任务时，花费在访问 GPU 内存（显存）上的时间百分比。|\n\n<br>",
    "520": "一级标题：系统硬件监控\n二级标题：NPU（Ascend）\n内容：\n![](./system-monitor/ascend.png)\n\n> 在多卡机器上，每个NPU的资源情况都会单独记录，最终在图表中展示多条图线。\n\n| 指标 | 描述 |\n|--------|------------|\n| NPU Utilization (%) | **NPU 利用率**，表示此NPU的计算资源占用百分比。|\n| NPU Memory Allocated (MB) | **NPU 显存使用率**，表示此GPU的显存占用量，以MB为单位。该指标对应图表的纵坐标上限为所有GPU中的最大总显存。|\n| NPU Memory Allocated (%) | **NPU 显存使用率**，表示此NPU的显存占用百分比。|\n| NPU Temperature (℃) | **NPU 温度**，表示此NPU的温度，以摄氏度为单位。|\n| NPU Power (W) | **NPU 功率**，表示此NPU的功率，以瓦特为单位。|\n\n<br>",
    "521": "一级标题：系统硬件监控\n二级标题：MLU（寒武纪）\n内容：\n![](./system-monitor/cambricon.png)\n\n> 在多卡机器上，每个MLU的资源情况都会单独记录，最终在图表中展示多条图线。\n\n| 指标 | 描述 |\n|--------|------------|\n| MLU Utilization (%) | **MLU 利用率**，表示此MLU的计算资源占用百分比。|\n| MLU Memory Allocated (MB) | **MLU 显存使用率**，表示此MLU的显存占用量，以MB为单位。该指标对应图表的纵坐标上限为所有MLU中的最大总显存。|\n| MLU Memory Allocated (%) | **MLU 显存使用率**，表示此MLU的显存占用百分比。|\n| MLU Temperature (℃) | **MLU 温度**，表示此MLU的温度，以摄氏度为单位。|\n| MLU Power (W) | **MLU 功率**，表示此MLU的功率，以瓦特为单位。|\n\n<br>",
    "522": "一级标题：系统硬件监控\n二级标题：XPU（昆仑芯）\n内容：\n![](./system-monitor/kunlunxin.png)\n\n> 在多卡机器上，每个XPU的资源情况都会单独记录，最终在图表中展示多条图线。\n\n| 指标 | 描述 |\n|--------|------------|\n| XPU Utilization (%) | **XPU 利用率**，表示此XPU的计算资源占用百分比。|\n| XPU Memory Allocated (MB) | **XPU 显存使用率**，表示此XPU的显存占用量，以MB为单位。该指标对应图表的纵坐标上限为所有XPU中的最大总显存。|\n| XPU Memory Allocated (%) | **XPU 显存使用率**，表示此XPU的显存占用百分比。|\n| XPU Temperature (℃) | **XPU 温度**，表示此XPU的温度，以摄氏度为单位。|\n| XPU Power (W) | **XPU 功率**，表示此XPU的功率，以瓦特为单位。|\n\n<br>",
    "523": "一级标题：系统硬件监控\n二级标题：GPU（摩尔线程）\n内容：\n![](./system-monitor/moorethread.png)\n\n> 在多卡机器上，每个摩尔线程GPU的资源情况都会单独记录，最终在图表中展示多条图线。\n\n| 指标 | 描述 |\n|--------|------------|\n| GPU Utilization (%) | **GPU 利用率**，表示此GPU的计算资源占用百分比。|\n| GPU Memory Allocated (MB) | **GPU 显存使用率**，表示此GPU的显存占用量，以MB为单位。该指标对应图表的纵坐标上限为所有GPU中的最大总显存。|\n| GPU Memory Allocated (%) | **GPU 显存使用率**，表示此GPU的显存占用百分比。|\n| GPU Temperature (℃) | **GPU 温度**，表示此GPU的温度，以摄氏度为单位。|\n| GPU Power (W) | **GPU 功率**，表示此GPU的功率，以瓦特为单位。|\n\n<br>",
    "524": "一级标题：系统硬件监控\n二级标题：GPU（沐曦）\n内容：\n![](./system-monitor/metax.png)\n\n> 在多卡机器上，每个沐曦GPU的资源情况都会单独记录，最终在图表中展示多条图线。\n\n| 指标 | 描述 |\n|--------|------------|\n| GPU Utilization (%) | **GPU 利用率**，表示此GPU的计算资源占用百分比。|\n| GPU Memory Allocated (MB) | **GPU 显存使用率**，表示此GPU的显存占用量，以MB为单位。该指标对应图表的纵坐标上限为所有GPU中的最大总显存。|\n| GPU Memory Allocated (%) | **GPU 显存使用率**，表示此GPU的显存占用百分比。|\n| GPU Temperature (℃) | **GPU 温度**，表示此GPU的温度，以摄氏度为单位。|\n| GPU Power (W) | **GPU 功率**，表示此GPU的功率，以瓦特为单位。|\n\n<br>",
    "525": "一级标题：系统硬件监控\n二级标题：DCU（海光）\n内容：\n> 在多卡机器上，每个海光DCU的资源情况都会单独记录，最终在图表中展示多条图线。\n\n| 指标 | 描述 |\n|--------|------------|\n| GPU Utilization (%) | **GPU 利用率**，表示此GPU的计算资源占用百分比。|\n| GPU Memory Allocated (MB) | **GPU 显存使用率**，表示此GPU的显存占用量，以MB为单位。该指标对应图表的纵坐标上限为所有GPU中的最大总显存。|\n| GPU Memory Allocated (%) | **GPU 显存使用率**，表示此GPU的显存占用百分比。|\n| GPU Temperature (℃) | **GPU 温度**，表示此GPU的温度，以摄氏度为单位。|\n| GPU Power (W) | **GPU 功率**，表示此GPU的功率，以瓦特为单位。|\n\n\n<br>",
    "526": "一级标题：系统硬件监控\n二级标题：CPU\n内容：\n| 指标 | 描述 |\n|--------|------------|\n| CPU Utilization (%) | **CPU 利用率**，表示此CPU的计算资源占用百分比。|\n| Process CPU Threads | **CPU 线程数**，表示当前运行的实验所使用的CPU总线程数。|\n\n<br>",
    "527": "一级标题：系统硬件监控\n二级标题：内存\n内容：\n| 指标 | 描述 |\n|--------|------------|\n| System Memory Utilization (%) | **系统内存使用率**，表示当前系统的内存占用百分比。|\n| Process Memory In Use (non-swap) (MB) | **进程占用内存**，当前进程实际占用的物理内存量（不包含交换区），直观反映实验运行时的内存消耗。|\n| Process Memory Utilization (MB) | **进程分配内存**，当前进程分配的内存量（包含交换区），不一定是实际使用的内存量。|\n| Process Memory Available （non-swap） (MB) | **进程可用内存**，当前进程可用的物理内存量（不包含交换区），即当前进程可以使用的内存量。|\n\n<br>",
    "528": "一级标题：系统硬件监控\n二级标题：硬盘\n内容：\n| 指标 | 描述 |\n|--------|------------|\n| Disk IO Utilization (MB) | **硬盘I/O**，表示硬盘的读写速度，以MB/s为单位。读速率和写速率会在图表中作为两条图线，分开展示。|\n| Disk Utilization (%) | **硬盘使用情况**，表示当前系统盘的使用率，以百分比为单位。|\n\n在Linux平台，取根目录`/`的使用率；若操作系统为Windows，则取系统盘（通常是`C:`）的使用率。\n\n<br>",
    "529": "一级标题：系统硬件监控\n二级标题：网络\n内容：\n| 指标 | 描述 |\n|--------|------------|\n| Network Traffic (KB) | **网络I/O**，表示网络的读写速度，以KB/s为单位。接收速率和发送速率会在图表中作为两条图线，分开展示。|\n\n> 表示网络的读写速度，以KB/s为单位。接收速率和发送速率会在图表中作为两条图线，分开展示。",
    "530": "一级标题：使用OpenAPI获取实验数据\n二级标题：无\n内容：\n基于 SwanLab 云端功能, 在 SDK 端提供访问 **开放 API（OpenAPI）** 的能力, 允许用户通过编程方式在本地环境中操作云端 **实验/项目/工作空间** 资源。\n\n![](./py-openapi/logo.jpg)\n\n通过开放 API 的形式, 用户可以在本地编程环境中:\n\n- 获取实验数据、个人信息、工作空间信息、项目列表等\n- 进行实验的自动管理（如查询、组织、元数据编辑等）\n- 更方便地与其他工具集成（如 CI/CD、实验调度等）\n\n利用好此特性可极大提升 SDK 的灵活性和可扩展性, 方便构建高级用法或扩展体系",
    "531": "一级标题：使用OpenAPI获取实验数据\n二级标题：支持的API列表\n内容：\n下表列出了SwanLab OpenAPI支持的所有方法，点击API名称可跳转到详细说明：\n\n| API名称 | 分类 | 功能描述 | Ready |\n|---------|------|----------|------|\n| [`list_workspaces`](#list-workspaces) | WorkSpace | 获取当前用户的所有工作空间(组织)列表 | ✅ |\n| [`list_projects`](#list-projects) | Project | 获取指定工作空间下的所有项目列表 | ✅ |\n| [`delete_project`](#delete-project) | Project | 删除一个项目 | ✅ |\n| [`list_experiments`](#list-experiments) | Experiment | 获取指定项目下的所有实验列表 | ✅ |\n| [`get_experiment`](#get-experiment) | Experiment | 获取一个实验的详细信息（实验名、配置、环境等） | ✅ |\n| [`get_summary`](#get-summary) | Experiment | 获取一个实验的Summary信息，包含实验跟踪指标的最终值和最大最小值 | ✅ |\n| [`get_metrics`](#get-metrics) | Experiment | 获取一个实验指标的值 |  ✅ |\n| [`delete_experiment`](#delete-experiment) | Experiment | 删除一个实验 | ✅ |",
    "532": "一级标题：使用OpenAPI获取实验数据\n二级标题：介绍\n内容：\n> 前置条件：需要在编程环境下登录过SwanLab账号。\n\n要使用 SwanLab 的开放 API, 只需实例化一个 `OpenApi` 对象。\n\n```python\nfrom swanlab import OpenApi\n\nmy_api = OpenApi() # 使用本地登录信息\nprint(my_api.list_workspaces().data) # 获取当前用户的工作空间列表\n```\n\n如果你需要获取其他用户的数据：\n```python\nfrom swanlab import OpenApi\n\nother_api = OpenApi(api_key='other_api_key') # 使用另一个账户的api_key\nprint(other_api.list_workspaces().data)\n```\n\n\n具体来说, **OpenApi**的认证逻辑如下：\n\n1. 如果显式提供了`api_key`参数, 则优先使用该`api_key`进行身份认证, 可以在[这里](https://swanlab.cn/space/~/settings)查看自己的 API 密钥；\n2. 否则,使用本地的认证信息。",
    "533": "一级标题：使用OpenAPI获取实验数据\n二级标题：常用参数\n内容：\n### 实验ID `exp_id`\n\n实验的唯一标识符**CUID**, 即`exp_id`, 可通过`list_experiments`方法获取对应的`cuid`字段\n\n要查看某一个实验的CUID, 可在云端版网页的\"环境\"标签页查看\"实验ID\"一行, 点击即可复制此实验的CUID\n\n![](./py-openapi/exp_id.png)\n\n### 工作空间名 `username`\n\n工作空间名即`username`, 用于标识用户所在的工作空间:\n\n- 若为个人空间, `username`即为用户的用户名\n- 若为组织空间, `username`为该组织的组织ID\n\n`username`可以通过`list_workspaces`方法获取, 返回的工作空间列表中每个元素的`username`字段即为工作空间名\n\n一般的, 若在开放API调用中不指定`username`, 则**默认**为当前用户的个人空间",
    "534": "一级标题：使用OpenAPI获取实验数据\n二级标题：模型定义\n内容：\n在使用开放 API 时, 获取到的部分云端资源组成较为复杂, 如实验、项目等, 难以用简单的Python数据类型表示\n\n因此, 这些资源在开放API的返回值中被定义为了对象, 支持 IDE 的自动补全与类型检查, 从而方便用户进行操作\n\n例如, 要获取一个实验对象的开始时间, 可以用:\n\n```python\napi_response: ApiResponse = my_api.get_experiment(project=\"project1\", exp_cuid=\"cuid1\")\nmy_exp: Experiment = api_response.data\ncreated_time: str = my_exp.createdAt\n```\n\n或者, 要获取一个项目对象所属工作空间的名字, 可以用:\n\n```python\napi_response: ApiResponse = my_api.list_projects()\nmy_project: Project = api_response.data[0]\nworkspace_name: str = my_project.group[\"name\"]\n```\n\n对于一个模型, 其属性可通过以下三种方式访问:\n\n- `my_exp.createdAt`\n- `my_exp[\"createdAt\"]`\n- `my_exp.get(\"createdAt\")`\n\n> Note: 模型可以通过字典风格访问, 但不是真正的字典, 可以通过`my_exp_dict: Dict = my_exp.model_dump()`获取此时模型对应的字典\n\n### API 响应 `ApiResponse`\n\n开放 API 方法返回`swanlab.api.openapi.types.ApiResponse`对象, 包含以下字段:\n\n| 字段 | 类型 |描述 |\n| --- | --- | --- |\n| `code` | `int` | HTTP 状态码 |\n| `errmsg` | `str` | 错误信息, 如果状态码不为`2XX`则非空 |\n| `data` | `Any` | 返回的具体数据, 下面API文档中提到的返回值即为该字段 |\n\n### 实验模型 `Experiment`\n\n实验对象的类型为`swanlab.api.openapi.types.Experiment`, 包含以下字段:\n\n| 字段 | 类型 | 描述 |\n| --- | --- | --- |\n| `cuid` | `str` | 实验CUID, 唯一标识符 |\n| `name` | `str` | 实验名 |\n| `description` | `str` | 实验描述 |\n| `state` | `str` | 实验状态, `FINISHED` 或 `RUNNING` |\n| `show` | `bool` | 显示状态 |\n| `createdAt` | `str` | 创建时间, 格式如 `2024-11-23T12:28:04.286Z` |\n| `finishedAt` | `str` | 完成时间, 格式如 `2024-11-23T12:28:04.286Z`, 若不存在则为 None |\n| `user` | `Dict[str, str]` | 实验创建者, 包含 `username` 与 `name` |\n| `profile` | `dict` | 详细包含了实验的所有配置信息, 如用户自定义配置与Python运行环境等 |\n\n### 项目模型 `Project`\n\n项目对象的类型为`swanlab.api.openapi.types.Project`, 包含以下字段:\n\n| 字段 | 类型 | 描述 |\n| --- | --- | --- |\n| `cuid` | `str` | 项目CUID, 唯一标识符 |\n| `name` | `str` | 项目名 |\n| `description` | `str` | 项目描述 |\n| `visibility` | `str` | 可见性, `PUBLIC` 或 `PRIVATE` |\n| `createdAt` | `str` | 创建时间, 格式如 `2024-11-23T12:28:04.286Z` |\n| `updatedAt` | `str` | 更新时间, 格式如 `2024-11-23T12:28:04.286Z` |\n| `group` | `Dict[str, str]` | 工作空间信息, 包含 `type`, `username`, `name` |\n| `count` | `Dict[str, int]` | 项目的统计信息, 如实验个数, 协作者数量等 |",
    "535": "一级标题：使用OpenAPI获取实验数据\n二级标题：OpenAPIs\n内容：\n每个开放 API 都是`OpenApi`对象的一个方法\n\n下面是所有可用的SwanLab 开放 API\n\n### WorkSpace\n\n#### `list_workspaces`\n\n获取当前用户的所有工作空间(组织)列表。\n\n**返回值**\n\n`data` `(List[Dict])`: 用户加入的工作空间列表, 每个元素是一个字典, 包含工作空间的基础信息:\n\n| 字段 | 类型 | 描述 |\n| --- | --- | --- |\n| `name` | `str` | 工作空间名称 |\n| `username` | `str` | 工作空间唯一标识(用于组织相关的 URL) |\n| `role` | `str` | 用户在该工作空间中的角色, 为 `OWNER` 或 `MEMBER` |\n\n**示例**\n\n::: code-group\n\n```python [获取工作区列表]\nfrom swanlab import OpenApi\nmy_api = OpenApi()\n\nmy_api.list_workspaces().data\n\"\"\"\n[\n    {\n        \"name\": \"workspace1\",\n        \"username\": \"kites-test3\",\n        \"role\": \"OWNER\"\n    },\n    {\n        \"name\": \"hello-openapi\",\n        \"username\": \"kites-test2\",\n        \"role\": \"MEMBER\"\n    }\n]\n\"\"\"\n```\n\n```python [获取第一个工作区名称]\nfrom swanlab import OpenApi\nmy_api = OpenApi()\n\nmy_api.list_workspaces().data[0][\"name\"]\n\"\"\"\n\"workspace1\"\n\"\"\"\n```\n\n```python [获取响应状态码]\nfrom swanlab import OpenApi\nmy_api = OpenApi()\n\nmy_api.list_workspaces().code\n\"\"\"\n200\n\"\"\"\n```\n\n:::\n\n<br>\n\n---\n\n### Experiment\n\n#### `list_experiments`\n\n获取指定项目下的所有实验列表\n\n**方法参数**\n\n| 参数  | 类型 | 描述 |\n| --- | --- | --- |\n| `project` | `str` | 项目名 |\n| `username` | `str` | 工作空间名, 默认为用户个人空间 |\n\n**返回值**\n\n`data` `(List[Experiment])`: 包含实验[(Experiment)](#实验模型-experiment)对象的列表\n\n**示例**\n\n::: code-group\n\n```python [获取实验列表]\nmy_api.list_experiments(project=\"project1\").data\n\"\"\"\n[\n    {\n        \"cuid\": \"cuid1\",\n        \"name\": \"experiment1\",\n        \"description\": \"Description 1\",\n        \"state\": \"RUNNING\",\n        \"show\": true,\n        \"createdAt\": \"2024-11-23T12:28:04.286Z\",\n        \"finishedAt\": null,\n        \"user\": {\n            \"username\": \"kites-test3\",\n            \"name\": \"Kites Test\"\n        },\n        \"profile\": {\n            \"config\": {\n                \"lr\": 0.001,\n                \"epochs\": 10\n            }\n        }\n    },\n    ...\n]\n\"\"\"\n```\n\n```python [获取第一个实验的CUID]\nmy_api.list_experiments(project=\"project1\").data[0].cuid\n\"\"\"\n\"cuid1\"\n\"\"\"\n```\n\n```python [获取第一个实验的名称]\nmy_api.list_experiments(project=\"project1\").data[0].name\n\"\"\"\n\"experiment1\"\n\"\"\"\n```\n\n:::\n\n<br>\n\n#### `get_experiment`\n\n获取一个实验的详细信息\n\n**方法参数**\n\n| 参数 | 类型 | 描述 |\n| --- | --- | --- |\n| `project` | `str` | 项目名 |\n| `exp_id` | `str` | 实验CUID, 唯一标识符, 可通过`list_experiments`获取, 也可在云端版实验\"环境\"标签页查看 |\n| `username` | `str` | 工作空间名, 默认为用户个人空间 |\n\n**返回值**\n\n`data` `(Experiment)`: 返回一个实验[(Experiment)](#实验模型-experiment)类型的对象, 包含实验的详细信息\n\n**示例**\n\n::: code-group\n\n```python [获取实验信息]\nmy_api.get_experiment(project=\"project1\", exp_id=\"cuid1\").data\n\"\"\"\n{\n    \"cuid\": \"cuid1\",\n    \"name\": \"experiment1\",\n    \"description\": \"This is a test experiment\",\n    \"state\": \"FINISHED\",\n    \"show\": true,\n    \"createdAt\": \"2024-11-23T12:28:04.286Z\",\n    \"finishedAt\": \"2024-11-25T15:56:48.123Z\",\n    \"user\": {\n        \"username\": \"kites-test3\",\n        \"name\": \"Kites Test\"\n    },\n    \"profile\": {\n        \"conda\": \"...\",\n        \"requirements\": \"...\",\n        ...\n    }\n}\n\"\"\"\n```\n\n```python [获取实验的状态]\nmy_api.get_experiment(project=\"project1\", exp_id=\"cuid1\").data.state\n\"\"\"\n\"FINISHED\"\n\"\"\"\n```\n\n```python [获取实验的创建者用户名]\nmy_api.get_experiment(project=\"project1\", exp_id=\"cuid1\").data.user[\"username\"]\n\"\"\"\n\"kites-test3\"\n\"\"\"\n```\n\n:::\n\n<br>\n\n#### `delete_experiment`\n\n删除一个实验\n\n**方法参数**\n| 参数 | 类型 | 描述 |\n| --- | --- | --- |\n| `project` | `str` | 项目名 |\n| `exp_id` | `str` | 实验CUID, 唯一标识符, 可通过`list_experiments`获取, 也可在云端版实验\"环境\"标签页查看 |\n| `username` | `str` | 工作空间名, 默认为用户个人空间 |\n\n**返回值**\n`data` `(dict)`: 空字典, 仅表示删除操作成功\n\n**示例**\n\n::: code-group\n\n```python [删除实验]\nmy_api.delete_experiment(project=\"project1\", exp_id=\"cuid1\")\n```\n\n:::\n\n<br>\n\n#### `get_summary`\n\n获取一个实验的概要信息, 包含实验跟踪指标的最终值和最大最小值, 以及其对应的步数\n\n**方法参数**\n\n| 参数 | 类型 | 描述 |\n| --- | --- | --- |\n| `project` | `str` | 项目名 |\n| `exp_id` | `str` | 实验CUID, 唯一标识符, 可通过`list_experiments`获取, 也可在云端版实验\"环境\"标签页查看 |\n| `username` | `str` | 工作空间名, 默认为用户个人空间 |\n\n**返回值**\n\n`data` `(Dict[str, Dict])`: 返回一个字典, 包含实验的概要信息\n\n字典中的每个键是一个指标名称, 值是一个结构如下的字典:\n\n| 字段 | 类型 | 描述 |\n| --- | --- | --- |\n| `step` | `int` | 最后一个步数 |\n| `value` | `float` | 最后一个步数的指标值 |\n| `min` | `Dict[str, float]` | 最小值对应的步数和指标值 |\n| `max` | `Dict[str, float]` | 最大值对应的步数和指标值 |\n\n\n**示例**\n\n::: code-group\n\n```python [获取实验概要信息]\nmy_api.get_summary(project=\"project1\", exp_id=\"cuid1\").data\n\"\"\"\n{\n    \"loss\": {\n        \"step\": 47,\n        \"value\": 0.1907215012216071,\n        \"min\": {\n            \"step\": 33,\n            \"value\": 0.1745886406861026\n        },\n        \"max\": {\n            \"step\": 0,\n            \"value\": 0.7108771095136294\n        }\n    },\n    ...\n}\n\"\"\"\n```\n\n\n```python [获取指标的最大值]\nmy_api.get_summary(project=\"project1\", exp_id=\"cuid1\").data[\"loss\"][\"max\"][\"value\"]\n\"\"\"\n0.7108771095136294\n\"\"\"\n```\n\n```python [获取指标最小值所在步]\nmy_api.get_summary(project=\"project1\", exp_id=\"cuid1\").data[\"loss\"][\"min\"][\"step\"]\n\"\"\"\n33\n\"\"\"\n```\n:::\n\n<br>\n\n#### get_metrics\n\n获取一个实验的指标值\n\n**方法参数**\n\n| 参数 | 类型 | 描述 |\n| --- | --- | --- |\n| `exp_id` | `str` | 实验CUID, 唯一标识符, 可通过`list_experiments`获取, 也可在云端版实验\"环境\"标签页查看 |\n| `keys` | `Union[str, List[str]]` | 指标名列表, 即swanlab.log({key: value})中的key, 可在网站查看, 也可通过`get_summary`获取 |\n\n**返回值**\n\n`data` `(DataFrame)`: 返回一个DataFrame, 包含实验的指标值\n\n**示例**\n\n::: code-group\n\n```python [获取实验指标]\nmy_api.get_metrics(exp_id=\"cuid1\", keys=[\"loss\", \"acc\"]).data\n\"\"\"\n          loss  loss_timestamp       acc  acc_timestamp\nstep\n1     0.336772   1751712864853  0.670422  1751712864852\n2     0.338035   1751712864858  0.830018  1751712864857\n3     0.282654   1751712864862  0.794594  1751712864862\n4     0.258216   1751712864866  0.832750  1751712864866\n5     0.097542   1751712864871  0.901684  1751712864871\n6     0.092955   1751712864875  0.907544  1751712864875\n7     0.149327   1751712864879  0.942524  1751712864879\n8     0.131631   1751712864884  0.921309  1751712864883\n\"\"\"\n```\n\n:::\n\n\n<br>\n\n---\n\n\n### Project\n\n#### `list_projects`\n\n获取指定工作空间下的所有项目列表\n\n**方法参数**\n\n| 参数  | 类型 | 描述 |\n| --- | --- | --- |\n| `username` | `str` | 工作空间名, 默认为用户个人空间 |\n| `detail` | `bool` | 是否项目统计信息, 默认为 True |\n\n**返回值**\n\n`data` `(List[Project])`: 包含项目[(Project)](#项目模型-project)对象的列表\n\n**示例**\n\n::: code-group\n\n```python [获取项目列表]\nmy_api.list_projects().data\n\"\"\"\n[\n    {\n        \"cuid\": \"project1\",\n        \"name\": \"Project 1\",\n        \"description\": \"Description 1\",\n        \"visibility\": \"PUBLIC\",\n        \"createdAt\": \"2024-11-23T12:28:04.286Z\",\n        \"updatedAt\": null,\n        \"group\": {\n            \"type\": \"PERSON\",\n            \"username\": \"kites-test3\",\n            \"name\": \"Kites Test\"\n        },\n        \"count\": {\n            \"experiments\": 4,\n            \"contributors\": 1,\n            \"children\": 0,\n            \"runningExps\": 0\n        }\n    },\n    ...\n]\n\"\"\"\n```\n\n:::\n\n#### `delete_project`\n\n删除一个项目\n\n**方法参数**\n\n| 参数 | 类型 | 描述 |\n| --- | --- | --- |\n| `project` | `str` | 项目名 |\n| `username` | `str` | 工作空间名, 默认为用户个人空间 |\n\n**返回值**\n\n`data` `(dict)`: 空字典, 仅表示删除操作成功\n\n**示例**\n\n::: code-group\n\n```python [删除项目]\nmy_api.delete_project(project=\"project1\")\n```\n\n:::\n\n<br>",
    "536": "一级标题：查看实验结果\n二级标题：无\n内容：\n使用SwanLab丰富的实验仪表盘，一站式管理和可视化AI模型训练结果。\n\n[[toc]]",
    "537": "一级标题：查看实验结果\n二级标题：云端同步\n内容：\n无论您在哪里训练模型 —— **自己的电脑、实验室的服务器集群、还是云上的实例**，我们都能轻松收集与汇总您的训练数据，并且随时随地访问训练进展，哪怕是在手机上。\n\n您也无需花时间将终端的输出截图或粘贴到Excel，也无需管理来自不同计算机的Tensorboard文件，用SwanLab就能轻松搞定。\n\n![](./view-result/cloud.jpg)",
    "538": "一级标题：查看实验结果\n二级标题：📱 移动端看实验\n内容：\n你一定遇到过，实验正在training，但你不在电脑旁边 —— 也许在运动、在通勤、或者刚刚起床，十分想瞄一眼实验的进展和结果。这个时候，手机+SwanLab，会是绝佳组合。[查看详情](../general/app.md)\n\n![](../general/app/android.png)",
    "539": "一级标题：查看实验结果\n二级标题：表格视图\n内容：\n通过表格视图比较每次训练实验，看看哪些超参数发生了变化。\n表格视图默认会将数据以`[实验名]-[元信息]-[配置]-[指标]`的顺序排序。\n\n![view-result](/assets/view-result-1.jpg)",
    "540": "一级标题：查看实验结果\n二级标题：图表对比视图\n内容：\n通过**图表对比视图**可以将每个实验的图表进行整合，生成一个多实验对比图表视图。\n在多实验图表当中，可以清晰地对比不同实验在同一个指标下的变化情况与性能差异。\n\n![chart-comparison](/assets/chart-comparison.jpg)",
    "541": "一级标题：查看实验结果\n二级标题：日志\n内容：\n在实验开始到结束，SwanLab会记录下从`swanlab.init`到实验结束的终端输出，并记录在实验的「日志」选项卡，可以随时查看、复制与下载。我们也支持通过搜索找到关键信息。\n\n![logging](/assets/logging.jpg)",
    "542": "一级标题：查看实验结果\n二级标题：环境\n内容：\n在实验开始后，SwanLab会记录下训练相关的环境参数，包括：\n\n- **基础数据**：运行时间、主机名、操作系统、Python版本、Python解释器、运行目录、命令行、Git仓库URL、Git分支、Git提交、日志文件目录、SwanLab版本\n- **系统硬件**：CPU核心数、内存大小、GPU数量、GPU型号、GPU显存\n- **Python库**：运行环境下的所有Python库\n\n![environment](/assets/environment.jpg)",
    "543": "一级标题：什么是实验跟踪\n二级标题：无\n内容：\n**实验跟踪** 是指在机器学习模型开发过程中，记录每个实验从开始到结束的**超参数、指标、硬件、环境、日志**等数据，并在UI界面进行**组织**和**呈现**的过程。实验跟踪的目的是帮助研究人员更有效地**管理**和**分析**实验结果，以便更好地理解模型性能的变化，进而优化模型开发过程。\n\n::: warning 🤔简单来说\n实验跟踪的作用可以理解为，在进行机器学习实验时，记录下实验的各个关键信息，**为后续模型的进化提供“弹药”**。\n:::\n\n![](./what-is-experiment-track/overview.jpg)\n\n与**实验跟踪**息息相关的，是**可视化**、**可复现性**、**实验比较**以及**团队协作**。\n\n1. **📊 可视化**: 通过UI界面对实验跟踪数据进行可视化，可以让训练师**直观地看到实验每一步**的结果，**分析指标走势**，判断哪些**变化**导致了模型效果的提升，从而**整体性地提升模型迭代效率**。\n\n![](./what-is-experiment-track/visualization.jpg)\n\n<br>\n\n2. **♻️ 可复现性**: 实验从跑通到可用，再到SOTA，往往需要经历**大量试验**，而一些非常好的结果可能出现在中前期。但如果没有实验跟踪和可视化，训练师难以记住这些结果，从而导致大量优秀的实验结果**记不清细节或被遗忘**。而通过SwanLab的实验跟踪和可视化功能，可以帮助训练师随时**回顾**这些结果，大大提高了可复现性与整体效率。\n\n![](./what-is-experiment-track/card.jpg)\n\n<br>\n\n3. **🆚 实验比较**: 训练师可以通过SwanLab**轻松地比较**多组实验结果，分析哪些变化导致了性能提升，从而**快速找到最优的训练策略**。\n\n![](./what-is-experiment-track/table.jpg)\n\n<br>\n\n4. **👥 团队协作**: 通过SwanLab的**实验分享、团队空间、多人协同**实验等功能，无缝地共享训练进展和心得经验，打通团队成员之间的信息孤岛，**提高团队协作效率**。",
    "544": "一级标题：什么是实验跟踪\n二级标题：SwanLab是如何进行实验跟踪的？\n内容：\n**SwanLab**帮助你只需使用几行代码，便可以跟踪机器学习实验，并在交互式仪表板中查看与比较结果。跟踪流程：\n\n1. 创建SwanLab实验。\n2. 将超参数字典（例如学习率或模型类型）存储到您的配置中 (swanlab.config)。\n3. 在训练循环中随时间记录指标 (swanlab.log)，例如准确性acc和损失loss。\n\n下面的伪代码演示了常见的**SwanLab实验跟踪工作流**：\n\n```python\n# 1. 创建1个SwanLab实验\nswanlab.init(project=\"my-project-name\")\n\n# 2. 存储模型的输入或超参数\nswanlab.config.learning_rate = 0.01\n\n# 这里写模型的训练代码\n...\n\n# 3. 记录随时间变化的指标以可视化表现\nswanlab.log({\"loss\": loss})\n```",
    "545": "一级标题：什么是实验跟踪\n二级标题：如何开始？\n内容：\n探索以下资源以了解SwanLab实验跟踪：\n\n- 阅读[快速开始](/guide_cloud/general/quick-start)\n- 探索本章以了解如何：\n  - [创建一个实验](/guide_cloud/experiment_track/create-experiment)\n  - [配置实验](/guide_cloud/experiment_track/set-experiment-config.md)\n  - [记录指标](/guide_cloud/experiment_track/log-experiment-metric.md)\n  - [查看实验结果](/guide_cloud/experiment_track/view-result.md)\n- 在[API文档](/api/api-index)中探索SwanLab Python 库。",
    "546": "一级标题：在手机上使用SwanLab\n二级标题：无\n内容：\n你一定遇到过，实验正在training，但你不在电脑旁边 —— 也许在运动、在通勤、或者刚刚起床，十分想瞄一眼实验的进展和结果。这个时候，**手机+SwanLab**，会是绝佳组合。\n\n下面将介绍如何将SwanLab添加到你的手机主屏幕，以接近APP的体验，快速访问SwanLab。",
    "547": "一级标题：在手机上使用SwanLab\n二级标题：安卓\n内容：\n流程示例图如下所示，以Chrome浏览器为例：\n\n![alt text](/zh/guide_cloud/general/app/android.png)\n\n1. 在你的手机浏览器上，访问[swanlab.cn](https://swanlab.cn)\n2. 点击右上角三个点按钮后，在菜单中点击 **「添加到主屏幕」**\n3. 在弹窗中，选择 **「安装」** 或 **「创建快捷方式」** 均可\n4. 回到桌面，现在你在主屏幕上就可以找到 **SwanLab\"APP\"** 了！",
    "548": "一级标题：在手机上使用SwanLab\n二级标题：iOS\n内容：\n流程示例图如下所示，以Safari浏览器为例：\n\n![alt text](/zh/guide_cloud/general/app/ios.png)\n\n1. 在你的手机浏览器上，访问[swanlab.cn](https://swanlab.cn)\n2. 点击底部中间的分享按钮后，在菜单中点击 **「添加到主屏幕」**\n3. 在弹窗中，编辑应用名称，点击右上角的 **「添加」**\n4. 回到桌面，现在你在主屏幕上就可以找到 **SwanLab\"APP\"** 了！",
    "549": "一级标题：⚡️更新日志\n二级标题：无\n内容：\n::: warning 更新指南\n升级到最新版：`pip install -U swanlab`\nGithub: https://github.com/SwanHubX/SwanLab\n:::",
    "550": "一级标题：⚡️更新日志\n二级标题：v0.6.8 - 2025.7.29\n内容：\n**🚀新增功能**\n- 侧边栏支持**实验筛选、排序**\n- 表格视图上线**列控制面板**，能够方便地实现列的隐藏与显示\n- **多API Key管理**上线，让你的数据更安全\n- [swanlab sync](/guide_cloud/experiment_track/sync-logfile.md) 提高了对日志文件完整性的兼容，适配训练崩溃等场景\n- 新图表类型-[PR曲线](/api/py-pr_curve.md)、[ROC曲线](/api/py-roc_curve.md)、[混淆矩阵](/api/py-confusion_matrix.md)上线\n- 开放接口新增**获取实验指标**接口\n\n**🤔优化**\n- 增加 日语、俄语 语言支持\n- 实验卡片中的配置表格支持一键折叠/展开\n- 修复了一些问题",
    "551": "一级标题：⚡️更新日志\n二级标题：v0.6.7 - 2025.7.17\n内容：\n**🚀新增功能**\n- 更强大的折线图配置，支持灵活配置线型、颜色、粗细、网格和图例位置\n- 支持`swanlab.Video`数据类型，支持记录与可视化GIF格式文件\n- 全局图表仪表盘支持配置Y轴与最大显示实验数\n- 更强大的文本图表，适配大语言模型训练场景\n\n**🤔优化**\n- 最大实验名提升到250个字符\n- 修复了一些问题",
    "552": "一级标题：⚡️更新日志\n二级标题：v0.6.5 - 2025.7.5\n内容：\n**🚀新增功能**\n- 支持**resume断点续训**\n- 支持小折线图局部放大\n- 支持配置单个折线图平滑\n\n**⚙️优化**\n- 大幅改进了图像图表放大后的交互效果\n\n**🔌集成**\n- 🤗集成[accelerate](https://github.com/huggingface/accelerate)框架，[文档](/guide_cloud/integration/integration-huggingface-accelerate.md)增强分布式训练中的实验记录体验；\n- 集成[ROLL](https://github.com/alibaba/ROLL)框架，[文档](/guide_cloud/integration/integration-roll.md)增强分布式训练中的实验记录体验；\n- 集成[Ray](https://github.com/ray-project/ray)框架，[文档](/guide_cloud/integration/integration-ray.md)增强分布式训练中的实验记录体验；\n\n**🔌插件**\n- 新增`LogdirFileWriter`插件，支持将文件写入到日志文件夹\n\n\n**生态**\n- 阿里云计算巢服务上架：[指引](/guide_cloud/self_host/alibabacloud-computenest.md)",
    "553": "一级标题：⚡️更新日志\n二级标题：v0.6.4 - 2025.6.18\n内容：\n**🚀新增功能**\n- 新增与[AREAL](https://github.com/inclusionAI/AReaL)框架的集成，[PR](https://github.com/inclusionAI/AReaL/pull/98)\n- 支持鼠标Hover到侧边栏实验时，高亮相应曲线\n- 支持跨组对比折线图\n- 启用渐进式图表渲染，提高页面加载速度\n- 支持设置实验名裁剪规则\n\n**⚙️修复**\n- 修复了`local`模式下，日志文件无法正确`sync`和`watch`的问题",
    "554": "一级标题：⚡️更新日志\n二级标题：v0.6.3 - 2025.6.12\n内容：\n**🚀新增功能**\n- 新增`swnalab.echarts.table`，支持创建表格图表\n- 昇腾/沐曦/海光/寒武纪/昆仑芯 硬件监控 增加显存（MB）记录\n- `swanlab sync`支持一次多日志上传\n- 工作区增加`公开/私有`筛选\n- 表格视图增加`最新/最大/最小值`切换模块",
    "555": "一级标题：⚡️更新日志\n二级标题：v0.6.2 - 2025.6.9\n内容：\n**🚀新增功能**\n- 新增`swanlab sync`命令，支持将本地日志同步到SwanLab云端/私有化部署端\n- 支持在本地存储完整的实验日志文件",
    "556": "一级标题：⚡️更新日志\n二级标题：v0.6.1 - 2025.6.5\n内容：\n**🚀新增功能**\n- 鼠标放到表头，可以显示缩略的名称了\n- 表格视图增加「展开子表」功能\n- 硬件监控支持海光DCU\n- 硬件监控支持获取昇腾NPU的功耗信息\n\n**🤔优化**\n- 优化了HuggigngFace accelerate框架的集成\n- 默认不再打印重复step log warning",
    "557": "一级标题：⚡️更新日志\n二级标题：v0.6.0 - 2025.6.1\n内容：\n**🚀新增功能**\n- 支持 **图表自由拖拽**\n- 支持ECharts自定义图表，增加包括柱状图、饼状图、直方图在内的20+图表类型\n- 硬件监控已支持 **沐曦** 显卡\n- 集成 [PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP) 框架",
    "558": "一级标题：⚡️更新日志\n二级标题：v0.5.9 - 2025.5.25\n内容：\n**🚀新增功能**\n-  📒 日志记录支持了标准错误流，EvalScope / PyTorch Lightning等这些框架的日志记录体验大幅提升\n-  💻 硬件监控已支持 **摩尔线程** 显卡\n-  🔐 新增运行命令记录的安全防护功能，API Key将被自动隐藏\n-  ⚙️ 设置新增「默认空间」和「默认可见性」配置，可以指定你的项目默认创建在哪个组织下啦！",
    "559": "一级标题：⚡️更新日志\n二级标题：v0.5.8 - 2025.5.13\n内容：\n**🚀新增功能**\n\n- 新增**实验Tag**功能\n- 新增折线图 **Log Scale** 功能\n- 新增 **实验分组拖拽** 功能\n- 新增实验卡片中**配置**与**指标**表格下载功能\n- 新增[开放接口](/zh/api/py-openapi.md)，支持通过API获取SwanLab数据\n- 大幅优化了指标传输性能，提升上千指标的传输速度\n- 集成`paddlenlp`框架\n\n**🤔优化**\n- 优化了个人主页的一系列交互\n\n**生态**\n- 腾讯云云应用上架：[指引](/zh/guide_cloud/self_host/tencentcloud-app.md)",
    "560": "一级标题：⚡️更新日志\n二级标题：v0.5.6 - 2025.4.23\n内容：\n**🚀新增功能**\n\n- 折线图支持**图表配置**功能，本次更新支持配置图表的X、Y轴范围；主标题；X、Y轴标题\n- 图表搜索支持**正则表达式**\n- SwanLab私有化部署版，已支持离线激活验证\n- 支持**昆仑芯XPU**的环境记录与硬件监控\n- 适配对使用`uv`环境下的pip环境记录\n- 环境记录支持记录**Linux发行版**（如Ubuntu、CentOS、Kylin等）\n\n**🤔优化**\n- 修复了侧边栏一键隐藏实验的一些问题",
    "561": "一级标题：⚡️更新日志\n二级标题：v0.5.5 - 2025.4.7\n内容：\n**🚀新增功能**\n- 新增`swanlab.Molecule`数据类型，支持生物化学分子可视化，为AlphaFold等AI4Science训练任务提供更好的训练体验\n- 实验表格，现在支持记忆你的排序、筛选、列拖拽了！\n- 支持了寒武纪MLU的温度和功率指标记录\n- 新增SWANLAB_PROJ、SWANLAB_WORKSPACE、SWANLAB_EXP_NAME三个环境变量\n- 环境中支持显示寒武纪MLU Logo\n\n**🌍生态**\n- 大模型评估框架[EvalScope](https://github.com/modelscope/evalscope) 已集成SwanLab！：https://github.com/modelscope/evalscope/pull/453\n\n**🤔优化**\n- 优化了网页加载性能",
    "562": "一级标题：⚡️更新日志\n二级标题：v0.5.4 - 2025.3.31\n内容：\n**🚀新增功能**\n- 新增`swanlab.Settings`方法，支持更精细化的实验行为控制，进一步增强开放性\n- 支持了寒武纪MLU的硬件记录和资源监控\n- 昇腾NPU的硬件记录支持记录CANN版本\n- 英伟达GPU的硬件记录支持记录GPU架构和cuda核心数\n- 英伟达GPU的硬件监控支持记录“GPU 访问内存所花费的时间百分比”\n- 「个人主页」支持显示你所在的「组织」\n- 「概览」页支持编辑\"项目描述\"文本\n\n**🤔优化**\n- 修复了sync_wandb的一些问题\n- 修复了Obejct3D类的一些问题\n- 优化「常规」设置样式\n- 大幅优化了打开项目的性能\n\n**🔌插件**\n- 官方插件增加Slack通知、Discord通知，进一步打通海外生态",
    "563": "一级标题：⚡️更新日志\n二级标题：v0.5.3 - 2025.3.20\n内容：\n![swanlab x huggingface](./changelog/hf.png)\n\n**🚀新增功能**\n- SwanLab已正式加入 **🤗HuggingFace生态**！Transformers 4.50.0版本开始 正式将SwanLab集成为实验跟踪工具，在TrainingArguments中加入`report_to=\"swanlab\"`即可开始跟踪训练。\n- 新增了`swanlab.Object3D`，支持记录三维点云，[文档](/api/py-object3d)\n- 硬件监控支持了 GPU显存（MB）、磁盘利用率、网络上下行 的记录\n\n**优化**\n- 修复了一些问题",
    "564": "一级标题：⚡️更新日志\n二级标题：v0.5.0 - 2025.3.12\n内容：\n![logo](../self_host/docker-deploy/swanlab-docker.jpg)\n\n**🎉🎉SwanLab私有化部署（社区版）现已重磅发布！！**[部署文档](/guide_cloud/self_host/docker-deploy.md)\n\n**🚀新增功能**\n- `swanlab.init`新增参数`callbacks`，支持在初始化时注册回调函数，以支持各式各样的自定义插件类\n- 新增`swanlab.register_callback()`，支持在`init`外部注册回调函数，[文档](/api/py-register-callback.html)\n- `swanlab.login()`升级，新增`host`、`web_host`、`save`参数，适配了私有化部署服务的特性，同时支持不将用户登录凭证写入本地，以适应共用服务器场景。[文档](/zh/api/py-login.md)\n- `swanlab login`升级，新增`host`、`web_host`、`api-key`参数，[文档](/zh/api/cli-swanlab-login.md)\n- 新增支持使用`swanlab.sync_mlflow()`将MLFlow项目同步到SwanLab，[文档](/guide_cloud/integration/integration-mlflow.md)\n\n**🤔优化**\n- 我们大幅优化了sdk架构，提升了sdk在大量metric场景下的性能\n- 实验侧边栏可以拉伸了！\n- 实验页面右上角增加了「Git代码」按钮，一键跳转到对应的仓库\n\n**🔌插件**：\n- 新增**通知类插件**，支持在训练结束时使用**邮件、飞书、钉钉、企业微信**进行通知\n- 新增**记录类插件**，支持在训练过程中将元数据、配置、指标写入到**本地CSV文件**",
    "565": "一级标题：⚡️更新日志\n二级标题：v0.4.12 - 2025.3.8\n内容：\n**优化**\n- 修复了一些问题",
    "566": "一级标题：⚡️更新日志\n二级标题：v0.4.11 - 2025.3.5\n内容：\n**优化**\n- 修复了部分版本W&B格式转换报错的问题\n- 修复了一些交互问题",
    "567": "一级标题：⚡️更新日志\n二级标题：v0.4.10 - 2025.3.4\n内容：\n**🚀新增功能**\n- 新增了和[DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio)的集成, [文档](/guide_cloud/integration/integration-diffsynth-studio.md)\n- 新增支持转换 **MLFlow** 实验到 SwanLab，[文档](/guide_cloud/integration/integration-mlflow.md)\n- 新增**项目描述**，支持给你的项目记一些简短的笔记\n\n**优化**\n- 修复了在OpenEuler系统上无法正确记录CPU型号的问题",
    "568": "一级标题：⚡️更新日志\n二级标题：v0.4.9 - 2025.2.28\n内容：\n**🚀新增功能**\n- 新增了`移动实验`功能\n- 对一些集成Callback类增加了`update_config`方法\n- `run`新增`get_url()`和`get_project_url()`方法，支持获取实验和项目的URL\n\n**优化**\n- 修复了在部分Linux系统上CPU品牌获取不到的问题",
    "569": "一级标题：⚡️更新日志\n二级标题：v0.4.8 - 2025.2.16\n内容：\n**🚀新增功能**\n- 新增了和Modelscope Swift的集成，[文档](/guide_cloud/integration/integration-swift.md)\n- 新增了`添加分组`和`移动图表到其他分组`功能\n\n**优化**\n- 修复了sdk的一些问题",
    "570": "一级标题：⚡️更新日志\n二级标题：v0.4.7 - 2025.2.11\n内容：\n**🚀新增功能**\n- `swanlab.log`支持了参数`print_to_console`，开启后可以将`swanlab.log`的`key`、`value`以字典的形式打印到终端\n- `swanlab.init`支持了对`name`、`notes`参数的适配，等价于`experiment_name`和`description`",
    "571": "一级标题：⚡️更新日志\n二级标题：v0.4.6 - 2025.2.3\n内容：\n**🚀新增功能**\n- 新增与LLM强化学习框架[verl](https://github.com/volcengine/verl)的集成，[文档](/guide_cloud/integration/integration-verl.md)\n- `swanlab.log`支持了嵌套字典传入\n\n**优化**\n- 优化了在PyTorch Lightning框架下的分布式训练优化",
    "572": "一级标题：⚡️更新日志\n二级标题：v0.4.5 - 2025.1.22\n内容：\n**🚀新增功能**\n- 新增`swanlab.sync_tensorboardX()`和`swanlab.sync_tensorboard_torch()`：支持使用TensorboardX或PyTorch.utils.tensorboard跟踪实验时，同步指标到SwanLab\n\n**优化**\n- 优化了`sync_wandb()`的代码兼容性",
    "573": "一级标题：⚡️更新日志\n二级标题：v0.4.3 - 2025.1.17\n内容：\n**🚀新增功能**\n- 新增`swanlab.sync_wandb()`：支持使用Weights&Biases跟踪实验时，同步指标到SwanLab，[文档](/guide_cloud/integration/integration-wandb.md)\n- 新增在使用框架集成时，配置项将记录所使用的框架\n\n**优化**\n- 改进了表格视图的交互，增加了行列拖拽、筛选、排序交互\n- 大幅优化了工作区加载的性能\n- 大幅优化了日志渲染的性能\n- 改进了在未登录的计算机上，执行`swanlab.init()`的交互\n- 修复了一些已知问题",
    "574": "一级标题：⚡️更新日志\n二级标题：元旦节更新\n内容：\n**🚀新增功能**\n- 升级了图表平滑，网页刷新后状态将仍然保留\n- 更新了图表大小修改，现在可以通过拖拽图表的右下角来改变大小\n\n**⚙️问题修复**\n- 修复了没有实验时，项目设置不显示删除的bug",
    "575": "一级标题：⚡️更新日志\n二级标题：v0.4.2 - 2024.12.24\n内容：\n**🚀新增功能**\n- 新增密码登录\n- 新增项目设置页\n\n**优化**\n- 修复在一些设备上运行硬件监控会warning的问题",
    "576": "一级标题：⚡️更新日志\n二级标题：v0.4.0 - 2024.12.15\n内容：\n🎉万众期待的硬件监控功能（云端版）已经上线，支持**CPU、NPU、GPU**的系统级信息监控：\n\n- **CPU**：利用率、线程数\n- **内存**：利用率、进程利用率、可用内存\n- **Nvidia GPU**：利用率、显存分配、温度、功耗\n- **Ascend NPU**：利用率、HBM分配、温度\n\n更多信息的监控已经在路上！\n\nby Cunyue",
    "577": "一级标题：⚡️更新日志\n二级标题：v0.3.28 - 2024.12.6\n内容：\n> 🍥公告：硬件监控功能即将推出！\n\n**🚀新增功能**\n- 新增与LightGBM的集成\n- 新增与XGBoost的集成\n\n**优化**\n- 提高了对日志记录时单行长度的限制\n- 改善了部分性能，为0.4.0版本做准备",
    "578": "一级标题：⚡️更新日志\n二级标题：v0.3.27 - 2024.11.26\n内容：\n**🚀新增功能**\n- 新增华为昇腾NPU显卡检测\n- 新增与青云基石智算(Coreshub)的集成",
    "579": "一级标题：⚡️更新日志\n二级标题：新UI上线！\n内容：\n![alt text](/assets/new-homepage.png)\n\n**🚀我们改进了什么**\n- 从用户体验出发，上线全新的官网和UI界面\n- 上线个人/组织主页\n- 增加「黑夜模式」\n- 全面优化的「新手快速开始」，增加了框架集成和案例\n- 优化「图表对比视图」的实验选择逻辑",
    "580": "一级标题：⚡️更新日志\n二级标题：v0.3.25 - 2024.11.11\n内容：\n**🚀新增功能**\n- 🎉[VSCode插件](https://marketplace.visualstudio.com/items?itemName=SwanLab.swanlab&ssr=false#overview)已上线\n- 新增与Keras框架的集成\n- 新增`run.public`方法，支持获取实验的项目名、实验名、链接等信息，[#732](https://github.com/SwanHubX/SwanLab/pull/732)",
    "581": "一级标题：⚡️更新日志\n二级标题：v0.3.22 - 2024.10.18\n内容：\n**🚀新增功能**\n- 🎉基线社区Beta版本已上线：https://swanlab.cn/benchmarks\n- 新增与PaddleYolo的集成，[文档](/guide_cloud/integration/integration-paddleyolo.md)\n\n**修复问题**\n- 修复了在多组并行实验提交时，出现sqlite并行读写报错的问题，[#715](https://github.com/SwanHubX/SwanLab/issues/715)\n- 修复了在CPU品牌记录的兼容性问题",
    "582": "一级标题：⚡️更新日志\n二级标题：v0.3.21 - 2024.9.26\n内容：\n**🚀新增功能**\n- [组织创建](/guide_cloud/general/organization.md)已全面开放，每个组织上限为15人。\n- 实验名现已支持「重名」，并使用新的一套新建实验名体系。",
    "583": "一级标题：⚡️更新日志\n二级标题：v0.3.19 - 2024.9.2\n内容：\n**🚀新增功能**\n- （内测）新增任务式训练`swanlab task`的网盘存储功能\n\n**优化**\n- 【环境】增加对CPU品牌的记录\n\n**问题修复**\n- 修复了在Win命令行下`swanlab login`容易出现误操作引发的问题",
    "584": "一级标题：⚡️更新日志\n二级标题：v0.3.17 - 2024.8.18\n内容：\n1. 完成了对云端图表库以及前端的代码重构，改进了大量交互\n2. 修复了实验表格中侧边栏未加载实验没有正常显示参数的问题\n3. 修复了requests包引起的部分用户网络连接错误的问题\n4. 【环境】增加对NVIDIA驱动版本的记录\n5. 本地看版支持对已占用的端口自动续新端口了",
    "585": "一级标题：⚡️更新日志\n二级标题：v0.3.16 - 2024.7.31\n内容：\n**🚀新增功能**\n- （内测）新增任务式训练`swanlab task`功能\n- 新增与`torchtune`的集成，[文档](/guide_cloud/integration/integration-pytorch-torchtune)\n\n**优化**\n- `swanlab.init`增加参数`public`，可用于设置创建的新项目的可见性，默认为`False`\n- 用`swanlab.init`创建的项目默认可见性改为私有\n- 新增了`swanlab.config`对`dataclass`类型的支持\n\n**问题修复**\n- 修复了在conda-forge环境下import swanlab会提示缺乏依赖库的问题",
    "586": "一级标题：⚡️更新日志\n二级标题：v0.3.14 - 2024.7.20\n内容：\n**问题修复**\n- 修复环境依赖安装问题\n- 修复在Windows系统上存在的一些适配问题",
    "587": "一级标题：⚡️更新日志\n二级标题：v0.3.13 - 2024.6.27\n内容：\n**🚀新增功能**\n- 新增支持修改实验颜色\n\n**⚡️改进**\n- 优化了在Google CoLab、Jupyter Notebook下的一些问题\n- 优化了错误日志收集与打印\n\n**问题修复**\n- 修复了Windows系统下运行的一些问题\n- 修复了在Hydra等框架上的终端打印问题\n- 修复了了在mmengine集成中SwanlabVisBackend的save_dir不能为None的问题",
    "588": "一级标题：⚡️更新日志\n二级标题：v0.3.11 - 2024.6.14\n内容：\n**🚀新增功能**\n- 环境记录增加PID和Python Verbose\n- 支持修改项目可见性\n- 离线看版命令修改为`swanlab watch [LOG PATH]`\n\n**⚡️改进**\n- 优化了Python环境搜索的性能\n- 优化了SwanLab库的架构\n\n**问题修复**\n- 修复了离线看版启动失败的问题",
    "589": "一级标题：⚡️更新日志\n二级标题：v0.3.10 - 2024.6.10\n内容：\n**问题修复**\n- 修复了部分文本上传时会出现编码错误的问题\n- 修复了环境信息没有正确上传的问题",
    "590": "一级标题：⚡️更新日志\n二级标题：v0.3.9 - 2024.6.8\n内容：\n**🚀新增功能**\n- `swanlab logout`：支持在终端退出SwanLab账号\n\n**👥集成**\n- 增加与HuggingFace Accelerate的集成，[文档](/guide_cloud/integration/integration-huggingface-accelerate.md)\n\n**⚡️改进**\n- 改进了媒体文件上传的稳定性\n\n**问题修复**\n- 修复了nvml库的兼容性问题\n- 解决在实验结束时上传大量媒体文件可能引发的409错误\n- 修复了在部分机器上会出现OSError的问题",
    "591": "一级标题：⚡️更新日志\n二级标题：v0.3.8 - 2024.5.31\n内容：\n**⚡️改进**\n- 改进了与ultralytics在ddp场景下的集成\n- swanlab.init时增加最新版本的提示\n\n**问题修复**\n\n- 修复了当log的value为`inf`会导致线程崩溃的问题\n- 修复了训练时间过长时，部分图片上传会失败的问题",
    "592": "一级标题：⚡️更新日志\n二级标题：v0.3.6 - 2024.5.28\n内容：\n**问题修复**\n\n- 修复了部分logging日志无法上传的问题\n- 修复了`swanlab login`无法登陆的问题",
    "593": "一级标题：⚡️更新日志\n二级标题：v0.3.4 - 2024.5.27\n内容：\n**🚀新增功能**\n- `swanlab.init`增加参数`mode`，支持新模式`disabled`\n- 支持批量删除实验\n\n**⚡️改进**\n- 优化ultralytics集成代码\n\n**👥集成**\n- 与Stable Baseline3集成，[指引](/guide_cloud/integration/integration-sb3.md)",
    "594": "一级标题：⚡️更新日志\n二级标题：v0.3.3 - 2024.5.22\n内容：\n**👥集成**\n- 与Weights & Biases集成，支持将wandb项目转换为`SwanLab`项目，[指引](/guide_cloud/integration/integration-wandb.md)\n- 与Ultralytics集成，[指引](/guide_cloud/integration/integration-ultralytics.md)\n- 与fastai集成，[指引](/guide_cloud/integration/integration-fastai.md)",
    "595": "一级标题：⚡️更新日志\n二级标题：v0.3.2 - 2024.5.17\n内容：\n**👥集成**\n- 与Tensorboard集成，支持将`Tensorboard`日志文件转换为`SwanLab`实验，[指引](/guide_cloud/integration/integration-tensorboard.md)\n\n**🚀新增功能**\n- 支持下载折线图为PNG图像\n- SwanLab实验可以被嵌入到在线文档中了（飞书/Notion等支持嵌入网页的在线文档）\n- 表格视图支持导出CSV\n- 表格视图支持仅看指标\n\n**⚡️改进**\n- 优化了折线图与表格视图的数值显示\n\n**⚙️修复问题**\n- 修复了在Windows系统下，`swanlab.config`载入`hydra`配置文件时，config表格的显示Bug\n- 解决SwanLab在jupyter Notebook中的登录问题",
    "596": "一级标题：⚡️更新日志\n二级标题：v0.3.1 - 2024.5.3\n内容：\n**⚡️改进**\n- `swanlog`日志文件夹默认增加一个`.gitignore`\n\n**⚙️修复问题**\n- 修复`swanlab.init`的config不兼容Omegaconfig等类型的问题",
    "597": "一级标题：团队使用SwanLab\n二级标题：无\n内容：",
    "598": "一级标题：团队使用SwanLab\n二级标题：创建组织\n内容：\n在主页的左上方，点击“创建组织”按钮，填写组织名、组织ID等信息，即可完成组织创建。\n\n<div align=\"center\">\n<img src=\"/assets/organization-create.jpg\" width=\"400\">\n</div>",
    "599": "一级标题：团队使用SwanLab\n二级标题：邀请成员\n内容：\n<div align=\"center\">\n<img src=\"./organization/invite.png\">\n</div>\n\n在组织空间下，点击「设置」-「常规」，在「成员」栏下，点击「邀请成员」按钮，将邀请链接分享给要加入组织的成员。\n\n<div align=\"center\">\n<img src=\"./organization/join.png\">\n</div>\n\n成员点击邀请链接，提交申请后，经管理员审核通过，即可完成加入。",
    "600": "一级标题：团队使用SwanLab\n二级标题：将实验上传到组织空间\n内容：\n在默认情况下（即不设置`workspace`参数），你的项目会被上传到个人空间下。\n想要上传到组织空间下，则将`swanlab.init`的`workspace`参数设置为组织的组织名（不是组织昵称）即可。\n\n```python\nimport swanlab\n\nswanlab.init(\n    workspace=\"[组织名username]\"\n)\n```\n\n如果组织里的多个人想要在一个项目下协作，则只需要将`swanlab.init`的`project`参数设置为同一个即可。",
    "601": "一级标题：🚀快速开始\n二级标题：无\n内容：\n安装 SwanLab 并在几分钟内开始跟踪你的人工智能实验。\n\n![quick-start-1](./quick_start/quick-start.png)",
    "602": "一级标题：🚀快速开始\n二级标题：1. 安装SwanLab\n内容：\n使用 [pip](https://pip.pypa.io/en/stable/) 在Python3环境的计算机上安装swanlab库。\n\n打开命令行，输入：\n\n```bash\npip install swanlab\n```\n\n按下回车，等待片刻完成安装。\n\n> 如果遇到安装速度慢的问题，可以指定国内源安装：\n> `pip install swanlab -i https://mirrors.cernet.edu.cn/pypi/web/simple`",
    "603": "一级标题：🚀快速开始\n二级标题：2. 登录账号\n内容：\n> 如果你还没有SwanLab账号，请在 [官网](https://swanlab.cn) 免费注册。\n\n打开命令行，输入：\n\n```bash\nswanlab login\n```\n\n当你看到如下提示时：\n\n```bash\nswanlab: Logging into swanlab cloud.\nswanlab: You can find your API key at: https://swanlab.cn/settings\nswanlab: Paste an API key from your profile and hit enter, or press 'CTRL-C' to quit:\n```\n\n在[用户设置](https://swanlab.cn/settings)页面复制您的 **API Key**，粘贴后按下回车（你不会看到粘贴后的API Key，请放心这是正常的），即可完成登录。之后无需再次登录。\n\n::: info\n\n如果你的计算机不太支持`swanlab login`的登录方式，也可以使用python脚本登录：\n\n```python\nimport swanlab\nswanlab.login(api_key=\"你的API Key\", save=True)\n```\n\n:::",
    "604": "一级标题：🚀快速开始\n二级标题：3. 开启一个实验并跟踪超参数\n内容：\n在Python脚本中，我们用`swanlab.init`创建一个SwanLab实验，并向`config`参数传递将一个包含超参数键值对的字典：\n\n```python\nimport swanlab\n\nrun = swanlab.init(\n    # 设置项目\n    project=\"my-project\",\n    # 跟踪超参数与实验元数据\n    config={\n        \"learning_rate\": 0.01,\n        \"epochs\": 10,\n    },\n)\n```\n\n`run`是SwanLab的基本组成部分，你将经常使用它来记录与跟踪实验指标。",
    "605": "一级标题：🚀快速开始\n二级标题：4. 记录实验指标\n内容：\n在Python脚本中，用`swanlab.log`记录实验指标（比如准确率acc和损失值loss）。\n\n用法是将一个包含指标的字典传递给`swanlab.log`：\n\n```python\nswanlab.log({\"accuracy\": acc, \"loss\": loss})\n```",
    "606": "一级标题：🚀快速开始\n二级标题：5. 完整代码，在线查看可视化看板\n内容：\n我们将上面的步骤整合为下面所示的完整代码：\n\n```python (5,25)\nimport swanlab\nimport random\n\n# 初始化SwanLab\nrun = swanlab.init(\n    # 设置项目\n    project=\"my-project\",\n    # 跟踪超参数与实验元数据\n    config={\n        \"learning_rate\": 0.01,\n        \"epochs\": 10,\n    },\n)\n\nprint(f\"学习率为{run.config.learning_rate}\")\n\noffset = random.random() / 5\n\n# 模拟训练过程\nfor epoch in range(2, run.config.epochs):\n    acc = 1 - 2**-epoch - random.random() / epoch - offset\n    loss = 2**-epoch + random.random() / epoch + offset\n    print(f\"epoch={epoch}, accuracy={acc}, loss={loss}\")\n    # 记录指标\n    swanlab.log({\"accuracy\": acc, \"loss\": loss})\n```\n\n运行代码，访问[SwanLab](https://swanlab.cn)，查看在每个训练步骤中，你使用SwanLab记录的指标（准确率和损失值）的改进情况。\n\n![quick-start-1](./quick_start/line-chart.png)",
    "607": "一级标题：🚀快速开始\n二级标题：下一步是什么\n内容：\n1. 查看SwanLab如何[记录多媒体内容](/guide_cloud/experiment_track/log-media)（图片、音频、文本、...）\n1. 查看SwanLab记录[MNIST手写体识别](/examples/mnist.md)的案例\n2. 查看与其他框架的[集成](/guide_cloud/integration/index.md)\n3. 查看如何通过SwanLab与[团队协作](/guide_cloud/general/organization.md)",
    "608": "一级标题：🚀快速开始\n二级标题：常见问题\n内容：\n### 1. 在哪里可以找到我的API Key？\n\n登陆SwanLab网站后，API Key将显示在[用户设置](https://swanlab.cn/settings)页面上。\n\n### 2. 我可以离线使用SwanLab吗？\n\n可以，具体流程请查看[自托管部分](/guide_cloud/self_host/docker-deploy.md)。",
    "609": "一级标题：欢迎使用SwanLab\n二级标题：无\n内容：\n[官网](https://swanlab.cn) · [框架集成](/guide_cloud/integration/integration-huggingface-transformers.html) · [Github](https://github.com/swanhubx/swanlab) · [快速开始](/guide_cloud/general/quick-start.md) · [同步Wandb](/guide_cloud/integration/integration-wandb.md#_1-同步跟踪) · [基线社区](https://swanlab.cn/benchmarks)\n\n::: warning 🎉 私有化部署版正式上线！\n私有化部署版支持在本地使用到与公有云版体验相当的功能，部署方式见[此文档](/guide_cloud/self_host/docker-deploy.md)\n:::\n\n![alt text](/assets/product-swanlab-1.png)\n\nSwanLab 是一款**开源、轻量**的 AI 模型训练跟踪与可视化工具，提供了一个**跟踪、记录、比较、和协作实验**的平台。\n\nSwanLab 面向人工智能研究者，设计了友好的Python API 和漂亮的UI界面，并提供**训练可视化、自动日志记录、超参数记录、实验对比、多人协同等功能**。在SwanLab上，研究者能基于直观的可视化图表发现训练问题，对比多个实验找到研究灵感，并通过**在线网页**的分享与基于组织的**多人协同训练**，打破团队沟通的壁垒，提高组织训练效率。\n\n借助SwanLab，科研人员可以沉淀自己的每一次训练经验，与合作者无缝地交流和协作，机器学习工程师可以更快地开发可用于生产的模型。",
    "610": "一级标题：欢迎使用SwanLab\n二级标题：📹在线演示\n内容：\n| [ResNet50 猫狗分类][demo-cats-dogs] | [Yolov8-COCO128 目标检测][demo-yolo] |\n| :--------: | :--------: |\n| [![][demo-cats-dogs-image]][demo-cats-dogs] | [![][demo-yolo-image]][demo-yolo] |\n| 跟踪一个简单的 ResNet50 模型在猫狗数据集上训练的图像分类任务。 | 使用 Yolov8 在 COCO128 数据集上进行目标检测任务，跟踪训练超参数和指标。 |\n\n| [Qwen2 指令微调][demo-qwen2-sft] | [LSTM Google 股票预测][demo-google-stock] |\n| :--------: | :--------: |\n| [![][demo-qwen2-sft-image]][demo-qwen2-sft] | [![][demo-google-stock-image]][demo-google-stock] |\n| 跟踪 Qwen2 大语言模型的指令微调训练，完成简单的指令遵循。 | 使用简单的 LSTM 模型在 Google 股价数据集上训练，实现对未来股价的预测。 |\n\n| [ResNeXt101 音频分类][demo-audio-classification] | [Qwen2-VL COCO数据集微调][demo-qwen2-vl] |\n| :--------: | :--------: |\n| [![][demo-audio-classification-image]][demo-audio-classification] | [![][demo-qwen2-vl-image]][demo-qwen2-vl] |\n| 从ResNet到ResNeXt在音频分类任务上的渐进式实验过程 | 基于Qwen2-VL多模态大模型，在COCO2014数据集上进行Lora微调。 |\n\n| [EasyR1 多模态LLM RL训练][demo-easyr1-rl] | [Qwen2.5-0.5B GRPO训练][demo-qwen2-grpo] |\n| :--------: | :--------: |\n| [![][demo-easyr1-rl-image]][demo-easyr1-rl] | [![][demo-qwen2-grpo-image]][demo-qwen2-grpo] |\n| 使用EasyR1框架进行多模态LLM RL训练 | 基于Qwen2.5-0.5B模型在GSM8k数据集上进行GRPO训练 |\n\n视频Demo：\n\n<video controls src=\"./what_is_swanlab/demo.mp4\"></video>",
    "611": "一级标题：欢迎使用SwanLab\n二级标题：SwanLab能做什么？\n内容：\n**1. 📊 实验指标与超参数跟踪**: 极简的代码嵌入您的机器学习 pipeline，跟踪记录训练关键指标\n\n- ☁️ 支持**云端**使用（类似Weights & Biases），随时随地查看训练进展。[手机看实验的方法](https://docs.swanlab.cn/guide_cloud/general/app.html)\n- 🌸 **可视化训练过程**: 通过UI界面对实验跟踪数据进行可视化，可以让训练师直观地看到实验每一步的结果，分析指标走势，判断哪些变化导致了模型效果的提升，从而整体性地提升模型迭代效率。\n- 📝 **超参数记录**、**指标总结**、**表格分析**\n- **支持的元数据类型**：标量指标、图像、音频、文本、视频、3D点云、生物化学分子、Echarts自定义图表...\n\n![swanlab-table](/assets/molecule.gif)\n\n- **支持的图表类型**：折线图、媒体图（图像、音频、文本）、3D点云、生物化学分子、柱状图、散点图、箱线图、热力图、饼状图、雷达图...\n\n![swanlab-echarts](./what_is_swanlab/echarts.png)\n\n- **LLM生成内容可视化组件**：为大语言模型训练场景打造的文本内容可视化图表，支持Markdown渲染\n\n![swanlab-llm-content](/assets/text-chart.gif)\n\n- **后台自动记录**：日志logging、硬件环境、Git 仓库、Python 环境、Python 库列表、项目运行目录\n- **断点续训记录**：支持在训练完成/中断后，补充新的指标数据到同个实验中\n\n\n**2. ⚡️ 全面的框架集成**: PyTorch、🤗HuggingFace Transformers、PyTorch Lightning、🦙LLaMA Factory、MMDetection、Ultralytics、PaddleDetetion、LightGBM、XGBoost、Keras、Tensorboard、Weights&Biases、OpenAI、Swift、XTuner、Stable Baseline3、Hydra 在内的 **40+** 框架\n\n![](/assets/integrations.png)\n\n**3. 💻 硬件监控**: 支持实时记录与监控CPU、GPU（**英伟达Nvidia**、**沐曦MetaX**、**摩尔线程MooreThread**）、NPU（**昇腾Ascend**）、MLU（**寒武纪MLU**）、XPU（**昆仑芯KunlunX**）、内存的系统级硬件指标\n\n**4. 📦 实验管理**: 通过专为训练场景设计的集中式仪表板，通过整体视图速览全局，快速管理多个项目与实验\n\n**5. 🆚 比较结果**: 通过在线表格与对比图表比较不同实验的超参数和结果，挖掘迭代灵感\n\n![](./what_is_swanlab/chart3.png)\n\n**6. 👥 在线协作**: 您可以与团队进行协作式训练，支持将实验实时同步在一个项目下，您可以在线查看团队的训练记录，基于结果发表看法与建议\n\n**7. ✉️ 分享结果**: 复制和发送持久的 URL 来共享每个实验，方便地发送给伙伴，或嵌入到在线笔记中\n\n**8. 💻 支持自托管**: 支持离线环境使用，自托管的社区版同样可以查看仪表盘与管理实验，[使用攻略](#-自托管)\n\n**9. 🔌 插件拓展**: 支持通过插件拓展SwanLab的使用场景，比如 [飞书通知](https://docs.swanlab.cn/plugin/notification-lark.html)、[Slack通知](https://docs.swanlab.cn/plugin/notification-slack.html)、[CSV记录器](https://docs.swanlab.cn/plugin/writer-csv.html)等",
    "612": "一级标题：欢迎使用SwanLab\n二级标题：为什么使用SwanLab？\n内容：\n与软件工程不同，人工智能是一个**实验性学科**，产生灵感、快速试验、验证想法 是AI研究的主旋律。而记录下实验过程和灵感，就像化学家记录实验手稿一样，是每一个AI研究者、研究组织**形成积累、提升加速度**的核心。\n\n先前的实验记录方法，是在计算机前盯着终端打印的输出，复制粘贴日志文件（或TFEvent文件），**粗糙的日志对灵感的涌现造成了障碍，离线的日志文件让研究者之间难以形成合力**。\n\n与之相比，SwanLab提供了一套云端AI实验跟踪方案，面向训练过程，提供了训练可视化、实验跟踪、超参数记录、日志记录、多人协同等功能，研究者能轻松**通过直观的可视化图表找到迭代灵感，并且通过在线链接的分享与基于组织的多人协同训练**，打破团队沟通的壁垒。\n\n> 以往的AI研究的分享和开源更关注结果，而我们更关注过程。<br>\n> 社区用户对SwanLab的产品评价可以归结为**简洁易用、提升效率与迭代迅速**<br>\n> ——泽毅，SwanLab 联合创始人\n\n<img src=\"./what_is_swanlab/carton.png\" width=\"350\">\n\n更重要的是，SwanLab是开源的，由一帮热爱开源的机器学习工程师与社区共同构建，我们提供了完全自托管的版本，可以保证你的数据安全与隐私性。\n\n希望以上信息和这份指南可以帮助你了解这款产品，我们相信 SwanLab 能够帮助到你。",
    "613": "一级标题：欢迎使用SwanLab\n二级标题：从哪里开始\n内容：\n- [快速开始](/guide_cloud/general/quick-start.md): SwanLab入门教程，五分钟玩转实验跟踪！\n- [API文档](/api/api-index.md): 完整的API文档\n- [在线支持](/guide_cloud/community/online-support.md): 加入社区、反馈问题和联系我们\n- [自托管](/guide_cloud/self_host/docker-deploy.md): 自托管（私有化部署）使用方式教程\n- [案例](/examples/mnist.md): 查看SwanLab与各个深度学习任务的案例",
    "614": "一级标题：欢迎使用SwanLab\n二级标题：与熟悉产品的对比\n内容：\n### Tensorboard vs SwanLab\n\n- **☁️支持在线使用**：\n  通过SwanLab可以方便地将训练实验在云端在线同步与保存，便于远程查看训练进展、管理历史项目、分享实验链接、发送实时消息通知、多端看实验等。而Tensorboard是一个离线的实验跟踪工具。\n\n- **👥多人协作**：\n  在进行多人、跨团队的机器学习协作时，通过SwanLab可以轻松管理多人的训练项目、分享实验链接、跨空间交流讨论。而Tensorboard主要为个人设计，难以进行多人协作和分享实验。\n\n- **💻持久、集中的仪表板**：\n  无论你在何处训练模型，无论是在本地计算机上、在实验室集群还是在公有云的GPU实例中，你的结果都会记录到同一个集中式仪表板中。而使用TensorBoard需要花费时间从不同的机器复制和管理 TFEvent文件。\n\n- **💪更强大的表格**：\n  通过SwanLab表格可以查看、搜索、过滤来自不同实验的结果，可以轻松查看数千个模型版本并找到适合不同任务的最佳性能模型。 TensorBoard 不适用于大型项目。\n\n\n### W&B vs SwanLab\n\n- Weights and Biases 是一个必须联网使用的闭源MLOps平台\n\n- SwanLab 不仅支持联网使用，也支持开源、免费、自托管的版本",
    "615": "一级标题：欢迎使用SwanLab\n二级标题：训练框架集成\n内容：\n将你最喜欢的框架与 SwanLab 结合使用！\n下面是我们已集成的框架列表，欢迎提交 [Issue](https://github.com/swanhubx/swanlab/issues) 来反馈你想要集成的框架。\n\n**基础框架**\n- [PyTorch](/guide_cloud/integration/integration-pytorch.html)\n- [MindSpore](/guide_cloud/integration/integration-ascend.html)\n- [Keras](/guide_cloud/integration/integration-keras.html)\n\n**专有/微调框架**\n- [PyTorch Lightning](/guide_cloud/integration/integration-pytorch-lightning.html)\n- [HuggingFace Transformers](/guide_cloud/integration/integration-huggingface-transformers.html)\n- [LLaMA Factory](/guide_cloud/integration/integration-llama-factory.html)\n- [Modelscope Swift](/guide_cloud/integration/integration-swift.html)\n- [DiffSynth-Studio](/guide_cloud/integration/integration-diffsynth-studio.html)\n- [Sentence Transformers](/guide_cloud/integration/integration-sentence-transformers.html)\n- [OpenMind](https://modelers.cn/docs/zh/openmind-library/1.0.0/basic_tutorial/finetune/finetune_pt.html#%E8%AE%AD%E7%BB%83%E7%9B%91%E6%8E%A7)\n- [Torchtune](/guide_cloud/integration/integration-pytorch-torchtune.html)\n- [XTuner](/guide_cloud/integration/integration-xtuner.html)\n- [MMEngine](/guide_cloud/integration/integration-mmengine.html)\n- [FastAI](/guide_cloud/integration/integration-fastai.html)\n- [LightGBM](/guide_cloud/integration/integration-lightgbm.html)\n- [XGBoost](/guide_cloud/integration/integration-xgboost.html)\n\n\n**计算机视觉**\n- [Ultralytics](/guide_cloud/integration/integration-ultralytics.html)\n- [MMDetection](/guide_cloud/integration/integration-mmdetection.html)\n- [MMSegmentation](/guide_cloud/integration/integration-mmsegmentation.html)\n- [PaddleDetection](/guide_cloud/integration/integration-paddledetection.html)\n- [PaddleYOLO](/guide_cloud/integration/integration-paddleyolo.html)\n\n**强化学习**\n- [Stable Baseline3](/guide_cloud/integration/integration-sb3.html)\n- [veRL](/guide_cloud/integration/integration-verl.html)\n- [HuggingFace trl](/guide_cloud/integration/integration-huggingface-trl.html)\n- [EasyR1](/guide_cloud/integration/integration-easyr1.html)\n- [AReaL](/guide_cloud/integration/integration-areal.html)\n- [ROLL](/guide_cloud/integration/integration-roll.html)\n\n**其他框架：**\n- [Tensorboard](/guide_cloud/integration/integration-tensorboard.html)\n- [Weights&Biases](/guide_cloud/integration/integration-wandb.html)\n- [MLFlow](/guide_cloud/integration/integration-mlflow.html)\n- [HuggingFace Accelerate](/guide_cloud/integration/integration-huggingface-accelerate.html)\n- [Hydra](/guide_cloud/integration/integration-hydra.html)\n- [Omegaconf](/guide_cloud/integration/integration-omegaconf.html)\n- [OpenAI](/guide_cloud/integration/integration-openai.html)\n- [ZhipuAI](/guide_cloud/integration/integration-zhipuai.html)\n\n[更多集成](/guide_cloud/integration/integration-pytorch-lightning.html)",
    "616": "一级标题：欢迎使用SwanLab\n二级标题：在线支持\n内容：\n- **[GitHub Issues](https://github.com/SwanHubX/SwanLab/issues)**：反馈使用SwanLab时遇到的错误和问题\n\n- **电子邮件支持**：反馈关于使用SwanLab的问题\n  - 产品: <contact@swanlab.cn>, <zeyi.lin@swanhub.co>(产品经理邮箱)\n\n- **微信群与飞书群**: 见[在线支持](/guide_cloud/community/online-support.md)\n\n- **微信公众号**:\n\n<div align=\"center\">\n<img src=\"/assets/wechat_public_account.jpg\" width=300>\n</div>\n\n\n<!-- link -->\n\n[release-shield]: https://img.shields.io/github/v/release/swanhubx/swanlab?color=369eff&labelColor=black&logo=github&style=flat-square\n[release-link]: https://github.com/swanhubx/swanlab/releases\n\n[license-shield]: https://img.shields.io/badge/license-apache%202.0-white?labelColor=black&style=flat-square\n[license-shield-link]: https://github.com/SwanHubX/SwanLab/blob/main/LICENSE\n\n[last-commit-shield]: https://img.shields.io/github/last-commit/swanhubx/swanlab?color=c4f042&labelColor=black&style=flat-square\n[last-commit-shield-link]: https://github.com/swanhubx/swanlab/commits/main\n\n[pypi-version-shield]: https://img.shields.io/pypi/v/swanlab?color=orange&labelColor=black&style=flat-square\n[pypi-version-shield-link]: https://pypi.org/project/swanlab/\n\n[pypi-downloads-shield]: https://static.pepy.tech/badge/swanlab?labelColor=black&style=flat-square\n[pypi-downloads-shield-link]: https://pepy.tech/project/swanlab\n\n[swanlab-cloud-shield]: https://img.shields.io/badge/Product-SwanLab云端版-636a3f?labelColor=black&style=flat-square\n[swanlab-cloud-shield-link]: https://swanlab.cn/\n\n[wechat-shield]: https://img.shields.io/badge/WeChat-微信-4cb55e?labelColor=black&style=flat-square\n[wechat-shield-link]: https://docs.swanlab.cn/guide_cloud/community/online-support.html\n\n[colab-shield]: https://colab.research.google.com/assets/colab-badge.svg\n[colab-shield-link]: https://colab.research.google.com/drive/1RWsrY_1bS8ECzaHvYtLb_1eBkkdzekR3?usp=sharing\n\n[github-stars-shield]: https://img.shields.io/github/stars/swanhubx/swanlab?labelColor&style=flat-square&color=ffcb47\n[github-stars-link]: https://github.com/swanhubx/swanlab\n\n[github-issues-shield]: https://img.shields.io/github/issues/swanhubx/swanlab?labelColor=black&style=flat-square&color=ff80eb\n[github-issues-shield-link]: https://github.com/swanhubx/swanlab/issues\n\n[github-contributors-shield]: https://img.shields.io/github/contributors/swanhubx/swanlab?color=c4f042&labelColor=black&style=flat-square\n[github-contributors-link]: https://github.com/swanhubx/swanlab/graphs/contributors\n\n[demo-cats-dogs]: https://swanlab.cn/@ZeyiLin/Cats_Dogs_Classification/runs/jzo93k112f15pmx14vtxf/chart\n[demo-cats-dogs-image]: /assets/example-catsdogs.png\n\n[demo-yolo]: https://swanlab.cn/@ZeyiLin/ultratest/runs/yux7vclmsmmsar9ear7u5/chart\n[demo-yolo-image]: /assets/example-yolo.png\n\n[demo-qwen2-sft]: https://swanlab.cn/@ZeyiLin/Qwen2-fintune/runs/cfg5f8dzkp6vouxzaxlx6/chart\n[demo-qwen2-sft-image]: /assets/example-qwen2.png\n\n[demo-google-stock]:https://swanlab.cn/@ZeyiLin/Google-Stock-Prediction/charts\n[demo-google-stock-image]: /assets/example-lstm.png\n\n[demo-audio-classification]:https://swanlab.cn/@ZeyiLin/PyTorch_Audio_Classification/charts\n[demo-audio-classification-image]: /assets/example-audio-classification.png\n\n[demo-qwen2-vl]:https://swanlab.cn/@ZeyiLin/Qwen2-VL-finetune/runs/pkgest5xhdn3ukpdy6kv5/chart\n[demo-qwen2-vl-image]: /assets/example-qwen2-vl.jpg\n\n[demo-easyr1-rl]:https://swanlab.cn/@Kedreamix/easy_r1/runs/wzezd8q36bb6dlza6wtpc/chart\n[demo-easyr1-rl-image]: /assets/example-easyr1-rl.png\n\n[demo-qwen2-grpo]:https://swanlab.cn/@kmno4/Qwen-R1/runs/t0zr3ak5r7188mjbjgdsc/chart\n[demo-qwen2-grpo-image]: /assets/example-qwen2-grpo.png\n\n[tracking-swanlab-shield-link]:https://swanlab.cn\n[tracking-swanlab-shield]: https://raw.githubusercontent.com/SwanHubX/assets/main/badge2.svg\n\n[visualize-swanlab-shield-link]:https://swanlab.cn\n[visualize-swanlab-shield]: https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg\n\n[dockerhub-shield]: https://img.shields.io/docker/v/swanlab/swanlab-next?color=369eff&label=docker&labelColor=black&logoColor=white&style=flat-square\n[dockerhub-link]: https://hub.docker.com/r/swanlab/swanlab-next/tags",
    "617": "一级标题：⚡️集成框架一览\n二级标题：无\n内容：\n将你最喜欢的框架与 SwanLab 结合使用！\n下面是我们已集成的框架列表，欢迎提交 [Issue](https://github.com/swanhubx/swanlab/issues) 来反馈你想要集成的框架。",
    "618": "一级标题：⚡️集成框架一览\n二级标题：基础框架\n内容：\n- [PyTorch](/guide_cloud/integration/integration-pytorch.html)\n- [MindSpore](/guide_cloud/integration/integration-ascend.html)\n- [Keras](/guide_cloud/integration/integration-keras.html)",
    "619": "一级标题：⚡️集成框架一览\n二级标题：专有/微调框架\n内容：\n- [PyTorch Lightning](/guide_cloud/integration/integration-pytorch-lightning.html)\n- [HuggingFace Transformers](/guide_cloud/integration/integration-huggingface-transformers.html)\n- [LLaMA Factory](/guide_cloud/integration/integration-llama-factory.html)\n- [Modelscope Swift](/guide_cloud/integration/integration-swift.html)\n- [DiffSynth-Studio](/guide_cloud/integration/integration-diffsynth-studio.html)\n- [Sentence Transformers](/guide_cloud/integration/integration-sentence-transformers.html)\n- [PaddleNLP](/guide_cloud/integration/integration-paddlenlp.html)\n- [OpenMind](https://modelers.cn/docs/zh/openmind-library/1.0.0/basic_tutorial/finetune/finetune_pt.html#%E8%AE%AD%E7%BB%83%E7%9B%91%E6%8E%A7)\n- [Torchtune](/guide_cloud/integration/integration-pytorch-torchtune.html)\n- [XTuner](/guide_cloud/integration/integration-xtuner.html)\n- [MMEngine](/guide_cloud/integration/integration-mmengine.html)\n- [FastAI](/guide_cloud/integration/integration-fastai.html)\n- [LightGBM](/guide_cloud/integration/integration-lightgbm.html)\n- [XGBoost](/guide_cloud/integration/integration-xgboost.html)",
    "620": "一级标题：⚡️集成框架一览\n二级标题：评估框架\n内容：\n- [EvalScope](/guide_cloud/integration/integration-evalscope.html)",
    "621": "一级标题：⚡️集成框架一览\n二级标题：计算机视觉\n内容：\n- [Ultralytics](/guide_cloud/integration/integration-ultralytics.html)\n- [MMDetection](/guide_cloud/integration/integration-mmdetection.html)\n- [MMSegmentation](/guide_cloud/integration/integration-mmsegmentation.html)\n- [PaddleDetection](/guide_cloud/integration/integration-paddledetection.html)\n- [PaddleYOLO](/guide_cloud/integration/integration-paddleyolo.html)",
    "622": "一级标题：⚡️集成框架一览\n二级标题：强化学习\n内容：\n- [Stable Baseline3](/guide_cloud/integration/integration-sb3.html)\n- [veRL](/guide_cloud/integration/integration-verl.html)\n- [HuggingFace trl](/guide_cloud/integration/integration-huggingface-trl.html)\n- [EasyR1](/guide_cloud/integration/integration-easyr1.html)\n- [AReaL](/guide_cloud/integration/integration-areal.html)\n- [ROLL](/guide_cloud/integration/integration-roll.html)",
    "623": "一级标题：⚡️集成框架一览\n二级标题：其他框架\n内容：\n- [Tensorboard](/guide_cloud/integration/integration-tensorboard.html)\n- [Weights&Biases](/guide_cloud/integration/integration-wandb.html)\n- [MLFlow](/guide_cloud/integration/integration-mlflow.html)\n- [HuggingFace Accelerate](/guide_cloud/integration/integration-huggingface-accelerate.html)\n- [Ray](/guide_cloud/integration/integration-ray.html)\n- [Hydra](/guide_cloud/integration/integration-hydra.html)\n- [Omegaconf](/guide_cloud/integration/integration-omegaconf.html)\n- [OpenAI](/guide_cloud/integration/integration-openai.html)\n- [ZhipuAI](/guide_cloud/integration/integration-zhipuai.html)",
    "624": "一级标题：将SwanLab集成到你的库\n二级标题：无\n内容：\n本指南提供了如何将SwanLab集成到您的Python库中的最佳实践，以获得强大的实验跟踪、GPU和系统监控、超参数记录等功能。\n\n下面我们将介绍，如果您正在处理的代码库比单个 Python 训练脚本或 Jupyter Notebook 更复杂时，我们整理的最佳实践。\n\n**🪵目录：**\n\n[[toc]]",
    "625": "一级标题：将SwanLab集成到你的库\n二级标题：1. 补充Requirements\n内容：\n在开始之前，请决定是否在您的库的依赖项中要求 SwanLab：\n\n### 1.1 将swanlab作为依赖项\n\n```plaintext\ntorch==2.5.0\n...\nswanlab==0.4.*\n```\n\n### 1.2 将swanlab作为可选安装\n\n有两种设置swanlab成为可选安装的方法。\n\n1. 在代码中使用try-except语句，当用户没有安装swanlab时，抛出错误。\n\n```python\ntry:\n    import swanlab\nexcept ImportError:\n    raise ImportError(\n        \"You are trying to use swanlab which is not currently installed.\"\n        \"Please install it using pip install swanlab\"\n    )\n```\n\n2. 如果你要构建Python包，请将`swanlab`作为可选依赖项添加到`pyproject.toml`文件中：\n\n```toml\n[project]\nname = \"my_awesome_lib\"\nversion = \"0.1.0\"\ndependencies = [\n    \"torch\",\n    \"transformers\"\n]\n\n[project.optional-dependencies]\ndev = [\n    \"swanlab\"\n]\n```",
    "626": "一级标题：将SwanLab集成到你的库\n二级标题：2. 用户登录\n内容：\n您的用户有几种方法可以登录SwanLab：\n\n::: code-group\n\n```bash [命令行]\nswanlab login\n```\n\n```python [Python]\nimport swanlab\nswanlab.login()\n```\n\n```bash [环境变量(Bash)]\nexport SWANLAB_API_KEY=$YOUR_API_KEY\n```\n\n```python [环境变量(Python)]\nimport os\nos.environ[\"SWANLAB_API_KEY\"] = \"zxcv1234...\"\n```\n\n:::\n\n如果用户是第一次使用`swanlab`而没有遵循上述任何步骤，则当您的脚本调用`swanlab.init`时，系统会自动提示他们登录。",
    "627": "一级标题：将SwanLab集成到你的库\n二级标题：3. 启动SwanLab实验\n内容：\n实验是SwanLab的计算单元。通常，你可以为每个实验创建一个`Experiment`对象，并使用`swanlab.init`方法启动实验。\n\n### 3.1 初始化实验\n\n初始化SwanLab，并在您的代码种启动实验：\n\n```python\nswanlab.init()\n```\n\n你可以为这个实验提供项目名、实验名、工作空间等参数：\n\n```python\nswanlab.init(\n    project=\"my_project\",\n    experiment_name=\"my_experiment\",\n    workspace=\"my_workspace\",\n    )\n```\n\n::: warning 最好把 swanlab.init 放在哪里？\n\n您的库应该尽早创建SwanLab实验，因为SwanLab会自动收集控制台中的任何输出，这将使得调试更加容易。\n\n:::\n\n### 3.2 配置三种启动模式\n\n你可以通过`mode`参数来配置SwanLab的启动模式：\n\n::: code-group\n\n```python [云端模式]\nswanlab.init(\n    mode=\"cloud\",  # 默认模式\n    )\n```\n\n```python [本地模式]\nswanlab.init(\n    mode=\"local\",\n    )\n```\n\n```python [禁用模式]\nswanlab.init(\n    mode=\"disabled\",\n    )\n```\n\n:::\n\n- **云端模式**：默认模式。SwanLab会将实验数据上传到一个web服务器（SwanLab官方云或您自行部署的私有云）。\n- **本地模式**：SwanLab不会将实验数据上传到云端，但会记录一个特殊的`swanlog`目录，可以被`dashboard`插件打开进行可视化。\n- **禁用模式**：SwanLab不会收集任何数据，代码执行到`swanlab`相关代码时将不做任何处理。\n\n### 3.3 定义实验超参数/配置\n\n使用swanlab实验配置(config)，您可以在创建SwanLab实验时提供有关您的模型、数据集等的元数据。您可以使用这些信息来比较不同的实验并快速了解主要差异。\n\n您可以记录的典型配置参数包括：\n\n- 模型名称、版本、架构参数等\n- 数据集名称、版本、训练/测试数据数等。\n- 训练参数，例如学习率、批量大小、优化器等。\n\n以下代码片段显示了如何记录配置：\n\n```python\nconfig = {\"learning_rate\": 0.001, ...}\nswanlab.init(..., config=config)\n```\n\n**更新配置**：\n\n使用`swanlab.config.update`方法来更新配置。在定义config字典后获取参数时，用此方法更新config字典非常方便。\n\n例如，你可能希望在实例化模型后，添加模型的参数：\n\n```python\nswanlab.config.update({\"model_params\": \"1.5B\"})\n```",
    "628": "一级标题：将SwanLab集成到你的库\n二级标题：4. 记录数据到SwanLab\n内容：\n创建一个字典，其中key是指标的名称，value是指标的值。将此字典对象传递给`swanlab.log`：\n\n::: code-group\n\n```python [记录一组指标]\nmetrics = {\"loss\": 0.5, \"accuracy\": 0.8}\nswanlab.log(metrics)\n```\n\n```python [循环记录指标]\nfor epoch in range(NUM_EPOCHS):\n    for input, ground_truth in data:\n        prediction = model(input)\n        loss = loss_fn(prediction, ground_truth)\n        metrics = { \"loss\": loss }\n        swanlab.log(metrics)\n```\n\n:::\n\n如果您有很多指标，则可以在指标名称中使用前缀（如 `train/...` 和 `val/...`）。在 UI 中，SwanLab将自动对它们进行分组，来隔离不同门类的图表数据：\n\n```python\nmetrics = {\n    \"train/loss\": 0.5,\n    \"train/accuracy\": 0.8,\n    \"val/loss\": 0.6,\n    \"val/accuracy\": 0.7,\n}\nswanlab.log(metrics)\n```\n\n有关`swanlab.log`的更多信息，请参阅[记录指标](../experiment_track/log-experiment-metric)章节。",
    "629": "一级标题：将SwanLab集成到你的库\n二级标题：5. 高级集成\n内容：\n您还可以在以下集成中查看高级 SwanLab 集成的形态：\n\n- [HuggingFace Transformers](../integration/integration-huggingface-transformers.md)\n- [PyTorch Lightning](../integration/integration-pytorch-lightning.md)",
    "630": "一级标题：AREAL\n二级标题：无\n内容：\n[AReaL](https://github.com/inclusionAI/AReaL)（Ant Reasoning RL）是由蚂蚁研究院强化学习实验室（RL Lab） 开发的一套开源 、完全异步的强化学习训练系统， 适用于大型推理模型。该系统基于开源项目 [RealHF](https://github.com/openpsi-project/ReaLHF) 致力于开源，提供训练细节、数据以及复现结果所需的基础设施，并提供模型本身。\n\n<img src=\"./areal/logo.png\" width=\"200\">\n\nAReaL项目已集成SwanLab，指引可见此文档：[Areal - monitoring-the-training-process](https://inclusionai.github.io/AReaL/tutorial/quickstart_legacy.html#monitoring-the-training-process)",
    "631": "一级标题：Argparse\n二级标题：无\n内容：\n`argparse` 是 Python 标准库中的一个模块，用于解析命令行参数和选项。通过 argparse，开发者可以轻松地编写用户友好的命令行接口，定义命令行参数的名称、类型、默认值、帮助信息等。\n\n`argparse` 与swanlab的集成非常简单，直接将创建好的argparse对象传递给swanlab.config，即可记录为超参数：\n\n```python\nimport argparse\nimport swanlab\n\n# 初始化Argparse\nparser = argparse.ArgumentParser()\nparser.add_argument('--epochs', default=20)\nparser.add_argument('--lr', default=0.001)\nargs = parser.parse_args()\n\nswanlab.init(config=args)\n```\n\n运行案例：\n```bash\npython main.py --epochs 100 --lr 1e-4\n```\n\n![alt text](/assets/ig-argparse.png)",
    "632": "一级标题：Ascend NPU & MindSpore\n二级标题：无\n内容：\nSwanLab支持[Ascend系列显卡](https://www.hiascend.com/)的硬件检测和[mindspore](https://www.mindspore.cn/)项目的训练跟踪。（计划20241215硬件监控上线）\n\nSwanLab实验记录Ascend NPU信息截图：\n\n![device](/assets/guide_cloud/integration/ascend/device_mask.png)",
    "633": "一级标题：Ascend NPU & MindSpore\n二级标题：简介\n内容：\n本案例使用实现的IMDB数据集情感分类任务。并使用SwanLab跟踪模型训练进展。",
    "634": "一级标题：Ascend NPU & MindSpore\n二级标题：任务介绍\n内容：\nIMDB情感分类任务是一种自然语言处理任务，旨在分析IMDB（Internet Movie Database）电影评论中的文本内容，以判断评论的情感倾向，通常分为正面（Positive）和负面（Negative）两类。该任务广泛用于研究情感分析技术，尤其是在监督学习和深度学习领域。\n\n数据集中通常包含预处理好的评论文本及其对应的情感标签，每条评论均标注为正面或负面。如下图：\n\n![data_image](/assets/guide_cloud/integration/ascend/data_image.png)\n\nLSTM（Long Short-Term Memory）是一种改进的循环神经网络，专为处理和预测序列数据中的长距离依赖而设计。与传统RNN相比，LSTM通过引入**记忆单元**和**门机制**，能够有效缓解梯度消失和梯度爆炸问题，使其在长序列数据的建模中表现优异。使用LSTM能轻松完成IMDB的语言情感分类任务。关于LSTM的具体原理建议参考[大神博客](https://blog.csdn.net/zhaojc1995/article/details/80572098)\n\n![lstm](/assets/guide_cloud/integration/ascend/lstm.png)\n\n本代码参考[MindSpore官方文档](https://www.mindspore.cn/tutorials/zh-CN/r2.4.1/nlp/sentiment_analysis.html#%E6%95%B0%E6%8D%AE%E9%9B%86%E9%A2%84%E5%A4%84%E7%90%86)，进行整理并简化了部分实现.",
    "635": "一级标题：Ascend NPU & MindSpore\n二级标题：环境安装\n内容：\n### 克隆项目\n\n附上[github项目链接](https://github.com/ShaohonChen/mindspore_imdb_train.git)和下载命令\n\n```bash\ngit clone https://github.com/ShaohonChen/mindspore_imdb_train.git\n```\n\n如果访问不了github可在本博客后文找到[代码章节](#代码章节)\n\n推荐还是用github ;)\n\n### CPU环境安装\n\n可以在CPU环境下安装MindSpore，虽然看起来没有Pytorch那么好用，但实际上文档还是写的很细的，真的很细，看得出华为工程师的严谨orz。配合sheng腾卡使用的话是非常有潜力的框架（MAC死活打不出sheng字）。\n\n官方安装文档[link](https://www.mindspore.cn/install/)\n\n也可以直接使用如下命令安装：\n\n```bash\npip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/2.4.1/MindSpore/unified/x86_64/mindspore-2.4.1-cp311-cp311-linux_x86_64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.tuna.tsinghua.edu.cn/simple\n```\n\n验证安装成功命令：\n\n```bash\npython -c \"import mindspore;mindspore.set_context(device_target='CPU');mindspore.run_check()\"\n```\n\n如果输出如下信息说明MindSpore安装成功了：\n\n```bash\nMindSpore version: 2.4.1\nThe result of multiplication calculation is correct, MindSpore has been installed on platform [CPU] successfully!\n```\n\n### 华为Ascend NPU显卡环境安装\n\n由于华为Ascend环境安装较为复杂，建议参考[MindSpore安装教程和踩坑记录](///)教程完成MindSpore环境安装。下面简述MindSpore安装过程\n\n>本博客写的时间是2024年12月6日，安装的版本是**MindSpore2.4.1**，因为感觉MindSpore变动会比较大特意记录一下时间和版本。\n\n#### 驱动安装&验证\n\n首先得确定有NPU卡和NPU相关驱动，驱动是**8.0.RC3.beta1**，如果没安装可以参考[CANN官方安装教程](https://www.hiascend.com/document/detail/zh/canncommercial/80RC3/softwareinst/instg/instg_0000.html?Mode=PmIns&OS=Ubuntu&Software=cannToolKit)\n\n完成安装后检测方法是运行\n\n```bash\nnpu-smi info\n```\n\n可以看到如下信息的话就表示驱动已经安装完成了。\n\n![npu-smi](/assets/guide_cloud/integration/ascend/a_mask.png)\n\n#### 安装MindSpore\n\n个人比较推荐使用conda安装，这样环境比较好管理，自动安装的依赖项也比较多\n\n首先需要安装前置依赖的包：\n\n```bash\npip install sympy\npip install \"numpy>=1.20.0,<2.0.0\"\npip install /usr/local/Ascend/ascend-toolkit/latest/lib64/te-*-py3-none-any.whl\npip install /usr/local/Ascend/ascend-toolkit/latest/lib64/hccl-*-py3-none-any.whl\n```\n\n如果本地下载比较慢可以使用带国内源版本的命令\n\n```bash\npip install sympy -i https://mirrors.cernet.edu.cn/pypi/web/simple\npip install \"numpy>=1.20.0,<2.0.0\" -i https://mirrors.cernet.edu.cn/pypi/web/simple\npip install /usr/local/Ascend/ascend-toolkit/latest/lib64/te-*-py3-none-any.whl -i https://mirrors.cernet.edu.cn/pypi/web/simple\npip install /usr/local/Ascend/ascend-toolkit/latest/lib64/hccl-*-py3-none-any.whl  -i https://mirrors.cernet.edu.cn/pypi/web/simple\n```\n\nconda安装MindSpore方法如下：\n\n```bash\nconda install mindspore=2.4.1 -c mindspore -c conda-forge\n```\n\n因为某些众所周知的原因，有时候conda源会失效，反应出来就是conda安装mindspore时会进度一直为0%，如下图：\n\n![condainstallfailed](/assets/guide_cloud/integration/ascend/b.png)\n\n可以使用如下方法指定国内源：\n\n```bash\nconda install mindspore=2.4.1 -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/MindSpore/ -c conda-forge\n```\n\npip安装MindSpore命令如下：\n\n```bash\npip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/2.4.1/MindSpore/unified/aarch64/mindspore-2.4.1-cp311-cp311-linux_aarch64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.tuna.tsinghua.edu.cn/simple\n```\n\n安装完成后可以使用如下命令进行测试\n\n```bash\npython -c \"import mindspore;mindspore.set_context(device_target='Ascend');mindspore.run_check()\"\n```\n\n如果这步出现报错可以参考本文后面[环境安装疑难杂症](#环境安装疑难杂症)章节\n\n出现版本号信息和计算验证便意味着安装成功\n\n```bash\nMindSpore version:  2.4.1\nThe result of multiplication calculation is correct, MindSpore has been installed on platform [Ascend] successfully!\n```\n\n也附上官方安装教程链接[mindspore官方安装教程](https://www.mindspore.cn/install)，注意本教程使用的是[Mindspore 2.4.1](https://www.mindspore.cn/versions#2.4.1)，建议环境与本教程保持一致。\n\n此外本教程使用[SwanLab](https://swanlab.cn)进行训练过程跟踪，SwanLab支持对Ascend系列NPU进行硬件识别和跟踪。\n\n### 记得安装SwanLab ;)\n\n安装方法：\n\n```bash\npip install swanlab\n```",
    "636": "一级标题：Ascend NPU & MindSpore\n二级标题：数据集&词编码文件准备\n内容：\n### 数据集准备\n\nLinux使用如下命令完成下载+解压\n\n```bash\nwget -P ./data/ https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\ntar -xzvf data/aclImdb_v1.tar.gz -C data/\n```\n\n如果下载太慢可以使用[华为云提供的国内链接](https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/aclImdb_v1.tar.gz)下载。并且在`./data/`目录下解压。\n\n> 如果解压不了tar.gz推荐安装[7zip解压器](https://www.7-zip.org/)，开源且通用的解压器\n\n### 词编码器准备\n\n使用如下命令下载+解压词编码器文件\n\n```bash\nwget -P ./embedding/ https://nlp.stanford.edu/data/glove.6B.zip\nunzip embedding/glove.6B.zip -d embedding/\n```\n\n如果下载太慢可以使用[华为云提供的国内链接](https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/glove.6B.zip)下载。并且在`./embedding/`目录下解压。",
    "637": "一级标题：Ascend NPU & MindSpore\n二级标题：开始训练\n内容：\n使用如下命令开始训练\n\n```\npython train.py\n```\n\n可是这\n\n> 如果提示登录swanlab，可以参考[如何登录SwanLab](https://docs.swanlab.cn/guide_cloud/general/quick-start.html#_2-%E7%99%BB%E5%BD%95%E8%B4%A6%E5%8F%B7)，这样将能够使用**云上看版**随时查看训练过程与结果。\n\n完成设置便可以在云上实时看到训练进展，我的实验记录可参考[完整实验记录](https://swanlab.cn/@ShaohonChen/Ascend_IMDB_CLS/charts)\n\n![log_img](/assets/guide_cloud/integration/ascend/log_img.png)\n\n并且附上其他脚本与在线实验记录：\n\n| 内容  | 训练命令  | 实验log  |\n|--------|--------|--------|\n| 基线 | `python train.py configs/baseline.json` | [log](https://swanlab.cn/@ShaohonChen/Ascend_IMDB_CLS/runs/qhl47nxl23tc4oycr6pmg/chart) |\n| CPU运行 | `python train.py configs/baseline.json CPU` | [log](https://swanlab.cn/@ShaohonChen/Ascend_IMDB_CLS/runs/s60wuicmwaitxe2v401ry/chart) |\n| 双层LSTM | `python train.py configs/two_layer.json` | [log](https://swanlab.cn/@ShaohonChen/Ascend_IMDB_CLS/runs/ydrgxvnqhjfrimzdj3oh4/chart) |\n| 小batch数 | `python train.py configs/small_batch.json` | [log](https://swanlab.cn/@ShaohonChen/Ascend_IMDB_CLS/runs/uovjgenfzcnxrl9gup900/chart) |\n| 隐藏层加大 | `python train.py configs/large_hs.json` | [log](https://swanlab.cn/@ShaohonChen/Ascend_IMDB_CLS/runs/eki6pa1him482w4jcc7gn/chart) |\n| 学习率加大 | `python train.py configs/large_hs.json` | [log](https://swanlab.cn/@ShaohonChen/Ascend_IMDB_CLS/runs/if3o10o6nf3am87f4ou62/chart) |\n\n相关超参数和最终结果可在[图标视图查看](https://swanlab.cn/@ShaohonChen/Ascend_IMDB_CLS/overview)\n\n![log_table](/assets/guide_cloud/integration/ascend/log_table.png)\n\n> PS: 观察了下日志，发现还是训练量不足，应该增大些训练量（40-50epoch比较合适）",
    "638": "一级标题：Ascend NPU & MindSpore\n二级标题：代码章节\n内容：\n如果访问不了github也提供一段测试代码，不过就是没法使用其他超参数了T_T\n\n```python\n# 读取训练参数+初始化日志记录\nimport os\nimport sys\nimport json\nimport mindspore as ms\nimport swanlab\n\n# ms.set_context(device_target=\"CPU\") # 使用CPU\nms.set_context(device_target=\"Ascend\")  # 使用NPU\n\nargs={  # 超参数\n    \"hidden_size\": 256,\n    \"output_size\": 1,\n    \"num_layers\": 2,\n    \"lr\": 0.001,\n    \"num_epochs\": 10,\n    \"batch_size\": 64,\n    \"report_interval\": 10\n}\n\nexp_name = \"baseline\"\nswanlab.init(project=\"Ascend_IMDB_CLS\", experiment_name=exp_name, config=args)\n\n\n# 构造数据集\nimport mindspore.dataset as ds\n\n\nclass IMDBData:\n    label_map = {\"pos\": 1, \"neg\": 0}\n\n    def __init__(self, path, mode=\"train\"):\n        self.docs, self.labels = [], []\n        for label in self.label_map.keys():\n            doc_dir = os.path.join(path, mode, label)\n            doc_list = os.listdir(doc_dir)\n            for fname in doc_list:\n                with open(os.path.join(doc_dir, fname)) as f:\n                    doc = f.read()\n                    doc = doc.lower().split()\n                    self.docs.append(doc)\n                    self.labels.append([self.label_map[label]])\n\n    def __getitem__(self, idx):\n        return self.docs[idx], self.labels[idx]\n\n    def __len__(self):\n        return len(self.docs)\n\n\nimdb_path = \"data/aclImdb\"\nimdb_train = ds.GeneratorDataset(\n    IMDBData(imdb_path, \"train\"), column_names=[\"text\", \"label\"], shuffle=True\n)\nimdb_test = ds.GeneratorDataset(\n    IMDBData(imdb_path, \"test\"), column_names=[\"text\", \"label\"], shuffle=False\n)\n\n# 构造embedding词表\nimport numpy as np\n\n\ndef load_glove(glove_path):\n    embeddings = []\n    tokens = []\n    with open(os.path.join(glove_path, \"glove.6B.100d.txt\"), encoding=\"utf-8\") as gf:\n        for glove in gf:\n            word, embedding = glove.split(maxsplit=1)\n            tokens.append(word)\n            embeddings.append(np.fromstring(embedding, dtype=np.float32, sep=\" \"))\n    # 添加 <unk>, <pad> 两个特殊占位符对应的embedding\n    embeddings.append(np.random.rand(100))\n    embeddings.append(np.zeros((100,), np.float32))\n\n    vocab = ds.text.Vocab.from_list(\n        tokens, special_tokens=[\"<unk>\", \"<pad>\"], special_first=False\n    )\n    embeddings = np.array(embeddings).astype(np.float32)\n    return vocab, embeddings\n\n\nvocab, embeddings = load_glove(\"./embedding\")\nprint(f\"VOCAB SIZE: {len(vocab.vocab())}\")\n\n# 数据预处理\nimport mindspore as ms\n\nlookup_op = ds.text.Lookup(vocab, unknown_token=\"<unk>\")\npad_op = ds.transforms.PadEnd([500], pad_value=vocab.tokens_to_ids(\"<pad>\"))\ntype_cast_op = ds.transforms.TypeCast(ms.float32)\n\nimdb_train = imdb_train.map(operations=[lookup_op, pad_op], input_columns=[\"text\"])\nimdb_train = imdb_train.map(operations=[type_cast_op], input_columns=[\"label\"])\n\nimdb_test = imdb_test.map(operations=[lookup_op, pad_op], input_columns=[\"text\"])\nimdb_test = imdb_test.map(operations=[type_cast_op], input_columns=[\"label\"])\n\nimdb_train, imdb_valid = imdb_train.split([0.7, 0.3])\n\nprint(f\"TRAIN SET SIZE: {len(imdb_train)}\")\nprint(f\"VALID SET SIZE: {len(imdb_valid)}\")\nprint(f\"TEST SET SIZE: {len(imdb_test)}\")\n\nimdb_train = imdb_train.batch(args[\"batch_size\"], drop_remainder=True)\nimdb_valid = imdb_valid.batch(args[\"batch_size\"], drop_remainder=True)\n\n\n# LSTM分类器实现\nimport math\nimport mindspore as ms\nimport mindspore.nn as nn\nimport mindspore.ops as ops\nfrom mindspore.common.initializer import Uniform, HeUniform\n\n\nclass LSTM_CLS(nn.Cell):\n    def __init__(self, embeddings, hidden_dim, output_dim, n_layers, pad_idx):\n        super().__init__()\n        vocab_size, embedding_dim = embeddings.shape\n        self.embedding = nn.Embedding(\n            vocab_size,\n            embedding_dim,\n            embedding_table=ms.Tensor(embeddings),\n            padding_idx=pad_idx,\n        )\n        self.rnn = nn.LSTM(\n            embedding_dim, hidden_dim, num_layers=n_layers, batch_first=True\n        )\n        weight_init = HeUniform(math.sqrt(5))\n        bias_init = Uniform(1 / math.sqrt(hidden_dim * 2))\n        self.fc = nn.Dense(\n            hidden_dim, output_dim, weight_init=weight_init, bias_init=bias_init\n        )\n\n    def construct(self, inputs):\n        embedded = self.embedding(inputs)\n        _, (hidden, _) = self.rnn(embedded)\n        hidden = hidden[-1, :, :]\n        output = self.fc(hidden)\n        return output\n\n\nmodel = LSTM_CLS(\n    embeddings,\n    args[\"hidden_size\"],\n    args[\"output_size\"],\n    args[\"num_layers\"],\n    vocab.tokens_to_ids(\"<pad>\"),\n)\n\n# 损失函数与优化器\nloss_fn = nn.BCEWithLogitsLoss(reduction=\"mean\")\noptimizer = nn.Adam(model.trainable_params(), learning_rate=args[\"lr\"])\n\n# 训练过程实现\nfrom tqdm import tqdm\nimport time\n\n\ndef forward_fn(data, label):\n    logits = model(data)\n    loss = loss_fn(logits, label)\n    return loss\n\n\ngrad_fn = ms.value_and_grad(forward_fn, None, optimizer.parameters)\n\n\ndef train_step(data, label):\n    loss, grads = grad_fn(data, label)\n    optimizer(grads)\n    return loss\n\n\ndef train_one_epoch(model, train_dataset, epoch=0):\n    model.set_train()\n    total = train_dataset.get_dataset_size()\n    step_total = 0\n    last_time = time.time()\n    for i in train_dataset.create_tuple_iterator():\n        loss = train_step(*i)\n        step_total += 1\n        loss_item = loss.item()\n        if step_total % args[\"report_interval\"] == 1:\n            now_time = time.time()\n            per_batch_time = (now_time - last_time) / args[\"report_interval\"]\n            last_time = now_time\n            swanlab.log(\n                {\n                    \"train/epoch\": epoch,\n                    \"train/step\": step_total,\n                    \"train/loss\": loss_item,\n                    \"train/per_batch_time(s)\": per_batch_time,\n                }\n            )\n            print(\n                f\"[train epoch-{epoch:2d} step-{step_total:4d}/{total:4d}] loss:{loss_item:.4f} use_time:{per_batch_time:10.4f}s\"\n            )\n\n\n# 评估过程实现\ndef binary_accuracy(preds, y):\n    rounded_preds = np.around(ops.sigmoid(preds).asnumpy())\n    correct = (rounded_preds == y).astype(np.float32)\n    acc = correct.sum() / len(correct)\n    return acc\n\n\ndef evaluate(model, test_dataset, criterion, epoch=0, mode=\"eval\"):\n    last_time = time.time()\n    total = test_dataset.get_dataset_size()\n    epoch_loss = 0\n    epoch_acc = 0\n    model.set_train(False)\n    for i in test_dataset.create_tuple_iterator():\n        predictions = model(i[0])\n        loss = criterion(predictions, i[1])\n        epoch_loss += loss.asnumpy()\n        acc = binary_accuracy(predictions, i[1])\n        epoch_acc += acc\n\n    final_loss = float(epoch_loss / total)\n    final_acc = float(epoch_acc / total)\n    use_time = time.time() - last_time\n    swanlab.log(\n        {\n            f\"{mode}/loss\": final_loss,\n            f\"{mode}/acc\": final_acc,\n            f\"{mode}/use_time\": use_time,\n        }\n    )\n    print(\n        f\"[{mode} epoch-{epoch:2d} loss:{final_loss:.4f} acc:{final_acc*100:.2f}% use_time:{use_time:10.4f}s\"\n    )\n\n    return final_loss, final_acc\n\n\n# 开启训练=\nbest_valid_loss = float(\"inf\")\nsave_path = os.path.join(\"output\", exp_name)\nos.makedirs(save_path, exist_ok=True)\nckpt_file_name = os.path.join(save_path, \"sentiment-analysis.ckpt\")\n\n\nfor epoch in range(args[\"num_epochs\"]):\n    train_one_epoch(model, imdb_train, epoch)\n    valid_loss, _ = evaluate(model, imdb_valid, loss_fn, epoch)\n\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        ms.save_checkpoint(model, ckpt_file_name)\n\n\n# 开始测试\nparam_dict = ms.load_checkpoint(ckpt_file_name)\nms.load_param_into_net(model, param_dict)\nimdb_test = imdb_test.batch(64)\ntest_loss, test_acc = evaluate(model, imdb_test, loss_fn, mode=\"test\")\n\n# 开始预测\nscore_map = {1: \"Positive\", 0: \"Negative\"}\n\n\ndef predict_sentiment(model, vocab, sentence):\n    model.set_train(False)\n    tokenized = sentence.lower().split()\n    indexed = vocab.tokens_to_ids(tokenized)\n    tensor = ms.Tensor(indexed, ms.int32)\n    tensor = tensor.expand_dims(0)\n    prediction = model(tensor)\n    return score_map[int(np.round(ops.sigmoid(prediction).asnumpy()))]\n\n\npredict_sentiment(model, vocab, \"This film is great\")\npredict_sentiment(model, vocab, \"This film is terrible\")\n\n```",
    "639": "一级标题：Ascend NPU & MindSpore\n二级标题：疑难杂症\n内容：\n### 可能出现的问题一：MindSpore和CANN版本不对应\n\n务必确保MindSpore版本和驱动一致，否则会出现如下报错：\n\n```bash\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:11.112.000 [mindspore/run_check/_check_version.py:357] MindSpore version 2.3.1 and Ascend AI software package (Ascend Data Center Solution)version 7.5 does not match, the version of software package expect one of ['7.2', '7.3']. Please refer to the match info on: https://www.mindspore.cn/install\n/home/huawei/miniconda3/envs/mindspore231/lib/python3.10/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n  setattr(self, word, getattr(machar, word).flat[0])\n/home/huawei/miniconda3/envs/mindspore231/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n  return self._float_to_str(self.smallest_subnormal)\n/home/huawei/miniconda3/envs/mindspore231/lib/python3.10/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n  setattr(self, word, getattr(machar, word).flat[0])\n/home/huawei/miniconda3/envs/mindspore231/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n  return self._float_to_str(self.smallest_subnormal)\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:13.700.000 [mindspore/run_check/_check_version.py:375] MindSpore version 2.3.1 and \"te\" wheel package version 7.5 does not match. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:13.701.000 [mindspore/run_check/_check_version.py:382] MindSpore version 2.3.1 and \"hccl\" wheel package version 7.5 does not match. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:13.702.000 [mindspore/run_check/_check_version.py:396] Please pay attention to the above warning, countdown: 3\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:14.703.000 [mindspore/run_check/_check_version.py:396] Please pay attention to the above warning, countdown: 2\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:15.704.000 [mindspore/run_check/_check_version.py:396] Please pay attention to the above warning, countdown: 1\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:18.608.000 [mindspore/run_check/_check_version.py:357] MindSpore version 2.3.1 and Ascend AI software package (Ascend Data Center Solution)version 7.5 does not match, the version of software package expect one of ['7.2', '7.3']. Please refer to the match info on: https://www.mindspore.cn/install\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:18.608.000 [mindspore/run_check/_check_version.py:375] MindSpore version 2.3.1 and \"te\" wheel package version 7.5 does not match. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:18.608.000 [mindspore/run_check/_check_version.py:382] MindSpore version 2.3.1 and \"hccl\" wheel package version 7.5 does not match. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:18.608.000 [mindspore/run_check/_check_version.py:396] Please pay attention to the above warning, countdown: 3\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:19.609.000 [mindspore/run_check/_check_version.py:396] Please pay attention to the above warning, countdown: 2\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:20.611.000 [mindspore/run_check/_check_version.py:396] Please pay attention to the above warning, countdown: 1\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:21.614.000 [mindspore/run_check/_check_version.py:357] MindSpore version 2.3.1 and Ascend AI software package (Ascend Data Center Solution)version 7.5 does not match, the version of software package expect one of ['7.2', '7.3']. Please refer to the match info on: https://www.mindspore.cn/install\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:21.614.000 [mindspore/run_check/_check_version.py:375] MindSpore version 2.3.1 and \"te\" wheel package version 7.5 does not match. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:21.614.000 [mindspore/run_check/_check_version.py:382] MindSpore version 2.3.1 and \"hccl\" wheel package version 7.5 does not match. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:21.615.000 [mindspore/run_check/_check_version.py:396] Please pay attention to the above warning, countdown: 3\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:22.616.000 [mindspore/run_check/_check_version.py:396] Please pay attention to the above warning, countdown: 2\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:23.617.000 [mindspore/run_check/_check_version.py:396] Please pay attention to the above warning, countdown: 1\nMindSpore version:  2.3.1\nSegmentation fault (core dumped)\n```\n\n解决方法：装对版本即可解决。对于MindSpore2.4.1，安装**8.0.RC3.beta1**驱动\n\n### 可能出现的问题二：少装了前置的包\n\n这里面\n\n```bash\n[ERROR] ME(1051780:281473416683552,MainProcess):2024-12-06-12:39:02.460.00 [mindspore/run_check/_check_version.py:360] CheckFailed: cannot import name 'version' from 'te' (unknown location)\n[ERROR] ME(1051780:281473416683552,MainProcess):2024-12-06-12:39:02.460.00 [mindspore/run_check/_check_version.py:361] MindSpore relies on whl packages of \"te\" and \"hccl\" in the \"latest\" folder of the Ascend AI software package (Ascend Data Center Solution). Please check whether they are installed correctly or not, refer to the match info on: https://www.mindspore.cn/install\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/home/huawei/miniconda3/envs/mindspore241/lib/python3.11/site-packages/mindspore/__init__.py\", line 19, in <module>\n    from mindspore import common, dataset, mindrecord, train, log, amp\n...\nImportError: cannot import name 'util' from 'tbe.tvm.topi.cce' (unknown location)\nFatal Python error: PyThreadState_Get: the function must be called with the GIL held, but the GIL is released (the current Python thread state is NULL)\nPython runtime state: finalizing (tstate=0x00000000008aceb0)\n\nAborted (core dumped)\n```\n\n### 可能出现的问题三：pip安装阶段报错opc-tool 0.1.0 requires attrs, which is not installed\n\n若出现如下报错（之前安装的时候有概率pip会报如下错误）：\n\n```bash\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nauto-tune 0.1.0 requires decorator, which is not installed.\ndataflow 0.0.1 requires jinja2, which is not installed.\nopc-tool 0.1.0 requires attrs, which is not installed.\nopc-tool 0.1.0 requires decorator, which is not installed.\nopc-tool 0.1.0 requires psutil, which is not installed.\nschedule-search 0.0.1 requires absl-py, which is not installed.\nschedule-search 0.0.1 requires decorator, which is not installed.\nte 0.4.0 requires attrs, which is not installed.\nte 0.4.0 requires cloudpickle, which is not installed.\nte 0.4.0 requires decorator, which is not installed.\nte 0.4.0 requires ml-dtypes, which is not installed.\nte 0.4.0 requires psutil, which is not installed.\nte 0.4.0 requires scipy, which is not installed.\nte 0.4.0 requires tornado, which is not installed.\n```\n\n尝试使用如下命令解决：\n\n```bash\npip install attrs cloudpickle decorator jinja2 ml-dtypes psutil scipy tornado absl-py\n```\n\n### 可能出现的问题四：在测试或者实际训练的时候出现KeyError: 'op_debug_dir'\n\n出现如下情况大概率是没有运行环境变量命令。\n\n```bash\nTraceback (most recent call last):\n  File \"/home/huawei/miniconda3/envs/mindspore241/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/home/huawei/miniconda3/envs/mindspore241/lib/python3.11/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/huawei/.local/lib/python3.11/site-packages/te_fusion/parallel_compilation.py\", line 249, in exec_compilation_task\n    check_dict_paras(dict_ops)\n  File \"/home/huawei/.local/lib/python3.11/site-packages/te_fusion/parallel_compilation.py\", line 183, in check_dict_paras\n    if dict_ops['op_debug_dir'] == None or dict_ops['op_debug_dir'] == '':\n       ~~~~~~~~^^^^^^^^^^^^^^^^\nKeyError: 'op_debug_dir'\n```\n\n解决方法：使用如下命令设置环境变量\n\n```bash\n# control log level. 0-DEBUG, 1-INFO, 2-WARNING, 3-ERROR, 4-CRITICAL, default level is WARNING.\nexport GLOG_v=2\n\n# environment variables\nLOCAL_ASCEND=/usr/local/Ascend # 设置为软件包的实际安装路径\n\n# set environmet variables using script provided by CANN, swap \"ascend-toolkit\" with \"nnae\" if you are using CANN-nnae package instead\nsource ${LOCAL_ASCEND}/ascend-toolkit/set_env.sh\n```\n\n使用conda的时候发现似乎每次都要运行一次如上命令。如果想要永久解决这个问题，可以使用如下命令解决：\n\n```bash\nexport LOCAL_ASCEND=/usr/local/Ascend # 设置为软件包的实际安装路径\necho \"source ${LOCAL_ASCEND}/ascend-toolkit/set_env.sh\" >> ~/.bashrc\nsource ~/.bashrc\n```",
    "640": "一级标题：DiffSynth Studio\n二级标题：无\n内容：\n[DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio) 是 [ModelScope](https://modelscope.cn/) 推出的一个开源的扩散模型引擎，专注于图像与视频的风格迁移与生成任务。它通过优化架构设计（如文本编码器、UNet、VAE 等组件），在保持与开源社区模型兼容性的同时，显著提升计算性能，为用户提供高效、灵活的创作工具。\n\nDiffSynth Studio 支持多种扩散模型，包括 Wan-Video、StepVideo、HunyuanVideo、CogVideoX、FLUX、ExVideo、Kolors、Stable Diffusion 3 等。\n\n![](./diffsynth/logo.jpg)\n\n你可以使用DiffSynth Studio快速进行Diffusion模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n[[toc]]",
    "641": "一级标题：DiffSynth Studio\n二级标题：准备工作\n内容：\n**1. 克隆仓库并安装环境**\n\n```bash\ngit clone https://github.com/modelscope/DiffSynth-Studio.git\ncd DiffSynth-Studio\npip install -e .\npip install swanlab\npip install lightning lightning_fabric\n```\n\n**2. 准备数据集**\n\nDiffSynth Studio 的数据集需要按下面的格式进行构建，比如将图像数据存放在`data/dog`目录下：\n\n```bash\ndata/dog/\n└── train\n    ├── 00.jpg\n    ├── 01.jpg\n    ├── 02.jpg\n    ├── 03.jpg\n    ├── 04.jpg\n    └── metadata.csv\n```\n\n`metadata.csv` 文件需要按下面的格式进行构建：\n\n```csv\nfile_name,text\n00.jpg,一只小狗\n01.jpg,一只小狗\n02.jpg,一只小狗\n03.jpg,一只小狗\n04.jpg,一只小狗\n```\n\n> 这里有一份整理好格式的火影忍者数据集，[百度云](https://pan.baidu.com/s/1kPvkTV6gy2xWFRpyXRX0Yw?pwd=2p6h)，供参考与测试\n\n**3. 准备模型**\n\n这里以Kolors模型为例，下载模型权重和VAE权重：\n\n```bash\nmodelscope download --model=Kwai-Kolors/Kolors --local_dir models/kolors/Kolors\nmodelscope download --model=AI-ModelScope/sdxl-vae-fp16-fix --local_dir models/kolors/sdxl-vae-fp16-fix\n```",
    "642": "一级标题：DiffSynth Studio\n二级标题：设置SwanLab参数\n内容：\n在运行训练脚本时，添加`--use_swanlab`，即可将训练过程记录到SwanLab平台。\n\n如果你需要离线记录，可以添加`--swanlab_mode \"local\"`。\n\n```bash\nCUDA_VISIBLE_DEVICES=\"0\" python examples/train/kolors/train_kolors_lora.py \\\n...\n--use_swanlab \\  # [!code ++]\n--swanlab_mode \"cloud\"  # [!code ++]\n```",
    "643": "一级标题：DiffSynth Studio\n二级标题：开启训练\n内容：\n使用下面的命令即可开启训练，并使用SwanLab记录超参数、训练日志、loss曲线等信息：\n\n```bash {11,12}\nCUDA_VISIBLE_DEVICES=\"0\" python examples/train/kolors/train_kolors_lora.py \\\n--pretrained_unet_path models/kolors/Kolors/unet/diffusion_pytorch_model.safetensors \\\n--pretrained_text_encoder_path models/kolors/Kolors/text_encoder \\\n--pretrained_fp16_vae_path models/kolors/sdxl-vae-fp16-fix/diffusion_pytorch_model.safetensors \\\n--dataset_path data/dog \\\n--output_path ./models \\\n--max_epochs 10 \\\n--center_crop \\\n--use_gradient_checkpointing \\\n--precision \"16-mixed\" \\\n--use_swanlab \\\n--swanlab_mode \"cloud\"\n```\n\n![](./diffsynth/ui-1.png)\n\n![](./diffsynth/ui-2.png)",
    "644": "一级标题：DiffSynth Studio\n二级标题：补充\n内容：\n如果你想要自定义SwanLab的项目名、实验名等参数，可以：\n\n**1. 文生图任务**\n\n在`DiffSynth-Studio/diffsynth/trainers/text_to_image.py`文件中，找到`swanlab_logger`变量的位置，修改`project`和`name`参数：\n\n```python {6-7}\nif args.use_swanlab:\n    from swanlab.integration.pytorch_lightning import SwanLabLogger\n    swanlab_config = {\"UPPERFRAMEWORK\": \"DiffSynth-Studio\"}\n    swanlab_config.update(vars(args))\n    swanlab_logger = SwanLabLogger(\n        project=\"diffsynth_studio\",\n        name=\"diffsynth_studio\",\n        config=swanlab_config,\n        mode=args.swanlab_mode,\n        logdir=args.output_path,\n    )\n    logger = [swanlab_logger]\n```\n\n**2. Wan-Video文生视频任务**\n\n在`DiffSynth-Studio/examples/wanvideo/train_wan_t2v.py`文件中，找到`swanlab_logger`变量的位置，修改`project`和`name`参数：\n\n```python {6-7}\nif args.use_swanlab:\n    from swanlab.integration.pytorch_lightning import SwanLabLogger\n    swanlab_config = {\"UPPERFRAMEWORK\": \"DiffSynth-Studio\"}\n    swanlab_config.update(vars(args))\n    swanlab_logger = SwanLabLogger(\n        project=\"wan\",\n        name=\"wan\",\n        config=swanlab_config,\n        mode=args.swanlab_mode,\n        logdir=args.output_path,\n    )\n    logger = [swanlab_logger]\n```",
    "645": "一级标题：EasyR1\n二级标题：无\n内容：\n[EasyR1](https://github.com/hiyouga/EasyR1) 是基于[veRL](https://github.com/volcengine/verl)的一个高效、可扩展、多模态强化学习LLM训练框架。\n\n![](./easyr1/logo.png)\n\nEasyR1 受益于 veRL 的 HybridEngine 和 vLLM 0.7 的 SPMD mode，并适配了 Qwen2.5-VL 模型，在多模态几何题任务 Geometry3k 上通过 30 个 batch 的 GRPO 训练，即可提升 5% 验证集准确率。\n\n> **作者hiyouga**：EasyR1旨在钻研多模态 RL 训练的难点，由团队的算法同学和工程同学共同迭代框架效率、算法表现和扩展性，未来将会支持更多的 RL 算法和多模态模型。\n\n你可以使用EasyR1训练你的多模态RL模型，并使用SwanLab跟踪与可视化训练曲线。",
    "646": "一级标题：EasyR1\n二级标题：1. 准备工作\n内容：\n在执行下面的命令之前，请先确保你的环境中已经安装了Python>=3.9，CUDA和PyTorch。\n\n```bash\ngit clone https://github.com/hiyouga/EasyR1.git\ncd EasyR1\npip install -e .\npip install git+https://github.com/hiyouga/MathRuler.git\npip install swanlab\n```\n\n:::warning 注意\n\nEasyR1的依赖中有flash-attn，直接安装非常慢，请在[flash-attention预编译包](https://github.com/Dao-AILab/flash-attention/releases)中找到对应Python与CUDA版本的包，下载并安装。\n\n:::",
    "647": "一级标题：EasyR1\n二级标题：2. 训练Qwen2.5-7b数学模型\n内容：\n在`EasyR1`目录下，执行下面的命令，即可使用GRPO训练Qwen2.5-7b数学模型，并使用SwanLab进行跟踪与可视化：\n\n```bash\nbash examples/run_qwen2_5_7b_math_swanlab.sh\n```\n\n![](./easyr1/qwen_math.png)\n\n当然，这里我们可以剖析一下，由于EasyR1是原始 veRL 项目的一个干净分叉，所以继承了[veRL与SwanLab的集成](/guide_cloud/integration/integration-verl.md)。所以这里我们来看`run_qwen2_5_7b_math_swanlab.sh`文件：\n\n```sh {10}\nset -x\n\nexport VLLM_ATTENTION_BACKEND=XFORMERS\n\nMODEL_PATH=Qwen/Qwen2.5-7B-Instruct  # replace it with your local file path\n\npython3 -m verl.trainer.main \\\n    config=examples/grpo_example.yaml \\\n    worker.actor.model.model_path=${MODEL_PATH} \\\n    trainer.logger=['console','swanlab'] \\\n    trainer.n_gpus_per_node=4\n```\n\n只需要在`python3 -m verl.trainer.main`参数中加入一行`trainer.logger=['console','swanlab']`，即可使用SwanLab进行跟踪与可视化。",
    "648": "一级标题：EasyR1\n二级标题：3. 训练Qwen2.5-VL-7b多模态模型\n内容：\n在`EasyR1`目录下，执行下面的命令，即可使用GRPO训练Qwen2.5-VL-7b多模态模型，并使用SwanLab进行跟踪与可视化：\n\n```bash\nbash examples/run_qwen2_5_vl_7b_geo_swanlab.sh\n```",
    "649": "一级标题：EasyR1\n二级标题：4. 每轮评估时记录生成文本\n内容：\n如果你希望在每轮评估（val）时将生成的文本记录到SwanLab中，只需在命令行钟增加一行`val_generations_to_log=1`即可：\n\n```bash {6}\npython3 -m verl.trainer.main \\\n    config=examples/grpo_example.yaml \\\n    worker.actor.model.model_path=${MODEL_PATH} \\\n    trainer.logger=['console','swanlab'] \\\n    trainer.n_gpus_per_node=4 \\\n    val_generations_to_log=1\n```",
    "650": "一级标题：EasyR1\n二级标题：写在最后\n内容：\nEasyR1 是 [LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory) 作者 [hiyouga](https://github.com/hiyouga) 的全新开源项目，一个适用于多模态大模型的强化学习框架。感谢 [hiyouga](https://github.com/hiyouga) 为全球开源生态的贡献，SwanLab也将继续与AI开发者同行。",
    "651": "一级标题：EvalScope\n二级标题：无\n内容：\n[EvalScope](https://github.com/modelscope/evalscope) 是 [ModelScope](https://modelscope.cn/) 的官方模型评估和基准测试框架，专为满足各种评估需求而设计。它支持各种模型类型，包括大型语言模型、多模态模型、Embedding模型、Reranker模型和 CLIP 模型。\n\n![evalscope-logo](./evalscope/logo.png)\n\n该框架支持多种评估场景，如端到端的RAG评估、竞技场模式和推理性能测试。它内置了MMLU、CMMLU、C-Eval和GSM8K等基准和指标。与 [ms-swift](https://github.com/modelscope/ms-swift) 训练框架无缝集成，EvalScope实现了单击评估，为模型训练和评估提供全面支持 🚀。\n\n现在，你可以使用 EvalScope 评估LLM性能，同时使用SwanLab方便地跟踪、对比、可视化。\n\n[Demo](https://swanlab.cn/@ShaohonChen/perf_benchmark/overview)",
    "652": "一级标题：EvalScope\n二级标题：1. 准备工作\n内容：\n安装下面的环境：\n\n```bash\npip install evalscope\npip install swanlab\n```\n\n如果你需要扩展evalscope的更多功能，可以按需安装：\n\n```bash\npip install -e '.[opencompass]'   # Install OpenCompass backend\npip install -e '.[vlmeval]'       # Install VLMEvalKit backend\npip install -e '.[rag]'           # Install RAGEval backend\npip install -e '.[perf]'          # Install Perf dependencies\npip install -e '.[app]'           # Install visualization dependencies\npip install -e '.[all]'           # Install all backends (Native, OpenCompass, VLMEvalKit, RAGEval)\n```",
    "653": "一级标题：EvalScope\n二级标题：2. Qwen模型推理性能压测\n内容：\n如果你希望评估`Qwen2.5-0.5B-Instruct`在[openqa格式默认数据集](https://www.modelscope.cn/datasets/AI-ModelScope/HC3-Chinese)上的表现，同时使用`SwanLab`观测性能，可以运行下面的命令：\n\n```bash {5,6}\nexport CUDA_VISIBLE_DEVICES=0\nevalscope perf \\\n --model Qwen/Qwen2.5-0.5B-Instruct \\\n --dataset openqa \\\n --number 20 \\\n --parallel 2 \\\n --limit 5 \\\n --swanlab-api-key '你的API Key' \\\n --name 'qwen2.5-openqa' \\\n --temperature 0.9 \\\n --api local\n```\n\n其中`swanlab-api-key`是你的SwanLab API Key，`name`是实验名。\n如果你希望设置自定义项目名，可以去往`EvalScope`源码的 `evalscope/perf/benchmark.py` 的 `statistic_benchmark_metric_worker`函数，找到swanlab部分，修改`project`参数。\n\n**可视化效果案例：**\n\n![](./evalscope/show.png)",
    "654": "一级标题：EvalScope\n二级标题：上传到私有化部署版\n内容：\n如果你希望将评估结果上传到私有化部署版，可以先在命令行登录到私有化部署版。比如你的部署地址是`http://localhost:8000`，可以运行：\n\n```bash\nswanlab login --host http://localhost:8000\n```\n\n完成登录后，再运行`evalscope`的命令，就可以将评估结果上传到私有化部署版了。",
    "655": "一级标题：fastai\n二级标题：无\n内容：\n[fastai](https://github.com/fastai/fastai) 是一个基于 PyTorch 的高层次深度学习库，旨在使现代深度学习的应用更加容易和高效。它提供了一个简单的 API，使用户能够快速构建、训练和评估复杂的模型，而无需深入了解底层细节。\n\n你可以使用fastai快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。",
    "656": "一级标题：fastai\n二级标题：1.引入SwanLabCallback\n内容：\n```python\nfrom swanlab.integration.fastai import SwanLabCallback\n```\n**SwanLabCallback**是适配于fastai的日志记录类。\n\n**SwanLabCallback**可以定义的参数有：\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。\n- 你也可以在外部通过`swanlab.init`创建项目，集成会将实验记录到你在外部创建的项目中。",
    "657": "一级标题：fastai\n二级标题：2.传入训练器\n内容：\n```python\nfrom fastai.vision.all import *\nfrom swanlab.integration.fastai import SwanLabCallback\n\n...\n\n# 定义模型\nlearn = vision_learner(...)\n\n# 添加SwanLabCallback\nlearn.fit_one_cycle(5, cbs=SwanLabCallback)\n```",
    "658": "一级标题：fastai\n二级标题：3.案例-宠物分类\n内容：\n```python (2,16)\nfrom fastai.vision.all import *\nfrom swanlab.integration.fastai import SwanLabCallback\n\n# 加载数据\npath = untar_data(URLs.PETS)\ndls = ImageDataLoaders.from_name_re(\n    path, get_image_files(path / \"images\"), pat=r\"([^/]+)_\\d+.jpg$\", item_tfms=Resize(224)\n)\n\n# 定义模型\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\n\n# 添加SwanLabCallback\nlearn.fit_one_cycle(\n    5,\n    cbs=SwanLabCallback(\n        project=\"fastai-swanlab-integration-test\",\n        experiment_name=\"super-test\",\n        description=\"Test fastai integration with swanlab\",\n        logdir=\"./logs\",\n    ),\n)\n```",
    "659": "一级标题：🤗HuggingFace Accelerate\n二级标题：无\n内容：\nHuggingFace 的 [accelerate](https://huggingface.co/docs/accelerate/index) 是一个简化和优化深度学习模型训练与推理的开源库。\n\n> 🚀在几乎任何设备和分布式配置上启动、训练和使用PyTorch模型的简单方法，支持自动混合精度(包括fp8)，以及易于配置的FSDP和DeepSpeed\n\n它提供了高效的分布式训练和推理的工具，使开发者能够更轻松地在不同硬件设备上部署和加速模型。通过简单的几行代码改动，就可以轻松将现有的训练代码集成进 `torch_xla` 和 `torch.distributed` 这类平台，而无需为复杂的分布式计算架构烦恼，从而提升工作效率和模型性能。\n\n![hf-accelerate-image](./huggingface_accelerate/logo.png)\n\n你可以使用`accelerate`快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n> `accelerate`>=1.8.0 的版本，已官方集成了swanlab\n> 如果你的版本低于1.8.0，请使用 **SwanLabTracker集成**",
    "660": "一级标题：🤗HuggingFace Accelerate\n二级标题：1. 两行代码完成集成\n内容：\n```python {4,9}\nfrom accelerate import Accelerator\n\n# 告诉 Accelerator 对象使用 swanlab 进行日志记录\naccelerator = Accelerator(log_with=\"swanlab\")\n\n# 初始化您的 swanlab 实验，传递 swanlab 参数和任何配置信息\naccelerator.init_trackers(\n    ...\n    init_kwargs={\"swanlab\": {\"experiment_name\": \"hello_world\"}}\n    )\n```\n\n::: warning 补充信息\n1. swanlab项目名由`accelerator.init_trackers`的`project_name`参数指定\n2. 向`init_kwargs`传递的`swanlab`字典，key-value和`swanlab.init`的参数完全一致（除了project）。\n:::\n\n最小能跑代码：\n\n```python {4,10}\nfrom accelerate import Accelerator\n\n# Tell the Accelerator object to log with swanlab\naccelerator = Accelerator(log_with=\"swanlab\")\n\n# Initialise your swanlab experiment, passing swanlab parameters and any config information\naccelerator.init_trackers(\n    project_name=\"accelerator\",\n    config={\"dropout\": 0.1, \"learning_rate\": 1e-2},\n    init_kwargs={\"swanlab\": {\"experiment_name\": \"hello_world\"}}\n    )\n\nfor i in range(100):\n    # Log to swanlab by calling `accelerator.log`, `step` is optional\n    accelerator.log({\"train_loss\": 1.12, \"valid_loss\": 0.8}, step=i+1)\n\n# Make sure that the swanlab tracker finishes correctly\naccelerator.end_training()\n```",
    "661": "一级标题：🤗HuggingFace Accelerate\n二级标题：2. SwanLabTracker集成\n内容：\n如果你使用的是`accelerate<1.8.0`的版本，则可以使用SwanLabCallback集成。\n\n### 2.1 引入\n\n```bash\nfrom swanlab.integration.accelerate import SwanLabTracke\n```\n\n\n### 2.2 在初始化accelerate时指定日志记录器\n\n```python (1,7,9,12)\nfrom swanlab.integration.accelerate import SwanLabTracker\nfrom accelerate import Accelerator\n\n...\n\n# 创建SwanLab日志记录器\ntracker = SwanLabTracker(\"YOUR_SMART_PROJECT_NAME\")\n# 传入Accelerator\naccelerator = Accelerator(log_with=tracker)\n\n# 初始化所有日志记录器\naccelerator.init_trackers(\"YOUR_SMART_PROJECT_NAME\", config=config)\n\n# training code\n...\n```\n\n- 虽然上面的代码两次设定了项目名，实际上只有第一个项目名设置才起了作用\n\n- 显式调用`init_trackers`来初始化所有日志记录是`accelerate`的机制，第二次设置的项目名是当有多个日志记录器时,初始化内置的日志记录器的情况下才会用到。\n\n### 2.3 完整案例代码\n\n下面是一个使用accelerate进行cifar10分类，并使用SwanLab进行日志跟踪的案例：\n\n```python (10,45,46,47,71,90)\nimport torch\nimport torch.utils\nimport torch.utils.data\nimport torch.utils.data.dataloader\nimport torchvision\nfrom torchvision.models import resnet18, ResNet18_Weights\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nimport swanlab\nfrom swanlab.integration.accelerate import SwanLabTracker\n\n\ndef main():\n    # hyperparameters\n    config = {\n        \"num_epoch\": 5,\n        \"batch_num\": 16,\n        \"learning_rate\": 1e-3,\n    }\n\n    # Download the raw CIFAR-10 data.\n    transform = torchvision.transforms.Compose(\n        [\n            torchvision.transforms.ToTensor(),\n            torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n        ]\n    )\n    train_data = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n    test_data = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n    BATCH_SIZE = config[\"batch_num\"]\n    my_training_dataloader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n    my_testing_dataloader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n\n    # Using resnet18 model, make simple changes to fit the data set\n    my_model = resnet18(weights=ResNet18_Weights.DEFAULT)\n    my_model.conv1 = torch.nn.Conv2d(my_model.conv1.in_channels, my_model.conv1.out_channels, 3, 1, 1)\n    my_model.maxpool = torch.nn.Identity()\n    my_model.fc = torch.nn.Linear(my_model.fc.in_features, 10)\n\n    # Criterion and optimizer\n    criterion = torch.nn.CrossEntropyLoss()\n    my_optimizer = torch.optim.SGD(my_model.parameters(), lr=config[\"learning_rate\"], momentum=0.9)\n\n    # Init accelerate with swanlab tracker\n    tracker = SwanLabTracker(\"CIFAR10_TRAING\")\n    accelerator = Accelerator(log_with=tracker)\n    accelerator.init_trackers(\"CIFAR10_TRAING\", config=config)\n    my_model, my_optimizer, my_training_dataloader, my_testing_dataloader = accelerator.prepare(\n        my_model, my_optimizer, my_training_dataloader, my_testing_dataloader\n    )\n    device = accelerator.device\n    my_model.to(device)\n\n    # Get logger\n    logger = get_logger(__name__)\n\n    # Begin training\n\n    for ep in range(config[\"num_epoch\"]):\n        # train model\n        if accelerator.is_local_main_process:\n            print(f\"begin epoch {ep} training...\")\n        step = 0\n        for stp, data in enumerate(my_training_dataloader):\n            my_optimizer.zero_grad()\n            inputs, targets = data\n            outputs = my_model(inputs)\n            loss = criterion(outputs, targets)\n            accelerator.backward(loss)\n            my_optimizer.step()\n            accelerator.log({\"training_loss\": loss, \"epoch_num\": ep})\n            if accelerator.is_local_main_process:\n                print(f\"train epoch {ep} [{stp}/{len(my_training_dataloader)}] | train loss {loss}\")\n\n        # eval model\n        if accelerator.is_local_main_process:\n            print(f\"begin epoch {ep} evaluating...\")\n        with torch.no_grad():\n            total_acc_num = 0\n            for stp, (inputs, targets) in enumerate(my_testing_dataloader):\n                predictions = my_model(inputs)\n                predictions = torch.argmax(predictions, dim=-1)\n                # Gather all predictions and targets\n                all_predictions, all_targets = accelerator.gather_for_metrics((predictions, targets))\n                acc_num = (all_predictions.long() == all_targets.long()).sum()\n                total_acc_num += acc_num\n                if accelerator.is_local_main_process:\n                    print(f\"eval epoch {ep} [{stp}/{len(my_testing_dataloader)}] | val acc {acc_num/len(all_targets)}\")\n\n            accelerator.log({\"eval acc\": total_acc_num / len(my_testing_dataloader.dataset)})\n\n    accelerator.wait_for_everyone()\n    accelerator.save_model(my_model, \"cifar_cls.pth\")\n\n    accelerator.end_training()\n\n\nif __name__ == \"__main__\":\n    main()\n```",
    "662": "一级标题：🤗HuggingFace Transformers\n二级标题：无\n内容：\n[![](/assets/colab.svg)](https://colab.research.google.com/drive/1iYwrAM4ToCWt5p5hlrrkHlQqBIav_r2E?usp=sharing)\n\nHugging Face 的 [Transformers](https://github.com/huggingface/transformers) 是一个非常流行的开源库，它提供了大量预训练的模型，主要用于自然语言处理（NLP）任务。这个库的目标是使最新的模型能够易于使用，并支持多种框架，如 TensorFlow 和 PyTorch。\n\n![hf-transformers-image](/assets/ig-huggingface-transformers.png)\n\n你可以使用Transformers快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n> `transformers>=4.50.0` 的版本，已官方集成了SwanLab\n> 如果你的版本低于4.50.0，请使用[SwanLabCallback集成](#_4-swanlabcallback集成)。",
    "663": "一级标题：🤗HuggingFace Transformers\n二级标题：1. 一行代码完成集成\n内容：\n只需要在你的训练代码中，找到`TrainingArguments`部分，添加`report_to=\"swanlab\"`参数，即可完成集成。\n\n```python\nfrom transformers import TrainingArguments, Trainer\n\nargs = TrainingArguments(\n    ...,\n    report_to=\"swanlab\" # [!code ++]\n\n)\n\ntrainer = Trainer(..., args=args)\n```\n\n如果你想要设定一下实验名，以区分每次训练，可以设置`run_name`参数：\n\n```python\nargs = TrainingArguments(\n    ...,\n    report_to=\"swanlab\",\n    run_name=\"great_try_1\", # [!code ++]\n)\n```",
    "664": "一级标题：🤗HuggingFace Transformers\n二级标题：2. 自定义项目/工作空间\n内容：\n默认下，项目名会使用你运行代码的`目录名`，实验名等于`output_dir`。\n\n如果你想自定义项目名或工作空间，可以设置`SWANLAB_PROJECT`和`SWANLAB_WORKSPACE`环境变量：\n\n::: code-group\n\n```python\nimport os  # [!code ++]\n\nos.environ[\"SWANLAB_PROJECT\"]=\"qwen2-sft\"  # [!code ++]\nos.environ[\"SWANLAB_WORKSPACE\"]=\"EmotionMachine\"  # [!code ++]\n\n...\n\nfrom transformers import TrainingArguments, Trainer\n\nargs = TrainingArguments(\n    ...,\n    report_to=\"swanlab\",\n    run_name=\"great_try_1\",\n)\n\ntrainer = Trainer(..., args=args)\n```\n\n```bash [Command Line（Linux/MacOS）]\nexport SWANLAB_PROJECT=\"qwen2-sft\"\nexport SWANLAB_WORKSPACE=\"EmotionMachine\"\n```\n\n```bash [Command Line（Windows）]\nset SWANLAB_PROJECT=\"qwen2-sft\"\nset SWANLAB_WORKSPACE=\"EmotionMachine\"\n```\n\n:::",
    "665": "一级标题：🤗HuggingFace Transformers\n二级标题：3. 案例代码：Bert文本分类\n内容：\n```python\nimport evaluate\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\n\ndataset = load_dataset(\"yelp_review_full\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n\nmetric = evaluate.load(\"accuracy\")\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\n\ntraining_args = TrainingArguments(\n    output_dir=\"test_trainer\",\n    num_train_epochs=3,\n    logging_steps=50,\n    report_to=\"swanlab\", # [!code ++]\n    run_name=\"bert_train\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n```",
    "666": "一级标题：🤗HuggingFace Transformers\n二级标题：4. SwanLabCallback集成\n内容：\n如果你使用的是`Transformers<4.50.0`的版本，或者你希望更灵活地控制SwanLab的行为，则可以使用SwanLabCallback集成。\n\n### 4.1 引入SwanLabCallback\n\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\n```\n\n**SwanLabCallback**是适配于Transformers的日志记录类。\n\n**SwanLabCallback**可以定义的参数有：\n\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。\n- 你也可以在外部通过`swanlab.init`创建项目，集成会将实验记录到你在外部创建的项目中。\n\n### 4.2 传入Trainer\n\n```python (1,7,12)\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom transformers import Trainer, TrainingArguments\n\n...\n\n# 实例化SwanLabCallback\nswanlab_callback = SwanLabCallback(project=\"hf-visualization\")\n\ntrainer = Trainer(\n    ...\n    # 传入callbacks参数\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n```\n\n### 4.3 完整案例代码\n\n```python (4,41,50)\nimport evaluate\nimport numpy as np\nimport swanlab\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom datasets import load_dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\n\ndataset = load_dataset(\"yelp_review_full\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n\nmetric = evaluate.load(\"accuracy\")\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\n\ntraining_args = TrainingArguments(\n    output_dir=\"test_trainer\",\n    # 如果只需要用SwanLab跟踪实验，则将report_to参数设置为”none“\n    report_to=\"none\",\n    num_train_epochs=3,\n    logging_steps=50,\n)\n\n# 实例化SwanLabCallback\nswanlab_callback = SwanLabCallback(experiment_name=\"TransformersTest\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n    # 传入callbacks参数\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n```\n\n### 4.4 GUI效果展示\n\n超参数自动记录：\n\n![ig-hf-transformers-gui-1](./huggingface_transformers/card.jpg)\n\n指标记录：\n\n![ig-hf-transformers-gui-2](./huggingface_transformers/chart.jpg)\n\n\n### 4.5 拓展：增加更多回调\n\n试想一个场景，你希望在每个epoch结束时，让模型推理测试样例，并用swanlab记录推理的结果，那么你可以创建一个继承自`SwanLabCallback`的新类，增加或重构生命周期函数。比如：\n\n```python\nclass NLPSwanLabCallback(SwanLabCallback):\n    def on_epoch_end(self, args, state, control, **kwargs):\n        test_text_list = [\"example1\", \"example2\"]\n        log_text_list = []\n        for text in test_text_list:\n            result = model(text)\n            log_text_list.append(swanlab.Text(result))\n\n        swanlab.log({\"Prediction\": test_text_list}, step=state.global_step)\n```\n\n上面是一个在NLP任务下的新回调类，增加了`on_epoch_end`函数，它会在`transformers`训练的每个epoch结束时执行。\n\n查看全部的Transformers生命周期回调函数：[链接](https://github.com/huggingface/transformers/blob/main/src/transformers/trainer_callback.py#L311)",
    "667": "一级标题：🤗HuggingFace Transformers\n二级标题：5. 环境变量\n内容：\n参考：[HuggingFace Docs: transformers.integrations.SwanLabCallback](https://huggingface.co/docs/transformers/main/en/main_classes/callback#transformers.integrations.SwanLabCallback)",
    "668": "一级标题：🤗HuggingFace Trl\n二级标题：无\n内容：\n[TRL](https://github.com/huggingface/trl) (Transformers Reinforcement Learning，用强化学习训练Transformers模型) 是一个领先的Python库，旨在通过监督微调（SFT）、近端策略优化（PPO）和直接偏好优化（DPO）等先进技术，对基础模型进行训练后优化。TRL 建立在 🤗 Transformers 生态系统之上，支持多种模型架构和模态，并且能够在各种硬件配置上进行扩展。\n\n![logo](./huggingface_trl/logo.png)\n\n你可以使用Trl快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n[Demo](https://swanlab.cn/@ZeyiLin/trl-visualization/runs/q1uf2r4wmao7iomc5z1ff/overview)\n\n> `transformers>=4.50.0` 的版本，已官方集成了SwanLab\n> 如果你的版本低于4.50.0，请使用[SwanLabCallback集成](#_5-使用swanlabcallback)。",
    "669": "一级标题：🤗HuggingFace Trl\n二级标题：1. 一行代码集成\n内容：\n只需要在你的训练代码中，找到HF的`Config`部分（比如`SFTConfig`、`GRPOConfig`等），添加`report_to=\"swanlab\"`参数，即可完成集成。\n\n```python\nfrom trl import SFTConfig, SFTTrainer\n\nargs = SFTConfig(\n    ...,\n    report_to=\"swanlab\" # [!code ++]\n)\n\ntrainer = Trainer(..., args=args)\n```",
    "670": "一级标题：🤗HuggingFace Trl\n二级标题：2. 自定义项目名\n内容：\n默认下，项目名会使用你运行代码的`目录名`。\n\n如果你想自定义项目名，可以设置`SWANLAB_PROJECT`环境变量：\n\n::: code-group\n\n```python\nimport os\nos.environ[\"SWANLAB_PROJECT\"]=\"qwen2-sft\"\n```\n\n```bash [Command Line（Linux/MacOS）]\nexport SWANLAB_PROJECT=\"qwen2-sft\"\n```\n\n```bash [Command Line（Windows）]\nset SWANLAB_PROJECT=\"qwen2-sft\"\n```\n\n:::",
    "671": "一级标题：🤗HuggingFace Trl\n二级标题：3. 案例代码\n内容：\n使用Qwen2.5-0.5B-Instruct模型，使用Capybara数据集进行SFT训练：\n\n```python\nfrom trl import SFTConfig, SFTTrainer\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"trl-lib/Capybara\", split=\"train\")\n\ntraining_args = SFTConfig(\n    output_dir=\"Qwen/Qwen2.5-0.5B-SFT\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    num_train_epochs=1,\n    logging_steps=20,\n    learning_rate=2e-5,\n    report_to=\"swanlab\", # [!code ++]\n    )\n\ntrainer = SFTTrainer(\n    args=training_args,\n    model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n    train_dataset=dataset,\n)\n\ntrainer.train()\n```\n\nDPO、GRPO、PPO等同理，只需要将`report_to=\"swanlab\"`传入对应的`Config`即可。",
    "672": "一级标题：🤗HuggingFace Trl\n二级标题：4. GUI效果展示\n内容：\n**超参数自动记录：**\n\n![ig-hf-trl-gui-1](./huggingface_trl/ig-hf-trl-gui-1.png)\n\n**指标记录：**\n\n![ig-hf-trl-gui-2](./huggingface_trl/ig-hf-trl-gui-2.png)",
    "673": "一级标题：🤗HuggingFace Trl\n二级标题：5.使用SwanLabCallback\n内容：\n如果你使用的是`Transformers<4.50.0`的版本，或者你希望更灵活地控制SwanLab的行为，则可以使用SwanLabCallback集成。\n\n### 5.1 引入SwanLabCallback\n\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\n```\n\n**SwanLabCallback**是适配于Transformers的日志记录类。\n\n**SwanLabCallback**可以定义的参数有：\n\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。\n- 你也可以在外部通过`swanlab.init`创建项目，集成会将实验记录到你在外部创建的项目中。\n\n### 5.2 传入Trainer\n\n```python (1,7,12)\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom trl import SFTConfig, SFTTrainer\n\n...\n\n# 实例化SwanLabCallback\nswanlab_callback = SwanLabCallback(project=\"trl-visualization\")\n\ntrainer = SFTTrainer(\n    ...\n    # 传入callbacks参数\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n```\n\n### 5.3 完整案例代码\n\n使用Qwen2.5-0.5B-Instruct模型，使用Capybara数据集进行SFT训练：\n\n```python (3,7,26)\nfrom trl import SFTConfig, SFTTrainer\nfrom datasets import load_dataset\nfrom swanlab.integration.transformers import SwanLabCallback\n\ndataset = load_dataset(\"trl-lib/Capybara\", split=\"train\")\n\nswanlab_callback = SwanLabCallback(\n    project=\"trl-visualization\",\n    experiment_name=\"Qwen2.5-0.5B-SFT\",\n    description=\"测试使用trl框架sft训练\"\n)\n\ntraining_args = SFTConfig(\n    output_dir=\"Qwen/Qwen2.5-0.5B-SFT\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    num_train_epochs=1,\n    logging_steps=20,\n    learning_rate=2e-5,\n    report_to=\"none\",\n    )\n\ntrainer = SFTTrainer(\n    args=training_args,\n    model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n    train_dataset=dataset,\n)\n\ntrainer.train()\n```\n\nDPO、GRPO、PPO等同理，只需要将`SwanLabCallback`传入对应的`Trainer`即可。",
    "674": "一级标题：🤗HuggingFace Trl\n二级标题：5. 环境变量\n内容：\n参考：[HuggingFace Docs: transformers.integrations.SwanLabCallback](https://huggingface.co/docs/transformers/main/en/main_classes/callback#transformers.integrations.SwanLabCallback)",
    "675": "一级标题：Hydra\n二级标题：无\n内容：\n[hydra](https://hydra.cc/)是一个由Facebook AI Research创建的开源框架，旨在简化Python应用程序中配置的创建、管理和使用过程。Hydra通过使配置内容动态化和可组合，大大简化了处理多个配置集合的复杂性，特别是对于那些具有大量参数和需要在多种环境下运行的应用程序而言。\n\n![hydra-image](/assets/hydra-image.jpg)\n\n你可以继续使用 Hydra 进行配置管理，同时使用SwanLab的强大功能。",
    "676": "一级标题：Hydra\n二级标题：跟踪指标\n内容：\n和常规一样，用`swanlab.init`和`swanlab.log`跟踪你的指标。\n假设你的hydra配置文件为`configs/defaults.yaml`，则添加几行：\n\n```yaml\nswanlab:\n  project: \"my-project\"\n```\n\n\n在训练脚本中，将配置文件中的`project`传入：\n\n```python\nimport swanlab\nimport hydra\n\n@hydra.main(config_path=\"configs/\", config_name=\"defaults\")\ndef run_experiment(cfg):\n    run = swanlab.init(project=cfg.swanlab.project)\n    ...\n    swanlab.log({\"loss\": loss})\n```",
    "677": "一级标题：Hydra\n二级标题：跟踪超参数\n内容：\nHydra使用[omegaconf](https://omegaconf.readthedocs.io/en/2.1_branch/)作为与配置字典交互的默认方式。\n\n可以直接将OmegaConf的字典传递给`swanlab.config`：\n\n```python\n@hydra.main(config_path=\"configs/\", config_name=\"defaults\")\ndef run_experiment(cfg):\n    run = swanlab.init(project=cfg.swanlab.project,\n                       config=cfg,\n    )\n    ...\n    swanlab.log({\"loss\": loss})\n    model = Model(**swanlab.config.model.configs)\n```\n\n如果传递`cfg`时出现意外的结果，那么可以先转换`omegaconf.DictConfig`为原始类型：\n\n```python\n@hydra.main(config_path=\"configs/\", config_name=\"defaults\")\ndef run_experiment(cfg):\n    run = swanlab.init(project=cfg.swanlab.project,\n                       config=omegaconf.OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True)\n    ...\n    swanlab.log({\"loss\": loss})\n    model = Model(**swanlab.config.model.configs)\n```",
    "678": "一级标题：Keras\n二级标题：无\n内容：\n[Keras](https://keras.io/) 是一个用 Python 编写的高级神经网络 API，最初由 François Chollet 创建，并于 2017 年合并到 TensorFlow 中，但依然可以作为一个独立的框架使用。它是一个开源的深度学习框架，运行在 TensorFlow、Theano 或 Microsoft Cognitive Toolkit (CNTK) 等深度学习后端之上。\n\n![keras-image](/assets/ig-keras-1.png)\n\n你可以使用Keras快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n[在线演示](https://swanlab.cn/@ZeyiLin/keras_mnist/runs/9gzx3m1ga2q2xb6t6ekxb/chart)",
    "679": "一级标题：Keras\n二级标题：1. 引入SwanLabLogger\n内容：\n```python\nfrom swanlab.integration.keras import SwanLabLogger\n```",
    "680": "一级标题：Keras\n二级标题：2. 与model.fit配合\n内容：\n首先初始化SwanLab：\n\n```python\nswanlab.init(\n    project=\"keras_mnist\",\n    experiment_name=\"mnist_example\",\n    description=\"Keras MNIST Example\"\n    )\n```\n\n然后，在`model.fit`的`callbacks`参数中添加`SwanLabLogger`，即可完成集成：\n\n```python\nmodel.fit(..., callbacks=[SwanLabLogger()])\n```",
    "681": "一级标题：Keras\n二级标题：3. 案例-MNIST\n内容：\n```python\nfrom swanlab.integration.keras import SwanLabLogger\nimport tensorflow as tf\nimport swanlab\n\n# Initialize SwanLab\nswanlab.init(\n    project=\"keras_mnist\",\n    experiment_name=\"mnist_example\",\n    description=\"Keras MNIST Example\"\n    )\n\n# Load and preprocess MNIST data\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\nx_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n\n# Build a simple CNN model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Conv2D(64, 3, activation='relu'),\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Train the model with SwanLabLogger\nmodel.fit(\n    x_train,\n    y_train,\n    epochs=5,\n    validation_data=(x_test, y_test),\n    callbacks=[SwanLabLogger()]\n)\n```\n\n效果演示：\n\n![keras-image](/assets/ig-keras-2.png)\n\n[在线演示](https://swanlab.cn/@ZeyiLin/keras_mnist/runs/9gzx3m1ga2q2xb6t6ekxb/chart)",
    "682": "一级标题：LightGBM\n二级标题：无\n内容：\nLightGBM（Light Gradient Boosting Machine）是一种基于决策树算法的分布式梯度提升框架，由微软公司在2017年发布。它以高效、快速和准确著称，广泛应用于分类、回归和排序等机器学习任务。\n\n![lightgbm](/zh/guide_cloud/integration/lightgbm/logo.png)\n\n你可以使用LightGBM快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。",
    "683": "一级标题：LightGBM\n二级标题：1. 引入SwanLabCallback\n内容：\n```python\nfrom swanlab.integration.lightgbm import SwanLabCallback\n```\n\nSwanLabCallback是适配于LightGBM的日志记录类。",
    "684": "一级标题：LightGBM\n二级标题：2. 初始化SwanLab\n内容：\n```python\nswanlab.init(\n    project=\"lightgbm-example\",\n    experiment_name=\"breast-cancer-classification\"\n)\n```",
    "685": "一级标题：LightGBM\n二级标题：3. 传入`lgb.train`\n内容：\n```python\nimport lightgbm as lgb\n\ngbm = lgb.train(\n    ...\n    callbacks=[SwanLabCallback()]\n)\n```",
    "686": "一级标题：LightGBM\n二级标题：4. 完整测试代码\n内容：\n```python\nimport lightgbm as lgb\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport swanlab\nfrom swanlab.integration.lightgbm import SwanLabCallback\n\n# Step 1: 初始化swanlab\nswanlab.init(project=\"lightgbm-example\", experiment_name=\"breast-cancer-classification\")\n\n# Step 2: 加载数据集\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Step 3: 分割数据集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Step 4: 创建LightGBM数据集\ntrain_data = lgb.Dataset(X_train, label=y_train)\ntest_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n\n# Step 5: 设置参数\nparams = {\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'boosting_type': 'gbdt',\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9\n}\n\n# Step 6: 使用swanlab callback训练模型\nnum_round = 100\ngbm = lgb.train(\n    params,\n    train_data,\n    num_round,\n    valid_sets=[test_data],\n    callbacks=[SwanLabCallback()]\n)\n\n# Step 7: 预测\ny_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\ny_pred_binary = [1 if p >= 0.5 else 0 for p in y_pred]\n\n# Step 8: 评估模型\naccuracy = accuracy_score(y_test, y_pred_binary)\nprint(f\"模型准确率: {accuracy:.4f}\")\nswanlab.log({\"accuracy\": accuracy})\n\n# Step 9: 保存模型\ngbm.save_model('lightgbm_model.txt')\n\n# Step 10: 加载模型并预测\nbst_loaded = lgb.Booster(model_file='lightgbm_model.txt')\ny_pred_loaded = bst_loaded.predict(X_test)\ny_pred_binary_loaded = [1 if p >= 0.5 else 0 for p in y_pred_loaded]\n\n# Step 11: 评估加载模型\naccuracy_loaded = accuracy_score(y_test, y_pred_binary_loaded)\nprint(f\"加载模型后的准确率: {accuracy_loaded:.4f}\")\nswanlab.log({\"accuracy_loaded\": accuracy_loaded})\n\n# Step 12: 结束swanlab实验\nswanlab.finish()\n```",
    "687": "一级标题：LLaMA Factory\n二级标题：无\n内容：\n[[toc]]",
    "688": "一级标题：LLaMA Factory\n二级标题：0. 前言\n内容：\n![](/zh/guide_cloud/integration/llama_factory/0.png)\n\n我们非常高兴地宣布**SwanLab**与[LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory)建立合作伙伴关系，致力于为中国训练者提供优质、高效的大模型训练体验。\n\n现在你使用新版本的LLaMA Factory启动训练前，可以在WebUI的「SwanLab configurations」（中文：SwanLab参数设置）卡片中勾选「Use SwanLab」，就可以通过SwanLab强大的训练看板进行这一次大模型微调的跟踪、记录与可视化。\n\n![](/zh/guide_cloud/integration/llama_factory/1.png)\n\nLLaMA Factory 是一个用于微调大语言模型 (LLM) 的开源工具包，它提供了一个统一且高效的框架，支持 100 多个 LLM （包括Qwen、LLaMA、ChatGLM、Mistral等）的微调，涵盖了各种训练方法、数据集和先进算法。\n\n大语言模型的微调是一个上手门槛颇高的工作，LLaMA Factory通过提供用户友好的 Web UI 和命令行界面，结合其统一且高效的框架，大幅降低了大模型从微调到测试评估的上手门槛。\n\n为了提供用户更好的大模型微调过程监控与日志记录体验，我们与LLaMA Factory团队合作开展了两项举措：利用SwanLab增强LLaMA Factory的实验监控能力，以及在SwanLab中记录 LLaMA Factory的专属超参数。\n\n\n> LLaMA Factory：https://github.com/hiyouga/LLaMA-Factory\n> SwanLab：https://swanlab.cn\n> SwanLab开源仓库：https://github.com/SwanHubX/SwanLab\n> 实验过程：https://swanlab.cn/@ZeyiLin/llamafactory/runs/y79f9ri9jr1mkoh24a7g8/chart\n\n我们将以使用LLaMA Factory + SwanLab可视化微调Qwen2.5为案例。",
    "689": "一级标题：LLaMA Factory\n二级标题：1. 安装环境\n内容：\n首先，你需要确保你拥有Python3.8以上环境与Git工具，然后克隆仓库：\n\n```shellscript\ngit clone --depth 1 https://github.com/hiyouga/LLaMA-Factory\n```\n\n安装相关环境：\n\n```shellscript\ncd LLaMA-Factory\npip install -e \".[torch,metrics,swanlab]\"\n```\n\n> 如果你是昇腾NPU用户，可以访问：[华为NPU适配](https://llamafactory.readthedocs.io/zh-cn/latest/advanced/npu.html) 查看昇腾NPU版安装教程。",
    "690": "一级标题：LLaMA Factory\n二级标题：2. 使用LLaMA Board开启训练\n内容：\nLLaMA Board是基于Gradio的可视化微调界面，你可以通过下面的代码启动LLaMA Board：\n\n```shellscript\nllamafactory-cli webui\n```\n\n提示：LLaMA Factory默认的模型/数据集下载源是HuggingFace，如果你所在的网络环境对与HuggingFace下载并不友好，可以在启动LLaMA Board之前，将下载源设置为魔搭社区或魔乐社区：\n\n```shellscript\n# 下载源改为魔搭社区\nexport USE_MODELSCOPE_HUB=1 # Windows 使用 `set USE_MODELSCOPE_HUB=1`\n\n# 下载源改为魔乐社区\nexport USE_OPENMIND_HUB=1 # Windows 使用 `set USE_OPENMIND_HUB=1`\n```\n\n执行 llamafactory-cli webui 之后，你可以在浏览器看到下面的UI界面。本案例选择Qwen2-1.5B-instruct作为模型，alpaca\\_zh\\_demo作为数据集：\n\n![](/zh/guide_cloud/integration/llama_factory/2.png)\n\n在页面的下方，你会看到一个「SwanLab参数设置」的卡片，展开后，你就可以配置SwanLab的项目名、实验名、工作区、API 密钥以及模式等参数。\n\n> 如果你是第一次使用SwanLab，还需要在 swanlab.cn 注册一个账号获取专属的API密钥。\n\n我们勾&#x9009;**「使用SwanLab」：**\n\n![](/zh/guide_cloud/integration/llama_factory/3.png)\n\n现在，点&#x51FB;**「开始」按钮**，就可以开启微调：\n\n![](/zh/guide_cloud/integration/llama_factory/4.png)\n\n在完成载入模型、载入数据集，正式开启微调后，我们可以在命令行界面找到SwanLab部分：\n\n![](/zh/guide_cloud/integration/llama_factory/5.png)\n\n点击箭头对应的实验链接，就可以在**浏览器**中打开SwanLab实验跟踪看板：\n\n![](/zh/guide_cloud/integration/llama_factory/6.png)\n\n在「卡片」栏下的「配置」表中，第一个就会是LLamaFactory，标识了这次训练的使用框架。\n\n![](/zh/guide_cloud/integration/llama_factory/7.png)",
    "691": "一级标题：LLaMA Factory\n二级标题：3. 使用命令行开启训练\n内容：\nLLaMA Factory还支持通过yaml配置文件，在命令行中进行微调。\n\n我们编辑LLaMA Factory项目目录下的 **examples/train\\_lora/qwen2vl\\_lora\\_sft.yaml** 文件，在文件尾部增加：\n\n```yaml\n...\n\n### swanlab\nuse_swanlab: true\nswanlab_project: llamafactory\nswanlab_run_name: Qwen2-VL-7B-Instruct\n```\n\n然后运行：\n\n```shellscript\nllamafactory-cli train examples/train_lora/qwen2vl_lora_sft.yaml\n```\n\n在完成载入模型、载入数据集，正式开启微调后，与LLaMA Board一样，可以在命令行界面找到SwanLab部分，通过实验链接访问SwanLab实验看板。\n\n![](/zh/guide_cloud/integration/llama_factory/8.png)\n\n![](/zh/guide_cloud/integration/llama_factory/9.png)\n\n***\n\n致敬 LLaMA Factory 团队，感谢他们为开源社区提供了这么一个优秀的模型训练工具。随着我们的继续合作，敬请期待SwanLab工具为大模型训练师提供更深入、强大的实验跟踪功能。",
    "692": "一级标题：LLaMA Factory\n二级标题：4.附录：支持的参数\n内容：\n```yaml\n# swanlab\nuse_swanlab: true\nswanlab_project: your_project_name\nswanlab_run_name: your_experiment_name\nswanlab_workspace: your_workspace\nswanlab_mode: your_mode\nswanlab_api_key: your_api_key\n```\n\n> 更多可见：[LLaMA Factory - Github](https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md#%E5%AE%89%E8%A3%85-llama-factory) 中的`SwanLabArguments`类。",
    "693": "一级标题：MLFlow\n二级标题：无\n内容：\n[MLFlow](https://github.com/mlflow/mlflow) 是一个开源的机器学习生命周期管理平台，由 Databricks 创建并维护。它旨在帮助数据科学家和机器学习工程师更高效地管理机器学习项目的整个生命周期，包括实验跟踪、模型管理、模型部署和协作。MLflow 的设计是模块化的，可以与任何机器学习库、框架或工具集成。\n\n![mlflow](./mlflow/logo.png)\n\n:::warning 其他工具的同步教程\n\n- [TensorBoard](/guide_cloud/integration/integration-tensorboard.md)\n- [Weights & Biases](/guide_cloud/integration/integration-wandb.md)\n:::\n\n**你可以用两种方式将MLflow上的项目同步到SwanLab：**\n\n1. **同步跟踪**：如果你现在的项目使用了mlflow进行实验跟踪，你可以使用`swanlab.sync_mlflow()`命令，在运行训练脚本时同步记录指标到SwanLab。\n2. **转换已存在的项目**：如果你想要将mlflow上的项目复制到SwanLab，你可以使用`swanlab convert`，将mlflow上已存在的项目转换成SwanLab项目。\n\n::: info\n在当前版本暂仅支持转换标量图表。\n:::\n\n[[toc]]",
    "694": "一级标题：MLFlow\n二级标题：1. 同步跟踪\n内容：\n### 1.1 添加sync_mlflow命令\n\n在你的代码执行`mlflow.start_run()`之前的任何位置，添加一行`swanlab.sync()`命令，即可在运行训练脚本时同步记录指标到SwanLab。\n\n```python\nimport swanlab\n\nswanlab.sync_mlflow()\n\n...\n\nmlflow.start_run()\n```\n\n在上述这种代码写法中，`mlflow.start_run()`的同时会初始化swanlab，项目名、实验名和配置和`mlflow.start_run()`中的`experiment_name`、`run_name`、`log_param`一致，因此你不需要再手动初始化swanlab。\n\n\n### 1.2 另一种写法\n\n另一种用法是先手动初始化swanlab，再运行mlflow的代码。\n\n```python\nimport swanlab\n\nswanlab.init(...)\nswanlab.sync_mlflow()\n```\n\n在这种写法中，项目名、实验名、配置和`swanlab.init()`中的`project`、`experiment_name`、`config`一致，而后续`mlflow.start_run()`中的`experiment_name`、`run_name`会被忽略，`config`会更新进`swanlab.config`中。\n\n### 1.3 测试代码\n\n```python\nimport mlflow\nimport random\nimport swanlab\n\nswanlab.sync_mlflow()\n\nmlflow.set_experiment(\"mlflow_sync_test\")\n\nwith mlflow.start_run(run_name=\"test_run\"):\n    mlflow.log_param(\"learning_rate\", 0.01)\n    mlflow.log_params({\"batch_size\": 32, \"epochs\": 10})\n\n    for epoch in range(10):\n        acc = 1 - 2 ** -epoch - random.random() / epoch\n        loss = 2 ** -epoch + random.random() / epoch\n        mlflow.log_metric(\"accuracy\", acc, step=epoch)\n        mlflow.log_metric(\"loss\", loss, step=epoch)\n\n        mlflow.log_metrics({\n            \"precision\": acc * 0.9,\n            \"recall\": acc * 0.8\n        }, step=epoch)\n```",
    "695": "一级标题：MLFlow\n二级标题：2. 转换已经存在的项目\n内容：\n### 2.1 准备工作\n\n**（必须）mlflow服务的url链接**\n\n首先，需要记下mlflow服务的**url链接**，如`http://127.0.0.1:5000`。\n\n> 如果还没有启动mlflow服务，那么需要使用`mlflow ui`命令启动服务，并记下url链接。\n\n**（可选）实验ID**\n\n如果你只想转换其中的一组实验，那么在下图所示的地方，记下该实验ID。\n\n![](./mlflow/ui-1.png)\n\n### 2.2 方式一：命令行转换\n\n转换命令行：\n\n```bash\nswanlab convert -t mlflow --mlflow-url <MLFLOW_URL> --mlflow-exp <MLFLOW_EXPERIMENT_ID>\n```\n\n支持的参数如下：\n\n- `-t`: 转换类型，可选wandb、tensorboard和mlflow。\n- `-p`: SwanLab项目名。\n- `-w`: SwanLab工作空间名。\n- `--cloud`: (bool) 是否上传模式为\"cloud\"，默认为True\n- `-l`: logdir路径。\n- `--mlflow-url`: mlflow服务的url链接。\n- `--mlflow-exp`: mlflow实验ID。\n\n如果不填写`--mlflow-exp`，则会将指定项目下的全部实验进行转换；如果填写，则只转换指定的实验组。\n\n### 2.3 方式二：代码内转换\n\n```python\nfrom swanlab.converter import MLFLowConverter\n\nmlflow_converter = MLFLowConverter(project=\"mlflow_converter\")\n# mlflow_exp可选\nmlflow_converter.run(tracking_uri=\"http://127.0.0.1:5000\", experiment=\"1\")\n```\n\n效果与命令行转换一致。\n\n`MLFLowConverter`支持的参数：\n\n- `project`: SwanLab项目名。\n- `workspace`: SwanLab工作空间名。\n- `cloud`: (bool) 是否上传模式为\"cloud\"，默认为True。\n- `logdir`: wandb Run（项目下的某一个实验）的id。",
    "696": "一级标题：MMDetection\n二级标题：无\n内容：\n:::info 教程\n[mmdetection如何使用swanlab远程查看训练日志](https://zhuanlan.zhihu.com/p/699058426)\n:::\n\n<div align=\"center\">\n<img src=\"/assets/integration-mmdetection.png\" width=600>\n</div>\n\n[MMdetection](https://github.com/open-mmlab/mmdetection) 是一个由 [OpenMMLab](https://openmmlab.com/) 社区开发的深度学习训练框架，建立在 PyTorch 深度学习框架之上，旨在为研究人员和工程师提供一个高效、灵活、易于扩展的目标检测平台。MMDetection 支持多种主流的目标检测方法，并提供了大量预训练模型和丰富的配置选项，使得在目标检测任务中的应用和开发变得更加便捷。\n\n<div align=\"center\">\n<img src=\"/assets/integration-mmdetection-intro.png\">\n</div>\n\n可以通过修改MMDetection的配置文件来使用SwanLab作为实验记录工具。",
    "697": "一级标题：MMDetection\n二级标题：在配置文件中指定SwanLab作为VisBackend\n内容：\n确保你安装了SwanLab，或者使用`pip install -U swanlab`安装最新版。\n\n将如下内容添加到mmdetection的config文件中, 其中`init_kwargs`中填入的参数字典与`swanlab.init`的规则一致:\n\n```python\n# swanlab visualizer\ncustom_imports = dict(  # 引入SwanLab作为日志记录器，对于部分不支持custom_imports的项目可以直接初始化SwanlabVisBackend并加入vis_backends\n    imports=[\"swanlab.integration.mmengine\"], allow_failed_imports=False\n)\n\nvis_backends = [\n    dict(\n        type=\"SwanlabVisBackend\",\n        init_kwargs={ # swanlab.init 参数\n            \"project\": \"swanlab-mmengine\",\n            \"experiment_name\": \"Your exp\",  # 实验名称\n            \"description\": \"Note whatever you want\",  # 实验的描述信息\n        },\n    ),\n]\n\nvisualizer = dict(\n    type=\"Visualizer\",\n    vis_backends=vis_backends,\n    name=\"visualizer\",\n)\n```\n\n> 有关其他引入方法和更灵活的配置，可以参考[MMEngine接入SwanLab](https://docs.swanlab.cn/zh/guide_cloud/integration/integration-mmengine.html)",
    "698": "一级标题：MMDetection\n二级标题：使用案例：MMDetection训练faster-rcnn\n内容：\n首先克隆[MMDetction](https://github.com/open-mmlab/mmdetection)项目到本地。\n\n然后在faster-rnn对应的config文件（`configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py`）的最后增加下面的代码：\n\n```python\n_base_ = [\n    '../_base_/models/faster-rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n\n# swanlab\ncustom_imports = dict(  # 引入SwanLab作为日志记录器\n    imports=[\"swanlab.integration.mmengine\"], allow_failed_imports=False\n)\nvis_backends = [\n    dict(type=\"LocalVisBackend\"),\n    dict(\n        type=\"SwanlabVisBackend\",\n        init_kwargs={  # swanlab.init 参数\n            \"project\": \"MMDetection\",  # 项目名称\n            \"experiment_name\": \"faster-rcnn\",  # 实验名称\n            \"description\": \"faster-rcnn r50 fpn 1x coco\",  # 实验的描述信息\n        },\n    ),\n]\nvisualizer = dict(\n    type=\"DetLocalVisualizer\", vis_backends=vis_backends, name=\"visualizer\"\n)\n```\n\n**然后开启训练即可**：\n\n```bash\npython tools/train.py configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py\n```\n\n![ig-mmengine-1](/assets/ig-mmengine-1.png)\n\n**在swanlab中远程查看训练日志**：\n\n![ig-mmengine-2](/assets/ig-mmengine-2.png)",
    "699": "一级标题：MMEngine\n二级标题：无\n内容：\n[MMEngine](https://github.com/open-mmlab/mmengine) 是一个由 [OpenMMLab](https://openmmlab.com/) 社区开发的深度学习训练框架，专为深度学习研究和开发而设计。MMEngine 提供了一种高效、灵活且用户友好的方式来构建、训练和测试深度学习模型，尤其是在计算机视觉领域。它的目标是简化研究人员和开发者在深度学习项目中的工作流程，并提高其开发效率。\n\n<div align=\"center\">\n<img src=\"/assets/integration-mmengine.jpeg\" width=440>\n</div>\n\nMMEngine 为 OpenMMLab 算法库实现了下一代训练架构，为 OpenMMLab 中的 30 多个算法库提供了统一的执行基础。其核心组件包括训练引擎、评估引擎和模块管理。\n\nSwanLab将专为MMEngine设计的`SwanlabVisBackend`集成到MMEngine中，可用于记录训练、评估指标、记录实验配置、记录图像等。\n\n::: warning MM生态的其他集成\n\n- [MMPretrain](/zh/guide_cloud/integration/integration-mmpretrain.md)\n- [MMDetection](/zh/guide_cloud/integration/integration-mmdetection.md)\n- [MMSegmentation](/zh/guide_cloud/integration/integration-mmsegmentation.md)\n- [XTuner](/zh/guide_cloud/integration/integration-xtuner.md)\n\n:::",
    "700": "一级标题：MMEngine\n二级标题：MMEngine系列框架兼容性说明\n内容：\n使用mmengine的框架都可以使用如下方法引入SwanLab，比如MM官方框架 [mmdetection](https://docs.swanlab.cn/zh/guide_cloud/integration/integration-mmdetection.html)，[mmsegmentation](https://docs.swanlab.cn/zh/guide_cloud/integration/integration-mmsegmentation.html)等，以及[自己基于mmengine实现的训练框架](https://mmengine.readthedocs.io/zh-cn/latest/get_started/15_minutes.html)。\n\n> 可以在[OpenMMLab官方GitHub账号](https://github.com/open-mmlab)下查看有哪些优秀框架。\n\n部分框架比如[Xtuner](https://github.com/InternLM/xtuner)项目，其没有完全兼容mmengine，需要做一些简单改动，可以前往[SwanLab的Xtuner集成](https://docs.swanlab.cn/zh/guide_cloud/integration/integration-xtuner.html)查看如何在Xtuner中使用SwanLab。\n\nmmengine有两种引入SwanLab进行实验可视化跟踪的方法：",
    "701": "一级标题：MMEngine\n二级标题：使用方法一：训练脚本传入visualizer，开始训练\n内容：\n:::info\n可以参考[mmengine15分钟教程](https://mmengine.readthedocs.io/zh-cn/latest/get_started/15_minutes.html)将自己的训练代码适配mmengine\n:::\n\n确保你安装了SwanLab，或者使用`pip install -U swanlab`安装最新版。\n\n如果你按照官方案例使用了mmengine作为你的训练框架。只需在训练脚本中进行如下改动：\n1. 在初始化`visualizer`时加入SwanlabVisBackend\n2. 初始化`runner`传入`visualizer`即可：\n\n```python (10,20)\nfrom mmengine.visualization import Visualizer\nfrom mmengine.runner import Runner\n\nfrom swanlab.integration.mmengine import SwanlabVisBackend\n...\n# 初始化SwanLab\nswanlab_vis_backend = SwanlabVisBackend(init_kwargs={})# init args can be found in https://docs.swanlab.cn/zh/guide_cloud/integration/integration-mmengine.html\n# 初始化mmegine的Visulizer，并引入SwanLab作为Visual Backend\nvisualizer = Visualizer(\n    vis_backends=swanlab_vis_backend\n)\n\n# 构建mmengine的Runner\nrunner = Runner(\n    model,\n    work_dir='runs/gan/',\n    train_dataloader=train_dataloader,\n    train_cfg=train_cfg,\n    optim_wrapper=opt_wrapper_dict,\n    visualizer=visualizer,\n)\n\n# 开始训练\nrunner.train()\n```\n\n如果希望像平常使用swanlab那样指定实验名等信息，可以在实例化SwanlabVisBackend时在init_kwargs中指定参数，具体参考[init api](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/sdk.py#L71)，不过不像使用`swanlab.init`那样直接作为参数传入，而是需要构建字典。\n\n下面列举了两者在交互上的不同：\n\n直接使用`swanlab.init`:\n\n```python\nrun = swanlab.init(\n    project=\"cat-dog-classification\",\n    experiment_name=\"Resnet50\",\n    description=\"我的第一个人工智能实验\",\n)\n```\n\n使用`SwanlabVisBackend`，则是以字典的形式传入`init`的参数:\n\n```python\nswanlab_vis_backend = SwanlabVisBackend(\n    init_kwargs={\n        \"project\": \"cat-dog-classification\",\n        \"experiment_name\": \"Resnet50\",\n        \"description\": \"我的第一个人工智能实验\",\n    }\n)\n```",
    "702": "一级标题：MMEngine\n二级标题：使用方法二：config文件引入SwanlabVisBackend\n内容：\n:::info\n此方法对于大多数基于mmengine的训练框架都是适用的\n:::\n\n将如下内容添加到mm系列框架的任意config文件中, 其中`init_kwargs`中填入的参数字典与`swanlab.init`的规则一致:\n\n```python\n# swanlab visualizer\ncustom_imports = dict(  # 引入SwanLab作为日志记录器，对于部分不支持custom_imports的项目可以直接初始化SwanlabVisBackend并加入vis_backends\n    imports=[\"swanlab.integration.mmengine\"], allow_failed_imports=False\n)\n\nvis_backends = [\n    dict(\n        type=\"SwanlabVisBackend\",\n        init_kwargs={ # swanlab.init 参数\n            \"project\": \"swanlab-mmengine\",\n            \"experiment_name\": \"Your exp\",  # 实验名称\n            \"description\": \"Note whatever you want\",  # 实验的描述信息\n        },\n    ),\n]\n\nvisualizer = dict(\n    type=\"Visualizer\",\n    vis_backends=vis_backends,\n    name=\"visualizer\",\n)\n```\n\n可以使用如下代码测试config文件是否能够成功引入SwanLab，将上面的config文件保存为`my_swanlab_config.py`，创建一个`test_config.py`写入如下代码并运行：\n\n```python\nfrom mmengine.config import Config\nimport mmengine\n\nprint(mmengine.__version__)\ncfg = Config.fromfile(\n    \"my_swanlab_config.py\"\n)\n\nfrom mmengine.registry import VISUALIZERS\n\ncustom_vis = VISUALIZERS.build(cfg.visualizer)\nprint(custom_vis)\n\n```\n\n如果看到终端打印出类似如下信息，则表示成功引入了swanlab：\n\n```console\nMMEngine Version: 0.10.4\nSwanLab Version: 0.3.11\n<mmengine.visualization.visualizer.Visualizer object at 0x7f7cf15b1e20>\n```",
    "703": "一级标题：MMEngine\n二级标题：3.案例：MMEngine训练ResNet-50\n内容：\n:::info 参考MMEngine官方15分钟上手教程\n[15 分钟上手 MMENGINE](https://mmengine.readthedocs.io/zh-cn/latest/get_started/15_minutes.html)\n:::\n\n按照[MMEngine官方教程](https://mmengine.readthedocs.io/zh-cn/latest/get_started/installation.html)安装MMEngine。\n\n这里将安装环境的命令抄录下来，强烈建议按照官方文档安装，以环境为python3.11，CUDA12.1为例。\n\n```sh\n# with cuda12.1 or you can find torch version you want at pytorch.org\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\npip install -U openmim\nmim install mmengine\npip install swanlab\n```\n\n使用如下代码构建ResNet-50网络并引入Cifar10数据集开始训练\n\n```python\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.optim import SGD\nfrom torch.utils.data import DataLoader\n\nfrom mmengine.evaluator import BaseMetric\nfrom mmengine.model import BaseModel\nfrom mmengine.runner import Runner\nfrom mmengine.visualization import Visualizer\n\nfrom swanlab.integration.mmengine import SwanlabVisBackend\n\n\nclass MMResNet50(BaseModel):\n    def __init__(self):\n        super().__init__()\n        self.resnet = torchvision.models.resnet50()\n\n    def forward(self, imgs, labels, mode):\n        x = self.resnet(imgs)\n        if mode == \"loss\":\n            return {\"loss\": F.cross_entropy(x, labels)}\n        elif mode == \"predict\":\n            return x, labels\n\n\nclass Accuracy(BaseMetric):\n    def process(self, data_batch, data_samples):\n        score, gt = data_samples\n        self.results.append(\n            {\n                \"batch_size\": len(gt),\n                \"correct\": (score.argmax(dim=1) == gt).sum().cpu(),\n            }\n        )\n\n    def compute_metrics(self, results):\n        total_correct = sum(item[\"correct\"] for item in results)\n        total_size = sum(item[\"batch_size\"] for item in results)\n        return dict(accuracy=100 * total_correct / total_size)\n\n\nnorm_cfg = dict(mean=[0.491, 0.482, 0.447], std=[0.202, 0.199, 0.201])\ntrain_dataloader = DataLoader(\n    batch_size=32,\n    shuffle=True,\n    dataset=torchvision.datasets.CIFAR10(\n        \"data/cifar10\",\n        train=True,\n        download=True,\n        transform=transforms.Compose(\n            [\n                transforms.RandomCrop(32, padding=4),\n                transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                transforms.Normalize(**norm_cfg),\n            ]\n        ),\n    ),\n)\n\nval_dataloader = DataLoader(\n    batch_size=32,\n    shuffle=False,\n    dataset=torchvision.datasets.CIFAR10(\n        \"data/cifar10\",\n        train=False,\n        download=True,\n        transform=transforms.Compose(\n            [transforms.ToTensor(), transforms.Normalize(**norm_cfg)]\n        ),\n    ),\n)\n\nvisualizer = Visualizer(\n    vis_backends=SwanlabVisBackend(init_kwargs={})\n)  # init args can be found in https://docs.swanlab.cn/zh/guide_cloud/integration/integration-mmengine.html\n\nrunner = Runner(\n    model=MMResNet50(),\n    work_dir=\"./work_dir\",\n    train_dataloader=train_dataloader,\n    optim_wrapper=dict(optimizer=dict(type=SGD, lr=0.001, momentum=0.9)),\n    train_cfg=dict(by_epoch=True, max_epochs=5, val_interval=1),\n    val_dataloader=val_dataloader,\n    val_cfg=dict(),\n    val_evaluator=dict(type=Accuracy),\n    visualizer=visualizer,\n)\nrunner.train()\n\n```\n\n可以在[公开训练图表](https://swanlab.cn/@ShaohonChen/cifar10_with_resnet50/runs/f8znz8vj06huv6rm7j5a8/chart)查看到上脚本的训练结果。\n\n<div align=\"center\">\n<img src=\"/assets/integration-mmegine-train.png\" width=600>\n</div>",
    "704": "一级标题：MMPretrain\n二级标题：无\n内容：\n[MMPretrain](https://github.com/open-mmlab/mmpretrain) 是 [OpenMMLab](https://openmmlab.com/) 旗下的一个开源预训练模型库，专注于为计算机视觉任务提供高效、易用的预训练模型。\n\n<div align=\"center\">\n<img src=\"/assets/integration-mmpretrain.jpg\" width=440>\n</div>\n\n基于 PyTorch 构建，MMPretrain 旨在帮助研究人员和开发人员快速应用和评估预训练模型，从而提升下游任务的性能和效率。该库包含了多种预训练模型，如 ResNet、Vision Transformer（ViT）和 Swin Transformer 等，这些模型经过大规模数据集的训练，能够直接用于图像分类、目标检测和分割等任务。此外，MMPretrain 提供了灵活的配置系统和丰富的接口，用户可以方便地进行模型的加载、微调和评估。详细的文档和教程使得用户能够快速上手和应用，适用于学术研究和工业实践中的各种场景。通过使用 MMPretrain，用户可以显著减少模型训练时间，专注于模型优化和应用创新。\n\n可以通过修改MMPretrain的配置文件来使用SwanLab作为实验记录工具。",
    "705": "一级标题：MMPretrain\n二级标题：在配置文件中指定\n内容：\n确保你安装了SwanLab，或者使用`pip install -U swanlab`安装最新版。\n\n将如下内容添加到所使用的mmpretrain的config文件中, 其中`init_kwargs`中填入的参数字典与`swanlab.init`的规则一致:\n\n```python\n...\n# swanlab visualizer\ncustom_imports = dict(  # 引入SwanLab作为日志记录器，对于部分不支持custom_imports的项目可以直接初始化SwanlabVisBackend并加入vis_backends\n    imports=[\"swanlab.integration.mmengine\"], allow_failed_imports=False\n)\n\nvis_backends = [\n    dict(\n        type=\"SwanlabVisBackend\",\n        init_kwargs={ # swanlab.init 参数\n            \"project\": \"swanlab-mmengine\",\n            \"experiment_name\": \"Your exp\",  # 实验名称\n            \"description\": \"Note whatever you want\",  # 实验的描述信息\n        },\n    ),\n]\n\nvisualizer = dict(\n    type=\"Visualizer\",\n    vis_backends=vis_backends,\n    name=\"visualizer\",\n)\n...\n```\n\n> 有关其他引入方法和更灵活的配置，可以参考[MMEngine接入SwanLab](https://docs.swanlab.cn/zh/guide_cloud/integration/integration-mmengine.html)",
    "706": "一级标题：MMSegmentation\n二级标题：无\n内容：\n[MMSegmentation](https://github.com/open-mmlab/mmengine) 是一个由 [OpenMMLab](https://openmmlab.com/) 社区开发的深度学习训练框架，基于 PyTorch 构建，旨在为研究人员和开发人员提供便捷高效的图像分割解决方案。\n\n<div align=\"center\">\n<img src=\"/assets/integration-mmsegmentation.png\" width=440>\n</div>\n\n该工具箱采用模块化设计，提供多种预训练模型如 U-Net、DeepLabV3 和 PSPNet 等，支持语义分割、实例分割和全景分割任务。MMSegmentation 内置强大的数据处理功能和多种分割性能评价指标，如 mIoU 和 Dice 系数，能够全面评估模型性能。其灵活的配置系统允许用户快速进行实验配置和参数调整。\n\n<div align=\"center\">\n<img src=\"/assets/integration-mmsegmentation-demo.gif\">\n</div>\n\nMMSegmentation 提供详细的文档和示例，帮助用户快速上手，并支持分布式训练和模型加速推理。该工具箱广泛应用于医学图像分割、遥感图像分割和自动驾驶等领域。\n\n可以通过修改MMSegmentation的配置文件来使用SwanLab作为实验记录工具。",
    "707": "一级标题：MMSegmentation\n二级标题：在配置文件中指定\n内容：\n确保你安装了SwanLab，或者使用`pip install -U swanlab`安装最新版。\n\n将如下内容添加到所使用的mmsegmentation的config文件中, 其中`init_kwargs`中填入的参数字典与`swanlab.init`的规则一致:\n\n```python\n...\n# swanlab visualizer\ncustom_imports = dict(  # 引入SwanLab作为日志记录器，对于部分不支持custom_imports的项目可以直接初始化SwanlabVisBackend并加入vis_backends\n    imports=[\"swanlab.integration.mmengine\"], allow_failed_imports=False\n)\n\nvis_backends = [\n    dict(\n        type=\"SwanlabVisBackend\",\n        init_kwargs={ # swanlab.init 参数\n            \"project\": \"swanlab-mmengine\",\n            \"experiment_name\": \"Your exp\",  # 实验名称\n            \"description\": \"Note whatever you want\",  # 实验的描述信息\n        },\n    ),\n]\n\nvisualizer = dict(\n    type=\"Visualizer\",\n    vis_backends=vis_backends,\n    name=\"visualizer\",\n)\n...\n```\n\n> 有关其他引入方法和更灵活的配置，可以参考[MMEngine接入SwanLab](https://docs.swanlab.cn/zh/guide_cloud/integration/integration-mmengine.html)",
    "708": "一级标题：Omegaconf\n二级标题：无\n内容：\nOmegaConf 是一个用于处理配置的 Python 库，尤其适用于需要灵活配置和配置合并的场景。\nOmegaConf 与swanlab的集成非常简单，直接将`omegaconf`对象传递给`swanlab.config`，即可记录为超参数：\n\n```python\nfrom omegaconf import OmegaConf\nimport swanlab\n\ncfg = OmegaConf.load(\"config.yaml\")\nswanlab.init(config=cfg,)\n```\n\n如果传递`cfg`时出现意外的结果，那么可以先转换`omegaconf.DictConfig`为原始类型：\n\n```python\nfrom omegaconf import OmegaConf\nimport swanlab\n\ncfg = OmegaConf.load(\"config.yaml\")\nswanlab.init(config=OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True))\n\n```",
    "709": "一级标题：OpenAI\n二级标题：无\n内容：\n[openai](https://github.com/openai/openai-python)是ChatGPT在Python环境下使用的核心库。\n\n![openai](/assets/ig-openai.png)\n\n你可以使用openai获得ChatGPT的回复，同时使用SwanLab自动进行过程记录。",
    "710": "一级标题：OpenAI\n二级标题：1. 引入autolog\n内容：\n```python\nfrom swanlab.integration.openai import autolog\n```\n\n`autolog`是一个为openai适配的过程记录类，能够自动记录你的openai交互的过程。",
    "711": "一级标题：OpenAI\n二级标题：2. 传入参数\n内容：\n```python\nautolog(init={\"project\":\"openai_autologging\", \"experiment_name\":\"chatgpt4.0\"})\n```\n\n这里给`init`传入的参数与`swanlab.init`的参数形式完全一致。",
    "712": "一级标题：OpenAI\n二级标题：3. 自动记录\n内容：\n由于`openai`在1.0.0版本以后，采用了和先前不一样的API设计，所以下面分为两个版本：\n\n### openai>=1.0.0\n\n需要将`client=openai.OpenAI()`替换为`client=autolog.client`。\n\n```python\nfrom swanlab.integration.openai import autolog\n\nautolog(init=dict(experiment_name=\"openai_autologging\"))\nclient = autolog.client\n\n# chat_completion\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-0125\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n        {\"role\": \"user\", \"content\": \"Who won the world series in 2015?\"},\n    ],\n)\n\n# text_completion\nresponse2 = client.completions.create(model=\"gpt-3.5-turbo-instruct\", prompt=\"Write a song for jesus.\")\n```\n\n### openai<=0.28.0\n\n```python\nimport openai\nfrom swanlab.integration.openai import autolog\n\nautolog(init=dict(experiment_name=\"openai_logging123\"))\n\nchat_request_kwargs = dict(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers\"},\n        {\"role\": \"user\", \"content\": \"Where was it played?\"},\n    ],\n)\nresponse = openai.ChatCompletion.create(**chat_request_kwargs)\n```",
    "713": "一级标题：PaddleDetection\n二级标题：无\n内容：\n[PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection) 是百度基于其深度学习框架 PaddlePaddle 开发的一个端到端的目标检测开发工具包。它支持对象检测、实例分割、多对象跟踪和实时多人关键点检测，旨在帮助开发者更高效地进行目标检测模型的开发和训练。\n\n![PaddleDetection](/assets/ig-paddledetection-1.png)\n\n你可以使用PaddleDetection快速进行目标检测模型训练，同时使用SwanLab进行实验跟踪与可视化。",
    "714": "一级标题：PaddleDetection\n二级标题：1. 引入SwanLabCallback\n内容：\n首先在你clone的PaddleDetection项目中，找到`ppdet/engine/callbacks.py`文件，在代码的底部添加如下代码：\n\n```python\nclass SwanLabCallback(Callback):\n    def __init__(self, model):\n        super(SwanLabCallback, self).__init__(model)\n\n        try:\n            import swanlab\n            self.swanlab = swanlab\n        except Exception as e:\n            logger.error('swanlab not found, please install swanlab. '\n                         'Use: `pip install swanlab`.')\n            raise e\n\n        self.swanlab_params = {k[8:]: v for k, v in model.cfg.items() if k.startswith(\"swanlab_\")}\n\n        self._run = None\n        if dist.get_world_size() < 2 or dist.get_rank() == 0:\n            _ = self.run\n            self.run.config.update(self.model.cfg)\n\n        self.best_ap = -1000.\n        self.fps = []\n\n    @property\n    def run(self):\n        if self._run is None:\n            self._run = self.swanlab.get_run() or self.swanlab.init(**self.swanlab_params)\n        return self._run\n\n    def on_step_end(self, status):\n        if dist.get_world_size() < 2 or dist.get_rank() == 0 and status['mode'] == 'train':\n            training_status = status['training_staus'].get()\n            batch_time = status['batch_time']\n            data_time = status['data_time']\n            batch_size = self.model.cfg['{}Reader'.format(status['mode'].capitalize())]['batch_size']\n\n            ips = float(batch_size) / float(batch_time.avg)\n            metrics = {\n                \"train/\" + k: float(v) for k, v in training_status.items()\n            }\n            metrics.update({\n                \"train/ips\": ips,\n                \"train/data_cost\": float(data_time.avg),\n                \"train/batch_cost\": float(batch_time.avg)\n            })\n\n            self.fps.append(ips)\n            self.run.log(metrics)\n\n    def on_epoch_end(self, status):\n        if dist.get_world_size() < 2 or dist.get_rank() == 0:\n            mode = status['mode']\n            epoch_id = status['epoch_id']\n\n            if mode == 'train':\n                fps = sum(self.fps) / len(self.fps)\n                self.fps = []\n\n                end_epoch = self.model.cfg.epoch\n                if (epoch_id + 1) % self.model.cfg.snapshot_epoch == 0 or epoch_id == end_epoch - 1:\n                    save_name = str(epoch_id) if epoch_id != end_epoch - 1 else \"model_final\"\n                    tags = [\"latest\", f\"epoch_{epoch_id}\"]\n\n            elif mode == 'eval':\n                fps = status['sample_num'] / status['cost_time']\n\n                merged_dict = {\n                    f\"eval/{key}-mAP\": map_value[0]\n                    for metric in self.model._metrics\n                    for key, map_value in metric.get_results().items()\n                }\n                merged_dict.update({\n                    \"epoch\": status[\"epoch_id\"],\n                    \"eval/fps\": fps\n                })\n\n                self.run.log(merged_dict)\n\n                if status.get('save_best_model'):\n                    for metric in self.model._metrics:\n                        map_res = metric.get_results()\n                        key = next((k for k in ['bbox', 'keypoint', 'mask'] if k in map_res), None)\n\n                        if not key:\n                            logger.warning(\"Evaluation results empty, this may be due to \"\n                                           \"training iterations being too few or not \"\n                                           \"loading the correct weights.\")\n                            return\n\n                        if map_res[key][0] >= self.best_ap:\n                            self.best_ap = map_res[key][0]\n                            save_name = 'best_model'\n                            tags = [\"best\", f\"epoch_{epoch_id}\"]\n\n    def on_train_end(self, status):\n        self.run.finish()\n```",
    "715": "一级标题：PaddleDetection\n二级标题：2. 修改trainer代码\n内容：\n在`ppdet/engine/trainer.py`文件中，在`from .callbacks import`那一行添加SwanLabCallback：\n\n```python\nfrom .callbacks import Callback, ComposeCallback, LogPrinter, Checkpointer, WiferFaceEval, VisualDLWriter, SniperProposalsGenerator, WandbCallback, SemiCheckpointer, SemiLogPrinter, SwanLabCallback\n```\n\n接着，我们找到`Trainer`类的`__init_callbacks`方法，在`if self.mode == 'train':`下添加如下代码：\n\n```python\nif self.cfg.get('use_swanlab', False) or 'swanlab' in self.cfg:\n    self._callbacks.append(SwanLabCallback(self))\n```\n\n至此，你已经完成了SwanLab与PaddleYolo的集成！接下来，只需要在训练的配置文件中添加`use_swanlab: True`，即可开始可视化跟踪训练。",
    "716": "一级标题：PaddleDetection\n二级标题：3. 修改配置文件\n内容：\n我们以`yolov3_mobilenet_v1_roadsign`为例。\n\n在`configs/yolov3/yolov3_mobilenet_v1_roadsign.yml`文件中，在下面添加如下代码：\n\n```yaml\nuse_swanlab: true\nswanlab_project: PaddleYOLO # 可选\nswanlab_experiment_name: yolov3_mobilenet_v1_roadsign # 可选\nswanlab_description: 对PaddleYOLO的一次训练测试 # 可选\n# swanlab_workspace: swanhub # 组织名，可选\n```",
    "717": "一级标题：PaddleDetection\n二级标题：4. 开始训练\n内容：\n```bash\npython -u tools/train.py -c configs/yolov3/yolov3_mobilenet_v1_roadsign.yml --eval\n```\n\n在训练过程中，即可看到整个训练过程的日志，以及训练结束后自动生成的可视化图表。",
    "718": "一级标题：PaddleNLP\n二级标题：无\n内容：\n[PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP) 是一款基于飞桨深度学习框架的大语言模型(LLM)开发套件，支持在多种硬件上进行高效的大模型训练、无损压缩以及高性能推理。PaddleNLP 具备简单易用和性能极致的特点，致力于助力开发者实现高效的大模型产业级应用。\n\n![paddlenlp-image](./paddlenlp/logo.png)\n\n你可以使用`PaddleNLP`快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。",
    "719": "一级标题：PaddleNLP\n二级标题：1. 引入SwanLabCallback\n内容：\n```python\nfrom swanlab.integration.paddlenlp import SwanLabCallback\n```\n\n**SwanLabCallback**是适配于PaddleNLP的日志记录类。\n\n**SwanLabCallback**可以定义的参数有：\n\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。\n- 你也可以在外部通过`swanlab.init`创建项目，集成会将实验记录到你在外部创建的项目中。",
    "720": "一级标题：PaddleNLP\n二级标题：2. 传入Trainer\n内容：\n```python (1,7,12)\nfrom swanlab.integration.paddlenlp import SwanLabCallback\nfrom paddlenlp.trainer import  TrainingArguments, Trainer\n\n...\n\n# 实例化SwanLabCallback\nswanlab_callback = SwanLabCallback(project=\"paddlenlp-demo\")\n\ntrainer = Trainer(\n    ...\n    # 传入callbacks参数\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n```",
    "721": "一级标题：PaddleNLP\n二级标题：3. 完整案例代码\n内容：\n> 需要能连接上HuggingFace服务器下载数据集。\n\n```python {8,19,28}\n\"\"\"\n测试于：\npip install paddlepaddle-gpu==3.0.0 -i https://www.paddlepaddle.org.cn/packages/stable/cu126/\npip install paddlenlp==3.0.0b4\n\"\"\"\nfrom paddlenlp.trl import SFTConfig, SFTTrainer\nfrom datasets import load_dataset\nfrom swanlab.integration.paddlenlp import SwanLabCallback\n\ndataset = load_dataset(\"ZHUI/alpaca_demo\", split=\"train\")\n\ntraining_args = SFTConfig(\n    output_dir=\"Qwen/Qwen2.5-0.5B-SFT\",\n    device=\"gpu\",\n    per_device_train_batch_size=1,\n    logging_steps=20\n    )\n\nswanlab_callback = SwanLabCallback(\n    project=\"Qwen2.5-0.5B-SFT-paddlenlp\",\n    experiment_name=\"Qwen2.5-0.5B\",\n)\n\ntrainer = SFTTrainer(\n    args=training_args,\n    model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n    train_dataset=dataset,\n    callbacks=[swanlab_callback],\n)\ntrainer.train()\n```",
    "722": "一级标题：PaddleNLP\n二级标题：4. GUI效果展示\n内容：\n超参数自动记录：\n\n![ig-paddlenlp-gui-1](./paddlenlp/config.png)\n\n指标记录：\n\n![ig-paddlenlp-gui-2](./paddlenlp/chart.png)",
    "723": "一级标题：PaddleNLP\n二级标题：5 拓展：增加更多回调\n内容：\n试想一个场景，你希望在每个epoch结束时，让模型推理测试样例，并用swanlab记录推理的结果，那么你可以创建一个继承自`SwanLabCallback`的新类，增加或重构生命周期函数。比如：\n\n```python\nclass NLPSwanLabCallback(SwanLabCallback):\n    def on_epoch_end(self, args, state, control, **kwargs):\n        test_text_list = [\"example1\", \"example2\"]\n        log_text_list = []\n        for text in test_text_list:\n            result = model(text)\n            log_text_list.append(swanlab.Text(result))\n\n        swanlab.log({\"Prediction\": test_text_list}, step=state.global_step)\n```\n\n上面是一个在NLP任务下的新回调类，增加了`on_epoch_end`函数，它会在`transformers`训练的每个epoch结束时执行。",
    "724": "一级标题：PaddleYolo\n二级标题：无\n内容：\n[PaddleYolo](https://github.com/PaddlePaddle/PaddleYOLO) 是飞桨（PaddlePaddle）框架下的一个目标检测库，主要用于图像和视频中的物体检测。PaddleYOLO包含YOLO系列模型的相关代码，支持YOLOv3、PP-YOLO、PP-YOLOv2、PP-YOLOE、PP-YOLOE+、RT-DETR、YOLOX、YOLOv5、YOLOv6、YOLOv7、YOLOv8、YOLOv5u、YOLOv7u、YOLOv6Lite、RTMDet等模型\n\n你可以使用PaddleYolo快速进行目标检测模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n[Demo](https://swanlab.cn/@ZeyiLin/PaddleYOLO/runs/10zy8zickn2062kubch34/chart)",
    "725": "一级标题：PaddleYolo\n二级标题：1. 引入SwanLabCallback\n内容：\n首先在你clone的PaddleYolo项目中，找到`ppdet/engine/callbacks.py`文件，在代码的底部添加如下代码：\n\n```python\nclass SwanLabCallback(Callback):\n    def __init__(self, model):\n        super(SwanLabCallback, self).__init__(model)\n\n        try:\n            import swanlab\n            self.swanlab = swanlab\n        except Exception as e:\n            logger.error('swanlab not found, please install swanlab. '\n                         'Use: `pip install swanlab`.')\n            raise e\n\n        self.swanlab_params = {k[8:]: v for k, v in model.cfg.items() if k.startswith(\"swanlab_\")}\n\n        self._run = None\n        if dist.get_world_size() < 2 or dist.get_rank() == 0:\n            _ = self.run\n            self.run.config.update(self.model.cfg)\n\n        self.best_ap = -1000.\n        self.fps = []\n\n    @property\n    def run(self):\n        if self._run is None:\n            self._run = self.swanlab.get_run() or self.swanlab.init(**self.swanlab_params)\n        return self._run\n\n    def on_step_end(self, status):\n        if dist.get_world_size() < 2 or dist.get_rank() == 0 and status['mode'] == 'train':\n            training_status = status['training_staus'].get()\n            batch_time = status['batch_time']\n            data_time = status['data_time']\n            batch_size = self.model.cfg['{}Reader'.format(status['mode'].capitalize())]['batch_size']\n\n            ips = float(batch_size) / float(batch_time.avg)\n            metrics = {\n                \"train/\" + k: float(v) for k, v in training_status.items()\n            }\n            metrics.update({\n                \"train/ips\": ips,\n                \"train/data_cost\": float(data_time.avg),\n                \"train/batch_cost\": float(batch_time.avg)\n            })\n\n            self.fps.append(ips)\n            self.run.log(metrics)\n\n    def on_epoch_end(self, status):\n        if dist.get_world_size() < 2 or dist.get_rank() == 0:\n            mode = status['mode']\n            epoch_id = status['epoch_id']\n\n            if mode == 'train':\n                fps = sum(self.fps) / len(self.fps)\n                self.fps = []\n\n                end_epoch = self.model.cfg.epoch\n                if (epoch_id + 1) % self.model.cfg.snapshot_epoch == 0 or epoch_id == end_epoch - 1:\n                    save_name = str(epoch_id) if epoch_id != end_epoch - 1 else \"model_final\"\n                    tags = [\"latest\", f\"epoch_{epoch_id}\"]\n\n            elif mode == 'eval':\n                fps = status['sample_num'] / status['cost_time']\n\n                merged_dict = {\n                    f\"eval/{key}-mAP\": map_value[0]\n                    for metric in self.model._metrics\n                    for key, map_value in metric.get_results().items()\n                }\n                merged_dict.update({\n                    \"epoch\": status[\"epoch_id\"],\n                    \"eval/fps\": fps\n                })\n\n                self.run.log(merged_dict)\n\n                if status.get('save_best_model'):\n                    for metric in self.model._metrics:\n                        map_res = metric.get_results()\n                        key = next((k for k in ['bbox', 'keypoint', 'mask'] if k in map_res), None)\n\n                        if not key:\n                            logger.warning(\"Evaluation results empty, this may be due to \"\n                                           \"training iterations being too few or not \"\n                                           \"loading the correct weights.\")\n                            return\n\n                        if map_res[key][0] >= self.best_ap:\n                            self.best_ap = map_res[key][0]\n                            save_name = 'best_model'\n                            tags = [\"best\", f\"epoch_{epoch_id}\"]\n\n    def on_train_end(self, status):\n        self.run.finish()\n```",
    "726": "一级标题：PaddleYolo\n二级标题：2. 修改trainer代码\n内容：\n在`ppdet/engine/trainer.py`文件中，在`from .callbacks import`那一行添加SwanLabCallback：\n\n```python\nfrom .callbacks import Callback, ComposeCallback, LogPrinter, Checkpointer, VisualDLWriter, WandbCallback, SwanLabCallback\n```\n\n接着，我们找到`Trainer`类的`__init_callbacks`方法，在`if self.mode == 'train':`下添加如下代码：\n\n```python\nif self.cfg.get('use_swanlab', False) or 'swanlab' in self.cfg:\n    self._callbacks.append(SwanLabCallback(self))\n```\n\n至此，你已经完成了SwanLab与PaddleYolo的集成！接下来，只需要在训练的配置文件中添加`use_swanlab: True`，即可开始可视化跟踪训练。",
    "727": "一级标题：PaddleYolo\n二级标题：3. 修改配置文件\n内容：\n我们以`yolov3_mobilenet_v1_roadsign`为例。\n\n在`configs/yolov3/yolov3_mobilenet_v1_roadsign.yml`文件中，在下面添加如下代码：\n\n```yaml\nuse_swanlab: true\nswanlab_project: PaddleYOLO # 可选\nswanlab_experiment_name: yolov3_mobilenet_v1_roadsign # 可选\nswanlab_description: 对PaddleYOLO的一次训练测试 # 可选\n# swanlab_workspace: swanhub # 组织名，可选\n```",
    "728": "一级标题：PaddleYolo\n二级标题：4. 开始训练\n内容：\n```bash\npython -u tools/train.py -c configs/yolov3/yolov3_mobilenet_v1_roadsign.yml --eval\n```\n\n在训练过程中，即可看到整个训练过程的日志，以及训练结束后自动生成的可视化图表。\n\n![paddleyolo-image](/assets/ig-paddleyolo.png)",
    "729": "一级标题：PyTorch Lightning\n二级标题：无\n内容：\n[![](/assets/colab.svg)](https://colab.research.google.com/drive/1g1s86qobSvIuaFVxzDgzyZ-B4VzdTCym?usp=sharing)\n\n[PyTorch Lightning](https://github.com/Lightning-AI/pytorch-lightning)是一个开源的机器学习库，它建立在 PyTorch 之上，旨在帮助研究人员和开发者更加方便地进行深度学习模型的研发。Lightning 的设计理念是将模型训练中的繁琐代码（如设备管理、分布式训练等）与研究代码（模型架构、数据处理等）分离，从而使研究人员可以专注于研究本身，而不是底层的工程细节。\n\n![pytorch-lightning-image](/assets/ig-pytorch-lightning.png)\n\n你可以使用PyTorch Lightning快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。",
    "730": "一级标题：PyTorch Lightning\n二级标题：1. 引入SwanLabLogger\n内容：\n```python\nfrom swanlab.integration.pytorch_lightning import SwanLabLogger\n```\n\n**SwanLabLogger**是适配于PyTorch Lightning的日志记录类。\n\n**SwanLabLogger**可以定义的参数有：\n\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。\n- 你也可以在外部通过`swanlab.init`创建项目，集成会将实验记录到你在外部创建的项目中。",
    "731": "一级标题：PyTorch Lightning\n二级标题：2. 传入Trainer\n内容：\n```python (6,11)\nimport pytorch_lightning as pl\n\n...\n\n# 实例化SwanLabLogger\nswanlab_logger = SwanLabLogger(project=\"lightning-visualization\")\n\ntrainer = pl.Trainer(\n    ...\n    # 传入callbacks参数\n    logger=swanlab_logger,\n)\n\ntrainer.fit(...)\n```",
    "732": "一级标题：PyTorch Lightning\n二级标题：3. 完整案例代码\n内容：\n```python (1,65,70)\nfrom swanlab.integration.pytorch_lightning import SwanLabLogger\n\nimport importlib.util\nimport os\n\nimport pytorch_lightning as pl\nfrom torch import nn, optim, utils\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\n\n# define any number of nn.Modules (or use your current ones)\nencoder = nn.Sequential(nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 3))\ndecoder = nn.Sequential(nn.Linear(3, 128), nn.ReLU(), nn.Linear(128, 28 * 28))\n\n\n# define the LightningModule\nclass LitAutoEncoder(pl.LightningModule):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def training_step(self, batch, batch_idx):\n        # training_step defines the train loop.\n        # it is independent of forward\n        x, y = batch\n        x = x.view(x.size(0), -1)\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        loss = nn.functional.mse_loss(x_hat, x)\n        # Logging to TensorBoard (if installed) by default\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        # test_step defines the test loop.\n        # it is independent of forward\n        x, y = batch\n        x = x.view(x.size(0), -1)\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        loss = nn.functional.mse_loss(x_hat, x)\n        # Logging to TensorBoard (if installed) by default\n        self.log(\"test_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n        return optimizer\n\n\n# init the autoencoder\nautoencoder = LitAutoEncoder(encoder, decoder)\n\n\n# setup data\ndataset = MNIST(os.getcwd(), train=True, download=True, transform=ToTensor())\ntrain_dataset, val_dataset = utils.data.random_split(dataset, [55000, 5000])\ntest_dataset = MNIST(os.getcwd(), train=False, download=True, transform=ToTensor())\n\ntrain_loader = utils.data.DataLoader(train_dataset)\nval_loader = utils.data.DataLoader(val_dataset)\ntest_loader = utils.data.DataLoader(test_dataset)\n\nswanlab_logger = SwanLabLogger(\n    project=\"swanlab_example\",\n    experiment_name=\"example_experiment\",\n)\n\ntrainer = pl.Trainer(limit_train_batches=100, max_epochs=5, logger=swanlab_logger)\n\n\ntrainer.fit(model=autoencoder, train_dataloaders=train_loader, val_dataloaders=val_loader)\ntrainer.test(dataloaders=test_loader)\n\n```",
    "733": "一级标题：PyTorch Lightning\n二级标题：4. 注意：如多次调用trainer.fit\n内容：\n如果你在一次进程中多次调用`trainer.fit`（如N折交叉验证），那么需要在`trainer.fit`之后添加一行：\n\n```python\nswanlab_logger.experiment.finish()\n# 或swanlab.finish()\n```\n\n示例程序：\n\n```python\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom sklearn.model_selection import KFold\nimport pytorch_lightning as pl\nfrom swanlab.integration.pytorch_lightning import SwanLabLogger\nimport datetime\nimport argparse\n\nclass RandomDataset(Dataset):\n    def __init__(self, size=100):\n        self.x = torch.randn(size, 10)\n        self.y = (self.x.sum(dim=1) > 0).long()  # 简单分类任务\n\n    def __len__(self):\n        return len(self.x)\n\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n\nclass SimpleClassifier(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = torch.nn.Linear(10, 2)\n        self.loss_fn = torch.nn.CrossEntropyLoss()\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss_fn(logits, y)\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss_fn(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log(\"val_loss\", loss)\n        self.log(\"val_acc\", acc)\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=1e-3)\n\ndef main(args):\n    dataset = RandomDataset(size=100)\n    kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n\n    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n        print(f\"\\nFold {fold + 1}/3\")\n\n        train_loader = DataLoader(Subset(dataset, train_idx), batch_size=16, shuffle=True)\n        val_loader = DataLoader(Subset(dataset, val_idx), batch_size=16)\n\n        # 日志名称\n        current_time = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n        run_name = f\"{args.save_name}_fold{fold + 1}_{current_time}\"\n\n        swanlab_logger = SwanLabLogger(\n            project=\"swanlab_example\",\n            experiment_name=run_name,\n        )\n\n        model = SimpleClassifier()\n\n        trainer = pl.Trainer(\n            max_epochs=5,\n            logger=swanlab_logger,\n            log_every_n_steps=1\n        )\n\n        trainer.fit(model, train_loader, val_loader)\n        swanlab_logger.experiment.finish()\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--save_name\", type=str, default=\"test_swan\")\n    args = parser.parse_args()\n    main(args)\n```",
    "734": "一级标题：Torchtune\n二级标题：无\n内容：\n[Torchtune](https://github.com/pytorch/torchtune)是一个 PyTorch 库，用于轻松编写、微调和试验LLMs。\n\n你可以使用`torchtune`快速进行LLM微调，同时使用SwanLab进行实验跟踪与可视化。",
    "735": "一级标题：Torchtune\n二级标题：1. 修改配置文件，引入SwanLabLogger\n内容：\n我们以使用`torchtune`微调Google的`gemma-2b`模型为例。\n\ntorchtune在微调一个模型时，需要训练者先准备一个配置文件，如用QLoRA微调Gemma-2b模型：[2B_qlora_single_device.yaml](https://github.com/pytorch/torchtune/blob/main/recipes/configs/gemma/2B_qlora_single_device.yaml)。\n\n下载后，编辑这个配置文件。我们在文件中找到下面的代码段：\n\n```yaml\n# Logging\nmetric_logger:\n  _component_: torchtune.utils.metric_logging.DiskLogger\n  log_dir: ${output_dir}\n```\n\n将该代码段替换为：\n\n```yaml\n# Logging\nmetric_logger:\n  _component_: swanlab.integration.torchtune.SwanLabLogger\n  project: \"gemma-fintune\"\n  experiment_name: \"gemma-2b\"\n  log_dir: ${output_dir}\n```\n\n其中，`_component_`对应的`swanlab.integration.torchtune.SwanLabLogger`是适配于PyTorch torchtune的日志记录类。而`project`、`experiment_name`等则是创建SwanLab项目传入的参数，支持传入的参数与[swanlab.init](/zh/api/py-init.html)规则一致。",
    "736": "一级标题：Torchtune\n二级标题：2. 开始训练\n内容：\n```bash\ntune run lora_finetune_single_device --config 2B_qlora_single_device.yaml\n```",
    "737": "一级标题：PyTorch\n二级标题：无\n内容：\n[![](/assets/colab.svg)](https://colab.research.google.com/drive/1RWsrY_1bS8ECzaHvYtLb_1eBkkdzekR3?usp=sharing)\n\n在学术研究者当中，[PyTorch](https://pytorch.org/) 是最流行的 Python 深度学习框架。\n\n![PyTorch](/assets/ig-pytorch.png)\n\n你可以使用PyTorch进行深度学习模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n::: warning Pytorch生态的其他集成\n\n- [Lightning](/guide_cloud/integration/integration-pytorch-lightning.md)\n- [Torchtune](/guide_cloud/integration/integration-pytorch-torchtune.md)\n\n:::",
    "738": "一级标题：PyTorch\n二级标题：记录Tensor图像\n内容：\n你可以将带有图像数据的PyTorch `Tensors`传递给`swanlab.Image`，`swanlab.Image`将使用`torchvision`把它们转换成图像：\n\n```python\nimage_tensors = ...  # shape为[B, C, H, W]的Tensor图像\nswanlab.log({\"examples\": [swanlab.Image(im) for im in image_tensors]})\n```",
    "739": "一级标题：Ray\n二级标题：无\n内容：\n[Ray](https://github.com/ray-project/ray) 是一个分布式计算框架，专为大规模并行任务和强化学习应用设计。它由加州大学伯克利分校的研究团队开发，旨在简化构建高性能、可扩展的分布式应用程序的过程。Ray 支持 Python 和 Java，并且可以轻松集成到现有的机器学习、数据处理和强化学习工作流中。\n\n![ray](./ray/logo.png)\n\nSwanLab 支持 Ray 的实验记录，通过 `SwanLabLoggerCallback` 可以方便地记录实验指标和超参数。",
    "740": "一级标题：Ray\n二级标题：1. 引入SwanLabCallback\n内容：\n```python\nfrom swanlab.integration.ray import SwanLabLoggerCallback\n```\n\n`SwanLabLoggerCallback` 是适配于 `Ray` 的日志记录类。\n\n`SwanLabLoggerCallback`可以定义的参数有：\n- `project`: 项目名称\n- `workspace`: 工作空间名称\n- 其他和`swanlab.init`一致的参数",
    "741": "一级标题：Ray\n二级标题：2. 与`tune.Tuner`集成\n内容：\n```python\ntuner = tune.Tuner(\n    ...\n    run_config=tune.RunConfig(\n        callbacks=[SwanLabLoggerCallback(project=\"Ray_Project\")],\n    ),\n)\n```",
    "742": "一级标题：Ray\n二级标题：3. 完整案例\n内容：\n```python\nimport random\nfrom ray import tune\nfrom swanlab.integration.ray import SwanLabLoggerCallback\n\ndef train_func(config):\n    offset = random.random() / 5\n    for epoch in range(2, config[\"epochs\"]):\n        acc = 1 - (2 + config[\"lr\"]) ** -epoch - random.random() / epoch - offset\n        loss = (2 + config[\"lr\"]) ** -epoch + random.random() / epoch + offset\n        tune.report({\"acc\": acc, \"loss\": loss})\n\n\ntuner = tune.Tuner(\n    train_func,\n    param_space={\n        \"lr\": tune.grid_search([0.001, 0.01, 0.1, 1.0]),\n        \"epochs\": 10,\n    },\n    run_config=tune.RunConfig(\n        callbacks=[SwanLabLoggerCallback(project=\"Ray_Project\")],\n    ),\n)\nresults = tuner.fit()\n```\n\n![ray-tune](./ray/demo.png)",
    "743": "一级标题：ROLL\n二级标题：无\n内容：\n[ROLL](https://github.com/alibaba/ROLL) 是一个高效且用户友好的强化学习库，专为利用大规模 GPU 资源的大型语言模型 (LLM) 而设计。它显著提升了 LLM 在人类偏好对齐、复杂推理和多轮代理交互等关键领域的性能。\n\nROLL 利用 Ray 的多角色分布式架构实现灵活的资源分配和异构任务调度，并集成 Megatron-Core、SGLang 和 vLLM 等尖端技术来加速模型训练和推理。\n\n![ROLL](./roll/logo.png)\n\n在ROLL中使用SwanLab非常简单，只需要设置一些参数即可，详情参考 [agentic_pipeline_config.yaml](https://github.com/alibaba/ROLL/blob/main/tests/pipeline/agentic_pipeline_config.yaml) 中的`track_with: swanlab`部分。",
    "744": "一级标题：Stable-Baseline3\n二级标题：无\n内容：\n[![](/assets/colab.svg)](https://colab.research.google.com/drive/1JfU4oCKCS7FQE_AXqZ3k9Bt1vmK-6pMO?usp=sharing)\n\n\nStable Baselines3 (SB3) 是一个强化学习的开源库，基于 PyTorch 框架构建。它是 Stable Baselines 项目的继任者，旨在提供一组可靠且经过良好测试的RL算法实现，便于研究和应用。StableBaseline3主要被应用于机器人控制、游戏AI、自动驾驶、金融交易等领域。\n\n![sb3](/assets/ig-sb3.png)\n\n你可以使用sb3快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。",
    "745": "一级标题：Stable-Baseline3\n二级标题：1.引入SwanLabCallback\n内容：\n```python\nfrom swanlab.integration.sb3 import SwanLabCallback\n```\n\n**SwanLabCallback**是适配于 Stable Baselines3 的日志记录类。\n\n**SwanLabCallback**可以定义的参数有：\n\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。\n- 你也可以在外部通过`swanlab.init`创建项目，集成会将实验记录到你在外部创建的项目中。",
    "746": "一级标题：Stable-Baseline3\n二级标题：2.传入model.learn\n内容：\n```python (1,7)\nfrom swanlab.integration.sb3 import SwanLabCallback\n\n...\n\nmodel.learn(\n    ...\n    callback=SwanLabCallback(),\n)\n```\n在`model.learn`的`callback`参数传入`SwanLabCallback`实例，即可开始跟踪。",
    "747": "一级标题：Stable-Baseline3\n二级标题：3.完整案例代码\n内容：\n下面是一个PPO模型的简单训练案例，使用SwanLab做训练可视化和监控：\n\n```python (6,31)\nimport gymnasium as gym\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nimport swanlab\nfrom swanlab.integration.sb3 import SwanLabCallback\n\n\nconfig = {\n    \"policy_type\": \"MlpPolicy\",\n    \"total_timesteps\": 25000,\n    \"env_name\": \"CartPole-v1\",\n}\n\n\ndef make_env():\n    env = gym.make(config[\"env_name\"], render_mode=\"rgb_array\")\n    env = Monitor(env)\n    return env\n\n\nenv = DummyVecEnv([make_env])\nmodel = PPO(\n    config[\"policy_type\"],\n    env,\n    verbose=1,\n)\n\nmodel.learn(\n    total_timesteps=config[\"total_timesteps\"],\n    callback=SwanLabCallback(\n        project=\"PPO\",\n        experiment_name=\"MlpPolicy\",\n        verbose=2,\n    ),\n)\n\nswanlab.finish()\n\n```",
    "748": "一级标题：Sentence Transformers\n二级标题：无\n内容：\n[Sentence Transformers](https://github.com/UKPLab/sentence-transformers)(又名SBERT)是访问、使用和训练文本和图像嵌入（Embedding）模型的Python库。\n\n![](/assets/ig-sentence-transformers.png)\n\n你可以使用Sentence Transformers快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。",
    "749": "一级标题：Sentence Transformers\n二级标题：1. 引入SwanLabCallback\n内容：\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\n```\n\n**SwanLabCallback**是适配于HuggingFace系列工具（Transformers等）的日志记录类。\n\n**SwanLabCallback**可以定义的参数有：\n\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。\n- 你也可以在外部通过`swanlab.init`创建项目，集成会将实验记录到你在外部创建的项目中。",
    "750": "一级标题：Sentence Transformers\n二级标题：2. 传入Trainer\n内容：\n```python (1,7,12)\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom sentence_transformers import SentenceTransformer, SentenceTransformerTrainer\n\n...\n\n# 实例化SwanLabCallback\nswanlab_callback = SwanLabCallback(project=\"hf-visualization\")\n\ntrainer = SentenceTransformerTrainer(\n    ...\n    # 传入callbacks参数\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n```",
    "751": "一级标题：Sentence Transformers\n二级标题：3.完整案例代码\n内容：\n```python (4,12,19)\nfrom datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer, SentenceTransformerTrainer\nfrom sentence_transformers.losses import MultipleNegativesRankingLoss\nfrom swanlab.integration.transformers import SwanLabCallback\n\nmodel = SentenceTransformer(\"bert-base-uncased\")\n\ntrain_dataset = load_dataset(\"sentence-transformers/all-nli\", \"pair\", split=\"train[:10000]\")\neval_dataset = load_dataset(\"sentence-transformers/all-nli\", \"triplet\", split=\"dev\")\nmnrl_loss = MultipleNegativesRankingLoss(model)\n\nswanlab_callback = SwanLabCallback(project=\"sentence-transformers\", experiment_name=\"bert-all-nli\")\n\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    loss=mnrl_loss,\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n```",
    "752": "一级标题：Modelscope Swift\n二级标题：无\n内容：\n> SwanLab已经与Swift官方集成，见：[#3142](https://github.com/modelscope/ms-swift/pull/3142)\n> 可视化在线Demo：[swift-robot](https://swanlab.cn/@ZeyiLin/swift-robot/runs/9lc9rmmwm4hh7ay1vkzd7/chart)\n\n[Modelscope魔搭社区](https://modelscope.cn/) 的 [Swift](https://github.com/modelscope/swift) 是一个集模型训练、微调、推理、部署于一体的框架。\n\n![logo](./swift/logo.png)\n\n🍲 **ms-swift** 是 ModelScope 社区提供的官方框架，用于微调和部署大型语言模型和多模态大型模型。它目前支持 **450+** 大型模型和 **150+** 多模态大型模型的训练（预训练、微调、人工对齐）、推理、评估、量化和部署。\n\n🍔 此外，ms-swift 还采用了**最新的训练技术**，包括 **LoRA、QLoRA、Llama-Pro、LongLoRA、GaLore、Q-GaLore、LoRA+、LISA、DoRA、FourierFt、ReFT、UnSloth 和 Liger 等轻量级技术**，以及 **DPO、GRPO、RM、PPO、KTO、CPO、SimPO 和 ORPO** 等人工对齐训练方法。\n\nms-swift 支持使用 vLLM 和 LMDeploy 加速推理、评估和部署模块，并支持使用 GPTQ、AWQ 和 BNB 等技术进行模型量化。此外，ms-swift 还提供了基于 Gradio 的 Web UI 和丰富的最佳实践。\n\n你可以使用Swift快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n[[toc]]",
    "753": "一级标题：Modelscope Swift\n二级标题：0. 安装ms-swift和swanlab\n内容：\n安装ms-swift（>=3.1.1）：\n\n```bash\npip install ms-swift\n```\n\n安装swanlab：\n\n```bash\npip install swanlab\n```",
    "754": "一级标题：Modelscope Swift\n二级标题：1. CLI微调\n内容：\n你只需要在ms-swift的CLI中添加`--report_to`和`--swanlab_project`两个参数，即可使用SwanLab进行实验跟踪与可视化：\n\n```bash\nswift sft \\\n    ...\n    --report_to swanlab \\  # [!code ++]\n    --swanlab_project swift-robot \\  # [!code ++]\n    ...\n```\n\n下面是在swift官方的CLI微调案例，中结合SwanLab的示例（见代码最后）：\n\n```bash {29-30}\n# 22GB\nCUDA_VISIBLE_DEVICES=0 \\\nswift sft \\\n    --model Qwen/Qwen2.5-7B-Instruct \\\n    --train_type lora \\\n    --dataset 'AI-ModelScope/alpaca-gpt4-data-zh#500' \\\n              'AI-ModelScope/alpaca-gpt4-data-en#500' \\\n              'swift/self-cognition#500' \\\n    --torch_dtype bfloat16 \\\n    --num_train_epochs 1 \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --learning_rate 1e-4 \\\n    --lora_rank 8 \\\n    --lora_alpha 32 \\\n    --target_modules all-linear \\\n    --gradient_accumulation_steps 16 \\\n    --eval_steps 50 \\\n    --save_steps 50 \\\n    --save_total_limit 5 \\\n    --logging_steps 5 \\\n    --max_length 2048 \\\n    --output_dir output \\\n    --system 'You are a helpful assistant.' \\\n    --warmup_ratio 0.05 \\\n    --dataloader_num_workers 4 \\\n    --model_author swift \\\n    --model_name swift-robot \\\n    --report_to swanlab \\\n    --swanlab_project swift-robot\n```\n\n运行指令后，就可以在SwanLab看到训练过程：\n\n![](./swift/dashboard-1.png)\n\n支持的完整参数：\n\n- `swanlab_token`: SwanLab的api-key\n- `swanlab_project`: swanlab的project\n- `swanlab_workspace`: 默认为None，会使用api-key对应的username\n- `swanlab_exp_name`: 实验名，可以为空，为空时默认传入--output_dir的值\n- `swanlab_mode`: 可选cloud和local，云模式或者本地模式",
    "755": "一级标题：Modelscope Swift\n二级标题：2. WebUI微调\n内容：\nSwift不仅支持CLI微调，还为开发者提供非常方便的**WebUI（网页端）**的微调界面。你同样可以在WebUI当中启动SwanLab跟踪实验。\n\n启动WebUI方式：\n\n```bash\nswift web-ui\n```\n\n启动后，会自动打开浏览器，显示微调界面（或者访问 `http://localhost:7860/` ）：\n\n![ig-swift-2](./swift/dashboard-2.png)\n\n在下方的「训练记录」模块中，在`训练记录方式`部分选择`swanlab`：\n\n![ig-swift-3](./swift/webui-1.png)\n\n你还可以在「训练记录」模块的其他填写更细致的swanlab参数，包括：\n\n- `swanlab_token`: SwanLab的api-key\n- `swanlab_project`: swanlab的project\n- `swanlab_workspace`: 默认为None，会使用api-key对应的username\n- `swanlab_exp_name`: 实验名，可以为空，为空时默认传入--output_dir的值\n- `swanlab_mode`: 可选cloud和local，云模式或者本地模式\n\n然后，点击「🚀开始训练」按钮，即可启动训练，并使用SwanLab跟踪实验：\n\n![ig-swift-4](./swift/webui-2.png)",
    "756": "一级标题：Modelscope Swift\n二级标题：3. Python代码微调\n内容：\n**3.1 引入SwanLabCallback**\n\n因为`Swift`的`trainer`集成自`transformers`，所以可以直接使用`swanlab`与`huggingface`集成的`SwanLabCallback`：\n\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\n```\n\nSwanLabCallback可以定义的参数有：\n\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。 你也可以在外部通过swanlab.init创建项目，集成会将实验记录到你在外部创建的项目中。\n\n**3.2 引入Trainer**\n\n```python {1,7,11}\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom swift import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\n···\n\n#实例化SwanLabCallback\nswanlab_callback = SwanLabCallback(project=\"swift-visualization\")\n\ntrainer = Seq2SeqTrainer(\n    ...\n    callbacks=[swanlab_callback],\n    )\n\ntrainer.train()\n```\n\n**3.3 使用SwanLabCallback**\n\n> Lora微调一个Qwen2-0.5B模型\n\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom swift import Seq2SeqTrainer, Seq2SeqTrainingArguments\nfrom swift.llm import get_model_tokenizer, load_dataset, get_template, EncodePreprocessor\nfrom swift.utils import get_logger, find_all_linears, get_model_parameter_info, plot_images, seed_everything\nfrom swift.tuners import Swift, LoraConfig\nfrom swift.trainers import Seq2SeqTrainer, Seq2SeqTrainingArguments\nfrom functools import partial\nimport os\n\nlogger = get_logger()\nseed_everything(42)\n\n# Hyperparameters for training\n# model\nmodel_id_or_path = 'Qwen/Qwen2.5-3B-Instruct'  # model_id or model_path\nsystem = 'You are a helpful assistant.'\noutput_dir = 'output'\n\n# dataset\ndataset = ['AI-ModelScope/alpaca-gpt4-data-zh#500', 'AI-ModelScope/alpaca-gpt4-data-en#500',\n           'swift/self-cognition#500']  # dataset_id or dataset_path\ndata_seed = 42\nmax_length = 2048\nsplit_dataset_ratio = 0.01  # Split validation set\nnum_proc = 4  # The number of processes for data loading.\n# The following two parameters are used to override the placeholders in the self-cognition dataset.\nmodel_name = ['小黄', 'Xiao Huang']  # The Chinese name and English name of the model\nmodel_author = ['魔搭', 'ModelScope']  # The Chinese name and English name of the model author\n\n# lora\nlora_rank = 8\nlora_alpha = 32\n\n# training_args\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=output_dir,\n    learning_rate=1e-4,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_checkpointing=True,\n    weight_decay=0.1,\n    lr_scheduler_type='cosine',\n    warmup_ratio=0.05,\n    logging_first_step=True,\n    save_strategy='steps',\n    save_steps=50,\n    eval_strategy='steps',\n    eval_steps=50,\n    gradient_accumulation_steps=16,\n    num_train_epochs=1,\n    metric_for_best_model='loss',\n    save_total_limit=5,\n    logging_steps=5,\n    dataloader_num_workers=1,\n    data_seed=data_seed,\n)\n\noutput_dir = os.path.abspath(os.path.expanduser(output_dir))\nlogger.info(f'output_dir: {output_dir}')\n\n# Obtain the model and template, and add a trainable Lora layer on the model.\nmodel, tokenizer = get_model_tokenizer(model_id_or_path)\nlogger.info(f'model_info: {model.model_info}')\ntemplate = get_template(model.model_meta.template, tokenizer, default_system=system, max_length=max_length)\ntemplate.set_mode('train')\n\ntarget_modules = find_all_linears(model)\nlora_config = LoraConfig(task_type='CAUSAL_LM', r=lora_rank, lora_alpha=lora_alpha,\n                         target_modules=target_modules)\nmodel = Swift.prepare_model(model, lora_config)\nlogger.info(f'lora_config: {lora_config}')\n\n# Print model structure and trainable parameters.\nlogger.info(f'model: {model}')\nmodel_parameter_info = get_model_parameter_info(model)\nlogger.info(f'model_parameter_info: {model_parameter_info}')\n\n# Download and load the dataset, split it into a training set and a validation set,\n# and encode the text data into tokens.\ntrain_dataset, val_dataset = load_dataset(dataset, split_dataset_ratio=split_dataset_ratio, num_proc=num_proc,\n        model_name=model_name, model_author=model_author, seed=data_seed)\n\nlogger.info(f'train_dataset: {train_dataset}')\nlogger.info(f'val_dataset: {val_dataset}')\nlogger.info(f'train_dataset[0]: {train_dataset[0]}')\n\ntrain_dataset = EncodePreprocessor(template=template)(train_dataset, num_proc=num_proc)\nval_dataset = EncodePreprocessor(template=template)(val_dataset, num_proc=num_proc)\nlogger.info(f'encoded_train_dataset[0]: {train_dataset[0]}')\n\n# Print a sample\ntemplate.print_inputs(train_dataset[0])\n\n# Get the trainer and start the training.\nmodel.enable_input_require_grads()  # Compatible with gradient checkpointing\n\nswanlab_callback = SwanLabCallback(\n    project=\"swift-visualization\",\n    experiment_name=\"lora-qwen2-0.5b\",\n    description=\"Lora微调一个Qwen2-0.5B模型\"\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    data_collator=template.data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    template=template,\n    callbacks=[swanlab_callback],\n)\ntrainer.train()\n\nlast_model_checkpoint = trainer.state.last_model_checkpoint\nlogger.info(f'last_model_checkpoint: {last_model_checkpoint}')\n```\n\n运行可视化结果：\n\n![ig-swift-5](./swift/dashboard-3.png)",
    "757": "一级标题：Tensorboard\n二级标题：无\n内容：\n[TensorBoard](https://github.com/tensorflow/tensorboard) 是 Google TensorFlow 提供的一个可视化工具，用于帮助理解、调试和优化机器学习模型。它通过图形界面展示训练过程中的各种指标和数据，让开发者更直观地了解模型的性能和行为。\n\n![TensorBoard](/assets/ig-tensorboard.png)\n\n:::warning 其他工具的同步教程\n\n- [Wandb](/guide_cloud/integration/integration-wandb.md)\n- [MLFlow](/guide_cloud/integration/integration-mlflow.md)\n:::\n\n**你可以用两种方式将使用Tensorboard跟踪的项目同步到SwanLab：**\n\n- **同步跟踪**：如果你现在的项目使用了Tensorboard进行实验跟踪，你可以使用`swanlab.sync_tensorboardX()`或`swanlab.sync_tensorboard_torch()`命令，在运行训练脚本时同步记录指标到SwanLab。\n- **转换已存在的项目**：如果你想要将Tensorboard上的项目复制到SwanLab，你可以使用`swanlab convert`，将存放TFevent文件的目录转换成SwanLab项目。\n\n::: info\n在当前版本暂仅支持转换标量和图像图表。\n:::\n\n[[toc]]",
    "758": "一级标题：Tensorboard\n二级标题：1. 同步跟踪\n内容：\n### 1.1 TensorboardX: 添加sync_tensorboardX命令\n\n如果你使用的是TensorboardX，可以在代码执行`tensorboardX.SummaryWriter()`之前的任何位置，添加一行`swanlab.sync_tensorboardX()`命令，即可在训练时同步记录指标到SwanLab。\n\n```python\nimport swanlab\nfrom tensorboardX import SummaryWriter\n\nswanlab.sync_tensorboardX()\n\nwriter = SummaryWriter(log_dir='./runs')\n```\n\n### 1.2 PyTorch: 添加sync_tensorboard_torch命令\n\n如果你使用的是PyTorch自带的tensorboard，那么可以在代码执行`torch.utils.tensorboard.SummaryWriter()`之前的任何位置，添加一行`swanlab.sync_tensorboard_torch()`命令，即可在训练时同步记录指标到SwanLab。\n\n```python\nimport swanlab\nimport torch\n\nswanlab.sync_tensorboard_torch()\n\nwriter = torch.utils.tensorboard.SummaryWriter(log_dir='./runs')\n```\n\n### 1.3 另一种写法\n\n你也可以先手动初始化swanlab，再运行tensorboard的代码。\n\n::: code-group\n\n```python [TensorboardX]\nimport swanlab\nfrom tensorboardX import SummaryWriter\n\nswanlab.init(...)\nswanlab.sync_tensorboardX()\n\n...\n\nwriter = SummaryWriter(log_dir='./runs')\n```\n\n```python [PyTorch]\nimport swanlab\nfrom torch.utils.tensorboard import SummaryWriter\n\nswanlab.init(...)\nswanlab.sync_tensorboard_torch()\n\n...\n\nwriter = SummaryWriter(log_dir='./runs')\n```\n:::\n\n### 1.4 测试代码\n\n::: code-group\n\n```python [TensorboardX]\nimport swanlab\nfrom tensorboardX import SummaryWriter\n\nswanlab.sync_tensorboardX()\n\nwriter = SummaryWriter(log_dir='./runs')\n\nepochs = 10\noffset = random.random() / 5\nfor epoch in range(2, epochs):\n  acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n  loss = 2 ** -epoch + random.random() / epoch + offset\n\n  writer.add_scalar(\"acc\", acc, epoch)\n  writer.add_scalar(\"loss\", loss, epoch)\n```\n\n```python [PyTorch]\nimport swanlab\nfrom torch.utils.tensorboard import SummaryWriter\n\nswanlab.sync_tensorboard_torch()\n\nwriter = SummaryWriter(log_dir='./runs')\n\nepochs = 10\noffset = random.random() / 5\nfor epoch in range(2, epochs):\n  acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n  loss = 2 ** -epoch + random.random() / epoch + offset\n\n  writer.add_scalar(\"acc\", acc, epoch)\n  writer.add_scalar(\"loss\", loss, epoch)\n```\n\n:::",
    "759": "一级标题：Tensorboard\n二级标题：2. 转换已存在的项目\n内容：\n### 2.1 方式一：命令行转换\n\n```bash\nswanlab convert -t tensorboard --tb_logdir [TFEVENT_LOGDIR]\n```\n\n支持的参数如下：\n\n- `-t`: 转换类型，可选wandb、tensorboard和mlflow。\n- `-p`: SwanLab项目名。\n- `-w`: SwanLab工作空间名。\n- `--cloud`: (bool) 是否上传模式为\"cloud\"，默认为True\n- `-l`: logdir路径。\n- `--tb_logdir`: Tensorboard日志文件路径。\n\n这里的`[TFEVENT_LOGDIR]`是指你先前用Tensorboard记录实验时，生成的日志文件路径。\n\nSwanLab Converter将会自动检测文件路径及其子目录下的`tfevent`文件（默认子目录深度为3），并为每个`tfevent`文件创建一个SwanLab实验。\n\n### 2.2 方式二：代码内转换\n\n```python\nfrom swanlab.converter import TFBConverter\n\ntfb_converter = TFBConverter(convert_dir=\"[TFEVENT_LOGDIR]\")\ntfb_converter.run()\n```\n\n效果与命令行转换一致。\n\n### 2.3 参数列表\n\n| 参数 | 对应CLI参数       | 描述                  |\n| ---- | ---------- | --------------------- |\n| convert_dir    | -      | Tfevent文件路径       |\n| project    | -p, --project      | SwanLab项目名       |\n| workspace  | -w, --workspace      | SwanLab工作空间名 |\n| cloud    | --cloud      | 是否使用云端版，默认为True       |\n| logdir    | -l, --logdir      | SwanLab日志文件保存路径       |\n\n例子：\n\n```python\nfrom swanlab.converter import TFBConverter\n\ntfb_converter = TFBConverter(\n    convert_dir=\"./runs\",\n    project=\"Tensorboard-Converter\",\n    workspace=\"SwanLab\",\n    logdir=\"./logs\",\n    )\ntfb_converter.run()\n```\n\n与之作用相同的CLI：\n```bash\nswanlab convert -t tensorboard --tb_logdir ./runs -p Tensorboard-Converter -w SwanLab -l ./logs\n```\n\n执行上面的脚本，将会在`SwanLab`空间下，创建一个名为`Tensorboard-Converter`的项目，将`./runs`目录下tfevent文件创建为一个个swanlab实验，并将swanlab运行时产生的日志保存在`./logs`目录下。",
    "760": "一级标题：Tensorboard\n二级标题：3. API映射表\n内容：\n| 功能 | Tensorboard | SwanLab |\n| ---- | ---------- | --------------------- |\n| 创建实验    |  writer = SummaryWriter(logdir=\"./runs\")   | swanlab.init(logdir=\"./runs\")    |\n| 记录标量指标 | writer.add_scalar(key, value, step) | swanlab.log({key, value}, step=step) |\n| 记录多个标量指标 | writer.add_scalar(key1, value1, step)<br> writer.add_scalar(key2, value2, step) | swanlab.log({key1: value1, key2: value2}, step=step) |\n| 记录图像指标 | writer.add_image(key, data, step) | swanlab.log({key: swanlab.Image(data), step=step}) |\n| 记录文本指标 | writer.add_text(key, data, step) | swanlab.log({key: swanlab.Text(data)}, step=step) |\n| 记录音频指标 | writer.add_audio(key, data, step) | swanlab.log({key: swanlab.Audio(data), step=step}) |\n| 记录视频指标 | writer.add_video(key, data, step) | swanlab.log({key: swanlab.Video(data), step=step}) |\n| 记录PR曲线 | writer.add_pr_curve(key, labels, predictions, step) | swanlab.log({key: swanlab.PRCurve(labels, predictions), step=step}) |\n| 关闭实验 | writer.close() | swanlab.finish() |",
    "761": "一级标题：Ultralytics\n二级标题：无\n内容：\n[![](/assets/colab.svg)](https://colab.research.google.com/drive/1RAT2vSrvET4wEDd9syeDrgz0KBUDQAR1?usp=sharing)\n\n[Ultralytics](https://github.com/ultralytics/ultralytics) YOLOv8 是一款尖端、最先进的 （SOTA） 模型，它建立在以前 YOLO 版本的成功基础上，并引入了新功能和改进，以进一步提高性能和灵活性。YOLOv8 设计为快速、准确且易于使用，使其成为各种对象检测和跟踪、实例分割、图像分类和姿态估计任务的绝佳选择。\n\n![ultralytics](./ultralytics/logo.png)\n\n你可以使用Ultralytics快速进行计算机视觉模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n下面介绍两种引入SwanLab的方式：\n1. `add_swanlab_callback`：无需修改源码，适用于单卡训练场景\n2. `return_swanlab_callback`：需要修改源码，适用于单卡以及多卡DDP训练场景",
    "762": "一级标题：Ultralytics\n二级标题：1.1 引入add_swanlab_callback\n内容：\n```python\nfrom swanlab.integration.ultralytics import add_swanlab_callback\n```\n\n`add_swanlab_callback`的作用是为Ultralytics模型添加回调函数，以在模型训练的各个生命周期执行SwanLab记录。",
    "763": "一级标题：Ultralytics\n二级标题：1.2 代码案例\n内容：\n下面是使用yolov8n模型在coco数据集上的训练，只需将model传入`add_swanlab_callback`函数，即可完成与SwanLab的集成。\n\n```python {9}\nfrom ultralytics import YOLO\nfrom swanlab.integration.ultralytics import add_swanlab_callback\n\n\nif __name__ == \"__main__\":\n    model = YOLO(\"yolov8n.yaml\")\n    model.load()\n    # 添加swanlab回调\n    add_swanlab_callback(model)\n\n    model.train(\n        data=\"./coco128.yaml\",\n        epochs=3,\n        imgsz=320,\n    )\n```\n\n如果需要自定义SwanLab的项目、实验名等参数，则可以在`add_swanlab_callback`中添加：\n\n```python\nadd_swanlab_callback(\n    model,\n    project=\"ultralytics\",\n    experiment_name=\"yolov8n\",\n    description=\"yolov8n在coco128数据集上的训练。\",\n    mode=\"local\",\n    )\n```",
    "764": "一级标题：Ultralytics\n二级标题：2.1 多卡训练/DDP训练\n内容：\n> swanlab>=0.3.7\n\n在Ultralytics多卡训练的场景下，由于启动训练的方式与单卡完全不同，所以需要用一种不同的方式接入SwanLab回调。\n\n这是一个ultralytics开启DDP训练的样例代码：\n\n```python\nfrom ultralytics import YOLO\n\nif __name__ == \"__main__\":\n    model = YOLO(\"yolov8n.pt\")\n\n    model.train(\n        data=\"./coco128.yaml\",\n        epochs=3,\n        imgsz=320,\n        # 开启DDP\n        device=[0,1],\n    )\n```\n\n我们需要修改ultralytics的源码，去到`ultralytics/utils/callbacks/base.py`，找到`add_integration_callbacks`函数，添加下面的三行代码：\n\n```python (15,16,18)\ndef add_integration_callbacks(instance):\n    ...\n\n    # Load training callbacks\n    if \"Trainer\" in instance.__class__.__name__:\n        from .clearml import callbacks as clear_cb\n        from .comet import callbacks as comet_cb\n        from .dvc import callbacks as dvc_cb\n        from .mlflow import callbacks as mlflow_cb\n        from .neptune import callbacks as neptune_cb\n        from .raytune import callbacks as tune_cb\n        from .tensorboard import callbacks as tb_cb\n        from .wb import callbacks as wb_cb\n\n        from swanlab.integration.ultralytics import return_swanlab_callback\n        sw_cb = return_swanlab_callback()\n\n        callbacks_list.extend([..., sw_cb])\n```\n\n然后运行，就可以在ddp下正常跟踪实验了。\n\n如果需要自定义SwanLab的项目、实验名等参数，则可以在`return_swanlab_callback`中添加：\n\n```python\nreturn_swanlab_callback(\n    model,\n    project=\"ultralytics\",\n    experiment_name=\"yolov8n\",\n    description=\"yolov8n在coco128数据集上的训练。\",\n    mode=\"local\",\n    )\n```\n\n:::warning ps\n1. 写入源码之后，之后运行就不需要在训练脚本中增加`add_swanlab_callback`了。\n2. 项目名由model.train()的project参数定义，实验名由name参数定义。\n:::",
    "765": "一级标题：Ultralytics\n二级标题：2.2 代码案例\n内容：\n```python\nfrom ultralytics import YOLO\n\nif __name__ == \"__main__\":\n    model = YOLO(\"yolov8n.pt\")\n\n    model.train(\n        data=\"./coco128.yaml\",\n        epochs=3,\n        imgsz=320,\n        # 开启DDP\n        device=[0,1,2,3],\n        # 可以通过project参数设置SwanLab的project，name参数设置SwanLab的experiment_name\n        project=\"Ultralytics\",\n        name=\"yolov8n\"\n    )\n```",
    "766": "一级标题：Unsloth\n二级标题：无\n内容：\n[微信公众号文章](https://mp.weixin.qq.com/s/re7R7WhTYNuiDj0fSwAnWQ)\n\n[Unsloth](https://github.com/unslothai/unsloth) 是一个用于加速 LLM（大型语言模型）微调的轻量级库 。它与 Hugging Face 生态系统完全兼容，包括 Hub、transformers 和 PEFT 。\n\n![logo](./unsloth/logo.png)\n\n你可以使用Unsloth与Tranformers或TRL结合加速LLM模型训练，同时使用SwanLab进行实验跟踪与可视化。",
    "767": "一级标题：Unsloth\n二级标题：1. 引入SwanLabCallback\n内容：\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\n```\n\nSwanLabCallback是适配于Transformers的日志记录类。\n\nSwanLabCallback可以定义的参数有：\n\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。\n- 你也可以在外部通过swanlab.init创建项目，集成会将实验记录到你在外部创建的项目中。",
    "768": "一级标题：Unsloth\n二级标题：2. 传入Trainer\n内容：\n```python {1,7,12}\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom trl import GRPOTrainer\n\n...\n\n# 实例化SwanLabCallback\nswanlab_callback = SwanLabCallback(project=\"unsloth-example\")\n\ntrainer = GRPOTrainer(\n    ...\n    # 传入callbacks参数\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n```",
    "769": "一级标题：Unsloth\n二级标题：3. 与Unsloth结合的案例模板\n内容：\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom unsloth import FastLanguageModel, PatchFastRL\n\nPatchFastRL(\"GRPO\", FastLanguageModel)  # 对 TRL 进行补丁处理\nfrom trl import GRPOConfig, GRPOTrainer, ModelConfig, TrlParser\n\n...\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n...\n)\n\n# PEFT 模型\nmodel = FastLanguageModel.get_peft_model(\n...\n)\n\n# 实例化SwanLabCallback\nswanlab_callback = SwanLabCallback(\n  project=\"trl_integration\",\n  experiment_name=\"qwen2.5-sft\",\n  description=\"测试swanlab和trl的集成\",\n  config={\"framework\": \"🤗TRL\"},\n)\n\n# 定义GRPOTrainer\ntrainer = GRPOTrainer(\n    ...\n    # 传入callbacks参数\n    callbacks=[swanlab_callback],\n)\n\n#开启训练！\ntrainer.train()\n```",
    "770": "一级标题：verl\n二级标题：无\n内容：\n[verl](https://github.com/volcengine/verl) 是一个灵活、高效且可用于生产环境的强化学习（RL）训练框架，专为大型语言模型（LLMs）的后训练设计。它由字节跳动火山引擎团队开源，是 [HybridFlow](https://arxiv.org/abs/2409.19256) 论文的开源实现。\n\n<div style=\"text-align: center;\">\n    <img src=\"./verl/verl_logo.svg\" alt=\"verl_logo\" style=\"width: 70%;\">\n</div>\n\n**verl 具有以下特点，使其灵活且易于使用：**\n\n1. **易于扩展的多样化 RL 算法**：Hybrid 编程模型结合了单控制器和多控制器范式的优点，能够灵活表示并高效执行复杂的后训练数据流。用户只需几行代码即可构建 RL 数据流。\n\n2. **与现有 LLM 基础设施无缝集成的模块化 API**：通过解耦计算和数据依赖，verl 能够与现有的 LLM 框架（如 PyTorch FSDP、Megatron-LM 和 vLLM）无缝集成。此外，用户可以轻松扩展到其他 LLM 训练和推理框架。\n\n3. **灵活的设备映射和并行化**：支持将模型灵活地映射到不同的 GPU 组上，以实现高效的资源利用，并在不同规模的集群上具有良好的扩展性。\n\n4. **与流行的 HuggingFace 模型轻松集成**：verl 能够方便地与 HuggingFace 模型进行集成。\n\n**verl 也具有以下优势，使其运行速度快：**\n\n1. **最先进的吞吐量**：通过无缝集成现有的 SOTA LLM 训练和推理框架，verl 实现了高生成和训练吞吐量。\n\n2. **基于 3D-HybridEngine 的高效 Actor 模型重分片**：消除了内存冗余，并显著减少了在训练和生成阶段之间切换时的通信开销。\n\n更多信息可参考如下链接\n\n> * verl GitHub仓库链接: [https://github.com/volcengine/verl](https://github.com/volcengine/verl)\n> * 官方文档: [https://verl.readthedocs.io/en/latest/index.html](https://verl.readthedocs.io/en/latest/index.html)\n> * HybridFlow论文地址: [https://arxiv.org/pdf/2409.19256v2](https://arxiv.org/pdf/2409.19256v2)\n\n\n你可以使用verl快速进行大模型强化学习训练，同时使用SwanLab进行实验跟踪与可视化。",
    "771": "一级标题：verl\n二级标题：环境安装\n内容：\n需要环境：\n\n* Python: Version >= 3.9\n\n* CUDA: Version >= 12.1\n\n参考verl官方文档安装：[https://verl.readthedocs.io/en/latest/start/install.html](https://verl.readthedocs.io/en/latest/start/install.html)\n\n以及需要额外安装SwanLab\n\n```bash\npip install -U swanlab\n```",
    "772": "一级标题：verl\n二级标题：使用方法\n内容：\n以verl官方文档的[Post-train a LLM using PPO with GSM8K dataset](https://verl.readthedocs.io/en/latest/start/quickstart.html)为例。\n\n你仅需要通过在实验的启动命令中，增加`trainer.logger=['swanlab']`，即可选择swanlab进行实验跟踪。\n\n**完整的测试命令如下：**\n\n```bash {4}\nPYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo \\\n data.train_files=$HOME/data/gsm8k/train.parquet \\\n data.val_files=$HOME/data/gsm8k/test.parquet \\\n trainer.logger=['console','swanlab'] \\\n data.train_batch_size=256 \\\n data.val_batch_size=1312 \\\n data.max_prompt_length=512 \\\n data.max_response_length=256 \\\n actor_rollout_ref.model.path=Qwen/Qwen2.5-0.5B-Instruct \\\n actor_rollout_ref.actor.optim.lr=1e-6 \\\n actor_rollout_ref.actor.ppo_mini_batch_size=64 \\\n actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 \\\n actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8 \\\n actor_rollout_ref.rollout.tensor_model_parallel_size=1 \\\n actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \\\n actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 \\\n critic.optim.lr=1e-5 \\\n critic.model.path=Qwen/Qwen2.5-0.5B-Instruct \\\n critic.ppo_micro_batch_size_per_gpu=4 \\\n algorithm.kl_ctrl.kl_coef=0.001 \\\n trainer.val_before_train=False \\\n trainer.default_hdfs_dir=null \\\n trainer.n_gpus_per_node=1 \\\n trainer.nnodes=1 \\\n trainer.save_freq=10 \\\n trainer.test_freq=10 \\\n trainer.total_epochs=15 2>&1 | tee verl_demo.log\n```\n\n:::info\n如果你需要设置项目和实验名，可以设置`trainer.project_name`和`trainer.experiment_name`。\n如：\n```bash\nPYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo \\\n ...\n trainer.project_name=\"verl_demo\" \\\n trainer.experiment_name=\"ppo\" \\\n ...\n```\n:::\n\n如果启动训练时你还未登陆SwanLab，会出现如下提示。\n\n![select](./verl/select.png)\n\n选择**1、2**则为使用云端跟踪模式，选择后根据引导输入官网的API即可实现在线跟踪。可以在线查看训练跟踪结果。选择**3**则不上传训练数据，采用离线跟踪。\n\n\n当然，你也可以通过[环境变量](/api/environment-variable)的方式登陆或者设置跟踪模式：\n\n```bash\nexport SWANLAB_API_KEY=<你的登陆API>           # 设置在线跟踪模式API\nexport SWANLAB_LOG_DIR=<设置本地日志存储路径>    # 设置本地日志存储路径\nexport SWANLAB_MODE=<设置SwanLab的运行模式>     # 包含四种模式：cloud云端跟踪模式（默认）、cloud-only仅云端跟踪本地不保存文件、local本地跟踪模式、disabled完全不记录用于debug\n```",
    "773": "一级标题：verl\n二级标题：查看训练日志\n内容：\n完成登陆后会显示如下登陆信息：\n\n![track](./verl/track.png)\n\n运行进程，即可在[SwanLab官网](https://swanlab.cn)上查看训练日志：\n\n![remote](./verl/remote.png)\n\n更多使用方法可以参考[SwanLab查看使用结果](https://docs.swanlab.cn/guide_cloud/experiment_track/view-result.html)\n\n---\n\n如果你使用本地看板模式，则可以通过如下命令打开本地看板\n\n```bash\nswanlab watch\n```\n\n更多详细可以参考[SwanLab离线看板模式](https://docs.swanlab.cn/guide_cloud/self_host/offline-board.html)\n\n服务器设置端口号可以查看[离线看板端口号](https://docs.swanlab.cn/api/cli-swanlab-watch.html#%E8%AE%BE%E7%BD%AEip%E5%92%8C%E7%AB%AF%E5%8F%A3%E5%8F%B7)",
    "774": "一级标题：verl\n二级标题：每轮评估时记录生成文本\n内容：\n如果你希望在每轮评估（val）时将生成的文本记录到SwanLab中，只需在命令行钟增加一行`trainer.log_val_generations=1`即可：\n\n```bash {5}\nPYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo \\\n data.train_files=$HOME/data/gsm8k/train.parquet \\\n data.val_files=$HOME/data/gsm8k/test.parquet \\\n trainer.logger=['console','swanlab'] \\\n trainer.log_val_generations=1 \\\n ...\n```\n\n> 如果你希望每轮评估时生成多条结果，如10条，那么修改`trainer.log_val_generations=10`即可",
    "775": "一级标题：verl\n二级标题：断点续训\n内容：\n如果你训练时崩溃或希望补充实验，可以使用[resume](/guide_cloud/experiment_track/resume-experiment.html)功能来恢复实验。\n\n在verl训练中，你可以通过设置环境变量来执行resume：\n\n```bash\nexport SWANLAB_RESUME=must\nexport SWANLAB_RUN_ID=<exp_id>\n```",
    "776": "一级标题：Weights & Biases\n二级标题：无\n内容：\nWeights & Biases (Wandb) 是一个用于机器学习和深度学习项目的实验跟踪、模型优化和协作平台。W&B 提供了强大的工具来记录和可视化实验结果，帮助数据科学家和研究人员更好地管理和分享他们的工作。\n\n![wandb](/assets/ig-wandb.png)\n\n:::warning 其他工具的同步教程\n\n- [TensorBoard](/guide_cloud/integration/integration-tensorboard.md)\n- [MLFlow](/guide_cloud/integration/integration-mlflow.md)\n:::\n\n**你可以用两种方式将Wandb上的项目同步到SwanLab：**\n\n1. **同步跟踪**：如果你现在的项目使用了wandb进行实验跟踪，你可以使用`swanlab.sync_wandb()`命令，在运行训练脚本时同步记录指标到SwanLab。\n2. **转换已存在的项目**：如果你想要将wandb上的项目复制到SwanLab，你可以使用`swanlab convert`，将Wandb上已存在的项目转换成SwanLab项目。\n\n::: info\n在当前版本暂仅支持转换标量图表。\n:::\n\n[[toc]]",
    "777": "一级标题：Weights & Biases\n二级标题：1. 同步跟踪\n内容：\n### 1.1 添加sync_wandb命令\n\n在你的代码执行`wandb.init()`之前的任何位置，添加一行`swanlab.sync()`命令，即可在训练时同步wandb的指标到SwanLab。\n\n```python\nimport swanlab\n\nswanlab.sync_wandb()\n\n...\n\nwandb.init()\n```\n\n在上述这种代码写法中，`wandb.init()`的同时会初始化swanlab，项目名、实验名和配置和`wandb.init()`中的`project`、`name`、`config`一致，因此你不需要再手动初始化swanlab。\n\n:::info\n\n**`sync_wandb`支持设置两个参数：**\n\n- `mode`: swanlab的记录模式，支持cloud、local和disabled三种模式。\n- `wandb_run`: 如果此参数设置为**False**，则不会将数据上传到wandb，等同于设置wandb.init(mode=\"offline\")\n\n:::\n\n### 1.2 另一种写法\n\n另一种用法是先手动初始化swanlab，再运行wandb的代码。\n\n```python\nimport swanlab\n\nswanlab.init(...)\nswanlab.sync_wandb()\n\n...\n\nwandb.init()\n```\n\n在这种写法中，项目名、实验名、配置和`swanlab.init()`中的`project`、`experiment_name`、`config`一致，而后续`wandb.init()`中的`project`、`name`会被忽略，`config`会更新进`swanlab.config`中。\n\n### 1.3 测试代码\n\n```python\nimport wandb\nimport random\nimport swanlab\n\nswanlab.sync_wandb()\n# swanlab.init(project=\"sync_wandb\")\n\nwandb.init(\n  project=\"test\",\n  config={\"a\": 1, \"b\": 2},\n  name=\"test\",\n  )\n\nepochs = 10\noffset = random.random() / 5\nfor epoch in range(2, epochs):\n  acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n  loss = 2 ** -epoch + random.random() / epoch + offset\n\n  wandb.log({\"acc\": acc, \"loss\": loss})\n```\n\n![alt text](/assets/ig-wandb-4.png)",
    "778": "一级标题：Weights & Biases\n二级标题：2. 转换已存在的项目\n内容：\n### 2.1 找到你在wandb.ai上的projecy、entity和runid\n\nprojecy、entity和runid是转换所需要的（runid是可选的）。\nproject和entity的位置：\n![alt text](/assets/ig-wandb-2.png)\n\nrunid的位置：\n\n![alt text](/assets/ig-wandb-3.png)\n\n### 2.2 方式一：命令行转换\n\n首先，需要确保当前环境下，你已登录了wandb，并有权限访问目标项目。\n\n转换命令行：\n\n```bash\nswanlab convert -t wandb --wb-project [WANDB_PROJECT_NAME] --wb-entity [WANDB_ENTITY]\n```\n\n支持的参数如下：\n\n- `-t`: 转换类型，可选wandb与tensorboard。\n- `-p`: SwanLab项目名。\n- `-w`: SwanLab工作空间名。\n- `--mode`: (str) 选择模式，默认为\"cloud\"，可选 [\"cloud\", \"local\", \"offline\", \"disabled\"]\n- `-l`: logdir路径。\n- `--wb-project`：待转换的wandb项目名。\n- `--wb-entity`：wandb项目所在的空间名。\n- `--wb-runid`: wandb Run（项目下的某一个实验）的id。\n\n如果不填写`--wb-runid`，则会将指定项目下的全部Run进行转换；如果填写，则只转换指定的Run。\n\n---\n\n**异步转换方法（先将数据下载到本地，再上传到swanlab）**\n\n1. 数据下载到本地：\n\n```bash\nswanlab convert --mode 'offline' -t wandb --wb-project [WANDB_PROJECT_NAME] --wb-entity [WANDB_ENTITY]\n```\n\n2. 上传到swanlab：\n\n```bash\nswanlab sync [日志文件夹路径]\n```\n\n[swanlab sync文档](/zh/api/cli-swanlab-sync.md)\n\n\n### 2.3 方式二：代码内转换\n\n```python\nfrom swanlab.converter import WandbConverter\n\nwb_converter = WandbConverter()\n# wb_runid可选\nwb_converter.run(wb_project=\"WANDB_PROJECT_NAME\", wb_entity=\"WANDB_USERNAME\")\n```\n\n效果与命令行转换一致。\n\n`WandbConverter`支持的参数：\n\n- `project`: SwanLab项目名。\n- `workspace`: SwanLab工作空间名。\n- `mode`: (str) 选择模式，默认为\"cloud\"，可选 [\"cloud\", \"local\", \"offline\", \"disabled\"]\n- `logdir`: logdir路径。\n\n`WandbConverter.run`支持的参数：\n\n- `wb_project`: wandb项目名。\n- `wb_entity`: wandb项目所在的空间名。\n- `wb_runid`: wandb Run（项目下的某一个实验）的id。\n\n**异步转换方法（先将数据下载到本地，再上传到swanlab）**\n\n1. 数据下载到本地：\n\n```python\nfrom swanlab.converter import WandbConverter\n\nwb_converter = WandbConverter(mode=\"offline\")\n# wb_runid可选\nwb_converter.run(wb_project=\"WANDB_PROJECT_NAME\", wb_entity=\"WANDB_USERNAME\")\n```\n\n2. 上传到swanlab：\n\n```bash\nswanlab sync [日志文件夹路径]\n```\n\n[swanlab sync文档](/zh/api/cli-swanlab-sync.md)",
    "779": "一级标题：XGBoost\n二级标题：无\n内容：\nXGBoost（eXtreme Gradient Boosting）是一种高效、灵活且广泛使用的梯度提升框架，由陈天奇在2014年提出。它基于决策树算法，通过集成多个弱学习器（通常是决策树）来构建一个强大的预测模型。XGBoost在各种机器学习竞赛和实际应用中表现出色，尤其是在分类、回归和排序任务中。\n\n![xgboost](/zh/guide_cloud/integration/xgboost/logo.png)\n\n你可以使用XGBoost快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。",
    "780": "一级标题：XGBoost\n二级标题：1. 引入SwanLabCallback\n内容：\n```python\nfrom swanlab.integration.xgboost import SwanLabCallback\n```\n\nSwanLabCallback是适配于XGBoost的日志记录类。",
    "781": "一级标题：XGBoost\n二级标题：2. 初始化SwanLab\n内容：\n```python\nswanlab.init(\n    project=\"xgboost-example\",\n)\n```",
    "782": "一级标题：XGBoost\n二级标题：3. 传入`xgb.train`\n内容：\n```python\nimport xgboost as xgb\n\nbst = xgb.train(\n    ...\n    callbacks=[SwanLabCallback()]\n)\n```",
    "783": "一级标题：XGBoost\n二级标题：4. 完整测试代码\n内容：\n```python\nimport xgboost as xgb\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nimport swanlab\nfrom swanlab.integration.xgboost import SwanLabCallback\n\n# 初始化swanlab\nswanlab.init(\n    project=\"xgboost-breast-cancer\",\n    config={\n        \"learning_rate\": 0.1,\n        \"max_depth\": 3,\n        \"subsample\": 0.8,\n        \"colsample_bytree\": 0.8,\n        \"num_round\": 100\n    }\n)\n\n# 加载数据集\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# 将数据集分为训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 转换为DMatrix格式，这是XGBoost的内部数据格式\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n\n# 设置参数\nparams = {\n    'objective': 'binary:logistic',  # 二分类任务\n    'max_depth': 3,                  # 树的最大深度\n    'eta': 0.1,                      # 学习率\n    'subsample': 0.8,                # 样本采样比例\n    'colsample_bytree': 0.8,         # 特征采样比例\n    'eval_metric': 'logloss'         # 评估指标\n}\n\n# 训练模型\nnum_round = 100  # 迭代次数\nbst = xgb.train(\n    params,\n    dtrain,\n    num_round,\n    evals=[(dtrain, 'train'), (dtest, 'test')],\n    callbacks=[SwanLabCallback()]\n)\n\n# 进行预测\ny_pred = bst.predict(dtest)\ny_pred_binary = [round(value) for value in y_pred]  # 将概率转换为二分类结果\n\n# 评估模型\naccuracy = accuracy_score(y_test, y_pred_binary)\nprint(f\"Accuracy: {accuracy:.4f}\")\n\n# 打印分类报告\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred_binary, target_names=data.target_names))\n\n# 保存模型\nbst.save_model('xgboost_model.model')\n\n# 结束swanlab会话\nswanlab.finish()\n```",
    "784": "一级标题：Xtuner\n二级标题：无\n内容：\n[XTuner](https://github.com/InternLM/xtuner) 是一个高效、灵活、全能的轻量化大模型微调工具库。\n\n<div align=\"center\">\n<img src=\"/assets/integration-xtuner.png\" width=440>\n</div>\n\nXtuner支持与书生·浦语（InternLM）、Llama等多款开源大模型的适配，可执行增量预训练、指令微调、工具类指令微调等任务类型。硬件要求上，在Tesla T4、A100等传统数据中心之外，开发者最低使用消费级显卡便可进行训练，实现大模型特定需求能力。\n\n<div align=\"center\">\n<img src=\"/assets/integration-xtuner-intro.png\">\n</div>\n\nXtuner 支持通过 MMEngine 使用 SwanLab 进行在线跟踪，只需在配置文件中添加几行代码，就可以跟踪和可视化损失、显存占用等指标。",
    "785": "一级标题：Xtuner\n二级标题：使用SwanLab可视化跟踪Xtuner微调进展\n内容：\n打开要训练的配置文件（比如[qwen1_5_7b_chat_full_alpaca_e3.py](https://github.com/InternLM/xtuner/blob/main/xtuner/configs/qwen/qwen1_5/qwen1_5_7b_chat/qwen1_5_7b_chat_full_alpaca_e3.py)）），找到`visualizer`参数的位置，将它替换成：\n\n```python\n# set visualizer\nfrom mmengine.visualization import Visualizer\nfrom swanlab.integration.mmengine import SwanlabVisBackend\n\nvisualizer = dict(type=Visualizer, vis_backends=[dict(type=SwanlabVisBackend)])\n```\n\n然后照样运行微调命令，即可实现SwanLab实验跟踪：\n\n```bash\nxtuner train qwen1_5_7b_chat_full_alpaca_e3.py\n```\n\n---\n\n如果希望像平常使用SwanLab那样指定项目名、实验名等信息，可以在实例化`SwanlabVisBackend`时在`init_kwargs`参数中指定，可以参考 [swanlab init](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/sdk.py#L71) 查看可配置的参数。\n\n通过以字典的形式传入`init_kwargs`，该参数最终会传给 `swanlab.init` 方法，下面举了个指定项目名称的案例。\n\n```python (5)\nvisualizer = dict(\n  type=Visualizer,\n  vis_backends=[dict(\n        type=SwanlabVisBackend,\n        init_kwargs=dict(project='toy-example', experiment_name='Qwen'),\n    )])\n```\n\n有关MM系列的其他引入方法和更灵活的配置，可以参考[MMEngine接入SwanLab](https://docs.swanlab.cn/zh/guide_cloud/integration/integration-mmengine.html)。",
    "786": "一级标题：ZhipuAI\n二级标题：无\n内容：\n[zhipuai](https://github.com/MetaGLM/zhipuai-sdk-python-v4)是[智谱开放平台](https://open.bigmodel.cn/dev/api) 大模型接口的Python SDK，让开发者更便捷的调用智谱开放API。\n\n![](/assets/integration-zhipu.jpg)\n\n你可以使用zhipuai获得ChatGLM的回复，同时使用SwanLab自动进行过程记录。",
    "787": "一级标题：ZhipuAI\n二级标题：1. 引入autolog\n内容：\n```python\nfrom swanlab.integration.zhipuai import autolog\n```\n\nautolog是一个为zhipuai适配的过程记录类，能够自动记录你的zhipuai交互的过程。",
    "788": "一级标题：ZhipuAI\n二级标题：2. 传入参数\n内容：\n```python\nautolog(init=dict(project=\"zhipuai_logging\"))\n```\n\n这里给`init`传入的参数与`swanlab.init`的参数形式完全一致。",
    "789": "一级标题：ZhipuAI\n二级标题：3. 自动记录\n内容：\n```python\nfrom swanlab.integration.zhipuai import autolog\n\nautolog(init=dict(project=\"zhipuai_logging\"))\nclient = autolog.client\n\nresponse = client.chat.completions.create(\n    model=\"glm-4\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"作为一名营销专家，请为我的产品创作一个吸引人的slogan\"},\n        {\"role\": \"assistant\", \"content\": \"当然，为了创作一个吸引人的slogan，请告诉我一些关于您产品的信息\"},\n        {\"role\": \"user\", \"content\": \"智谱AI开放平台\"},\n        {\"role\": \"assistant\", \"content\": \"智启未来，谱绘无限一智谱AI，让创新触手可及!\"},\n        {\"role\": \"user\", \"content\": \"创造一个更精准、吸引人的slogan\"},\n    ],\n)\n\nresponse2 = client.chat.completions.create(\n    model=\"glm-4\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"谁获得了NBA2015年的总冠军\"},\n    ],\n)\n```",
    "790": "一级标题：阿里云计算巢应用部署\n二级标题：无\n内容：\n:::warning 关于第三方部署\n\n第三方部署是由社区贡献的部署方式，官方不保证能实时同步最新版本。\n\n:::\n\n目前 SwanLab 社区版本已上线阿里云计算巢服务市场，欢迎各位训练师通过阿里云一键部署使用~",
    "791": "一级标题：阿里云计算巢应用部署\n二级标题：⚠️ 前提条件\n内容：\n部署 SwanLab 社区版服务实例，需要对部分阿里云资源进行访问和创建操作。因此您的账号需要包含如下资源的权限。\n**说明**：当您的账号是RAM账号时，才需要添加此权限。\n\n| 权限策略名称                          | 备注                         |\n|---------------------------------|----------------------------|\n| AliyunECSFullAccess             | 管理云服务器服务（ECS）的权限           |\n| AliyunVPCFullAccess             | 管理专有网络（VPC）的权限             |\n| AliyunROSFullAccess             | 管理资源编排服务（ROS）的权限           |\n| AliyunComputeNestUserFullAccess | 管理计算巢服务（ComputeNest）的用户侧权限 |",
    "792": "一级标题：阿里云计算巢应用部署\n二级标题：💰 计费说明\n内容：\nSwanLab社区版在计算巢部署的费用主要涉及：\n\n- 所选vCPU与内存规格\n- 系统盘类型及容量\n- 公网带宽",
    "793": "一级标题：阿里云计算巢应用部署\n二级标题：🚀 部署流程\n内容：\n1. 访问计算巢SwanLab社区版[部署链接](https://computenest.console.aliyun.com/service/instance/create/cn-hangzhou?type=user&ServiceId=service-cb2da57160444c3ebdbf)\n，按提示填写部署参数：\n<img src=\"./alibabacloud-computenest/deploy_service_instance.jpg\" width=\"800\"/>\n\n2. 参数填写完成后可以看到对应询价明细，确认参数后点击**下一步：确认订单**。\n\n3. 确认订单完成后同意服务协议并点击**立即创建**进入部署阶段。\n\n4. 等待部署完成后就可以开始使用服务，进入服务实例详情点击服务地址。\n   <img src=\"./alibabacloud-computenest/get_service_instance.jpg\" width=\"800\"/>\n\n5. 访问服务地址注册账号并使用SwanLab服务。\n   <img src=\"./alibabacloud-computenest/swanlab_service.jpg\" width=\"800\"/>",
    "794": "一级标题：使用Docker进行部署\n二级标题：无\n内容：\n如果你想要使用SwanLab私有化部署（社区版），请按照下面的流程进行安装。\n\n![logo](./docker-deploy/swanlab-docker.jpg)",
    "795": "一级标题：使用Docker进行部署\n二级标题：先决条件\n内容：\n> 在安装 SwanLab 之前，请确保您的机器满足以下最低系统要求：\n>\n> - CPU >= 2核\n> - 内存 >= 4GB\n> - 存储空间 >= 20GB\n\nSwanLab 私有化部署版，需要使用 **Docker Compose** 进行安装与部署（暂不支持K8S部署），请根据你的操作系统，对表下面的表格选择正确的Docker及compose版本。\n\n**如果你已经安装了Docker，请跳过这一步。**\n\n| 操作系统               | 软件                                                   | 解释                                                                                                                                                                                                                                                                                                                                                 |\n| ---------------------- | ------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| macOS 10.14 或更高版本 | Docker Desktop                                         | 将 Docker 虚拟机 (VM) 设置为至少使用 2 个虚拟 CPU (vCPU) 和 8 GB 初始内存。否则，安装可能会失败。有关更多信息，请参阅[Mac 版 Docker Desktop 安装指南](https://docs.docker.com/desktop/install/mac-install/)。                                                                                                                                        |\n| Windows（启用了WSL 2） | Docker Desktop                                         | 我们建议将源代码和其他与 Linux 容器绑定的数据存储在 Linux 文件系统中，而不是 Windows 文件系统中。有关更多信息，请参阅 [Windows上使用WSL安装Linux](https://learn.microsoft.com/zh-cn/windows/wsl/install) 与 [在 Windows 上使用 WSL 2 后端的 Docker Desktop 安装指南](https://docs.docker.com/desktop/setup/install/windows-install/#wsl-2-backend)。 |\n| Linux                  | Docker 19.03 或更高版本 Docker Compose 1.28 或更高版本 | 有关如何安装Docker和Docker Compose 的更多信息，请参阅[Docker 安装指南](https://docs.docker.com/engine/install/)和[Docker Compose 安装指南](https://docs.docker.com/compose/install/)。                                                                                                                                                               |\n\n> 如果你还未安装Docker，可以运行我们提供的[安装脚本](https://docs.docker.com/desktop/install/mac-install/)。\n\n---\n\n**端口说明**\n\n如果你将SwanLab部署在服务器上，并希望能够远程访问与实验记录，那么请确保服务器开放以下两个端口：\n\n| 端口号 | 是否可配置 | 用途说明                                                      |\n| ------ | ---------- | ------------------------------------------------------------- |\n| 8000   | 是         | 网关服务端口，可用于接收外部请求，建议在公网环境中设置为 `80` |\n| 9000   | 否         | MinIO 签名端口，用于对象存储访问，端口固定不可修改            |\n\n> 由于网关服务端口（默认为`8000`）支持在部署前后修改，所以请确保你开放的是最终修改的端口。",
    "796": "一级标题：使用Docker进行部署\n二级标题：1. 克隆仓库\n内容：\n使用Git克隆`self-hosted`仓库：\n\n```bash\ngit clone https://github.com/SwanHubX/self-hosted.git\ncd self-hosted\n```",
    "797": "一级标题：使用Docker进行部署\n二级标题：2. 一键脚本安装\n内容：\n> 如果你使用的是Windows系统，请确保已安装并开启 WSL2 和 Docker Desktop\n> <img src=\"./docker-deploy/wsl-dockerinfo.png\" width=\"600\"/>\n\n> 在WSL2的文件系统中执行 `.sh` 安装脚本\n> <img src=\"./docker-deploy/wsl-bash.png\" width=\"600\"/>\n\n默认的安装脚本在`docker/install.sh`，直接执行即可一键安装所有需要的容器以及执行初始化配置。\n\n```bash\ncd ./docker\n./install.sh\n```\n\n默认脚本链接的镜像源在中国，所以中国地区的下载速度非常快！\n\n如果你需要使用 [DockerHub](https://hub.docker.com/) 作为镜像源，则可以使用下面的脚本进行安装：\n\n```bash\n./install-dockerhub.sh\n```",
    "798": "一级标题：使用Docker进行部署\n二级标题：3. 激活主账号\n内容：\nSwanLab社区版默认会使用`8000`端口，如果你使用的是默认配置，那么可以直接访问：`http://localhost:8000`，就可以访问到SwanLab社区版。\n\n> 也有可能社区版部署在了其他端口，请打开 Docker Desktop，找到`traefik`容器旁边的port映射，比如`64703:80`，那么你应该访问`http://localhost:64703`。\n\n![](./docker-deploy/create-account.png)\n\n现在，你需要激活你的主账号。激活需要1个License，个人使用可以免费在[SwanLab官网](https://swanlab.cn)申请一个，位置在 「设置」-「账户与许可证」。\n\n:::warning 离线验证\n\n在私有化部署 > `v1.1`的版本中，支持在离线环境下验证License。\n\n:::\n\n![](./docker-deploy/apply-license.png)\n\n拿到License后，回到激活页面，填写用户名、密码、确认密码和License，点击激活即可完成创建。\n\n![](./docker-deploy/quick-start.png)",
    "799": "一级标题：使用Docker进行部署\n二级标题：4. 开始你的第一个实验\n内容：\n在Python SDK完成登录：\n\n```bash\nswanlab login --host <IP地址>\n```\n\n> 如果你之前登录过swanlab，想要重新登录，请使用：\n> `swanlab login --host <IP地址> --relogin`。\n\n按回车，填写API Key，完成登录。之后你的SwanLab实验将会默认传到私有化部署的SwanLab上。\n\n---\n\n测试脚本：\n\n```bash\nimport swanlab\nimport random\n\n# 创建一个SwanLab项目\nswanlab.init(\n    # 设置项目名\n    project=\"my-awesome-project\",\n\n    # 设置超参数\n    config={\n        \"learning_rate\": 0.02,\n        \"architecture\": \"CNN\",\n        \"dataset\": \"CIFAR-100\",\n        \"epochs\": 10\n    }\n)\n\n# 模拟一次训练\nepochs = 10\noffset = random.random() / 5\nfor epoch in range(2, epochs):\n  acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n  loss = 2 ** -epoch + random.random() / epoch + offset\n\n  # 记录训练指标\n  swanlab.log({\"acc\": acc, \"loss\": loss})\n\n# [可选] 完成训练，这在notebook环境中是必要的\nswanlab.finish()\n```\n\n运行后在网页查看实验：\n\n![](./docker-deploy/test-experiment.png)",
    "800": "一级标题：使用Docker进行部署\n二级标题：升级版本\n内容：\n如果你想要将你本地的私有化部署版本升级到最新版，请使用下面的命令：\n\n```bash\n# 在你之前本地部署的 self-hosted 项目目录下\ncd ./docker\n./upgrade.sh\n```\n\n升级完成的命令行样式：\n\n![](./docker-deploy/upgrade.png)",
    "801": "一级标题：团队/企业版\n二级标题：无\n内容：\n私有化个人版（免费）目前支持现在公有云版的绝大部分功能，但不支持多人协作、创建组织、权限控制、统计看板等高级功能。\n\n对 团队版/企业版/多租户云版 有需求的伙伴，欢迎联系我们：[contact@swanlab.cn](mailto:contact@swanlab.cn)，并备注您的公司/机构与职位。",
    "802": "一级标题：常见问题\n二级标题：无\n内容：",
    "803": "一级标题：常见问题\n二级标题：如何修改端口？\n内容：\nSwanLab 自托管版本基于 [Docker](https://www.docker.com/) 部署，默认情况下使用 `8000` 端口，修改自托管服务默认访问端口实际上是修改 **swanlab-traefik** 容器的映射端口，分为以下两种情况：\n\n### 部署前修改\n\n安装脚本提供有一些配置可选项，包括数据存储位置和映射的端口，我们通过修改脚本启动参数来实现修改端口。\n\n- 执行 `install.sh` 安装脚本后，命令行会提示配置可选项，可以交互式输入对应的参数。在命令行输出 `2. Use the default port  (8000)? (y/n):` 后输入 `n`，然后会提示 `Enter a custom port:`，输入对应的端口号即可，例如 `80` 。\n\n```bash\n❯ bash install.sh\n🤩 Docker is installed, so let's get started.\n🧐 Checking if Docker is running...\n\n1. Use the default path  (./data)? (y/n):\n   The selected path is: ./data\n2. Use the default port  (8000)? (y/n):\n```\n\n- 启动脚本时添加参数，安装脚本提供有命令行参数 `-p` 可以用于修改端口，例如： `./install.sh -p 80`。\n\n> 更多命令行参数详见：[通过 Docker 部署](https://github.com/SwanHubX/self-hosted/tree/main/docker)\n\n### 部署后修改\n\n如果需要 SwanLab 服务部署完成后需要修改访问端口，则需要修改生成的 `docker-compose.yaml` 配置文件。\n\n在脚本执行的位置找到 `swanlab/` 目录，执行 `cd swanlab/` 后进入到 `swanlab` 目录下找到对应的 `docker-compose.yaml` 配置文件，然后修改 `traefik` 容器对应的端口 `ports`，如下所示：\n\n```yaml\n  traefik:\n    <<: *common\n    image: ccr.ccs.tencentyun.com/self-hosted/traefik:v3.0\n    container_name: swanlab-traefik\n    ports:\n      - \"8000:80\" # [!code --]\n      - \"80:80\" # [!code ++]\n```\n\n> 上面将访问端口修改为了 `80`\n\n修改完成后执行 `docker compose up -d` 重启容器，重启完成后即可通过 `http://{ip}:80` 访问",
    "804": "一级标题：常见问题\n二级标题：上传媒体文件报错怎么办\n内容：\n当你使用`swanlab.log`记录媒体文件，如图像、音频时，发现报错，如：\n\n```bash\nswanlab: Upload error: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The Access Key Id you provided does not exist in our records.\n```\n\n请检查你的服务器是否开放了`9000`端口，如果未开放，请在服务器防火墙/安全组中开放`9000`端口。",
    "805": "一级标题：离线看板接口文档\n二级标题：无\n内容：\n:::info 提示\n\n本 API 适用于 Swanlab **离线看板模式**。主要用于获取项目、实验、图表等数据，便于进行数据分析。\n\n:::",
    "806": "一级标题：离线看板接口文档\n二级标题：接口 1：获取项目详情\n内容：\n- **URL**：`/api/v1/project`\n- **方法**：`GET`\n- **接口说明**：获取当前 Swanlab 实例中加载的项目及其所有实验信息。\n\n### 响应示例\n\n```json\n{\n  \"code\": 0,\n  \"message\": \"success\",\n  \"data\":{\n    \"id\": 1,\n    \"name\": \"llamafactory\",\n    \"experiments\": [\n        {\n        \"id\": 1,\n        \"name\": \"Qwen2.5-7B/20250321-1130-16bed2e2\",\n        \"run_id\": \"run-20250321_125806-a3b1799d\",\n        \"status\": 0,\n        \"config\": { ... },\n        \"create_time\": \"2025-03-21T04:58:06.387383+00:00\"\n        },\n        ...\n    ]\n  }\n}\n```\n\n### 字段说明\n\n- `id` / `name`：项目唯一标识与名称。\n- `experiments`：该项目下的所有实验信息。\n- `logdir`：日志文件存储路径。\n- `charts`：图表数量。\n- `pinned_opened` / `hidden_opened`：控制面板的默认展开状态。\n\n---",
    "807": "一级标题：离线看板接口文档\n二级标题：接口 2：获取单个实验详情\n内容：\n- **URL**：`/api/v1/experiment/<experiment_id>`\n- **方法**：`GET`\n- **示例**：`/api/v1/experiment/1`\n- **接口说明**：获取指定实验的详细配置信息与系统环境。\n\n### 响应示例\n\n```json\n{\n  \"code\": 0,\n  \"message\": \"success\",\n  \"data\": {\n    \"id\": 1,\n    \"run_id\": \"run-20250321_125806-a3b1799d\",\n    \"name\": \"Qwen2.5-7B/20250321-1130-16bed2e2\",\n    \"config\": { ... },\n    \"system\": {\n      \"cpu\": { \"brand\": \"Intel...\", \"cores\": 104 },\n      \"gpu\": {\n        \"nvidia\": {\n          \"type\": [\"NVIDIA A100-PCIE-40GB\", ...],\n          \"memory\": [40, 40, 40, 40],\n          \"cuda\": \"11.6\"\n        }\n      },\n      \"os\": \"Linux...\",\n      \"python\": \"3.10.14\",\n      \"command\": \"/path/to/cli config.yaml\",\n      \"swanlab\": {\n        \"version\": \"0.5.2\",\n        \"logdir\": \"/path/to/logs\",\n        \"_monitor\": 3\n      }\n    }\n  }\n}\n```\n\n### 字段说明\n\n- `config`：实验的完整参数配置。\n- `system`：运行时主机的系统信息，包括 CPU、GPU、Python 版本、命令等。\n- `run_id`：实验的唯一标识符，通常与日志文件关联。\n\n---",
    "808": "一级标题：离线看板接口文档\n二级标题：接口 3：获取实验图表信息\n内容：\n- **URL**：`/api/v1/experiment/<experiment_id>/chart`\n- **方法**：`GET`\n- **示例**：`/api/v1/experiment/1/chart`\n- **接口说明**：获取指定实验的所有图表定义和元信息（如 loss 曲线、学习率曲线等）。\n\n### 响应示例\n\n```json\n{\n  \"code\": 0,\n  \"message\": \"success\",\n  \"data\": {\n    \"charts\": [\n      {\n        \"id\": 1,\n        \"name\": \"train/loss\",\n        \"type\": \"line\",\n        \"reference\": \"step\",\n        \"source\": [\"train/loss\"],\n        \"multi\": false\n      },\n      ...\n    ],\n    \"namespaces\": [\n      {\n        \"id\": 1,\n        \"name\": \"train\",\n        \"opened\": 1,\n        \"charts\": [1, 3, 5, 7]\n      }\n    ]\n  }\n}\n```\n\n### 字段说明\n\n- `charts`：图表定义列表，包括图表名称、类型、数据来源等。\n- `namespaces`：图表命名空间，用于分类展示。\n- `reference`：图表的 X 轴参考，5982 `step`（训练步数）。\n\n---",
    "809": "一级标题：离线看板接口文档\n二级标题：接口 4：获取指标数据\n内容：\n- **URL**：`/api/v1/experiment/<experiment_id>/tag/<namespace>/<metric_name>`\n- **方法**：`GET`\n- **示例**：`/api/v1/experiment/1/tag/train/loss`\n- **接口说明**：获取指定实验中某个具体指标的历史数据（如 loss、accuracy 等）。\n\n### 响应示例\n\n```json\n{\n  \"code\": 0,\n  \"message\": \"success\",\n  \"data\": {\n    \"sum\": 207,\n    \"max\": 1.7614,\n    \"min\": 0.8499,\n    \"experiment_id\": 1,\n    \"list\": [\n      {\n        \"index\": 1,\n        \"data\": 1.6858,\n        \"create_time\": \"2025-03-21T04:58:32.095272+00:00\"\n      },\n      ...,\n      {\n        \"index\": 207,\n        \"data\": 1.1845,\n        \"create_time\": \"2025-03-21T06:05:16.716693+00:00\",\n        \"_last\": true\n      }\n    ]\n  }\n}\n```\n\n### 字段说明\n\n- `sum`：数据总条数。\n- `max` / `min`：指标最大值与最小值。\n- `experiment_id`：所属实验 ID。\n- `list`：具体数据项，每项包括：\n  - `index`：数据点序号。\n  - `data`：具体数值。\n  - `create_time`：记录时间。\n  - `_last`：是否为最后一个数据点（仅最后一条为 true）。\n\n---",
    "810": "一级标题：离线看板接口文档\n二级标题：接口 5：获取实验最新日志\n内容：\n- **URL**：`/api/v1/experiment/<experiment_id>/recent_log`\n- **方法**：`GET`\n- **示例**：`/api/v1/experiment/1/recent_log`\n- **接口说明**：获取指定实验最新的日志输出。包括 Swanlab 自身日志信息和用户自定义的输出。\n\n### 响应示例\n\n```json\n{\n  \"code\": 0,\n  \"message\": \"success\",\n  \"data\": {\n    \"recent\": [\n      \"swanlab:\",\n      \"{'loss':\"\n    ],\n    \"logs\": [\n      \"swanlab: Tracking run with swanlab version 0.5.2\",\n      \"swanlab: Run data will be saved locally in /data/project/...\",\n      \"{'loss': 1.6858, 'grad_norm': ..., 'epoch': 0.02, ...}\",\n      \"...\"\n    ]\n  }\n}\n```\n\n### 字段说明\n\n- `recent`：最新日志段落，通常用于快速预览。\n- `logs`：日志输出列表，包含 swanlab 系统日志和运行中的配置、输出数据。\n\n---",
    "811": "一级标题：离线看板接口文档\n二级标题：接口 6：获取实验状态信息\n内容：\n- **URL**：`/api/v1/experiment/<experiment_id>/status`\n- **方法**：`GET`\n- **示例**：`/api/v1/experiment/1/status`\n- **接口说明**：获取指定实验的最新状态、更新时间、图表结构等信息。\n\n### 响应示例\n\n```json\n{\n  \"code\": 0,\n  \"message\": \"success\",\n  \"data\": {\n    \"status\": 0,\n    \"update_time\": \"2025-03-21T04:58:06.387487+00:00\",\n    \"finish_time\": null,\n    \"charts\": {\n      \"charts\": [\n        {\n          \"id\": 1,\n          \"name\": \"train/loss\",\n          \"type\": \"line\",\n          \"reference\": \"step\",\n          \"status\": 0,\n          \"source\": [\"train/loss\"],\n          \"multi\": false,\n          \"source_map\": {\"train/loss\": 1}\n        },\n        {\n          \"id\": 3,\n          \"name\": \"train/grad_norm\",\n          \"type\": \"line\",\n          \"reference\": \"step\",\n          \"status\": 0,\n          \"source\": [\"train/grad_norm\"],\n          \"multi\": false,\n          \"source_map\": {\"train/grad_norm\": 1}\n        },\n        {\n          \"id\": 5,\n          \"name\": \"train/learning_rate\",\n          \"type\": \"line\",\n          \"reference\": \"step\",\n          \"status\": 0,\n          \"source\": [\"train/learning_rate\"],\n          \"multi\": false,\n          \"source_map\": {\"train/learning_rate\": 1}\n        },\n        ...\n      ],\n      \"namespaces\": [\n        {\n          \"id\": 1,\n          \"name\": \"train\",\n          \"opened\": 1,\n          \"charts\": [1, 3, 5, 7, 9, 11]\n        }\n      ]\n    }\n  }\n}\n```\n\n### 字段说明\n\n- `status`：实验当前状态，整型（如 0 表示运行中）。\n- `update_time`：实验状态最近更新时间。\n- `finish_time`：实验完成时间，未完成为 `null`。\n- `charts`：实验中的图表结构信息。\n  - `charts`：图表定义数组，字段与 `/chart` 接口一致。\n  - `namespaces`：图表命名空间，标识图表分类与分组。\n\n---",
    "812": "一级标题：离线看板接口文档\n二级标题：接口 7：获取实验指标汇总\n内容：\n- **URL**：`/api/v1/experiment/<experiment_id>/summary`\n- **方法**：`GET`\n- **示例**：`/api/v1/experiment/1/summary`\n- **接口说明**：获取指定实验在当前状态下的各项关键指标的最新值汇总。\n\n### 响应示例\n\n```json\n{\n  \"code\": 0,\n  \"message\": \"success\",\n  \"data\": {\n    \"summaries\": [\n      { \"key\": \"train/loss\", \"value\": 1.1845 },\n      { \"key\": \"train/grad_norm\", \"value\": 1.0172306299209595 },\n      { \"key\": \"train/learning_rate\", \"value\": 0.000037463413651718303 },\n      { \"key\": \"train/epoch\", \"value\": 3.288 },\n      { \"key\": \"train/num_input_tokens_seen\", \"value\": 597776 },\n      { \"key\": \"train/global_step\", \"value\": 207 }\n    ]\n  }\n}\n```\n\n### 字段说明\n\n- `summaries`：包含多个指标的汇总值，每项包括：\n  - `key`：指标名称（如 `train/loss`）。\n  - `value`：该指标当前最新值。",
    "813": "一级标题：离线看板\n二级标题：无\n内容：\n:::warning 注意\n\n离线看板是SwanLab的历史功能，现阶段仅做简单维护，不再更新。\n\n如果您有私有化部署的需求，推荐使用[Docker版](/guide_cloud/self_host/docker-deploy)。\n\n:::\n\n离线看板是一种使用模式接近`tensorboard`的轻量级离线web看板。\n\nGithub：https://github.com/SwanHubX/SwanLab-Dashboard",
    "814": "一级标题：离线看板\n二级标题：安装\n内容：\n> 在swanlab>=0.5.0版本后，不再自带离线看板，需要使用dashboard扩展安装。\n\n使用离线看板，需要安装`swanlab`的`dashboard`扩展：\n\n```bash\npip install swanlab[dashboard]\n```",
    "815": "一级标题：离线看板\n二级标题：离线实验跟踪\n内容：\n在`swanlab.init`中设置`logdir`和`mode`这两个参数，即可离线跟踪实验：\n\n```python\n...\n\nswanlab.init(\n  logdir='./logs',\n  mode=\"local\",\n)\n\n...\n```\n\n- 参数`mode`设置为`local`，关闭将实验同步到云端\n- 参数`logdir`的设置是可选的，它的作用是指定了SwanLab日志文件的保存位置（默认保存在`swanlog`文件夹下）\n  - 日志文件会在跟踪实验的过程中被创建和更新，离线看板的启动也将基于这些日志文件\n\n其他部分和云端使用完全一致。",
    "816": "一级标题：离线看板\n二级标题：开启离线看板\n内容：\n打开终端，使用下面的指令，开启一个SwanLab仪表板:\n\n```bash\nswanlab watch ./logs\n```\n\n> 谐音：用swanlab看 ./logs 里的文件\n\n运行完成后，将启动一个后端服务，SwanLab会给你1个本地的URL链接（默认是http://127.0.0.1:5092）\n\n访问该链接，就可以在浏览器用离线看板查看实验了。\n\n[如何设置端口号和IP](/api/cli-swanlab-watch.md#设置ip和端口号)",
    "817": "一级标题：纯离线环境部署\n二级标题：无\n内容：\n> [!NOTE]\n>\n> 该教程适用于将 SwanLab 私有化部署在无法联网的服务器上。",
    "818": "一级标题：纯离线环境部署\n二级标题：部署流程\n内容：\n### 1. 下载镜像\n\n由于私有化版 [SwanLab](https://github.com/SwanHubX/self-hosted) 基于 Docker 部署，因此我们需要先在一台联网的机器上提前下载好所有镜像。\n\n> [!NOTE]\n>\n> 注意需要在相同 CPU 架构的服务器上下载镜像。比如你的服务器为 AMD64 架构，那么也需要在 AMD64 架构的服务器上拉取镜像，不能在 MacBook 这类采用 ARM64 架构的电脑上下载镜像。\n\n找到一台联网的电脑，确保其安装有 [Docker](https://docs.docker.com/engine/install/)，然后执行 [pull-images.sh](https://github.com/SwanHubX/self-hosted/blob/main/scripts/pull-images.sh) 脚本下载镜像包。执行完成后会得到一个 `swanlab_images.tar` 的压缩包。\n\n::: details pull-images.sh 脚本详情\n\n```shell\n#!/bin/bash\n\n# 定义要下载的镜像列表\nimages=(\n  \"ccr.ccs.tencentyun.com/self-hosted/traefik:v3.0\"\n  \"ccr.ccs.tencentyun.com/self-hosted/postgres:16.1\"\n  \"ccr.ccs.tencentyun.com/self-hosted/redis-stack-server:7.2.0-v15\"\n  \"ccr.ccs.tencentyun.com/self-hosted/clickhouse:24.3\"\n  \"ccr.ccs.tencentyun.com/self-hosted/logrotate:v1\"\n  \"ccr.ccs.tencentyun.com/self-hosted/fluent-bit:3.0\"\n  \"ccr.ccs.tencentyun.com/self-hosted/minio:RELEASE.2025-02-28T09-55-16Z\"\n  \"ccr.ccs.tencentyun.com/self-hosted/minio-mc:RELEASE.2025-04-08T15-39-49Z\"\n  \"ccr.ccs.tencentyun.com/self-hosted/swanlab-server:v1.1.1\"\n  \"ccr.ccs.tencentyun.com/self-hosted/swanlab-house:v1.1\"\n  \"ccr.ccs.tencentyun.com/self-hosted/swanlab-cloud:v1.1\"\n  \"ccr.ccs.tencentyun.com/self-hosted/swanlab-next:v1.1\"\n)\n\n# 下载镜像\nfor image in \"${images[@]}\"; do\n  docker pull \"$image\"\ndone\n\n# 保存镜像到文件\necho \"正在打包所有镜像到 swanlab_images.tar...\"\ndocker save -o ./swanlab_images.tar \"${images[@]}\"\n\necho \"所有镜像都打包至 swanlab_images.tar，可直接上传该文件到目标服务器!\"\n```\n\n:::\n\n###  2. 上传镜像到目标服务器\n\n可以使用 [sftp](https://www.ssh.com/academy/ssh/sftp-ssh-file-transfer-protocol) 等命令。例如：\n\n- 首先连接到服务器\n\n```bash\n$ sftp username@remote_host\n```\n\n- 上传文件\n\n```sftp\n> put swanlab_images.tar swanlab_images.tar\n```\n\n> [!TIP]\n>\n> 借助 [Termius](https://termius.com/) 这类 SSH 工具可以更方便地向服务器上传下载文件\n\n### 3. 加载镜像\n\n> [!NOTE]\n>\n> 需求确保服务器上安装有 [Docker](https://docs.docker.com/engine/install/)\n\n将镜像上传到目标服务器之后，需要加载镜像，命令如下：\n\n```bash\n$ docker load -i swanlab_images.tar\n```\n\n等待加载成功后，可以通过命令 `docker images` 查看镜像列表。\n\n```bash\n(base) root@swanlab:~# docker images\nREPOSITORY                                              TAG                            IMAGE ID       CREATED         SIZE\nccr.ccs.tencentyun.com/self-hosted/swanlab-server       v1.1.1                         a2b992161a68   8 days ago      1.46GB\nccr.ccs.tencentyun.com/self-hosted/swanlab-next         v1.1                           7a33e5b1afc5   3 weeks ago     265MB\nccr.ccs.tencentyun.com/self-hosted/swanlab-cloud        v1.1                           0bc15f138d79   3 weeks ago     53.3MB\nccr.ccs.tencentyun.com/self-hosted/swanlab-house        v1.1                           007b252f5b6c   3 weeks ago     48.5MB\nccr.ccs.tencentyun.com/self-hosted/minio-mc             RELEASE.2025-04-08T15-39-49Z   f33e36a42eec   5 weeks ago     84.1MB\nccr.ccs.tencentyun.com/self-hosted/clickhouse           24.3                           6ffc1e932ef1   2 months ago    942MB\nccr.ccs.tencentyun.com/self-hosted/fluent-bit           3.0                            97e65b999a4d   2 months ago    84.9MB\nccr.ccs.tencentyun.com/self-hosted/traefik              v3.0                           0f62db80c71d   2 months ago    190MB\nccr.ccs.tencentyun.com/self-hosted/minio                RELEASE.2025-02-28T09-55-16Z   377fe6127f60   2 months ago    180MB\nccr.ccs.tencentyun.com/self-hosted/redis-stack-server   7.2.0-v15                      110cc99f3057   3 months ago    520MB\nccr.ccs.tencentyun.com/self-hosted/postgres             16.1                           86414087c100   16 months ago   425MB\nccr.ccs.tencentyun.com/self-hosted/logrotate            v1                             e07b32a4bfda   6 years ago     45.6MB\n```\n\n### 4. 安装 SwanLab 服务\n\n在完成镜像载入之后，需要使用安装脚本完成服务安装并启动。\n\n首先在一台有网络的计算机上，使用 Git 克隆仓库到本地目录：\n\n```bash\n$ git clone https://github.com/SwanHubX/self-hosted.git\n```\n\n然后，将 `self-hosted` 文件夹上传到目标服务器。\n\n---\n\n在目标服务器，进入 `self-hosted` 目录，执行脚本 `./docker/install.sh` 用于安装，安装成功会看到以下标志：\n\n```bash\n$ ./docker/install.sh\n\n...\n   _____                    _           _\n  / ____|                  | |         | |\n | (_____      ____ _ _ __ | |     __ _| |__\n  \\___ \\ \\ /\\ / / _` | '_ \\| |    / _` | '_ \\\n  ____) \\ V  V / (_| | | | | |___| (_| | |_) |\n |_____/ \\_/\\_/ \\__,_|_| |_|______\\__,_|_.__/\n\n Self-Hosted Docker v1.1 - @SwanLab\n\n🎉 Wow, the installation is complete. Everything is perfect.\n🥰 Congratulations, self-hosted SwanLab can be accessed using {IP}:8000\n```\n\n> [!TIP]\n>\n> 默认脚本使用的镜像源在中国，所以中国地区不需要担心网络问题\n>\n> 如果你需要使用 [DockerHub](https://hub.docker.com/) 作为镜像源，可以使用下面的脚本进行安装：\n>\n> ```bash\n> $ ./docker/install-dockerhub.sh\n> ```\n\n脚本执行成功后，将会在当前目录下创建一个 `swanlab/` 目录，并在目录下生成两个文件：\n\n- `docker-compose.yaml`：用于 Docker Compose 的配置文件\n- `.env`：对应的密钥文件，保存数据库对应的初始化密码\n\n在 `swanlab` 目录下执行 `docker compose ps -a` 可以查看所有容器的运行状态：\n\n```bash\n$ docker compose ps -a\nNAME                 IMAGE                                                                   COMMAND                  SERVICE          CREATED          STATUS                    PORTS\nswanlab-clickhouse   ccr.ccs.tencentyun.com/self-hosted/clickhouse:24.3                      \"/entrypoint.sh\"         clickhouse       22 minutes ago   Up 22 minutes (healthy)   8123/tcp, 9000/tcp, 9009/tcp\nswanlab-cloud        ccr.ccs.tencentyun.com/self-hosted/swanlab-cloud:v1                     \"/docker-entrypoint.…\"   swanlab-cloud    22 minutes ago   Up 21 minutes             80/tcp\nswanlab-fluentbit    ccr.ccs.tencentyun.com/self-hosted/fluent-bit:3.0                       \"/fluent-bit/bin/flu…\"   fluent-bit       22 minutes ago   Up 22 minutes             2020/tcp\nswanlab-house        ccr.ccs.tencentyun.com/self-hosted/swanlab-house:v1                     \"./app\"                  swanlab-house    22 minutes ago   Up 21 minutes (healthy)   3000/tcp\nswanlab-logrotate    ccr.ccs.tencentyun.com/self-hosted/logrotate:v1                         \"/sbin/tini -- /usr/…\"   logrotate        22 minutes ago   Up 22 minutes\nswanlab-minio        ccr.ccs.tencentyun.com/self-hosted/minio:RELEASE.2025-02-28T09-55-16Z   \"/usr/bin/docker-ent…\"   minio            22 minutes ago   Up 22 minutes (healthy)   9000/tcp\nswanlab-next         ccr.ccs.tencentyun.com/self-hosted/swanlab-next:v1                      \"docker-entrypoint.s…\"   swanlab-next     22 minutes ago   Up 21 minutes             3000/tcp\nswanlab-postgres     ccr.ccs.tencentyun.com/self-hosted/postgres:16.1                        \"docker-entrypoint.s…\"   postgres         22 minutes ago   Up 22 minutes (healthy)   5432/tcp\nswanlab-redis        ccr.ccs.tencentyun.com/self-hosted/redis-stack-server:7.2.0-v15         \"/entrypoint.sh\"         redis            22 minutes ago   Up 22 minutes (healthy)   6379/tcp\nswanlab-server       ccr.ccs.tencentyun.com/self-hosted/swanlab-server:v1                    \"docker-entrypoint.s…\"   swanlab-server   22 minutes ago   Up 21 minutes (healthy)   3000/tcp\nswanlab-traefik      ccr.ccs.tencentyun.com/self-hosted/traefik:v3.0                         \"/entrypoint.sh trae…\"   traefik          22 minutes ago   Up 22 minutes (healthy)   0.0.0.0:8000->80/tcp, [::]:8000->80/tcp\n```\n\n通过执行 `docker compose logs <container_name>` 可以查看每个容器的日志。\n\n### 5. 访问 SwanLab\n\n安装成功后，可以通过 `http://localhost:8000` （默认端口为8000）直接打开网站。第一次打开需要激活主账户，流程见[文档](https://docs.swanlab.cn/guide_cloud/self_host/docker-deploy.html#_3-%E6%BF%80%E6%B4%BB%E4%B8%BB%E8%B4%A6%E5%8F%B7)。\n\n### 6. 升级 SwanLab\n\n如果你希望升级私有化部署版，那么回到联网的机器上，同步github上最新的`self-hosted`仓库，然后执行升级脚本：\n\n```bash\n$ ./docker/upgrade.sh\n```\n\n将升级后的镜像导出到目标服务器，载入镜像以覆盖之前的镜像。\n\n同时，将新同步的`self-hosted` 文件夹也上传到目标服务器（⚠️注意：不要覆盖存储原先私有化部署数据的文件夹）。\n\n然后在离线机器上，进入`self-hosted`目录，执行`./docker/upgrade.sh`进行升级。\n\n```bash\ncd self-hosted\n./docker/upgrade.sh\n```\n\n脚本运行完成后即完成升级。",
    "819": "一级标题：远程访问离线看板\n二级标题：无\n内容：\n`swanlab watch`命令让离线访问实验变得非常简单，而在机器学习训练中，使用远程服务器的情况是十分常见的。\n\n本节将教您：\n\n- 如何设定实验看板的IP和端口。\n- 如何在本机访问实验看板",
    "820": "一级标题：远程访问离线看板\n二级标题：准备工作\n内容：\n- `记下远程端IP`：比如你使用的是云服务器，那么它自带的公网IP（形如8.141.192.68）就是你之后本机访问实验看板的IP；如果你使用的是局域网服务器，那么则记下它的局域网IP。\n- `开放端口`：首先需要检查一下远程端的安全组/防火墙，比如你希望实验看板所用的端口为`5092`，那么需要检查服务器是否开放了该端口。\n\n> 可使用telnet <服务器IP> <端口号>命令查看linux服务器端口是否开放",
    "821": "一级标题：远程访问离线看板\n二级标题：在远程端设定实验看板的IP与端口\n内容：\n我们需要在远程端（跑训练所在的机器）运行实验看板服务。\n\n在swanlab watch命令中，可设置的参数主要有`-p`和`-h`：\n\n| API         | 描述                                     | 例子                                             |\n|-------------|------------------------------------------|--------------------------------------------------|\n| `-p, --port`| 设置实验看板Web服务运行的端口，默认为5092。 | `swanlab watch -p 8080`：将实验看板Web服务设置为8080端口 |\n| `-h, --host`| 设置实验看板Web服务运行的IP地址，默认为127.0.0.1。 | `swanlab watch -h 0.0.0.0`：将实验看板Web服务的IP地址设置为0.0.0.0 |\n\n\n一般远程访问实验看板需要将`-h`设置为`0.0.0.0`，`-p`的设置则根据你的需求。这里我们将端口设置为`8080`：\n\n```shell\nswanlab watch -h 0.0.0.0 -p 8080\n```\n\n运行上面的命令，得到：\n\n![image](/assets/self-host_im.jpg)",
    "822": "一级标题：远程访问离线看板\n二级标题：本机访问实验看板\n内容：\n这时我们在本机端打开浏览器，访问`远程端IP地址:端口号`。\n\n比如我的远程服务器的公网IP是`8.146.xxx.71`，端口号设置为`8080`，那么在浏览器就访问`8.146.xxx.71:8080`。",
    "823": "一级标题：腾讯云应用部署\n二级标题：无\n内容：\n:::warning 关于第三方部署\n\n第三方部署是由社区贡献的部署方式，官方不保证能实时同步最新版本。\n\n:::\n\n目前 SwanLab 自托管版本已上线腾讯云应用市场，欢迎各位训练师通过腾讯云开箱使用~\n\n![](./tencentcloud-app/head.png)\n\n- [SwanLab 腾讯云应用](https://app.cloud.tencent.com/detail/SPU_BHEEJEJCDD1984)",
    "824": "一级标题：腾讯云应用部署\n二级标题：先决条件\n内容：\n1. 首先需要一个腾讯云账号，并确保账号拥有 **安装云应用的权限**，参考链接：[腾讯云应用购买安装指引](https://cloud.tencent.com/document/product/1689/113848)\n\n2. 在 [腾讯云控制台-私有网络](https://console.cloud.tencent.com/vpc/vpc) 中，创建一个默认的 `VPC`（Vitual Private Cloud， 虚拟私有云），为云应用提供目标网络，\n目前支持的地域如下：\n    - 境内：南京; 北京; 广州; 成都; 上海; 重庆; 成都\n    - 境外：中国香港; 新加坡; 硅谷; 圣保罗; 法兰克福\n\n<img src=\"./tencentcloud-app/setup-vpc.png\" width=\"600\"/>\n\n以`南京`区域为例，CIDR与子网可以按需修改，必填项只有`名称`、`子网名称`与`可用区`\n\n<img src=\"./tencentcloud-app/setup-vpc-option.png\" width=\"600\"/>",
    "825": "一级标题：腾讯云应用部署\n二级标题：安装教程\n内容：\n1. 进入 [SwanLab 腾讯云应用](https://app.cloud.tencent.com/detail/SPU_BHEEJEJCDD1984) 页面，\n勾选 `我已阅读并同意《腾讯云云应用通用商品用户协议》`，并点击 `安装应用`，跳转到控制台界面\n\n<img src=\"./tencentcloud-app/intro.png\" width=\"800\"/>\n\n2. 在控制台界面，只需要配置 `目标网络`、`云服务器类型` 以及 `数据盘大小` 三项云资源设置：\n<img src=\"./tencentcloud-app/resource-option.png\" width=\"800\"/>\n\n各云资源代表的含义如下：\n\n| 配置项 | 说明 | 配置要求 |\n| ---- | ---- | ---- |\n| 目标网络 | 云服务托管地域 | 可以根据之前创建 `VPC` 的地域进行选择 |\n| 云服务器类型 | 云服务器实例配置 | 最低配置：<br>- CPU: ≥ 4 核<br>- 内存：≥ 8GB<br>- 系统存储空间：默认 40GB |\n| 数据盘大小 | 记录实验数据的硬盘大小 | 默认为 `100GB`，最低 `40GB` |\n\n云资源配置完成之后，点击 `下一步：确定资源`\n\n<img src=\"./tencentcloud-app/resource-confirm.png\" width=\"800\"/>\n\n3. 接着进入`确认订单信息`信息界面，腾讯云会根据上一步选用的云资源整理账单费用，此时需要确保腾讯云账号中有一定的余额。确认订单无误后，点击`允许服务角色调用其他云服务接口`，并点击 `下一步：安装应用`\n\n\n<img src=\"./tencentcloud-app/resource-setupapp.png\" width=\"800\"/>\n\n4. 接下来进入应用安装界面，需要等待所有资源创建并启动，需要等待 5 分钟左右\n\n<img src=\"./tencentcloud-app/app-setup.png\" width=\"600\"/>\n\n5. 完成之后，即可在腾讯云控制台界面看到已创建完成的云应用，点击 `打开应用`，即可使用自托管版的SwanLab\n\n<img src=\"./tencentcloud-app/open-app.png\" width=\"800\"/>\n\n\n:::info 提示\n\n在应用创建完成后，如果立即打开应用，可能会看到 404 页面，这是因为云服务器实例创建后需要执行一些容器初始化操作，稍等 1~2 分钟再打开即可。\n\n:::",
    "826": "一级标题：腾讯云应用部署\n二级标题：激活主账号\n内容：\n现在，你可以在腾讯云上使用自托管版本的 SwanLab\n\n<img src=\"./tencentcloud-app/swanlab-hello.png\" width=\"600\"/>\n\n个人使用可以免费在 [SwanLab官网](https://swanlab.cn) 申请一个License，位置在 「设置」-「账户与许可证」。\n\n<img src=\"./tencentcloud-app/swanlab-license-1.png\" width=\"600\"/>\n\n<img src=\"./tencentcloud-app/swanlab-license-2.png\" width=\"600\"/>\n\n输入账号、密码、License 后即可激活自托管版的 SwanLab\n\n<img src=\"./tencentcloud-app/swanlab-main.png\" width=\"600\"/>",
    "827": "一级标题：腾讯云应用部署\n二级标题：启动实验\n内容：\n在Python SDK完成登录：\n\n```bash\nswanlab login --host <IP地址>\n```\n\n> 如果你之前登录过swanlab，想要重新登录，请使用：\n> `swanlab login --host <IP地址> --relogin`。\n\n按回车，填写API Key，完成登录。之后你的SwanLab实验将会默认传到私有化部署的SwanLab上。\n\n---\n\n测试脚本：\n\n```bash\nimport swanlab\nimport random\n\n# 创建一个SwanLab项目\nswanlab.init(\n    # 设置项目名\n    project=\"my-awesome-project\",\n\n    # 设置超参数\n    config={\n        \"learning_rate\": 0.02,\n        \"architecture\": \"CNN\",\n        \"dataset\": \"CIFAR-100\",\n        \"epochs\": 10\n    }\n)\n\n# 模拟一次训练\nepochs = 10\noffset = random.random() / 5\nfor epoch in range(2, epochs):\n  acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n  loss = 2 ** -epoch + random.random() / epoch + offset\n\n  # 记录训练指标\n  swanlab.log({\"acc\": acc, \"loss\": loss})\n\n# [可选] 完成训练，这在notebook环境中是必要的\nswanlab.finish()\n```\n\n运行后，可在网页查看实验\n\n<img src=\"./tencentcloud-app/swanlab-dashboard.png\" width=\"600\"/>\n\n\n:::info 提示\n\n如您不再需要使用，请及时在 [腾讯云应用控制台](https://console.cloud.tencent.com/app) 中销毁应用，避免继续计费。\n\n:::",
    "828": "一级标题：版本对照表\n二级标题：无\n内容：\n| 版本 | 发布时间 | 主要更新 | 兼容的Python包版本 |\n| --- | --- | --- | --- |\n| v1.3 | 2025-07-08 | 同步发布时间的云端版更新 |  All |\n| v1.2 | 25-05-30 | 同步发布时间的云端版更新 |  <=0.6.4 |\n| v1.1 | 25-04-27 | 新增License离线验证功能；<br> 同步发布时间的云端版更新 |  <=0.6.4 |\n| v1.0 | 25-03-12 | 初始版本 |  <=0.5.5 |\n\n[升级版本](/guide_cloud/self_host/docker-deploy.html#升级版本)",
    "829": "一级标题：制作你的自定义插件\n二级标题：无\n内容：\n很开心，在`swanlab>=0.5.0`之后，我们正式开启了插件时代！\n\n插件是SwanLab诞生之初我们便一直探讨的话题，这不仅是增强SwanLab的功能与开放性，更是一种全新的视角来看待SwanLab ——\n\nSwanLab不只是1个训练跟踪工具与实验管理平台，同时可以是一个训练过程中的**数据核心**（比如Chrome core），`swanlab.init`与`swanlab.log`被赋予不同的意义。\n\n---\n\n我们将SwanLab的插件模式定义为三种类型：\n\n- **`Python库插件`**：SwanLab Python库中的回调类（Callback）。通过往SwanLab的生命周期阶段（比如`on_init`、`on_run`、`on_stop`等）注入代码的方式，来实现插件功能。\n- **`开放API插件`**：基于SwanLab平台提供的开放API，通过调用API进行组合的方式，来实现插件功能。\n- **`GUI插件`**：基于SwanLab平台开放的前端API，实现对图表、表格等组件的定制化。\n\n::: warning 👋 支持情况\n目前我们支持的插件类型为`Python库插件`，下面我将重点介绍如何制作你的`Python库插件`。\n:::",
    "830": "一级标题：制作你的自定义插件\n二级标题：认识SwanKitCallback类\n内容：\n> 仓库：[swanlab-toolkit](https://github.com/swanhubx/swanlab-toolkit)\n\n`SwanKitCallback`类是SwanLab的回调类，所有插件都必须继承自该类。\n\n```python\nfrom swankit.callback import SwanKitCallback\n```\n\n`SwanKitCallback`类中定义了所有SwanLab的生命周期阶段，你只需要重写你感兴趣的生命周期阶段即可：\n\n常用的生命周期阶段有：\n\n- `on_init`：初始化阶段，执行`swanlab.init`时调用\n- `before_init_experiment`：在初始化`SwanLabRun`之前调用\n- `on_run`：当`SwanLabRun`初始化完毕时调用\n- `on_log`：每次执行`swanlab.log`时调用\n- `on_stop`：停止阶段，当SwanLab停止时调用\n\n更多的生命周期阶段，请参考：[SwanKitCallback](https://github.com/SwanHubX/SwanLab-Toolkit/blob/main/swankit/callback/__init__.py)",
    "831": "一级标题：制作你的自定义插件\n二级标题：实现一个简单的插件\n内容：\n下面以1个案例为例，介绍如何实现一个插件。\n\n```python\nclass MyPlugin(SwanKitCallback):\n    def on_init(self, proj_name: str, workspace: str, logdir: str = None, *args, **kwargs):\n        print(f\"插件初始化: {proj_name} {workspace} {logdir}\")\n\n    def on_stop(self, error: str = None, *args, **kwargs):\n        print(f\"插件停止: {error}\")\n\n    def __str__(self):\n        return \"MyPlugin\"\n```\n\n这个插件实现的功能非常简单，就是在`swanlab.init()`调用时打印1条消息，在进程停止或`swanlab.finish()`调用时打印1条消息。\n\n而在SwanLab中使用着这个插件非常简单，只需要在`swanlab.init()`的`callbacks`参数中传入插件实例即可。\n\n```python {14,16}\nfrom swankit.callback import SwanKitCallback\nimport swanlab\n\nclass MyPlugin(SwanKitCallback):\n    def on_init(self, proj_name: str, workspace: str, logdir: str = None, *args, **kwargs):\n        print(f\"插件初始化: {proj_name} {workspace} {logdir}\")\n\n    def on_stop(self, error: str = None, *args, **kwargs):\n        print(f\"插件停止: {error}\")\n\n    def __str__(self):\n        return \"MyPlugin\"\n\nmy_plugin = MyPlugin()\n\nswanlab.init(callbacks=[my_plugin])\n```\n\n执行上述代码，你会在控制台看到\n\n![image](./custom-plugin/print.png)",
    "832": "一级标题：制作你的自定义插件\n二级标题：案例：指标打印与告警\n内容：\n我们来实现一个插件，这个插件的功能是打印指标，并当指标`acc`大于0.9时，打印1条消息，并发送告警。\n\n### 1. 定义插件\n\n> 在`SwanKitCallback`类中，定义了`on_log`方法，每次执行`swanlab.log`时都会调用该方法。\n\n```python\nclass ThresholdPlugin(SwanKitCallback):\n    def __init__(self, key: str, threshold: float = 0.9):\n        self.key = key\n        self.threshold = threshold\n\n    def on_log(self, data: dict, step: Optional[int] = None, *args, **kwargs):\n        print(f\"data: {data} step: {step}\")\n        if data[self.key] > self.threshold:\n            print(f\"{self.key} > {self.threshold} !!\")\n```\n\n### 2. 使用插件\n\n```python\nfrom swankit.callback import SwanKitCallback\nfrom typing import Optional\nimport swanlab\nimport random\n\nclass ThresholdPlugin(SwanKitCallback):\n    def __init__(self, key: str, threshold: float = 0.9):\n        self.key = key\n        self.threshold = threshold\n\n    def on_log(self, data: dict, step: Optional[int] = None, *args, **kwargs):\n        print(f\"data: {data} step: {step}\")\n        if data[self.key] > self.threshold:\n            print(f\"{self.key} > {self.threshold} !!\")\n\n    def __str__(self):\n        return \"ThresholdPlugin\"\n\nthreshold_plugin = ThresholdPlugin(key=\"acc\", threshold=0.9)\nswanlab.init(callbacks=[threshold_plugin])\n\n# 模拟一次训练\nepochs = 10\noffset = random.random() / 5\nfor epoch in range(2, epochs):\n  acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n  loss = 2 ** -epoch + random.random() / epoch + offset\n\n  # 记录训练指标\n  swanlab.log({\"acc\": acc, \"loss\": loss}, step=epoch)\n```\n\n执行上述代码，你会在控制台看到\n\n![image](./custom-plugin/threshold.png)",
    "833": "一级标题：制作你的自定义插件\n二级标题：学习更多插件\n内容：\n- [EmailCallback](/zh/plugin/notification-email.md)：训练完成/发生错误时，发送消息到邮箱\n- [LarkCallback](/zh/plugin/notification-lark.md)：训练完成/发生错误时，发送消息到飞书",
    "834": "一级标题：钉钉\n二级标题：无\n内容：\n![image](./notification-dingtalk/logo.jpg)\n\n如果你希望在训练完成/发生错误时，第一时间发送[钉钉](https://www.dingtalk.com/)信息通知你，那么非常推荐你使用钉钉通知插件。\n\n:::warning 改进插件\nSwanLab插件均为开源代码，你可以在[Github源代码](https://github.com/swanhubx/swanlab/blob/main/swanlab/plugin/notification.py)中查看，欢迎提交你的建议和PR！\n:::\n\n[[toc]]",
    "835": "一级标题：钉钉\n二级标题：准备工作\n内容：\n1. 在1个钉钉群（企业群）中，点击右上角的 **「设置」按钮**\n\n<img src=\"./notification-dingtalk/setting.png\" width=\"400\"/>\n\n2. 向下滚动，找到 **「机器人」**\n\n<img src=\"./notification-dingtalk/group-robot.png\" width=\"400\"/>\n\n3. 点击 **「添加机器人」**\n\n<img src=\"./notification-dingtalk/add-robot.png\" width=\"400\"/>\n\n4. 添加 **「自定义机器人」**\n\n<img src=\"./notification-dingtalk/custom-robot.png\" width=\"600\"/>\n\n<img src=\"./notification-dingtalk/add-robot-2.png\" width=\"600\"/>\n\n勾选「加签」，复制token到外部。\n\n<img src=\"./notification-dingtalk/add-robot-3.png\" width=\"600\"/>\n\n复制webhook，完成机器人创建：\n\n<img src=\"./notification-dingtalk/add-robot-4.png\" width=\"600\"/>\n\n至此，你完成了准备工作。",
    "836": "一级标题：钉钉\n二级标题：基本用法\n内容：\n使用钉钉通知插件的方法非常简单，只需要初始化1个`DingTalkCallback`对象：\n\n```python\nfrom swanlab.plugin.notification import DingTalkCallback\n\ndingtalk_callback = DingTalkCallback(\n    webhook_url=\"https://oapi.dingtalk.com/robot/xxxx\",\n    secret=\"xxxx\",\n)\n```\n\n然后将`dingtalk_callback`对象传入`swanlab.init`的`callbacks`参数中：\n\n```python\nswanlab.init(callbacks=[dingtalk_callback])\n```\n\n这样，当训练完成/发生错误时（触发`swanlab.finish()`），你将会收到钉钉信息通知。\n\n<img src=\"./notification-dingtalk/show.png\" width=\"600\"/>",
    "837": "一级标题：钉钉\n二级标题：自由提醒\n内容：\n你还可以使用`DingTalkCallback`对象的`send_msg`方法，发送自定义的钉钉信息。\n\n这在提醒你某些指标达到某个阈值时非常有用！\n\n```python\nif accuracy > 0.95:\n    # 自定义场景发送消息\n    dingtalk_callback.send_msg(\n        content=f\"Current Accuracy: {accuracy}\",  # 通知内容\n    )\n```",
    "838": "一级标题：钉钉\n二级标题：外部注册插件\n内容：\n<!--@include: ./shared-snippet.md-->",
    "839": "一级标题：钉钉\n二级标题：限制\n内容：\n- 钉钉通知插件的训练完成/异常通知，使用的是`SwanKitCallback`的`on_stop`生命周期回调，所以如果你的进程被突然`kill`，或者训练机异常关机，那么会因为无法触发`on_stop`回调，从而导致未发送钉钉通知。\n\n- 完善方案请期待`SwanLab`的`平台开放API`上线。",
    "840": "一级标题：Discord\n二级标题：无\n内容：\n如果你希望在训练完成/发生错误时，第一时间发送[Discord](https://discord.com/)信息通知你，那么非常推荐你使用Discord通知插件。\n\n![](./notification-discord/logo.jpg)\n\n:::warning 改进插件\nSwanLab插件均为开源代码，你可以在[Github源代码](https://github.com/swanhubx/swanlab/blob/main/swanlab/plugin/notification.py)中查看，欢迎提交你的建议和PR！\n:::\n\n[[toc]]",
    "841": "一级标题：Discord\n二级标题：准备工作\n内容：\n::: info 参考文档\n- [Discord-Webhook群机器人配置说明](https://support.discord.com/hc/en-us/articles/228383668-Intro-to-Webhooks)\n:::\n\n\n1. 选择您想要接收SwanLab事件通知的 Discord 频道\n\n\n2. 点击对应频道右侧的 **「⚙️」** 对应的 **「编辑频道」** 按钮\n\n<img src=\"./notification-discord/edit-channel.png\" width=\"400\"/>\n\n3. 展开菜单后，选择 **「整合」 -> 「Webhhook」**\n\n<img src=\"./notification-discord/integration-webhook.png\" width=\"400\"/>\n\n\n4. 点击选项卡 **「新Webhook」** 自动创建新的 webhook 机器人\n\n<img src=\"./notification-discord/new-webhook.png\" width=\"400\"/>\n\n5. 点击 **「复制 Webhook URL」** 即可获取到对应的 webhook 地址",
    "842": "一级标题：Discord\n二级标题：基本用法\n内容：\n使用Discord通知插件的方法非常简单，只需要初始化1个`DiscordCallback`对象：\n\n```python\nfrom swanlab.plugin.notification import DiscordCallback\n\ndiscord_callback = DiscordCallback(\n    webhook_url='https://discord.com/api/webhooks/xxxxx/xxx',\n    language='zh'\n)\n```\n\n然后将`discord_callback`对象传入`swanlab.init`的`callbacks`参数中：\n\n```python\nswanlab.init(callbacks=[discord_callback])\n```\n\n这样，当训练完成/发生错误时（触发`swanlab.finish()`），你将会收到Discord消息通知。\n\n\n\n<img src=\"./notification-discord/discord-finish.png\" width=\"500\"/>",
    "843": "一级标题：Discord\n二级标题：自由提醒\n内容：\n你还可以使用`DiscordCallback`对象的`send_msg`方法，发送自定义的的Discord消息。\n\n这在提醒你某些指标达到某个阈值时非常有用！\n\n```python\nif accuracy > 0.95:\n    # 自定义场景发送消息\n    discord_callback.send_msg(\n        content=f\"Current Accuracy: {accuracy}\",  # 通知内容\n    )\n```",
    "844": "一级标题：Discord\n二级标题：限制\n内容：\n- Discord通知插件的训练完成/异常通知，使用的是`SwanKitCallback`的`on_stop`生命周期回调，所以如果你的进程被突然`kill`，或者训练机异常关机，那么会因为无法触发`on_stop`回调，从而导致未发送Discord通知。\n\n- 完善方案请期待`SwanLab`的`平台开放API`上线。",
    "845": "一级标题：邮件通知\n二级标题：无\n内容：\n![image](./notification-email/logo.jpg)\n\n如果你希望在训练完成/发生错误时，第一时间发送邮件通知你，那么非常推荐你使用`邮件通知`插件。\n\n:::warning 改进插件\nSwanLab插件均为开源代码，你可以在[Github源代码](https://github.com/swanhubx/swanlab/blob/main/swanlab/plugin/notification.py)中查看，欢迎提交你的建议和PR！\n:::\n\n[[toc]]",
    "846": "一级标题：邮件通知\n二级标题：准备工作\n内容：\n在使用插件前，首先你需要准备开通你的邮箱的**STMP服务**。以QQ邮箱为例：\n\n**步骤 1：进入邮箱设置**\n\n- 进入QQ邮箱网页，点击顶部的 ​​“设置”​\n- 在设置菜单中，选择 ​​“账号”​ 选项。\n\n**​步骤 2：开启SMTP服务**\n\n- 找到 **“POP3/IMAP/SMTP/Exchange/CardDAV/CalDAV服务”**\n- 在“服务状态”旁边，点击 **“开启服务”**\n- 经过一些身份验证流程后，完成**STMP服务的开启**\n- （重要）保存给到你的**授权码**\n\n**​步骤 3：记录以下信息**\n- **SMTP服务器地址**: smtp.qq.com\n- **端口**: 465（SSL加密）或 587（TLS加密）\n- **发送邮箱**: 你的完整QQ邮箱地址（如 123456789@qq.com）\n- **密码**: 使用你刚刚获取的 ​授权码，而不是QQ邮箱的登录密码。\n\n其他的邮箱服务基本都支持STMP，可按照相似的流程开启服务。",
    "847": "一级标题：邮件通知\n二级标题：基本用法\n内容：\n使用邮件通知插件的方法非常简单，只需要初始化1个`EmailCallback`对象：\n\n```python\nfrom swanlab.plugin.notification import EmailCallback\n\n# 初始化邮件通知插件\nemail_callback = EmailCallback(\n    sender_email=\"<发送者邮箱，即开启SMTP服务的邮箱>\",\n    receiver_email=\"<接收者邮箱，即你想要收到邮件的邮件>\",\n    password=\"<你的授权码>\",\n    smtp_server=\"<你的邮箱服务器>\",\n    port=587,\n    language=\"zh\",\n)\n```\n\n然后将`email_callback`对象传入`swanlab.init`的`callbacks`参数中：\n\n```python\nswanlab.init(callbacks=[email_callback])\n```\n\n这样，当训练完成/发生错误时（触发`swanlab.finish()`），你将会收到邮件通知。\n\n\n![image](./notification-email/email.png)",
    "848": "一级标题：邮件通知\n二级标题：自由提醒\n内容：\n你还可以使用`EmailCallback`对象的`send_email`方法，发送自定义的邮件。\n\n这在提醒你某些指标达到某个阈值时非常有用！\n\n```python\nif accuracy > 0.95:\n    # 自定义场景发送邮件\n    email_callback.send_email(\n        subject=\"SwanLab | Accuracy > 0.95\",  # 邮件标题\n        content=f\"Current Accuracy: {accuracy}\",  # 邮件内容\n    )\n```",
    "849": "一级标题：邮件通知\n二级标题：外部注册插件\n内容：\n<!--@include: ./shared-snippet.md-->",
    "850": "一级标题：邮件通知\n二级标题：限制\n内容：\n- 邮件通知插件的训练完成/异常通知，使用的是`SwanKitCallback`的`on_stop`生命周期回调，所以如果你的进程被突然`kill`，或者训练机异常关机，那么会因为无法触发`on_stop`回调，从而导致未发送邮件通知。\n\n- 完善方案请期待`SwanLab`的`平台开放API`上线。",
    "851": "一级标题：飞书通知\n二级标题：无\n内容：\n![image](./notification-lark/logo.jpg)\n\n如果你希望在训练完成/发生错误时，第一时间发送飞书信息通知你，那么非常推荐你使用飞书通知插件。\n\n:::warning 改进插件\nSwanLab插件均为开源代码，你可以在[Github源代码](https://github.com/swanhubx/swanlab/blob/main/swanlab/plugin/notification.py)中查看，欢迎提交你的建议和PR！\n:::\n\n[[toc]]",
    "852": "一级标题：飞书通知\n二级标题：准备工作\n内容：\n::: info 参考文档\n- [自定义机器人API使用指南](https://open.feishu.cn/document/client-docs/bot-v3/add-custom-bot?lang=zh-CN#f62e72d5)·\n- [在飞书群组中使用机器人](https://www.feishu.cn/hc/zh-CN/articles/360024984973-%E5%9C%A8%E7%BE%A4%E7%BB%84%E4%B8%AD%E4%BD%BF%E7%94%A8%E6%9C%BA%E5%99%A8%E4%BA%BA)\n:::\n\n\n1. 在1个飞书群中，点击右上角的 **「···」-「设置」**\n\n<img src=\"./notification-lark/setting.png\" width=\"400\"/>\n\n2. 点击 **「群机器人」**\n\n<img src=\"./notification-lark/group-robot.png\" width=\"400\"/>\n\n3. 点击 **「添加机器人」**\n\n<img src=\"./notification-lark/add-robot.png\" width=\"400\"/>\n\n4. 添加 **「自定义机器人」**\n\n<img src=\"./notification-lark/custom-robot.png\" width=\"600\"/>\n\n<img src=\"./notification-lark/custom-robot-detail.png\" width=\"600\"/>\n\n5. 复制 **「Webhook 地址」和 「签名」**\n\n<img src=\"./notification-lark/webhookurl.png\" width=\"600\"/>\n\n至此，你完成了准备工作。",
    "853": "一级标题：飞书通知\n二级标题：基本用法\n内容：\n使用飞书通知插件的方法非常简单，只需要初始化1个`LarkCallback`对象：\n\n```python\nfrom swanlab.plugin.notification import LarkCallback\n\nlark_callback = LarkCallback(\n    webhook_url=\"https://open.larkoffice.com/open-apis/bot/v2/hook/xxxx\",\n    secret=\"xxxx\",\n)\n```\n\n然后将`lark_callback`对象传入`swanlab.init`的`callbacks`参数中：\n\n```python\nswanlab.init(callbacks=[lark_callback])\n```\n\n这样，当训练完成/发生错误时（触发`swanlab.finish()`），你将会收到飞书信息通知。\n\n<img src=\"./notification-lark/show.png\" width=\"600\"/>",
    "854": "一级标题：飞书通知\n二级标题：自由提醒\n内容：\n你还可以使用`LarkCallback`对象的`send_msg`方法，发送自定义的飞书信息。\n\n这在提醒你某些指标达到某个阈值时非常有用！\n\n```python\nif accuracy > 0.95:\n    # 自定义场景发送消息\n    lark_callback.send_msg(\n        content=f\"Current Accuracy: {accuracy}\",  # 通知内容\n    )\n```",
    "855": "一级标题：飞书通知\n二级标题：外部注册插件\n内容：\n<!--@include: ./shared-snippet.md-->",
    "856": "一级标题：飞书通知\n二级标题：限制\n内容：\n- 飞书通知插件的训练完成/异常通知，使用的是`SwanKitCallback`的`on_stop`生命周期回调，所以如果你的进程被突然`kill`，或者训练机异常关机，那么会因为无法触发`on_stop`回调，从而导致未发送飞书通知。\n\n- 完善方案请期待`SwanLab`的`平台开放API`上线。",
    "857": "一级标题：Slack\n二级标题：无\n内容：\n如果你希望在训练完成/发生错误时，第一时间发送[Slack](https://slack.com)信息通知你，那么非常推荐你使用Slack通知插件。\n\n![](./notification-slack/logo.jpg)\n\n:::warning 改进插件\nSwanLab插件均为开源代码，你可以在[Github源代码](https://github.com/swanhubx/swanlab/blob/main/swanlab/plugin/notification.py)中查看，欢迎提交你的建议和PR！\n:::\n\n[[toc]]",
    "858": "一级标题：Slack\n二级标题：准备工作\n内容：\n::: info 参考文档\n- [Slack-使用传入的webhooks发送消息](https://api.slack.com/messaging/webhooks)\n- [腾讯云-Slack群接收消息](https://cloud.tencent.com/document/product/1263/74219)\n:::\n\n\n1. 前往 [Slack-API](https://api.slack.com/apps) 页面，点击 **「Create an App」**\n\n<img src=\"./notification-slack/slack-create-app.png\" width=\"400\"/>\n\n\n2. 在弹窗中点击 **「From scratch」**\n\n<img src=\"./notification-slack/from-scratch.png\" width=\"400\"/>\n\n3. 填写 **「App Name」** ，并选择用于通知的 workspace，点击右下角的 **「Create App」**\n\n<img src=\"./notification-slack/name-app.png\" width=\"400\"/>\n\n4. 进入 App 配置菜单后，点击左侧的 **「Incoming Webhooks」**，并开启 **「Activate Incoming Webhooks」** 按钮；\n\n<img src=\"./notification-slack/slack-webhook-option.png\" width=\"400\"/>\n\n5. 在页面下方，点击 **「Add New Webhook to Workspace」**，将APP添加到工作区的频道中；\n\n\n<img src=\"./notification-slack/add-new-webhook-workspace.png\" width=\"400\"/>\n\n6. 在跳转的应用请求页面中，选择好APP要发送消息的频道，点击 **「允许」**\n\n<img src=\"./notification-slack/allow-channel.png\" width=\"400\"/>\n\n7.最后返回 APP 配置页面，复制APP的 Webhook URL\n\n<img src=\"./notification-slack/copy-url.png\" width=\"500\"/>",
    "859": "一级标题：Slack\n二级标题：基本用法\n内容：\n使用Slack通知插件的方法非常简单，只需要初始化1个`SlackCallback`对象：\n\n```python\nfrom swanlab.plugin.notification import SlackCallback\n\nslack_callback = SlackCallback(\n    webhook_url='https://hooks.slack.com/services/xxxx/xxxx/xxxx',\n    language='zh'\n)\n```\n\n然后将`slack_callback`对象传入`swanlab.init`的`callbacks`参数中：\n\n```python\nswanlab.init(callbacks=[slack_callback])\n```\n\n这样，当训练完成/发生错误时（触发`swanlab.finish()`），你将会收到Slack消息通知。\n\n\n<img src=\"./notification-slack/slack-finish.png\" width=\"500\"/>",
    "860": "一级标题：Slack\n二级标题：自由提醒\n内容：\n你还可以使用`SlackCallback`对象的`send_msg`方法，发送自定义的的Slack消息。\n\n这在提醒你某些指标达到某个阈值时非常有用！\n\n```python\nif accuracy > 0.95:\n    # 自定义场景发送消息\n    slack_callback.send_msg(\n        content=f\"Current Accuracy: {accuracy}\",  # 通知内容\n    )\n```",
    "861": "一级标题：Slack\n二级标题：限制\n内容：\n- Slack通知插件的训练完成/异常通知，使用的是`SwanKitCallback`的`on_stop`生命周期回调，所以如果你的进程被突然`kill`，或者训练机异常关机，那么会因为无法触发`on_stop`回调，从而导致未发送Slack通知。\n\n- 完善方案请期待`SwanLab`的`平台开放API`上线。",
    "862": "一级标题：企业微信\n二级标题：无\n内容：\n![](./notification-wxwork/logo.jpg)\n\n如果你希望在训练完成/发生错误时，第一时间发送[企业微信](https://work.weixin.qq.com/)信息通知你，那么非常推荐你使用企业微信通知插件。\n\n:::warning 改进插件\nSwanLab插件均为开源代码，你可以在[Github源代码](https://github.com/swanhubx/swanlab/blob/main/swanlab/plugin/notification.py)中查看，欢迎提交你的建议和PR！\n:::\n\n[[toc]]",
    "863": "一级标题：企业微信\n二级标题：准备工作\n内容：\n::: info 参考文档\n- [企业微信-群机器人配置说明](https://developer.work.weixin.qq.com/document/path/91770)\n:::\n1. 在企业微信群中，点击右上角的 **「···」-「添加群机器人」**\n\n<img src=\"./notification-wxwork/wxwork-setting.png\" width=\"400\"/>\n\n2. 在弹出的对话框中点击 **「添加机器人」**\n\n<img src=\"./notification-wxwork/wxwork-addrobot.png\" width=\"400\"/>\n\n3. 继续点击  **「新创建一个机器人」**\n\n<img src=\"./notification-wxwork/wxwork-createnewrobot.png\" width=\"400\"/>\n\n4. 为机器人添加名称，点击 **「添加机器人」**\n\n<img src=\"./notification-wxwork/wxwork-name.png\" width=\"400\"/>\n\n5. 企业微信的机器人只需要复制 **「Webhook地址」** 即可\n\n<img src=\"./notification-wxwork/wxwork-webhook.png\" width=\"400\"/>",
    "864": "一级标题：企业微信\n二级标题：基本用法\n内容：\n使用企业微信通知插件的方法非常简单，只需要初始化1个`WXWorkCallback`对象：\n\n```python\nfrom swanlab.plugin.notification import WXWorkCallback\n\nwxwork_callback = WXWorkCallback(\n    webhook_url=\"https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=xxxx\",\n)\n```\n\n然后将`wxwork_callback`对象传入`swanlab.init`的`callbacks`参数中：\n\n```python\nswanlab.init(callbacks=[wxwork_callback])\n```\n\n这样，当训练完成/发生错误时（触发`swanlab.finish()`），你将会收到企业微信消息通知。\n\n\n<img src=\"./notification-wxwork/wxwork-show.png\" width=\"500\"/>",
    "865": "一级标题：企业微信\n二级标题：自由提醒\n内容：\n你还可以使用`WXWorkCallback`对象的`send_msg`方法，发送自定义的的企业微信消息。\n\n这在提醒你某些指标达到某个阈值时非常有用！\n\n```python\nif accuracy > 0.95:\n    # 自定义场景发送消息\n    wxwork_callback.send_msg(\n        content=f\"Current Accuracy: {accuracy}\",  # 通知内容\n    )\n```",
    "866": "一级标题：企业微信\n二级标题：外部注册插件\n内容：\n<!--@include: ./shared-snippet.md-->",
    "867": "一级标题：企业微信\n二级标题：限制\n内容：\n- 企业微信通知插件的训练完成/异常通知，使用的是`SwanKitCallback`的`on_stop`生命周期回调，所以如果你的进程被突然`kill`，或者训练机异常关机，那么会因为无法触发`on_stop`回调，从而导致未发送企业微信通知。\n\n- 完善方案请期待`SwanLab`的`平台开放API`上线。",
    "868": "一级标题：插件一览\n二级标题：无\n内容：\n- [自定义插件](custom-plugin)",
    "869": "一级标题：插件一览\n二级标题：✈️ 通知类\n内容：\n- [邮件](notification-email)\n- [飞书](notification-lark)\n- [钉钉](notification-dingtalk)\n- [企业微信](notification-wxwork)\n- [Discord](notification-discord)\n- [Slack](notification-slack)",
    "870": "一级标题：插件一览\n二级标题：📝 记录类\n内容：\n- [文件记录器](writer-filelogdir)\n- [CSV表格](writer-csv)",
    "871": "一级标题：等价于 swanlab.init(callbaks=[...])\n二级标题：无\n内容：\nswanlab.register_callbacks([...])\n```",
    "872": "一级标题：CSV表格记录器\n二级标题：无\n内容：\n如果你希望在训练过程中，将一些配置信息、指标信息记录在本地的CSV文件中（格式和SwanLab网页中的“表格视图“一致），那么非常推荐你使用`CSV记录器`插件。\n\n:::warning 改进插件\nSwanLab插件均为开源代码，你可以在[Github源代码](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/plugin/writer.py)中查看，欢迎提交你的建议和PR！\n:::",
    "873": "一级标题：CSV表格记录器\n二级标题：插件用法\n内容：\n**1. 初始化CSV记录器：**\n\n```python\nfrom swanlab.plugin.writer import CSVWriter\n\ncsv_writer = CSVWriter(dir=\"logs\")\n```\n\n`dir`参数指定了CSV文件的保存路径，默认保存到当前工作目录。\n\n**2. 传入插件：**\n\n```python\nswanlab.init(\n    ...\n    callbacks=[csv_writer]\n)\n```\n\n执行代码后，就会在`logs`目录下生成一个`swanlab_run.csv`文件，并开始记录数据。后续的每一次训练，都会在该csv文件中添加新的行。\n\n如果想要指定其他文件名，可以传入`filename`参数：\n\n```python\ncsv_writer = CSVWriter(dir=\"logs\", filename=\"my_csv_file.csv\")\n```",
    "874": "一级标题：CSV表格记录器\n二级标题：示例代码\n内容：\n```python\nimport swanlab\nfrom swanlab.plugin.writer import CSVWriter\nimport random\n\ncsv_writer = CSVWriter(dir=\"logs\")\n\n# 创建一个SwanLab项目\nswanlab.init(\n    # 设置项目名\n    project=\"my-awesome-project\",\n\n    # 设置超参数\n    config={\n        \"learning_rate\": 0.02,\n        \"architecture\": \"CNN\",\n        \"dataset\": \"CIFAR-100\",\n        \"epochs\": 10,\n        \"batch_size\": 128\n    },\n    callbacks=[csv_writer]\n)\n\n# 模拟一次训练\nepochs = 10\noffset = random.random() / 5\nfor epoch in range(2, epochs):\n  acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n  loss = 2 ** -epoch + random.random() / epoch + offset\n\n  # 记录训练指标\n  swanlab.log({\"acc\": acc, \"loss2\": loss})\n\n# [可选] 完成训练，这在notebook环境中是必要的\nswanlab.finish()\n```",
    "875": "一级标题：CSV表格记录器\n二级标题：外部注册插件\n内容：\n<!--@include: ./shared-snippet.md-->",
    "876": "一级标题：文件记录器\n二级标题：无\n内容：\n如果你希望在训练开始时，指定一些文件复制到日志目录（run开头的目录下），那么非常推荐你使用`LogdirFileWriter`插件。\n\n:::warning 改进插件\nSwanLab插件均为开源代码，你可以在[Github源代码](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/plugin/writer.py#L175)中查看，欢迎提交你的建议和PR！\n:::",
    "877": "一级标题：文件记录器\n二级标题：插件用法\n内容：\n**1. 初始化LogdirFileWriter：**\n\n```python\nfrom swanlab.plugin.writer import LogdirFileWriter\n\nlogdirfile_writer = LogdirFileWriter(\n    sub_dir=\"code\",\n    files=[\n        \"config.yaml\",\n        \"README.md\",\n    ]\n)\n```\n\n- `sub_dir`参数如果不为None，则在run目录下创建1个sub_dir文件夹来保存文件\n- `files`参数指定了需要复制的文件列表（也支持仅传入1个str）。\n\n**2. 传入插件：**\n\n```python\nswanlab.init(\n    ...\n    callbacks=[logdirfile_writer]\n)\n```\n\n执行代码后，就会在`logdir`下对应的run开头目录下将`files`参数中的文件复制到该目录中（如果设置了`sub_dir`参数，则会复制到该子目录下）。",
    "878": "一级标题：文件记录器\n二级标题：示例代码\n内容：\n```python\nfrom swanlab.plugin.writer import LogdirFileWriter\nimport swanlab\n\nlogdirfile_writer = LogdirFileWriter(\n    sub_dir=\"code\",\n    file_path=[\"package.json\", \"README.md\"],\n)\n\nswanlab.init(project=\"test-plugin\", callbacks=[logdirfile_writer])\n\nswanlab.log({\"loss\": 0.2, \"acc\": 0.9})\nswanlab.finish()\n```\n\n![](./writer-filelogdir/paste.png)",
    "879": "一级标题：文件记录器\n二级标题：外部注册插件\n内容：\n<!--@include: ./shared-snippet.md-->",
    "880": "一级标题：定义数据\n二级标题：无\n内容：\nhours = [\"12a\", \"1a\", \"2a\", \"3a\", \"4a\", \"5a\", \"6a\", \"7a\", \"8a\", \"9a\", \"10a\", \"11a\", \"12p\", \"1p\", \"2p\", \"3p\", \"4p\", \"5p\", \"6p\", \"7p\", \"8p\", \"9p\", \"10p\", \"11p\"]\n\ndays = [\"Saturday\", \"Friday\", \"Thursday\", \"Wednesday\", \"Tuesday\", \"Monday\", \"Sunday\"]\n\ndata = [\n    [0, 0, 5], [0, 1, 1], [0, 2, 0], [0, 3, 0], [0, 4, 0], [0, 5, 0], [0, 6, 0], [0, 7, 0], [0, 8, 0], [0, 9, 0], [0, 10, 0], [0, 11, 2], [0, 12, 4], [0, 13, 1], [0, 14, 1], [0, 15, 3], [0, 16, 4], [0, 17, 6], [0, 18, 4], [0, 19, 4], [0, 20, 3], [0, 21, 3], [0, 22, 2], [0, 23, 5],\n    [1, 0, 7],[1, 1, 0],[1, 2, 0],[1, 3, 0],[1, 4, 0],[1, 5, 0],[1, 6, 0],[1, 7, 0],[1, 8, 0],[1, 9, 0],[1, 10, 5],[1, 11, 2],[1, 12, 2],[1, 13, 6],[1, 14, 9],[1, 15, 11],[1, 16, 6],[1, 17, 7],[1, 18, 8],[1, 19, 12],[1, 20, 5],[1, 21, 5],[1, 22, 7],[1, 23, 2],\n    [2, 0, 1],[2, 1, 1],[2, 2, 0],[2, 3, 0],[2, 4, 0],[2, 5, 0],[2, 6, 0],[2, 7, 0],[2, 8, 0],[2, 9, 0],[2, 10, 3],[2, 11, 2],[2, 12, 1],[2, 13, 9],[2, 14, 8],[2, 15, 10],[2, 16, 6],[2, 17, 5],[2, 18, 5],[2, 19, 5],[2, 20, 7],[2, 21, 4],[2, 22, 2],[2, 23, 4],\n    [3, 0, 7],[3, 1, 3],[3, 2, 0],[3, 3, 0],[3, 4, 0],[3, 5, 0],[3, 6, 0],[3, 7, 0],[3, 8, 1],[3, 9, 0],[3, 10, 5],[3, 11, 4],[3, 12, 7],[3, 13, 14],[3, 14, 13],[3, 15, 12],[3, 16, 9],[3, 17, 5],[3, 18, 5],[3, 19, 10],[3, 20, 6],[3, 21, 4],[3, 22, 4],[3, 23, 1],\n    [4, 0, 1],[4, 1, 3],[4, 2, 0],[4, 3, 0],[4, 4, 0],[4, 5, 1],[4, 6, 0],[4, 7, 0],[4, 8, 0],[4, 9, 2],[4, 10, 4],[4, 11, 4],[4, 12, 2],[4, 13, 4],[4, 14, 4],[4, 15, 14],[4, 16, 12],[4, 17, 1],[4, 18, 8],[4, 19, 5],[4, 20, 3],[4, 21, 7],[4, 22, 3],[4, 23, 0],\n    [5, 0, 2],[5, 1, 1],[5, 2, 0],[5, 3, 3],[5, 4, 0],[5, 5, 0],[5, 6, 0],[5, 7, 0],[5, 8, 2],[5, 9, 0],[5, 10, 4],[5, 11, 1],[5, 12, 5],[5, 13, 10],[5, 14, 5],[5, 15, 7],[5, 16, 11],[5, 17, 6],[5, 18, 0],[5, 19, 5],[5, 20, 3],[5, 21, 4],[5, 22, 2],[5, 23, 0],\n    [6, 0, 1],[6, 1, 0],[6, 2, 0],[6, 3, 0],[6, 4, 0],[6, 5, 0],[6, 6, 0],[6, 7, 0],[6, 8, 0],[6, 9, 0],[6, 10, 1],[6, 11, 0],[6, 12, 2],[6, 13, 1],[6, 14, 3],[6, 15, 4],[6, 16, 0],[6, 17, 0],[6, 18, 0],[6, 19, 0],[6, 20, 1],[6, 21, 2],[6, 22, 2],[6, 23, 6],\n]\ndata = [[d[1], d[0], d[2]] for d in data]\n\n# 创建echarts bar3d对象\nbar3d = swanlab.echarts.Bar3D()\n\n# 设置bar3d数据\nbar3d.add(\n    \"bar3d\",\n    data,\n    xaxis3d_opts=opts.Axis3DOpts(data=hours, type_=\"category\"),\n    yaxis3d_opts=opts.Axis3DOpts(data=days, type_=\"category\"),\n    zaxis3d_opts=opts.Axis3DOpts(data=data, type_=\"value\"),\n)\n\nbar3d.set_global_opts(\n        visualmap_opts=opts.VisualMapOpts(\n            max_=20,\n            range_color=[\n                \"#313695\",\n                \"#4575b4\",\n                \"#74add1\",\n                \"#abd9e9\",\n                \"#e0f3f8\",\n                \"#ffffbf\",\n                \"#fee090\",\n                \"#fdae61\",\n                \"#f46d43\",\n                \"#d73027\",\n                \"#a50026\",\n            ],\n        )\n    )\n\n# 记录到swanlab\nswanlab.log({\"bar3d\": bar3d})\n```",
    "881": "一级标题：定义数据\n二级标题：3D散点图 scatter3d\n内容：\n![scatter3d](/assets/py-echarts/scatter3d-1.png)\n\n```python\nimport asyncio\nfrom aiohttp import TCPConnector, ClientSession\nimport swanlab\nimport pyecharts.options as opts\n\nswanlab.init(project=\"swanlab-echarts-3d-demo\")\n\n# 定义数据\nasync def get_json_data(url: str) -> dict:\n    async with ClientSession(connector=TCPConnector(ssl=False)) as session:\n        async with session.get(url=url) as response:\n            return await response.json()\n\n\n# 获取echarts官方示例数据\ndata = asyncio.run(\n    get_json_data(\n        url=\"https://echarts.apache.org/examples/data/asset/data/nutrients.json\"\n    )\n)\n\n# 列名映射\nfield_indices = {\n    \"calcium\": 3,\n    \"calories\": 12,\n    \"carbohydrate\": 8,\n    \"fat\": 10,\n    \"fiber\": 5,\n    \"group\": 1,\n    \"id\": 16,\n    \"monounsat\": 14,\n    \"name\": 0,\n    \"polyunsat\": 15,\n    \"potassium\": 7,\n    \"protein\": 2,\n    \"saturated\": 13,\n    \"sodium\": 4,\n    \"sugars\": 9,\n    \"vitaminc\": 6,\n    \"water\": 11,\n}\n\n# 配置 config\nconfig_xAxis3D = \"protein\"\nconfig_yAxis3D = \"fiber\"\nconfig_zAxis3D = \"sodium\"\nconfig_color = \"fiber\"\nconfig_symbolSize = \"vitaminc\"\n\n# 构造数据\n\"\"\"\n数据结构为[[x, y, z, color, size, index]]\n例子：\n[[19.9, 0.4, 0.385, 0.4, 0.0769, 0],\n[35.8, 2, 0.717, 2, 0.138, 1],\n[23.5, 1.6, 0.78, 1.6, 0.0012, 2], ...]\n\"\"\"\ndata = [\n    [\n        item[field_indices[config_xAxis3D]],\n        item[field_indices[config_yAxis3D]],\n        item[field_indices[config_zAxis3D]],\n        item[field_indices[config_color]],\n        item[field_indices[config_symbolSize]],\n        index,\n    ]\n    for index, item in enumerate(data)\n]\n\n# 创建echarts scatter3d对象\nscatter3d = swanlab.echarts.Scatter3D()\n\n# 设置scatter3d数据\nscatter3d.add(\n    \"scatter3d\",\n    data,\n    xaxis3d_opts=opts.Axis3DOpts(name=config_xAxis3D, type_=\"value\"),\n    yaxis3d_opts=opts.Axis3DOpts(name=config_yAxis3D, type_=\"value\"),\n    zaxis3d_opts=opts.Axis3DOpts(name=config_zAxis3D, type_=\"value\"),\n    grid3d_opts=opts.Grid3DOpts(width=100, height=100, depth=100),\n)\nscatter3d.set_global_opts(\n        visualmap_opts=[\n            opts.VisualMapOpts(\n                type_=\"color\",\n                is_calculable=True,\n                dimension=3,\n                pos_top=\"10\",\n                max_=79 / 2,\n                range_color=[\n                    \"#1710c0\",\n                    \"#0b9df0\",\n                    \"#00fea8\",\n                    \"#00ff0d\",\n                    \"#f5f811\",\n                    \"#f09a09\",\n                    \"#fe0300\",\n                ],\n            ),\n            opts.VisualMapOpts(\n                type_=\"size\",\n                is_calculable=True,\n                dimension=4,\n                pos_bottom=\"10\",\n                max_=2.4 / 2,\n                range_size=[10, 40],\n            ),\n        ]\n    )\n\n# 记录到swanlab\nswanlab.log({\"scatter3d\": scatter3d})\n```",
    "882": "一级标题：定义数据\n二级标题：3D折线图 line3d\n内容：\n![line3d](/assets/py-echarts/line3d-1.png)\n\n```python\nimport math\nimport swanlab\nimport pyecharts.options as opts\nfrom pyecharts.faker import Faker\n\n\nswanlab.init(project=\"swanlab-echarts-3d-demo\")\n\n# 构造数据\ndata = []\nfor t in range(0, 25000):\n    _t = t / 1000\n    x = (1 + 0.25 * math.cos(75 * _t)) * math.cos(_t)\n    y = (1 + 0.25 * math.cos(75 * _t)) * math.sin(_t)\n    z = _t + 2.0 * math.sin(75 * _t)\n    data.append([x, y, z])\n\n\n# 创建echarts line3d对象\nline3d = swanlab.echarts.Line3D()\n\n# 设置line3d数据\nline3d.add(\n    \"line3d\",\n    data,\n    xaxis3d_opts=opts.Axis3DOpts(Faker.clock, type_=\"value\"),\n    yaxis3d_opts=opts.Axis3DOpts(Faker.week_en, type_=\"value\"),\n    grid3d_opts=opts.Grid3DOpts(width=100, depth=100),\n)\n\nline3d.set_global_opts(\n        visualmap_opts=opts.VisualMapOpts(\n            max_=30, min_=0, range_color=Faker.visual_color\n        ),\n    )\n\n# 记录到swanlab\nswanlab.log({\"line3d\": line3d})\n```",
    "883": "一级标题：定义数据\n二级标题：3D曲面图 3d_surface\n内容：\n![3d_surface](/assets/py-echarts/surface3d-1.png)\n\n```python\nimport math\nimport swanlab\nimport pyecharts.options as opts\nfrom typing import Union\n\nswanlab.init(project=\"swanlab-echarts-3d-demo\")\n\n# 构造数据\ndef float_range(start: int, end: int, step: Union[int, float], round_number: int = 2):\n    \"\"\"\n    浮点数 range\n    :param start: 起始值\n    :param end: 结束值\n    :param step: 步长\n    :param round_number: 精度\n    :return: 返回一个 list\n    \"\"\"\n    temp = []\n    while True:\n        if start < end:\n            temp.append(round(start, round_number))\n            start += step\n        else:\n            break\n    return temp\n\n\ndef surface3d_data():\n    for t0 in float_range(-3, 3, 0.05):\n        y = t0\n        for t1 in float_range(-3, 3, 0.05):\n            x = t1\n            z = math.sin(x**2 + y**2) * x / 3.14\n            yield [x, y, z]\n\n\n# 创建echarts surface3d对象\nsurface3d = swanlab.echarts.Surface3D()\n\n# 设置surface3d数据\nsurface3d.add(\n    \"surface3d\",\n    data=list(surface3d_data()),\n    xaxis3d_opts=opts.Axis3DOpts(type_=\"value\"),\n    yaxis3d_opts=opts.Axis3DOpts(type_=\"value\"),\n    grid3d_opts=opts.Grid3DOpts(width=100, height=40, depth=100),\n)\n\nsurface3d.set_global_opts(\n        visualmap_opts=opts.VisualMapOpts(\n            dimension=2,\n            max_=1,\n            min_=-1,\n            range_color=[\n                \"#313695\",\n                \"#4575b4\",\n                \"#74add1\",\n                \"#abd9e9\",\n                \"#e0f3f8\",\n                \"#ffffbf\",\n                \"#fee090\",\n                \"#fdae61\",\n                \"#f46d43\",\n                \"#d73027\",\n                \"#a50026\",\n            ],\n        )\n    )\n\n# 记录到swanlab\nswanlab.log({\"surface3d\": surface3d})\n```",
    "884": "一级标题：定义数据\n二级标题：无\n内容：\nweek_name_list = [\"周一\", \"周二\", \"周三\", \"周四\", \"周五\", \"周六\", \"周日\"]\nhigh_temperature = [11, 11, 15, 13, 12, 13, 10]\nlow_temperature = [1, -2, 2, 5, 3, 2, 0]\n\n# 创建echarts line对象\nline = swanlab.echarts.Line()\n\n# 设置line的轴\nline.add_xaxis(week_name_list)\n# 设置line的数据\nline.add_yaxis(\"high_temperature\", high_temperature)\nline.add_yaxis(\"low_temperature\", low_temperature)\n\n# 记录到swanlab\nswanlab.log({\"line\": line})\n```\n\n```python [样式-设置不透明度]\n\"\"\"\ndemo:\nhttps://swanlab.cn/@ZeyiLin/swanlab-echarts-demo/runs/trptzejp9037cimxd786e/chart#eDczbzM0-blJ6R3dXSFU=\n\"\"\"\n\nimport swanlab\nimport pyecharts.options as opts\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nweek_name_list = [\"周一\", \"周二\", \"周三\", \"周四\", \"周五\", \"周六\", \"周日\"]\nhigh_temperature = [11, 11, 15, 13, 12, 13, 10]\nlow_temperature = [1, -2, 2, 5, 3, 2, 0]\n\n# 创建echarts line对象\nline = swanlab.echarts.Line()\n# 设置line的轴\nline.add_xaxis(week_name_list)\n# 设置line的数据\nline.add_yaxis(\"high_temperature\", high_temperature)\nline.add_yaxis(\"low_temperature\", low_temperature)\n\n# 设置不透明度为0.5\nline.set_series_opts(areastyle_opts=opts.AreaStyleOpts(opacity=0.5))\n\n# 记录到swanlab\nswanlab.log({\"line_opacity\": line})\n```\n:::",
    "885": "一级标题：定义数据\n二级标题：柱状图 bar\n内容：\n![bar](/assets/py-echarts/bar-1.png)\n\n::: code-group\n\n```python [基础]\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nx = [\"a\", \"b\", \"c\"]\ny = [1, 2, 3]\n\n# 创建echarts bar对象\nbar = swanlab.echarts.Bar()\n\n# 设置x轴数据\nbar.add_xaxis(x)\n# 设置y轴数据\nbar.add_yaxis(\"value\", y)\n\n# 记录到swanlab\nswanlab.log({\"bar\": bar})\n```\n\n```python [水平方向]\n\"\"\"\ndemo:\nhttps://swanlab.cn/@ZeyiLin/swanlab-echarts-demo/runs/rtqyhofvc5080tpmdfxkz/chart#bGw5M2My-ZnRhOGRnWVE=\n\"\"\"\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nx = [\"a\", \"b\", \"c\"]\ny = [1, 2, 3]\n\n# 创建echarts bar对象\nbar = swanlab.echarts.Bar()\n\n# 设置x轴数据\nbar.add_xaxis(x)\n# 设置y轴数据\nbar.add_yaxis(\"value\", y)\n# 翻转\nbar.reversal_axis()\n\n# 记录到swanlab\nswanlab.log({\"bar_horizontal\": bar})\n```\n\n:::",
    "886": "一级标题：定义数据\n二级标题：饼状图 pie\n内容：\n![pie](/assets/py-echarts/pie-1.png)\n\n```python\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nx_data = [\"直接访问\", \"邮件营销\", \"联盟广告\", \"视频广告\", \"搜索引擎\"]\ny_data = [335, 310, 274, 235, 400]\n\n# 组合数据\ndata_pair = [list(z) for z in zip(x_data, y_data)]\ndata_pair.sort(key=lambda x: x[1])\n\n# 创建echarts pie对象\npie = swanlab.echarts.Pie()\n\n# 设置x轴数据并配置标签显示\npie.add(\n    \"访问来源\",\n    data_pair,\n    # 配置标签显示\n    label_opts={\n        \"formatter\": \"{b}: {d}%\",  # 显示百分比\n        \"position\": \"outside\"  # 标签位置\n    }\n)\n\n# 记录到swanlab\nswanlab.log({\"pie\": pie})\n```",
    "887": "一级标题：定义数据\n二级标题：热力图 heatmap\n内容：\n![heatmap](/assets/py-echarts/heatmap-1.png)\n\n:::code-group\n\n```python [基础]\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nhours = [\"12a\", \"1a\", \"2a\", \"3a\", \"4a\", \"5a\", \"6a\", \"7a\", \"8a\", \"9a\", \"10a\", \"11a\", \"12p\", \"1p\", \"2p\", \"3p\", \"4p\", \"5p\", \"6p\", \"7p\", \"8p\", \"9p\", \"10p\", \"11p\"]\n\ndays = [\"Saturday\", \"Friday\", \"Thursday\", \"Wednesday\", \"Tuesday\", \"Monday\", \"Sunday\"]\n\ndata = [\n    [0, 0, 5], [0, 1, 1], [0, 2, 0], [0, 3, 0], [0, 4, 0], [0, 5, 0], [0, 6, 0], [0, 7, 0], [0, 8, 0], [0, 9, 0], [0, 10, 0], [0, 11, 2], [0, 12, 4], [0, 13, 1], [0, 14, 1], [0, 15, 3], [0, 16, 4], [0, 17, 6], [0, 18, 4], [0, 19, 4], [0, 20, 3], [0, 21, 3], [0, 22, 2], [0, 23, 5],\n    [1, 0, 7],[1, 1, 0],[1, 2, 0],[1, 3, 0],[1, 4, 0],[1, 5, 0],[1, 6, 0],[1, 7, 0],[1, 8, 0],[1, 9, 0],[1, 10, 5],[1, 11, 2],[1, 12, 2],[1, 13, 6],[1, 14, 9],[1, 15, 11],[1, 16, 6],[1, 17, 7],[1, 18, 8],[1, 19, 12],[1, 20, 5],[1, 21, 5],[1, 22, 7],[1, 23, 2],\n    [2, 0, 1],[2, 1, 1],[2, 2, 0],[2, 3, 0],[2, 4, 0],[2, 5, 0],[2, 6, 0],[2, 7, 0],[2, 8, 0],[2, 9, 0],[2, 10, 3],[2, 11, 2],[2, 12, 1],[2, 13, 9],[2, 14, 8],[2, 15, 10],[2, 16, 6],[2, 17, 5],[2, 18, 5],[2, 19, 5],[2, 20, 7],[2, 21, 4],[2, 22, 2],[2, 23, 4],\n    [3, 0, 7],[3, 1, 3],[3, 2, 0],[3, 3, 0],[3, 4, 0],[3, 5, 0],[3, 6, 0],[3, 7, 0],[3, 8, 1],[3, 9, 0],[3, 10, 5],[3, 11, 4],[3, 12, 7],[3, 13, 14],[3, 14, 13],[3, 15, 12],[3, 16, 9],[3, 17, 5],[3, 18, 5],[3, 19, 10],[3, 20, 6],[3, 21, 4],[3, 22, 4],[3, 23, 1],\n    [4, 0, 1],[4, 1, 3],[4, 2, 0],[4, 3, 0],[4, 4, 0],[4, 5, 1],[4, 6, 0],[4, 7, 0],[4, 8, 0],[4, 9, 2],[4, 10, 4],[4, 11, 4],[4, 12, 2],[4, 13, 4],[4, 14, 4],[4, 15, 14],[4, 16, 12],[4, 17, 1],[4, 18, 8],[4, 19, 5],[4, 20, 3],[4, 21, 7],[4, 22, 3],[4, 23, 0],\n    [5, 0, 2],[5, 1, 1],[5, 2, 0],[5, 3, 3],[5, 4, 0],[5, 5, 0],[5, 6, 0],[5, 7, 0],[5, 8, 2],[5, 9, 0],[5, 10, 4],[5, 11, 1],[5, 12, 5],[5, 13, 10],[5, 14, 5],[5, 15, 7],[5, 16, 11],[5, 17, 6],[5, 18, 0],[5, 19, 5],[5, 20, 3],[5, 21, 4],[5, 22, 2],[5, 23, 0],\n    [6, 0, 1],[6, 1, 0],[6, 2, 0],[6, 3, 0],[6, 4, 0],[6, 5, 0],[6, 6, 0],[6, 7, 0],[6, 8, 0],[6, 9, 0],[6, 10, 1],[6, 11, 0],[6, 12, 2],[6, 13, 1],[6, 14, 3],[6, 15, 4],[6, 16, 0],[6, 17, 0],[6, 18, 0],[6, 19, 0],[6, 20, 1],[6, 21, 2],[6, 22, 2],[6, 23, 6],\n]\ndata = [[d[1], d[0], d[2] or \"-\"] for d in data]\n\n# 创建echarts heatmap对象\nheatmap = swanlab.echarts.HeatMap()\n\n# 设置x轴数据并配置标签显示\nheatmap.add_xaxis(hours)\nheatmap.add_yaxis(\n  \"Punch Card\",\n  days,\n  data,\n)\n\n# 记录到swanlab\nswanlab.log({\"heatmap\": heatmap})\n```\n\n```python [设置颜色映射范围]\n\"\"\"\ndemo:\nhttps://swanlab.cn/@ZeyiLin/swanlab-echarts-demo/runs/c1wm57rkfnwkyz7kaat8a/chart#OWJ5bWJl-c2M5bDFFc2I=\n\"\"\"\"\n\nimport swanlab\nfrom pyecharts import options as opts\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nhours = [\"12a\", \"1a\", \"2a\", \"3a\", \"4a\", \"5a\", \"6a\", \"7a\", \"8a\", \"9a\", \"10a\", \"11a\", \"12p\", \"1p\", \"2p\", \"3p\", \"4p\", \"5p\", \"6p\", \"7p\", \"8p\", \"9p\", \"10p\", \"11p\"]\n\ndays = [\"Saturday\", \"Friday\", \"Thursday\", \"Wednesday\", \"Tuesday\", \"Monday\", \"Sunday\"]\n\ndata = [\n    [0, 0, 5], [0, 1, 1], [0, 2, 0], [0, 3, 0], [0, 4, 0], [0, 5, 0], [0, 6, 0], [0, 7, 0], [0, 8, 0], [0, 9, 0], [0, 10, 0], [0, 11, 2], [0, 12, 4], [0, 13, 1], [0, 14, 1], [0, 15, 3], [0, 16, 4], [0, 17, 6], [0, 18, 4], [0, 19, 4], [0, 20, 3], [0, 21, 3], [0, 22, 2], [0, 23, 5],\n    [1, 0, 7],[1, 1, 0],[1, 2, 0],[1, 3, 0],[1, 4, 0],[1, 5, 0],[1, 6, 0],[1, 7, 0],[1, 8, 0],[1, 9, 0],[1, 10, 5],[1, 11, 2],[1, 12, 2],[1, 13, 6],[1, 14, 9],[1, 15, 11],[1, 16, 6],[1, 17, 7],[1, 18, 8],[1, 19, 12],[1, 20, 5],[1, 21, 5],[1, 22, 7],[1, 23, 2],\n    [2, 0, 1],[2, 1, 1],[2, 2, 0],[2, 3, 0],[2, 4, 0],[2, 5, 0],[2, 6, 0],[2, 7, 0],[2, 8, 0],[2, 9, 0],[2, 10, 3],[2, 11, 2],[2, 12, 1],[2, 13, 9],[2, 14, 8],[2, 15, 10],[2, 16, 6],[2, 17, 5],[2, 18, 5],[2, 19, 5],[2, 20, 7],[2, 21, 4],[2, 22, 2],[2, 23, 4],\n    [3, 0, 7],[3, 1, 3],[3, 2, 0],[3, 3, 0],[3, 4, 0],[3, 5, 0],[3, 6, 0],[3, 7, 0],[3, 8, 1],[3, 9, 0],[3, 10, 5],[3, 11, 4],[3, 12, 7],[3, 13, 14],[3, 14, 13],[3, 15, 12],[3, 16, 9],[3, 17, 5],[3, 18, 5],[3, 19, 10],[3, 20, 6],[3, 21, 4],[3, 22, 4],[3, 23, 1],\n    [4, 0, 1],[4, 1, 3],[4, 2, 0],[4, 3, 0],[4, 4, 0],[4, 5, 1],[4, 6, 0],[4, 7, 0],[4, 8, 0],[4, 9, 2],[4, 10, 4],[4, 11, 4],[4, 12, 2],[4, 13, 4],[4, 14, 4],[4, 15, 14],[4, 16, 12],[4, 17, 1],[4, 18, 8],[4, 19, 5],[4, 20, 3],[4, 21, 7],[4, 22, 3],[4, 23, 0],\n    [5, 0, 2],[5, 1, 1],[5, 2, 0],[5, 3, 3],[5, 4, 0],[5, 5, 0],[5, 6, 0],[5, 7, 0],[5, 8, 2],[5, 9, 0],[5, 10, 4],[5, 11, 1],[5, 12, 5],[5, 13, 10],[5, 14, 5],[5, 15, 7],[5, 16, 11],[5, 17, 6],[5, 18, 0],[5, 19, 5],[5, 20, 3],[5, 21, 4],[5, 22, 2],[5, 23, 0],\n    [6, 0, 1],[6, 1, 0],[6, 2, 0],[6, 3, 0],[6, 4, 0],[6, 5, 0],[6, 6, 0],[6, 7, 0],[6, 8, 0],[6, 9, 0],[6, 10, 1],[6, 11, 0],[6, 12, 2],[6, 13, 1],[6, 14, 3],[6, 15, 4],[6, 16, 0],[6, 17, 0],[6, 18, 0],[6, 19, 0],[6, 20, 1],[6, 21, 2],[6, 22, 2],[6, 23, 6],\n]\ndata = [[d[1], d[0], d[2] or \"-\"] for d in data]\n\n# 创建echarts heatmap对象\nheatmap = swanlab.echarts.HeatMap()\nheatmap.set_global_opts(\n    visualmap_opts=opts.VisualMapOpts(min_=0, max_=10, orient=\"horizontal\"),\n)\n\n# 设置x轴数据并配置标签显示\nheatmap.add_xaxis(hours)\nheatmap.add_yaxis(\n  \"Punch Card\",\n  days,\n  data,\n)\n\n# 记录到swanlab\nswanlab.log({\"heatmap_visualmapopts\": heatmap})\n```\n\n\n\n:::",
    "888": "一级标题：定义数据\n二级标题：散点图 scatter\n内容：\n![](/assets/py-echarts/scatter-1.png)\n\n```python\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\ndata = [\n    [10.0, 8.04],\n    [8.0, 6.95],\n    [13.0, 7.58],\n    [9.0, 8.81],\n    [11.0, 8.33],\n    [14.0, 9.96],\n    [6.0, 7.24],\n    [4.0, 4.26],\n    [12.0, 10.84],\n    [7.0, 4.82],\n    [5.0, 5.68],\n]\ndata.sort(key=lambda x: x[0])\nx_data = [d[0] for d in data]\ny_data = [d[1] for d in data]\n\n# 创建echarts scatter对象\nscatter = swanlab.echarts.Scatter()\n\n# 设置x轴数据并配置标签显示\nscatter.add_xaxis(x_data)\nscatter.add_yaxis(\n  \"\",\n  y_data,\n  symbol_size=20,\n)\n\n# 记录到swanlab\nswanlab.log({\"scatter\": scatter})\n```",
    "889": "一级标题：定义数据\n二级标题：雷达图 radar\n内容：\n![radar](/assets/py-echarts/radar-1.png)\n\n```python\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nv1 = [[4300, 10000, 28000, 35000, 50000, 19000]]\nv2 = [[5000, 14000, 28000, 31000, 42000, 21000]]\n\n# 创建echarts scatter对象\nradar = swanlab.echarts.Radar()\n\n# 设置雷达图维度与数据范围\nradar.add_schema(\n    schema=[\n        {\"name\": \"销售\", \"max\": 6500},\n        {\"name\": \"管理\", \"max\": 16000},\n        {\"name\": \"信息技术\", \"max\": 30000},\n        {\"name\": \"客服\", \"max\": 38000},\n        {\"name\": \"研发\", \"max\": 52000},\n        {\"name\": \"市场\", \"max\": 25000},\n    ]\n)\n\n# 添加数据1\nradar.add(\n    \"预算分配\",\n    v1,\n    color=\"#1f77b4\",\n)\n\n# 添加数据2\nradar.add(\n    \"实际开销\",\n    v2,\n    color=\"#ff7f0e\",\n)\n\n\n# 记录到swanlab\nswanlab.log({\"radar\": radar})\n```",
    "890": "一级标题：定义数据\n二级标题：箱线图 boxplot\n内容：\n![boxplot](/assets/py-echarts/boxplot-1.png)\n\n```python\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\ny_data = [\n    [850, 740, 900, 1070, 930, 850, 950, 980, 980, 880, 1000, 980, 930, 650, 760, 810, 1000, 1000, 960, 960, ],\n    [960, 940, 960, 940, 880, 800, 850, 880, 900, 840, 830, 790, 810, 880, 880, 830, 800, 790, 760, 800, ],\n    [880, 880, 880, 860, 720, 720, 620, 860, 970, 950, 880, 910, 850, 870, 840, 840, 850, 840, 840, 840, ],\n    [890, 810, 810, 820, 800, 770, 760, 740, 750, 760, 910, 920, 890, 860, 880, 720, 840, 850, 850, 780, ],\n    [890, 840, 780, 810, 760, 810, 790, 810, 820, 850, 870, 870, 810, 740, 810, 940, 950, 800, 810, 870, ],\n]\n\nscatter_data = [650, 620, 720, 720, 950, 970]\n# 创建echarts table对象\nboxplot = swanlab.echarts.Boxplot()\n\n# 设置表头\nboxplot.add_xaxis([\"expr 0\", \"expr 1\", \"expr 2\", \"expr 3\", \"expr 4\"])\nboxplot.add_yaxis(\"\", boxplot.prepare_data(y_data))\n\n# 记录到swanlab\nswanlab.log({\"boxplot\": boxplot})\n```",
    "891": "一级标题：定义数据\n二级标题：平行坐标系图 parallel\n内容：\n![parallel](/assets/py-echarts/parallel-1.png)\n\n```python\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nparallel_axis = [\n    {\"dim\": 0, \"name\": \"Price\"},\n    {\"dim\": 1, \"name\": \"Net Weight\"},\n    {\"dim\": 2, \"name\": \"Amount\"},\n    {\n        \"dim\": 3,\n        \"name\": \"Score\",\n        \"type\": \"category\",\n        \"data\": [\"Excellent\", \"Good\", \"OK\", \"Bad\"],\n    },\n]\n\ndata = [[12.99, 100, 82, \"Good\"], [9.99, 80, 77, \"OK\"], [20, 120, 60, \"Excellent\"]]\n\n# 创建echarts parallel对象\nparallel = swanlab.echarts.Parallel()\n\n# 设置parallel的轴\nparallel.add_schema(parallel_axis)\n# 设置parallel的数据\nparallel.add(\"data\", data=data)\n\n# 记录到swanlab\nswanlab.log({\"parallel\": parallel})\n```",
    "892": "一级标题：定义数据\n二级标题：仪表盘图 gauge\n内容：\n![gauge](/assets/py-echarts/gauge-1.png)\n\n```python\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 创建echarts gauge对象\ngauge = swanlab.echarts.Gauge()\ngauge.add(\"\", [(\"完成率\", 66.6)])\n\n# 记录到swanlab\nswanlab.log({\"gauge\": gauge})\n```",
    "893": "一级标题：定义数据\n二级标题：表格 table\n内容：\n![table](/assets/py-echarts/table-1.png)\n\n![](/assets/text-chart.gif)\n\n```python\nimport swanlab\n\nswanlab.init(\n    project=\"echarts-test\",\n)\n\n# 定义表头\nheaders = [\"NO\", \"Product\", \"Count\"]\n# 定义数据\nrows = [\n    [2, \"A\", 259],\n    [3, \"B\", 123],\n    [4, \"C\", 300],\n    [5, \"D\", 290],\n    [6, \"E\", 1145],\n]\n\n# 创建echarts table对象\ntable = swanlab.echarts.Table()\n\n# 添加数据\ntable.add(headers, rows)\n\n# 记录到swanlab\nswanlab.log({\"table\": table})\n```",
    "894": "一级标题：定义数据\n二级标题：树状图 tree\n内容：\n![tree](/assets/py-echarts/tree-1.png)\n\n```python\nimport swanlab\n\nswanlab.init(project=\"swanlab-echarts-demo\")\n\n# 构造数据\ndata = [\n    {\n        \"children\": [\n            {\"name\": \"B\"},\n            {\n                \"children\": [{\"children\": [{\"name\": \"I\"}], \"name\": \"E\"}, {\"name\": \"F\"}],\n                \"name\": \"C\",\n            },\n            {\n                \"children\": [\n                    {\"children\": [{\"name\": \"J\"}, {\"name\": \"K\"}], \"name\": \"G\"},\n                    {\"name\": \"H\"},\n                ],\n                \"name\": \"D\",\n            },\n        ],\n        \"name\": \"A\",\n    }\n]\n\n# 创建echarts tree对象\ntree = swanlab.echarts.Tree()\n\n# 设置tree数据\ntree.add(\"tree\", data=data)\n\n# 记录到swanlab\nswanlab.log({\"tree\": tree})\n```",
    "895": "一级标题：定义数据\n二级标题：桑基图 sankey\n内容：\n![sankey](/assets/py-echarts/sankey-1.png)\n\n```python\nimport swanlab\nfrom pyecharts import options as opts\n\nswanlab.init(project=\"swanlab-echarts-demo\")\n\n# 构造数据\nnodes = [\n    {\"name\": \"category1\"},\n    {\"name\": \"category2\"},\n    {\"name\": \"category3\"},\n    {\"name\": \"category4\"},\n    {\"name\": \"category5\"},\n    {\"name\": \"category6\"},\n]\n\nlinks = [\n    {\"source\": \"category1\", \"target\": \"category2\", \"value\": 10},\n    {\"source\": \"category2\", \"target\": \"category3\", \"value\": 15},\n    {\"source\": \"category3\", \"target\": \"category4\", \"value\": 20},\n    {\"source\": \"category5\", \"target\": \"category6\", \"value\": 25},\n]\n\n# 创建echarts sankey对象\nsankey = swanlab.echarts.Sankey()\n\n# 设置sankey数据\nsankey.add(\n    \"sankey\",\n    nodes=nodes,\n    links=links,\n    linestyle_opt=opts.LineStyleOpts(opacity=0.2, curve=0.5, color=\"source\"),\n    label_opts=opts.LabelOpts(position=\"right\"),\n)\n\n# 记录到swanlab\nswanlab.log({\"sankey\": sankey})\n```",
    "896": "一级标题：🤗 加速分布式训练\n二级标题：无\n内容：\n随着模型变得越来越大，并行性已经成为在有限硬件上训练更大模型和加速训练速度的策略，增加了数个数量级。在Hugging Face，我们创建了[🤗 加速](https://huggingface.co/docs/accelerate)库，以帮助用户在任何类型的分布式设置上轻松训练🤗 Transformers模型，无论是在一台机器上的多个GPU还是在多个机器上的多个GPU。在本教程中，了解如何自定义您的原生PyTorch训练循环，以启用分布式环境中的训练。",
    "897": "一级标题：🤗 加速分布式训练\n二级标题：设置\n内容：\n通过安装🤗 加速开始:\n\n```bash\npip install accelerate\n```\n\n然后导入并创建[`~accelerate.Accelerator`]对象。[`~accelerate.Accelerator`]将自动检测您的分布式设置类型，并初始化所有必要的训练组件。您不需要显式地将模型放在设备上。\n\n```py\n>>> from accelerate import Accelerator\n\n>>> accelerator = Accelerator()\n```",
    "898": "一级标题：🤗 加速分布式训练\n二级标题：准备加速\n内容：\n下一步是将所有相关的训练对象传递给[`~accelerate.Accelerator.prepare`]方法。这包括您的训练和评估DataLoader、一个模型和一个优化器:\n\n```py\n>>> train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n...     train_dataloader, eval_dataloader, model, optimizer\n... )\n```",
    "899": "一级标题：🤗 加速分布式训练\n二级标题：反向传播\n内容：\n最后一步是用🤗 加速的[`~accelerate.Accelerator.backward`]方法替换训练循环中的典型`loss.backward()`:\n\n```py\n>>> for epoch in range(num_epochs):\n...     for batch in train_dataloader:\n...         outputs = model(**batch)\n...         loss = outputs.loss\n...         accelerator.backward(loss)\n\n...         optimizer.step()\n...         lr_scheduler.step()\n...         optimizer.zero_grad()\n...         progress_bar.update(1)\n```\n\n如您在下面的代码中所见，您只需要添加四行额外的代码到您的训练循环中即可启用分布式训练！\n\n```diff\n+ from accelerate import Accelerator\n  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n\n+ accelerator = Accelerator()\n\n  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n  optimizer = AdamW(model.parameters(), lr=3e-5)\n\n- device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n- model.to(device)\n\n+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n+     train_dataloader, eval_dataloader, model, optimizer\n+ )\n\n  num_epochs = 3\n  num_training_steps = num_epochs * len(train_dataloader)\n  lr_scheduler = get_scheduler(\n      \"linear\",\n      optimizer=optimizer,\n      num_warmup_steps=0,\n      num_training_steps=num_training_steps\n  )\n\n  progress_bar = tqdm(range(num_training_steps))\n\n  model.train()\n  for epoch in range(num_epochs):\n      for batch in train_dataloader:\n-         batch = {k: v.to(device) for k, v in batch.items()}\n          outputs = model(**batch)\n          loss = outputs.loss\n-         loss.backward()\n+         accelerator.backward(loss)\n\n          optimizer.step()\n          lr_scheduler.step()\n          optimizer.zero_grad()\n          progress_bar.update(1)\n```",
    "900": "一级标题：🤗 加速分布式训练\n二级标题：训练\n内容：\n在添加了相关代码行后，可以在脚本或笔记本（如Colaboratory）中启动训练。\n\n### 用脚本训练\n\n如果您从脚本中运行训练，请运行以下命令以创建和保存配置文件:\n\n```bash\naccelerate config\n```\n\n然后使用以下命令启动训练:\n\n```bash\naccelerate launch train.py\n```\n\n### 用笔记本训练\n\n🤗 加速还可以在笔记本中运行，如果您计划使用Colaboratory的TPU，则可在其中运行。将负责训练的所有代码包装在一个函数中，并将其传递给[`~accelerate.notebook_launcher`]:\n\n```py\n>>> from accelerate import notebook_launcher\n\n>>> notebook_launcher(training_function)\n```\n\n有关🤗 加速及其丰富功能的更多信息，请参阅[文档](https://huggingface.co/docs/accelerate)。",
    "901": "一级标题：如何创建自定义流水线？\n二级标题：无\n内容：\n在本指南中，我们将演示如何创建一个自定义流水线并分享到 [Hub](https://hf.co/models)，或将其添加到 🤗 Transformers 库中。\n\n首先，你需要决定流水线将能够接受的原始条目。它可以是字符串、原始字节、字典或任何看起来最可能是期望的输入。\n尽量保持输入为纯 Python 语言，因为这样可以更容易地实现兼容性（甚至通过 JSON 在其他语言之间）。\n这些将是流水线 (`preprocess`) 的 `inputs`。\n\n然后定义 `outputs`。与 `inputs` 相同的策略。越简单越好。这些将是 `postprocess` 方法的输出。\n\n首先继承基类 `Pipeline`，其中包含实现 `preprocess`、`_forward`、`postprocess` 和 `_sanitize_parameters` 所需的 4 个方法。\n\n```python\nfrom transformers import Pipeline\n\n\nclass MyPipeline(Pipeline):\n    def _sanitize_parameters(self, **kwargs):\n        preprocess_kwargs = {}\n        if \"maybe_arg\" in kwargs:\n            preprocess_kwargs[\"maybe_arg\"] = kwargs[\"maybe_arg\"]\n        return preprocess_kwargs, {}, {}\n\n    def preprocess(self, inputs, maybe_arg=2):\n        model_input = Tensor(inputs[\"input_ids\"])\n        return {\"model_input\": model_input}\n\n    def _forward(self, model_inputs):\n        # model_inputs == {\"model_input\": model_input}\n        outputs = self.model(**model_inputs)\n        # Maybe {\"logits\": Tensor(...)}\n        return outputs\n\n    def postprocess(self, model_outputs):\n        best_class = model_outputs[\"logits\"].softmax(-1)\n        return best_class\n```\n\n这种分解的结构旨在为 CPU/GPU 提供相对无缝的支持，同时支持在不同线程上对 CPU 进行预处理/后处理。\n\n`preprocess` 将接受最初定义的输入，并将其转换为可供模型输入的内容。它可能包含更多信息，通常是一个 `Dict`。\n\n`_forward` 是实现细节，不应直接调用。`forward` 是首选的调用方法，因为它包含保障措施，以确保一切都在预期的设备上运作。\n如果任何内容与实际模型相关，它应该属于 `_forward` 方法，其他内容应该在 preprocess/postprocess 中。\n\n`postprocess` 方法将接受 `_forward` 的输出，并将其转换为之前确定的最终输出。\n\n`_sanitize_parameters` 存在是为了允许用户在任何时候传递任何参数，无论是在初始化时 `pipeline(...., maybe_arg=4)`\n还是在调用时 `pipe = pipeline(...); output = pipe(...., maybe_arg=4)`。\n\n`_sanitize_parameters` 的返回值是将直接传递给 `preprocess`、`_forward` 和 `postprocess` 的 3 个关键字参数字典。\n如果调用方没有使用任何额外参数调用，则不要填写任何内容。这样可以保留函数定义中的默认参数，这总是更\"自然\"的。\n\n在分类任务中，一个经典的例子是在后处理中使用 `top_k` 参数。\n\n```python\n>>> pipe = pipeline(\"my-new-task\")\n>>> pipe(\"This is a test\")\n[{\"label\": \"1-star\", \"score\": 0.8}, {\"label\": \"2-star\", \"score\": 0.1}, {\"label\": \"3-star\", \"score\": 0.05}\n{\"label\": \"4-star\", \"score\": 0.025}, {\"label\": \"5-star\", \"score\": 0.025}]\n\n>>> pipe(\"This is a test\", top_k=2)\n[{\"label\": \"1-star\", \"score\": 0.8}, {\"label\": \"2-star\", \"score\": 0.1}]\n```\n\n为了实现这一点，我们将更新我们的 `postprocess` 方法，将默认参数设置为 `5`，\n并编辑 `_sanitize_parameters` 方法，以允许这个新参数。\n\n```python\ndef postprocess(self, model_outputs, top_k=5):\n    best_class = model_outputs[\"logits\"].softmax(-1)\n    # Add logic to handle top_k\n    return best_class\n\n\ndef _sanitize_parameters(self, **kwargs):\n    preprocess_kwargs = {}\n    if \"maybe_arg\" in kwargs:\n        preprocess_kwargs[\"maybe_arg\"] = kwargs[\"maybe_arg\"]\n\n    postprocess_kwargs = {}\n    if \"top_k\" in kwargs:\n        postprocess_kwargs[\"top_k\"] = kwargs[\"top_k\"]\n    return preprocess_kwargs, {}, postprocess_kwargs\n```\n\n尽量保持简单输入/输出，最好是可 JSON 序列化的，因为这样可以使流水线的使用非常简单，而不需要用户了解新的对象类型。\n通常也相对常见地支持许多不同类型的参数以便使用（例如音频文件，可以是文件名、URL 或纯字节）。",
    "902": "一级标题：如何创建自定义流水线？\n二级标题：将其添加到支持的任务列表中\n内容：\n要将你的 `new-task` 注册到支持的任务列表中，你需要将其添加到 `PIPELINE_REGISTRY` 中：\n\n```python\nfrom transformers.pipelines import PIPELINE_REGISTRY\n\nPIPELINE_REGISTRY.register_pipeline(\n    \"new-task\",\n    pipeline_class=MyPipeline,\n    pt_model=AutoModelForSequenceClassification,\n)\n```\n\n如果需要，你可以指定一个默认模型，此时它应该带有一个特定的修订版本（可以是分支名称或提交哈希，这里我们使用了 `\"abcdef\"`），以及类型：\n\n```python\nPIPELINE_REGISTRY.register_pipeline(\n    \"new-task\",\n    pipeline_class=MyPipeline,\n    pt_model=AutoModelForSequenceClassification,\n    default={\"pt\": (\"user/awesome_model\", \"abcdef\")},\n    type=\"text\",  # current support type: text, audio, image, multimodal\n)\n```",
    "903": "一级标题：如何创建自定义流水线？\n二级标题：在 Hub 上分享你的流水线\n内容：\n要在 Hub 上分享你的自定义流水线，你只需要将 `Pipeline` 子类的自定义代码保存在一个 Python 文件中。\n例如，假设我们想使用一个自定义流水线进行句对分类，如下所示：\n\n```py\nimport numpy as np\n\nfrom transformers import Pipeline\n\n\ndef softmax(outputs):\n    maxes = np.max(outputs, axis=-1, keepdims=True)\n    shifted_exp = np.exp(outputs - maxes)\n    return shifted_exp / shifted_exp.sum(axis=-1, keepdims=True)\n\n\nclass PairClassificationPipeline(Pipeline):\n    def _sanitize_parameters(self, **kwargs):\n        preprocess_kwargs = {}\n        if \"second_text\" in kwargs:\n            preprocess_kwargs[\"second_text\"] = kwargs[\"second_text\"]\n        return preprocess_kwargs, {}, {}\n\n    def preprocess(self, text, second_text=None):\n        return self.tokenizer(text, text_pair=second_text, return_tensors=self.framework)\n\n    def _forward(self, model_inputs):\n        return self.model(**model_inputs)\n\n    def postprocess(self, model_outputs):\n        logits = model_outputs.logits[0].numpy()\n        probabilities = softmax(logits)\n\n        best_class = np.argmax(probabilities)\n        label = self.model.config.id2label[best_class]\n        score = probabilities[best_class].item()\n        logits = logits.tolist()\n        return {\"label\": label, \"score\": score, \"logits\": logits}\n```\n\n这个实现与框架无关，适用于 PyTorch 和 TensorFlow 模型。如果我们将其保存在一个名为\n`pair_classification.py` 的文件中，然后我们可以像这样导入并注册它：\n\n```py\nfrom pair_classification import PairClassificationPipeline\nfrom transformers.pipelines import PIPELINE_REGISTRY\nfrom transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification\n\nPIPELINE_REGISTRY.register_pipeline(\n    \"pair-classification\",\n    pipeline_class=PairClassificationPipeline,\n    pt_model=AutoModelForSequenceClassification,\n    tf_model=TFAutoModelForSequenceClassification,\n)\n```\n\n完成这些步骤后，我们可以将其与预训练模型一起使用。例如，`sgugger/finetuned-bert-mrpc`\n已经在 MRPC 数据集上进行了微调，用于将句子对分类为是释义或不是释义。\n\n```py\nfrom transformers import pipeline\n\nclassifier = pipeline(\"pair-classification\", model=\"sgugger/finetuned-bert-mrpc\")\n```\n\n然后，我们可以通过在 `Repository` 中使用 `save_pretrained` 方法将其分享到 Hub 上：\n\n```py\nfrom huggingface_hub import Repository\n\nrepo = Repository(\"test-dynamic-pipeline\", clone_from=\"{your_username}/test-dynamic-pipeline\")\nclassifier.save_pretrained(\"test-dynamic-pipeline\")\nrepo.push_to_hub()\n```\n\n这将会复制包含你定义的 `PairClassificationPipeline` 的文件到文件夹 `\"test-dynamic-pipeline\"` 中，\n同时保存流水线的模型和分词器，然后将所有内容推送到仓库 `{your_username}/test-dynamic-pipeline` 中。\n之后，只要提供选项 `trust_remote_code=True`，任何人都可以使用它：\n\n```py\nfrom transformers import pipeline\n\nclassifier = pipeline(model=\"{your_username}/test-dynamic-pipeline\", trust_remote_code=True)\n```",
    "904": "一级标题：如何创建自定义流水线？\n二级标题：将流水线添加到 🤗 Transformers\n内容：\n如果你想将你的流水线贡献给 🤗 Transformers，你需要在 `pipelines` 子模块中添加一个新模块，\n其中包含你的流水线的代码，然后将其添加到 `pipelines/__init__.py` 中定义的任务列表中。\n\n然后，你需要添加测试。创建一个新文件 `tests/test_pipelines_MY_PIPELINE.py`，其中包含其他测试的示例。\n\n`run_pipeline_test` 函数将非常通用，并在每种可能的架构上运行小型随机模型，如 `model_mapping` 和 `tf_model_mapping` 所定义。\n\n这对于测试未来的兼容性非常重要，这意味着如果有人为 `XXXForQuestionAnswering` 添加了一个新模型，\n流水线测试将尝试在其上运行。由于模型是随机的，所以不可能检查实际值，这就是为什么有一个帮助函数 `ANY`，它只是尝试匹配流水线的输出类型。\n\n你还 **需要** 实现 2（最好是 4）个测试。\n\n- `test_small_model_pt`：为这个流水线定义一个小型模型（结果是否合理并不重要），并测试流水线的输出。\n  结果应该与 `test_small_model_tf` 的结果相同。\n- `test_small_model_tf`：为这个流水线定义一个小型模型（结果是否合理并不重要），并测试流水线的输出。\n  结果应该与 `test_small_model_pt` 的结果相同。\n- `test_large_model_pt`（可选）：在一个真实的流水线上测试流水线，结果应该是有意义的。\n  这些测试速度较慢，应该被如此标记。这里的目标是展示流水线，并确保在未来的发布中没有漂移。\n- `test_large_model_tf`（可选）：在一个真实的流水线上测试流水线，结果应该是有意义的。\n  这些测试速度较慢，应该被如此标记。这里的目标是展示流水线，并确保在未来的发布中没有漂移。",
    "905": "一级标题：注意力机制\n二级标题：无\n内容：\n大多数 transformer 模型使用完全注意力机制，该机制采用正方形的注意力矩阵。当输入很长的文本时，这将导致巨大的计算瓶颈。Longformer 和 Reformer 是提高注意力机制效率的改进模型，它们使用稀疏化的注意力矩阵来加速训练。",
    "906": "一级标题：注意力机制\n二级标题：局部敏感哈希注意力机制（LSH attention）\n内容：\n[Reformer](model_doc/reformer)使用LSH（局部敏感哈希）的注意力机制。在计算softmax(QK^t)时，只有矩阵QK^t中的最大元素（在softmax维度上）会做出有用的贡献。所以对于Q中的每个查询q，我们只需要考虑K中与q接近的键k，这里使用了一个哈希函数来确定q和k是否接近。注意力掩码被修改以掩盖当前的词符（token）（除了第一个位置之外），因为这样会使得查询和键相等（因此非常相似）。由于哈希可能会有些随机性，所以在实践中使用多个哈希函数（由n_rounds参数确定）,然后一起求平均。",
    "907": "一级标题：注意力机制\n二级标题：局部注意力机制（Local attention）\n内容：\n[Longformer](model_doc/longformer)使用局部注意力机制:通常情况下，局部上下文（例如，左边和右边的两个词符是什么？）对于给定词符的操作已经足够了。此外，通过堆叠具有小窗口的注意力层，最后一层将拥有不仅仅是窗口内词符的感受野，这使得它们能构建整个句子的表示。\n\n一些预先选定的输入词符也被赋予全局注意力:对于这些少数词符，注意力矩阵可以访问所有词符（tokens），并且这个过程是对称的:所有其他词符除了它们局部窗口内的词符之外，也可以访问这些特定的词符。这在论文的图2d中有展示，下面是一个样本注意力掩码：\n\n<div class=\"flex justify-center\">\n    <img scale=\"50 %\" align=\"center\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/local_attention_mask.png\"/>\n</div>\n\n使用参数更少的注意力矩阵，可以让模型处理更长的输入序列。",
    "908": "一级标题：注意力机制\n二级标题：其他技巧\n内容：\n### 轴向位置编码\n\n[Reformer](model_doc/reformer)模型使用轴向位置编码：在传统的transformer模型中，位置编码矩阵E的大小是\\\\(l\\\\)乘以\\\\(d\\\\)，其中\\\\(l\\\\)是序列长度，\\\\(d\\\\)是隐藏状态的维度。如果你有非常长的文本，这个矩阵可能会非常大，将会占用大量的GPU显存。为了缓解这个问题，轴向位置编码将这个大矩阵E分解成两个较小的矩阵E1和E2，它们的维度分别是\\\\(l_{1} \\times d_{1}\\\\) 和\\\\(l_{2} \\times d_{2}\\\\)，满足\\\\(l_{1} \\times l_{2} = l\\\\)和\\\\(d_{1} + d_{2} = d\\\\)（通过长度的乘积，最终得到的矩阵要小得多）。在E中，对于时间步\\\\(j\\\\) 的嵌入是通过连接E1中时间步 \\\\(j \\% l1\\\\) 的嵌入和E2中时间步\\\\(j // l1\\\\)的嵌入来获得的。",
    "909": "一级标题：使用AutoClass加载预训练实例\n二级标题：无\n内容：\n由于存在许多不同的Transformer架构，因此为您的checkpoint创建一个可用架构可能会具有挑战性。通过`AutoClass`可以自动推断并从给定的checkpoint加载正确的架构, 这也是🤗 Transformers易于使用、简单且灵活核心规则的重要一部分。`from_pretrained()`方法允许您快速加载任何架构的预训练模型，因此您不必花费时间和精力从头开始训练模型。生成这种与checkpoint无关的代码意味着，如果您的代码适用于一个checkpoint，它将适用于另一个checkpoint - 只要它们是为了类似的任务进行训练的 - 即使架构不同。\n\n<Tip>\n\n请记住，架构指的是模型的结构，而checkpoints是给定架构的权重。例如，[BERT](https://huggingface.co/google-bert/bert-base-uncased)是一种架构，而`google-bert/bert-base-uncased`是一个checkpoint。模型是一个通用术语，可以指代架构或checkpoint。\n\n\n</Tip>\n\n在这个教程中，学习如何：\n\n* 加载预训练的分词器（`tokenizer`）\n* 加载预训练的图像处理器(`image processor`)\n* 加载预训练的特征提取器(`feature extractor`)\n* 加载预训练的处理器(`processor`)\n* 加载预训练的模型。",
    "910": "一级标题：使用AutoClass加载预训练实例\n二级标题：AutoTokenizer\n内容：\n几乎所有的NLP任务都以`tokenizer`开始。`tokenizer`将您的输入转换为模型可以处理的格式。\n\n使用[`AutoTokenizer.from_pretrained`]加载`tokenizer`：\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n```\n\n然后按照如下方式对输入进行分词：\n\n```py\n>>> sequence = \"In a hole in the ground there lived a hobbit.\"\n>>> print(tokenizer(sequence))\n{'input_ids': [101, 1999, 1037, 4920, 1999, 1996, 2598, 2045, 2973, 1037, 7570, 10322, 4183, 1012, 102],\n 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n```",
    "911": "一级标题：使用AutoClass加载预训练实例\n二级标题：AutoImageProcessor\n内容：\n对于视觉任务，`image processor`将图像处理成正确的输入格式。\n\n```py\n>>> from transformers import AutoImageProcessor\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n```",
    "912": "一级标题：使用AutoClass加载预训练实例\n二级标题：AutoFeatureExtractor\n内容：\n对于音频任务,`feature extractor`将音频信号处理成正确的输入格式。\n\n使用[`AutoFeatureExtractor.from_pretrained`]加载`feature extractor`：\n\n```py\n>>> from transformers import AutoFeatureExtractor\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\n...     \"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\"\n... )\n```",
    "913": "一级标题：使用AutoClass加载预训练实例\n二级标题：AutoProcessor\n内容：\n多模态任务需要一种`processor`，将两种类型的预处理工具结合起来。例如，[LayoutLMV2](model_doc/layoutlmv2)模型需要一个`image processor`来处理图像和一个`tokenizer`来处理文本；`processor`将两者结合起来。\n\n使用[`AutoProcessor.from_pretrained`]加载`processor`：\n\n\n```py\n>>> from transformers import AutoProcessor\n\n>>> processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n```",
    "914": "一级标题：使用AutoClass加载预训练实例\n二级标题：AutoModel\n内容：\n<frameworkcontent>\n<pt>\n\n最后，`AutoModelFor`类让你可以加载给定任务的预训练模型（参见[这里](model_doc/auto)获取可用任务的完整列表）。例如，使用[`AutoModelForSequenceClassification.from_pretrained`]加载用于序列分类的模型：\n\n```py\n>>> from transformers import AutoModelForSequenceClassification\n\n>>> model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n```\n\n轻松地重复使用相同的checkpoint来为不同任务加载模型架构：\n\n\n```py\n>>> from transformers import AutoModelForTokenClassification\n\n>>> model = AutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n```\n\n<Tip warning={true}>\n\n对于PyTorch模型，`from_pretrained()`方法使用`torch.load()`，它内部使用已知是不安全的`pickle`。一般来说，永远不要加载来自不可信来源或可能被篡改的模型。对于托管在Hugging Face Hub上的公共模型，这种安全风险在一定程度上得到了缓解，因为每次提交都会进行[恶意软件扫描](https://huggingface.co/docs/hub/security-malware)。请参阅[Hub文档](https://huggingface.co/docs/hub/security)以了解最佳实践，例如使用GPG进行[签名提交验证](https://huggingface.co/docs/hub/security-gpg#signing-commits-with-gpg)。\n\nTensorFlow和Flax的checkpoints不受影响，并且可以在PyTorch架构中使用`from_tf`和`from_flax`关键字参数,通过`from_pretrained`方法进行加载,来绕过此问题。\n\n</Tip>\n\n一般来说，我们建议使用`AutoTokenizer`类和`AutoModelFor`类来加载预训练的模型实例。这样可以确保每次加载正确的架构。在下一个[教程](preprocessing)中，学习如何使用新加载的`tokenizer`, `image processor`, `feature extractor`和`processor`对数据集进行预处理以进行微调。\n\n</pt>\n<tf>\n最后，`TFAutoModelFor`类允许您加载给定任务的预训练模型（请参阅[这里](model_doc/auto)获取可用任务的完整列表）。例如，使用[`TFAutoModelForSequenceClassification.from_pretrained`]加载用于序列分类的模型：\n\n```py\n>>> from transformers import TFAutoModelForSequenceClassification\n\n>>> model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n```\n\n轻松地重复使用相同的checkpoint来为不同任务加载模型架构：\n\n```py\n>>> from transformers import TFAutoModelForTokenClassification\n\n>>> model = TFAutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n```\n一般来说，我们推荐使用`AutoTokenizer`类和`TFAutoModelFor`类来加载模型的预训练实例。这样可以确保每次加载正确的架构。在下一个[教程](preprocessing)中，学习如何使用新加载的`tokenizer`, `image processor`, `feature extractor`和`processor`对数据集进行预处理以进行微调。\n\n</tf>\n</frameworkcontent>",
    "915": "一级标题：基于BERT进行的相关研究（BERTology）\n二级标题：无\n内容：\n当前，一个新兴的研究领域正致力于探索大规模 transformer 模型（如BERT）的内部工作机制，一些人称之为“BERTology”。以下是这个领域的一些典型示例：\n\n\n- BERT Rediscovers the Classical NLP Pipeline by Ian Tenney, Dipanjan Das, Ellie Pavlick:\n  https://huggingface.co/papers/1905.05950\n- Are Sixteen Heads Really Better than One? by Paul Michel, Omer Levy, Graham Neubig: https://huggingface.co/papers/1905.10650\n- What Does BERT Look At? An Analysis of BERT's Attention by Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D.\n  Manning: https://huggingface.co/papers/1906.04341\n- CAT-probing: A Metric-based Approach to Interpret How Pre-trained Models for Programming Language Attend Code Structure: https://huggingface.co/papers/2210.04633\n\n\n为了助力这一新兴领域的发展，我们在BERT/GPT/GPT-2模型中增加了一些附加功能，方便人们访问其内部表示，这些功能主要借鉴了Paul Michel的杰出工作(https://huggingface.co/papers/1905.10650)：\n\n\n- 访问BERT/GPT/GPT-2的所有隐藏状态，\n- 访问BERT/GPT/GPT-2每个注意力头的所有注意力权重，\n- 检索注意力头的输出值和梯度，以便计算头的重要性得分并对头进行剪枝，详情可见论文：https://huggingface.co/papers/1905.10650。\n\n为了帮助您理解和使用这些功能，我们添加了一个具体的示例脚本：[bertology.py](https://github.com/huggingface/transformers-research-projects/tree/main/bertology/run_bertology.py)，该脚本可以对一个在 GLUE 数据集上预训练的模型进行信息提取与剪枝。",
    "916": "一级标题：实例化大型模型\n二级标题：无\n内容：\n当你想使用一个非常大的预训练模型时，一个挑战是尽量减少对内存的使用。通常从PyTorch开始的工作流程如下：\n\n1. 用随机权重创建你的模型。\n2. 加载你的预训练权重。\n3. 将这些预训练权重放入你的随机模型中。\n\n步骤1和2都需要完整版本的模型在内存中，这在大多数情况下不是问题，但如果你的模型开始达到几个GB的大小，这两个副本可能会让你超出内存的限制。更糟糕的是，如果你使用`torch.distributed`来启动分布式训练，每个进程都会加载预训练模型并将这两个副本存储在内存中。\n\n<Tip>\n\n请注意，随机创建的模型使用“空”张量进行初始化，这些张量占用内存空间但不填充它（因此随机值是给定时间内该内存块中的任何内容）。在第3步之后，对未初始化的权重执行适合模型/参数种类的随机初始化（例如正态分布），以尽可能提高速度！\n\n</Tip>\n\n在本指南中，我们将探讨 Transformers 提供的解决方案来处理这个问题。请注意，这是一个积极开发的领域，因此这里解释的API在将来可能会略有变化。",
    "917": "一级标题：实例化大型模型\n二级标题：分片checkpoints\n内容：\n自4.18.0版本起，占用空间超过10GB的模型检查点将自动分成较小的片段。在使用`model.save_pretrained(save_dir)`时，您最终会得到几个部分`checkpoints`（每个的大小都小于10GB）以及一个索引，该索引将参数名称映射到存储它们的文件。\n\n您可以使用`max_shard_size`参数来控制分片之前的最大大小。为了示例的目的，我们将使用具有较小分片大小的普通大小的模型：让我们以传统的BERT模型为例。\n\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"google-bert/bert-base-cased\")\n```\n\n如果您使用 [`PreTrainedModel.save_pretrained`](模型预训练保存) 进行保存，您将得到一个新的文件夹，其中包含两个文件：模型的配置和权重：\n\n```py\n>>> import os\n>>> import tempfile\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir)\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model.bin']\n```\n\n现在让我们使用最大分片大小为200MB：\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     print(sorted(os.listdir(tmp_dir)))\n['config.json', 'pytorch_model-00001-of-00003.bin', 'pytorch_model-00002-of-00003.bin', 'pytorch_model-00003-of-00003.bin', 'pytorch_model.bin.index.json']\n```\n\n在模型配置文件最上方，我们可以看到三个不同的权重文件，以及一个`index.json`索引文件。这样的`checkpoint`可以使用[`~PreTrainedModel.from_pretrained`]方法完全重新加载：\n\n```py\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     new_model = AutoModel.from_pretrained(tmp_dir)\n```\n\n对于大型模型来说，这样做的主要优点是在上述工作流程的步骤2中，每个`checkpoint`的分片在前一个分片之后加载，从而将内存中的内存使用限制在模型大小加上最大分片的大小。\n\n在后台，索引文件用于确定`checkpoint`中包含哪些键以及相应的权重存储在哪里。我们可以像加载任何json一样加载该索引，并获得一个字典：\n\n```py\n>>> import json\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     with open(os.path.join(tmp_dir, \"pytorch_model.bin.index.json\"), \"r\") as f:\n...         index = json.load(f)\n\n>>> print(index.keys())\ndict_keys(['metadata', 'weight_map'])\n```\n\n目前元数据仅包括模型的总大小。我们计划在将来添加其他信息：\n```py\n>>> index[\"metadata\"]\n{'total_size': 433245184}\n```\n\n权重映射是该索引的主要部分，它将每个参数的名称（通常在PyTorch模型的`state_dict`中找到）映射到存储该参数的文件：\n\n```py\n>>> index[\"weight_map\"]\n{'embeddings.LayerNorm.bias': 'pytorch_model-00001-of-00003.bin',\n 'embeddings.LayerNorm.weight': 'pytorch_model-00001-of-00003.bin',\n ...\n```\n\n如果您想直接在模型内部加载这样的分片`checkpoint`，而不使用 [`PreTrainedModel.from_pretrained`](就像您会为完整`checkpoint`执行 `model.load_state_dict()` 一样)，您应该使用 [`modeling_utils.load_sharded_checkpoint`]：\n\n\n```py\n>>> from transformers.modeling_utils import load_sharded_checkpoint\n\n>>> with tempfile.TemporaryDirectory() as tmp_dir:\n...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")\n...     load_sharded_checkpoint(model, tmp_dir)\n```",
    "918": "一级标题：实例化大型模型\n二级标题：低内存加载\n内容：\n分片`checkpoints`在上述工作流的第2步中降低了内存使用，但为了在低内存环境中使用该模型，我们建议使用基于 Accelerate 库的工具。\n\n请阅读以下指南以获取更多信息：[使用 Accelerate 进行大模型加载](./main_classes/model#large-model-loading)",
    "919": "一级标题：聊天模型的模板\n二级标题：无\n内容：",
    "920": "一级标题：聊天模型的模板\n二级标题：介绍\n内容：\nLLM 的一个常见应用场景是聊天。在聊天上下文中，不再是连续的文本字符串构成的语句（不同于标准的语言模型），\n聊天模型由一条或多条消息组成的对话组成，每条消息都有一个“用户”或“助手”等 **角色**，还包括消息文本。\n\n与`Tokenizer`类似，不同的模型对聊天的输入格式要求也不同。这就是我们添加**聊天模板**作为一个功能的原因。\n聊天模板是`Tokenizer`的一部分。用来把问答的对话内容转换为模型的输入`prompt`。\n\n\n让我们通过一个快速的示例来具体说明，使用`BlenderBot`模型。\nBlenderBot有一个非常简单的默认模板，主要是在对话轮之间添加空格：\n\n```python\n>>> from transformers import AutoTokenizer\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n\n>>> chat = [\n...    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n...    {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n...    {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n... ]\n\n>>> tokenizer.apply_chat_template(chat, tokenize=False)\n\" Hello, how are you?  I'm doing great. How can I help you today?   I'd like to show off how chat templating works!</s>\"\n```\n\n注意，整个聊天对话内容被压缩成了一整个字符串。如果我们使用默认设置的`tokenize=True`，那么该字符串也将被tokenized处理。\n不过，为了看到更复杂的模板实际运行，让我们使用`mistralai/Mistral-7B-Instruct-v0.1`模型。\n\n```python\n>>> from transformers import AutoTokenizer\n>>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n\n>>> chat = [\n...   {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n...   {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n...   {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n... ]\n\n>>> tokenizer.apply_chat_template(chat, tokenize=False)\n\"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]\"\n```\n\n可以看到，这一次tokenizer已经添加了[INST]和[/INST]来表示用户消息的开始和结束。\nMistral-instruct是有使用这些token进行训练的，但BlenderBot没有。",
    "921": "一级标题：聊天模型的模板\n二级标题：我如何使用聊天模板？\n内容：\n正如您在上面的示例中所看到的，聊天模板非常容易使用。只需构建一系列带有`role`和`content`键的消息，\n然后将其传递给[`~PreTrainedTokenizer.apply_chat_template`]方法。\n另外，在将聊天模板用作模型预测的输入时，还建议使用`add_generation_prompt=True`来添加[generation prompt](#什么是generation-prompts)。\n\n这是一个准备`model.generate()`的示例，使用`Zephyr`模型：\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = \"HuggingFaceH4/zephyr-7b-beta\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)  # You may want to use bfloat16 and/or move to GPU here\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n ]\ntokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\nprint(tokenizer.decode(tokenized_chat[0]))\n```\n这将生成Zephyr期望的输入格式的字符串。它看起来像这样：\n```text\n<|system|>\nYou are a friendly chatbot who always responds in the style of a pirate</s>\n<|user|>\nHow many helicopters can a human eat in one sitting?</s>\n<|assistant|>\n```\n\n现在我们已经按照`Zephyr`的要求传入prompt了，我们可以使用模型来生成对用户问题的回复：\n\n```python\noutputs = model.generate(tokenized_chat, max_new_tokens=128)\nprint(tokenizer.decode(outputs[0]))\n```\n\n输出结果是：\n\n```text\n<|system|>\nYou are a friendly chatbot who always responds in the style of a pirate</s>\n<|user|>\nHow many helicopters can a human eat in one sitting?</s>\n<|assistant|>\nMatey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all.\n```\n啊，原来这么容易！",
    "922": "一级标题：聊天模型的模板\n二级标题：有自动化的聊天`pipeline`吗？\n内容：\n有的，[`TextGenerationPipeline`]。这个`pipeline`的设计是为了方便使用聊天模型。让我们再试一次 Zephyr 的例子，但这次使用`pipeline`：\n\n```python\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", \"HuggingFaceH4/zephyr-7b-beta\")\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\nprint(pipe(messages, max_new_tokens=256)['generated_text'][-1])\n```\n\n```text\n{'role': 'assistant', 'content': \"Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all.\"}\n```\n\n[`TextGenerationPipeline`]将负责处理所有的`tokenized`并调用`apply_chat_template`，一旦模型有了聊天模板，您只需要初始化pipeline并传递消息列表！",
    "923": "一级标题：聊天模型的模板\n二级标题：什么是\"generation prompts\"?\n内容：\n您可能已经注意到`apply_chat_template`方法有一个`add_generation_prompt`参数。\n这个参数告诉模板添加模型开始答复的标记。例如，考虑以下对话：\n\n```python\nmessages = [\n    {\"role\": \"user\", \"content\": \"Hi there!\"},\n    {\"role\": \"assistant\", \"content\": \"Nice to meet you!\"},\n    {\"role\": \"user\", \"content\": \"Can I ask a question?\"}\n]\n```\n\n这是`add_generation_prompt=False`的结果，使用ChatML模板：\n```python\ntokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n\"\"\"<|im_start|>user\nHi there!<|im_end|>\n<|im_start|>assistant\nNice to meet you!<|im_end|>\n<|im_start|>user\nCan I ask a question?<|im_end|>\n\"\"\"\n```\n\n下面这是`add_generation_prompt=True`的结果：\n\n```python\ntokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\"\"\"<|im_start|>user\nHi there!<|im_end|>\n<|im_start|>assistant\nNice to meet you!<|im_end|>\n<|im_start|>user\nCan I ask a question?<|im_end|>\n<|im_start|>assistant\n\"\"\"\n```\n\n这一次我们添加了模型开始答复的标记。这可以确保模型生成文本时只会给出答复，而不会做出意外的行为，比如继续用户的消息。\n记住，聊天模型只是语言模型，它们被训练来继续文本，而聊天对它们来说只是一种特殊的文本！\n你需要用适当的控制标记来引导它们，让它们知道自己应该做什么。\n\n并非所有模型都需要生成提示。一些模型，如BlenderBot和LLaMA，在模型回复之前没有任何特殊标记。\n在这些情况下，`add_generation_prompt`参数将不起作用。`add_generation_prompt`参数取决于你所使用的模板。",
    "924": "一级标题：聊天模型的模板\n二级标题：我可以在训练中使用聊天模板吗？\n内容：\n可以！我们建议您将聊天模板应用为数据集的预处理步骤。之后，您可以像进行任何其他语言模型训练任务一样继续。\n在训练时，通常应该设置`add_generation_prompt=False`，因为添加的助手标记在训练过程中并不会有帮助。\n让我们看一个例子：\n\n```python\nfrom transformers import AutoTokenizer\nfrom datasets import Dataset\n\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n\nchat1 = [\n    {\"role\": \"user\", \"content\": \"Which is bigger, the moon or the sun?\"},\n    {\"role\": \"assistant\", \"content\": \"The sun.\"}\n]\nchat2 = [\n    {\"role\": \"user\", \"content\": \"Which is bigger, a virus or a bacterium?\"},\n    {\"role\": \"assistant\", \"content\": \"A bacterium.\"}\n]\n\ndataset = Dataset.from_dict({\"chat\": [chat1, chat2]})\ndataset = dataset.map(lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=False, add_generation_prompt=False)})\nprint(dataset['formatted_chat'][0])\n```\n结果是：\n```text\n<|user|>\nWhich is bigger, the moon or the sun?</s>\n<|assistant|>\nThe sun.</s>\n```\n\n这样，后面你可以使用`formatted_chat`列，跟标准语言建模任务中一样训练即可。",
    "925": "一级标题：聊天模型的模板\n二级标题：高级：聊天模板是如何工作的？\n内容：\n模型的聊天模板存储在`tokenizer.chat_template`属性上。如果没有设置，则将使用该模型的默认模板。\n让我们来看看`BlenderBot`的模板：\n```python\n\n>>> from transformers import AutoTokenizer\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n\n>>> tokenizer.chat_template\n\"{% for message in messages %}{% if message['role'] == 'user' %}{{ ' ' }}{% endif %}{{ message['content'] }}{% if not loop.last %}{{ '  ' }}{% endif %}{% endfor %}{{ eos_token }}\"\n```\n\n这看着有点复杂。让我们添加一些换行和缩进，使其更易读。\n请注意，默认情况下忽略每个块后的第一个换行以及块之前的任何前导空格，\n使用Jinja的`trim_blocks`和`lstrip_blocks`标签。\n这里，请注意空格的使用。我们强烈建议您仔细检查模板是否打印了多余的空格！\n```\n{% for message in messages %}\n    {% if message['role'] == 'user' %}\n        {{ ' ' }}\n    {% endif %}\n    {{ message['content'] }}\n    {% if not loop.last %}\n        {{ '  ' }}\n    {% endif %}\n{% endfor %}\n{{ eos_token }}\n```\n\n如果你之前不了解[Jinja template](https://jinja.palletsprojects.com/en/3.1.x/templates/)。\nJinja是一种模板语言，允许你编写简单的代码来生成文本。\n在许多方面，代码和语法类似于Python。在纯Python中，这个模板看起来会像这样：\n```python\nfor idx, message in enumerate(messages):\n    if message['role'] == 'user':\n        print(' ')\n    print(message['content'])\n    if not idx == len(messages) - 1:  # Check for the last message in the conversation\n        print('  ')\nprint(eos_token)\n```\n\n这里使用Jinja模板处理如下三步：\n1. 对于每条消息，如果消息是用户消息，则在其前面添加一个空格，否则不打印任何内容\n2. 添加消息内容\n3. 如果消息不是最后一条，请在其后添加两个空格。在最后一条消息之后，打印`EOS`。\n\n这是一个简单的模板，它不添加任何控制tokens，也不支持`system`消息（常用于指导模型在后续对话中如何表现）。\n但 Jinja 给了你很大的灵活性来做这些事情！让我们看一个 Jinja 模板，\n它可以实现类似于LLaMA的prompt输入（请注意，真正的LLaMA模板包括`system`消息，请不要在实际代码中使用这个简单模板！）\n```\n{% for message in messages %}\n    {% if message['role'] == 'user' %}\n        {{ bos_token + '[INST] ' + message['content'] + ' [/INST]' }}\n    {% elif message['role'] == 'system' %}\n        {{ '<<SYS>>\\\\n' + message['content'] + '\\\\n<</SYS>>\\\\n\\\\n' }}\n    {% elif message['role'] == 'assistant' %}\n        {{ ' '  + message['content'] + ' ' + eos_token }}\n    {% endif %}\n{% endfor %}\n```\n\n这里稍微看一下，就能明白这个模板的作用：它根据每条消息的“角色”添加对应的消息。\n`user`、`assistant`、`system`的消息需要分别处理，因为它们代表不同的角色输入。",
    "926": "一级标题：聊天模型的模板\n二级标题：高级：编辑聊天模板\n内容：\n### 如何创建聊天模板？\n\n很简单，你只需编写一个jinja模板并设置`tokenizer.chat_template`。你也可以从一个现有模板开始，只需要简单编辑便可以！\n例如，我们可以采用上面的LLaMA模板，并在助手消息中添加\"[ASST]\"和\"[/ASST]\"：\n```\n{% for message in messages %}\n    {% if message['role'] == 'user' %}\n        {{ bos_token + '[INST] ' + message['content'].strip() + ' [/INST]' }}\n    {% elif message['role'] == 'system' %}\n        {{ '<<SYS>>\\\\n' + message['content'].strip() + '\\\\n<</SYS>>\\\\n\\\\n' }}\n    {% elif message['role'] == 'assistant' %}\n        {{ '[ASST] '  + message['content'] + ' [/ASST]' + eos_token }}\n    {% endif %}\n{% endfor %}\n```\n\n现在，只需设置`tokenizer.chat_template`属性。下次使用[`~PreTrainedTokenizer.apply_chat_template`]时，它将使用您的新模板！\n此属性将保存在`tokenizer_config.json`文件中，因此您可以使用[`~utils.PushToHubMixin.push_to_hub`]将新模板上传到 Hub，\n这样每个人都可以使用你模型的模板！\n\n```python\ntemplate = tokenizer.chat_template\ntemplate = template.replace(\"SYS\", \"SYSTEM\")  # Change the system token\ntokenizer.chat_template = template  # Set the new template\ntokenizer.push_to_hub(\"model_name\")  # Upload your new template to the Hub!\n```\n\n由于[`~PreTrainedTokenizer.apply_chat_template`]方法是由[`TextGenerationPipeline`]类调用，\n因此一旦你设置了聊天模板，您的模型将自动与[`TextGenerationPipeline`]兼容。\n### “默认”模板是什么？\n\n在引入聊天模板（chat_template）之前，聊天prompt是在模型中通过硬编码处理的。为了向前兼容，我们保留了这种硬编码处理聊天prompt的方法。\n如果一个模型没有设置聊天模板，但其模型有默认模板，`TextGenerationPipeline`类和`apply_chat_template`等方法将使用该模型的聊天模板。\n您可以通过检查`tokenizer.default_chat_template`属性来查找`tokenizer`的默认模板。\n\n这是我们纯粹为了向前兼容性而做的事情，以避免破坏任何现有的工作流程。即使默认的聊天模板适用于您的模型，\n我们强烈建议通过显式设置`chat_template`属性来覆盖默认模板，以便向用户清楚地表明您的模型已经正确的配置了聊天模板，\n并且为了未来防范默认模板被修改或弃用的情况。\n### 我应该使用哪个模板？\n\n在为已经训练过的聊天模型设置模板时，您应确保模板与模型在训练期间看到的消息格式完全匹配，否则可能会导致性能下降。\n即使您继续对模型进行训练，也应保持聊天模板不变，这样可能会获得最佳性能。\n这与`tokenization`非常类似，在推断时，你选用跟训练时一样的`tokenization`，通常会获得最佳性能。\n\n如果您从头开始训练模型，或者在微调基础语言模型进行聊天时，您有很大的自由选择适当的模板！\nLLMs足够聪明，可以学会处理许多不同的输入格式。我们为没有特定类别模板的模型提供一个默认模板，该模板遵循\n`ChatML` format格式要求，对于许多用例来说，\n这是一个很好的、灵活的选择。\n\n默认模板看起来像这样：\n\n```\n{% for message in messages %}\n    {{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}\n{% endfor %}\n```\n\n\n如果您喜欢这个模板，下面是一行代码的模板形式，它可以直接复制到您的代码中。这一行代码还包括了[generation prompts](#什么是\"generation prompts\"?)，\n但请注意它不会添加`BOS`或`EOS`token。\n如果您的模型需要这些token，它们不会被`apply_chat_template`自动添加，换句话说，文本的默认处理参数是`add_special_tokens=False`。\n这是为了避免模板和`add_special_tokens`逻辑产生冲突，如果您的模型需要特殊tokens，请确保将它们添加到模板中！\n\n```\ntokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n```\n\n该模板将每条消息包装在`<|im_start|>`和`<|im_end|>`tokens里面，并将角色简单地写为字符串，这样可以灵活地训练角色。输出如下：\n```text\n<|im_start|>system\nYou are a helpful chatbot that will do its best not to say anything so stupid that people tweet about it.<|im_end|>\n<|im_start|>user\nHow are you?<|im_end|>\n<|im_start|>assistant\nI'm doing great!<|im_end|>\n```\n\n`user`，`system`和`assistant`是对话助手模型的标准角色，如果您的模型要与[`TextGenerationPipeline`]兼容，我们建议你使用这些角色。\n但您可以不局限于这些角色，模板非常灵活，任何字符串都可以成为角色。\n\n### 如何添加聊天模板？\n\n如果您有任何聊天模型，您应该设置它们的`tokenizer.chat_template`属性，并使用[`~PreTrainedTokenizer.apply_chat_template`]测试，\n然后将更新后的`tokenizer`推送到 Hub。\n即使您不是模型所有者，如果您正在使用一个空的聊天模板或者仍在使用默认的聊天模板，\n请发起一个[pull request](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)，以便正确设置该属性！\n\n一旦属性设置完成，就完成了！`tokenizer.apply_chat_template`现在将在该模型中正常工作，\n这意味着它也会自动支持在诸如`TextGenerationPipeline`的地方！\n\n通过确保模型具有这一属性，我们可以确保整个社区都能充分利用开源模型的全部功能。\n格式不匹配已经困扰这个领域并悄悄地损害了性能太久了，是时候结束它们了！",
    "927": "一级标题：聊天模型的模板\n二级标题：高级：模板写作技巧\n内容：\n如果你对Jinja不熟悉，我们通常发现编写聊天模板的最简单方法是先编写一个简短的Python脚本，按照你想要的方式格式化消息，然后将该脚本转换为模板。\n\n请记住，模板处理程序将接收对话历史作为名为`messages`的变量。每条`message`都是一个带有两个键`role`和`content`的字典。\n您可以在模板中像在Python中一样访问`messages`，这意味着您可以使用`{% for message in messages %}`进行循环，\n或者例如使用`{{ messages[0] }}`访问单个消息。\n\n您也可以使用以下提示将您的代码转换为Jinja：\n### For循环\n\n在Jinja中，for循环看起来像这样：\n\n```\n{% for message in messages %}\n{{ message['content'] }}\n{% endfor %}\n```\n\n请注意，`{{ expression block }}`中的内容将被打印到输出。您可以在表达式块中使用像`+`这样的运算符来组合字符串。\n### If语句\n\nJinja中的if语句如下所示：\n\n```\n{% if message['role'] == 'user' %}\n{{ message['content'] }}\n{% endif %}\n```\n注意Jinja使用`{% endfor %}`和`{% endif %}`来表示`for`和`if`的结束。\n\n### 特殊变量\n\n在您的模板中，您将可以访问`messages`列表，但您还可以访问其他几个特殊变量。\n这些包括特殊`token`，如`bos_token`和`eos_token`，以及我们上面讨论过的`add_generation_prompt`变量。\n您还可以使用`loop`变量来访问有关当前循环迭代的信息，例如使用`{% if loop.last %}`来检查当前消息是否是对话中的最后一条消息。\n\n以下是一个示例，如果`add_generation_prompt=True`需要在对话结束时添加`generate_prompt`：\n\n\n```\n{% if loop.last and add_generation_prompt %}\n{{ bos_token + 'Assistant:\\n' }}\n{% endif %}\n```\n\n### 空格的注意事项\n\n我们已经尽可能尝试让Jinja忽略除`{{ expressions }}`之外的空格。\n然而，请注意Jinja是一个通用的模板引擎，它可能会将同一行文本块之间的空格视为重要，并将其打印到输出中。\n我们**强烈**建议在上传模板之前检查一下，确保模板没有在不应该的地方打印额外的空格！",
    "928": "一级标题：社区\n二级标题：无\n内容：\n这个页面汇集了社区开发的🤗Transformers相关的资源。",
    "929": "一级标题：社区\n二级标题：社区资源\n内容：\n| 资源     |      描述      |      作者      |\n|:----------|:-------------|------:|\n| [Hugging Face Transformers Glossary Flashcards](https://www.darigovresearch.com/huggingface-transformers-glossary-flashcards) | 这是一套基于 [Transformers文档术语表](glossary) 的抽认卡，它们已被整理成可以通过 [Anki](https://apps.ankiweb.net/) （一款专为长期知识保留而设计的开源、跨平台的应用）来进行学习和复习的形式。使用方法参见： [介绍如何使用抽认卡的视频](https://www.youtube.com/watch?v=Dji_h7PILrw)。 | [Darigov Research](https://www.darigovresearch.com/) |",
    "930": "一级标题：社区\n二级标题：社区笔记本\n内容：\n| 笔记本     |      描述      |      作者      |      |\n|:----------|:-------------|:-------------|------:|\n| [Fine-tune a pre-trained Transformer to generate lyrics](https://github.com/AlekseyKorshuk/huggingartists) | 如何通过微调GPT-2模型来生成你最喜欢的艺术家风格的歌词 |  [Aleksey Korshuk](https://github.com/AlekseyKorshuk) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb) |\n| [Train T5 in Tensorflow 2](https://github.com/snapthat/TF-T5-text-to-text) | 如何使用 Tensorflow 2 训练 T5 可以完成任何任务。本笔记本演示了如何使用 SQUAD 在 Tensorflow 2 中实现问答任务 | [Muhammad Harris](https://github.com/HarrisDePerceptron) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/snapthat/TF-T5-text-to-text/blob/master/snapthatT5/notebooks/TF-T5-Datasets%20Training.ipynb) |\n| [Train T5 on TPU](https://github.com/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb)  | 如何使用 Transformers 和 Nlp 在 SQUAD 上训练 T5 | [Suraj Patil](https://github.com/patil-suraj) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=QLGiFCDqvuil) |\n| [Fine-tune T5 for Classification and Multiple Choice](https://github.com/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb)  | 如何使用 PyTorch Lightning 的text-to-text格式对 T5 进行微调以完成分类和多项选择任务 |  [Suraj Patil](https://github.com/patil-suraj) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb) |\n| [Fine-tune DialoGPT on New Datasets and Languages](https://github.com/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb)  | 如何在新数据集上微调 DialoGPT 模型，以实现开放式对话聊天机器人 |  [Nathan Cooper](https://github.com/ncoop57) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb) |\n| [Long Sequence Modeling with Reformer](https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb)  | 如何使用 Reformer 对长达 500,000 个 token 的序列进行训练 |  [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb)  |\n| [Fine-tune BART for Summarization](https://github.com/ohmeow/ohmeow_website/blob/master/posts/2021-05-25-mbart-sequence-classification-with-blurr.ipynb) | 如何使用 blurr 对 BART 进行微调，以便使用 fastai 进行汇总 | [Wayde Gilliam](https://ohmeow.com/) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ohmeow/ohmeow_website/blob/master/posts/2021-05-25-mbart-sequence-classification-with-blurr.ipynb) |\n| [Fine-tune a pre-trained Transformer on anyone's tweets](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb) | 如何通过微调 GPT-2 模型生成以你最喜欢的 Twitter 帐户风格发布的推文 |  [Boris Dayma](https://github.com/borisdayma) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb) |\n| [Optimize 🤗 Hugging Face models with Weights & Biases](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb) | 展示 W&B 与 Hugging Face 集成的完整教程 | [Boris Dayma](https://github.com/borisdayma) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb) |\n| [Pretrain Longformer](https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb)  | 如何构建现有预训练模型的“长”版本 |  [Iz Beltagy](https://beltagy.net) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb) |\n| [Fine-tune Longformer for QA](https://github.com/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb) | 如何针对问答任务微调长模型 | [Suraj Patil](https://github.com/patil-suraj) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb) |\n| [Evaluate Model with 🤗nlp](https://github.com/patrickvonplaten/notebooks/blob/master/How_to_evaluate_Longformer_on_TriviaQA_using_NLP.ipynb) | 如何使用`nlp`库在TriviaQA数据集上评估Longformer模型| [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1m7eTGlPmLRgoPkkA7rkhQdZ9ydpmsdLE?usp=sharing) |\n| [Fine-tune T5 for Sentiment Span Extraction](https://github.com/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb)  | 如何使用PyTorch Lightning以text-to-text的格式对T5进行微调，以进行情感跨度提取 |  [Lorenzo Ampil](https://github.com/enzoampil) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb) |\n| [Fine-tune DistilBert for Multiclass Classification](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb) | 如何使用 PyTorch 微调 DistilBert 进行多类分类 | [Abhishek Kumar Mishra](https://github.com/abhimishra91) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb)|\n|[Fine-tune BERT for Multi-label Classification](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb)|如何使用 PyTorch 对 BERT 进行微调以进行多标签分类|[Abhishek Kumar Mishra](https://github.com/abhimishra91) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb)|\n|[Fine-tune T5 for Summarization](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb)|如何在 PyTorch 中微调 T5 进行总结并使用 WandB 跟踪实验|[Abhishek Kumar Mishra](https://github.com/abhimishra91) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb)|\n|[Speed up Fine-Tuning in Transformers with Dynamic Padding / Bucketing](https://github.com/ELS-RD/transformers-notebook/blob/master/Divide_Hugging_Face_Transformers_training_time_by_2_or_more.ipynb)|如何通过使用动态填充/桶排序将微调速度提高两倍|[Michael Benesty](https://github.com/pommedeterresautee) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1CBfRU1zbfu7-ijiOqAAQUA-RJaxfcJoO?usp=sharing)|\n|[Pretrain Reformer for Masked Language Modeling](https://github.com/patrickvonplaten/notebooks/blob/master/Reformer_For_Masked_LM.ipynb)| 如何训练一个带有双向自注意力层的Reformer模型 | [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1tzzh0i8PgDQGV3SMFUGxM7_gGae3K-uW?usp=sharing)|\n|[Expand and Fine Tune Sci-BERT](https://github.com/lordtt13/word-embeddings/blob/master/COVID-19%20Research%20Data/COVID-SciBERT.ipynb)| 如何在 CORD 数据集上增加 AllenAI 预训练的 SciBERT 模型的词汇量，并对其进行流水线化 | [Tanmay Thakur](https://github.com/lordtt13) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1rqAR40goxbAfez1xvF3hBJphSCsvXmh8)|\n|[Fine Tune BlenderBotSmall for Summarization using the Trainer API](https://github.com/lordtt13/transformers-experiments/blob/master/Custom%20Tasks/fine-tune-blenderbot_small-for-summarization.ipynb)| 如何使用Trainer API在自定义数据集上对BlenderBotSmall进行微调以进行文本摘要 | [Tanmay Thakur](https://github.com/lordtt13) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/19Wmupuls7mykSGyRN_Qo6lPQhgp56ymq?usp=sharing)|\n|[Fine-tune Electra and interpret with Integrated Gradients](https://github.com/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb) | 如何对Electra模型进行微调以进行情感分析，并使用Captum集成梯度来解释预测结果 | [Eliza Szczechla](https://elsanns.github.io) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb)|\n|[fine-tune a non-English GPT-2 Model with Trainer class](https://github.com/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb) | 如何使用 Trainer 类微调非英语 GPT-2 模型 | [Philipp Schmid](https://www.philschmid.de) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb)|\n|[Fine-tune a DistilBERT Model for Multi Label Classification task](https://github.com/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb) | 如何针对多标签分类任务微调 DistilBERT 模型 | [Dhaval Taunk](https://github.com/DhavalTaunk08) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb)|\n|[Fine-tune ALBERT for sentence-pair classification](https://github.com/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb) | 如何针对句子对分类任务对 ALBERT 模型或其他基于 BERT 的模型进行微调 | [Nadir El Manouzi](https://github.com/NadirEM) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb)|\n|[Fine-tune Roberta for sentiment analysis](https://github.com/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb) | 如何微调 Roberta 模型进行情绪分析 | [Dhaval Taunk](https://github.com/DhavalTaunk08) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb)|\n|[Evaluating Question Generation Models](https://github.com/flexudy-pipe/qugeev) | 你的 seq2seq 转换器模型生成的问题的答案有多准确？ | [Pascal Zoleko](https://github.com/zolekode) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1bpsSqCQU-iw_5nNoRm_crPq6FRuJthq_?usp=sharing)|\n|[Classify text with DistilBERT and Tensorflow](https://github.com/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb) | 如何在 TensorFlow 中微调 DistilBERT 以进行文本分类 | [Peter Bayerle](https://github.com/peterbayerle) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb)|\n|[Leverage BERT for Encoder-Decoder Summarization on CNN/Dailymail](https://github.com/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb) | 如何在CNN/Dailymail摘要任务上使用*google-bert/bert-base-uncased*检查点对*EncoderDecoderModel*进行热启动 | [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb)|\n|[Leverage RoBERTa for Encoder-Decoder Summarization on BBC XSum](https://github.com/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb) | 如何在BBC/XSum摘要任务上使用*FacebookAI/roberta-base*检查点对共享的*EncoderDecoderModel*进行热启动 | [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb)|\n|[Fine-tune TAPAS on Sequential Question Answering (SQA)](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb) | 如何在Sequential Question Answering (SQA)数据集上使用*tapas-base*检查点对*TapasForQuestionAnswering*进行微调 | [Niels Rogge](https://github.com/nielsrogge) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb)|\n|[Evaluate TAPAS on Table Fact Checking (TabFact)](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb) | 如何结合使用 🤗 数据集和 🤗 transformers 库，使用*tapas-base-finetuned-tabfact*检查点评估经过微调的*TapasForSequenceClassification* | [Niels Rogge](https://github.com/nielsrogge) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb)|\n|[Fine-tuning mBART for translation](https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb) | 如何使用 Seq2SeqTrainer 对 mBART 进行微调以实现印地语到英语的翻译 | [Vasudev Gupta](https://github.com/vasudevgupta7) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb)|\n|[Fine-tune LayoutLM on FUNSD (a form understanding dataset)](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb) | 如何在FUNSD数据集上对*LayoutLMForTokenClassification*进行微调以从扫描文档中提取信息 | [Niels Rogge](https://github.com/nielsrogge) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb)|\n|[Fine-Tune DistilGPT2 and Generate Text](https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb) | 如何微调 DistilGPT2 并生成文本 | [Aakash Tripathi](https://github.com/tripathiaakash) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb)|\n|[Fine-Tune LED on up to 8K tokens](https://github.com/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb) | 如何对LED模型在PubMed数据集上进行微调以进行长文本摘要 | [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb)|\n|[Evaluate LED on Arxiv](https://github.com/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb) | 如何有效评估LED模型的长远发展 | [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb)|\n|[Fine-tune LayoutLM on RVL-CDIP (a document image classification dataset)](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb) | 如何在 RVL-CDIP 数据集上微调*LayoutLMForSequenceClassification*以进行扫描文档分类 | [Niels Rogge](https://github.com/nielsrogge) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb)|\n|[Wav2Vec2 CTC decoding with GPT2 adjustment](https://github.com/voidful/huggingface_notebook/blob/main/xlsr_gpt.ipynb) | 如何通过语言模型调整解码 CTC 序列 | [Eric Lam](https://github.com/voidful) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1e_z5jQHYbO2YKEaUgzb1ww1WwiAyydAj?usp=sharing)|\n|[Fine-tune BART for summarization in two languages with Trainer class](https://github.com/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb) | 如何使用Trainer类对BART模型进行多语言摘要任务的微调 | [Eliza Szczechla](https://github.com/elsanns) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb)|\n|[Evaluate Big Bird on Trivia QA](https://github.com/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb) | 评估BigBird模型在长文档问答任务上的性能，特别是在Trivia QA数据集上| [Patrick von Platen](https://github.com/patrickvonplaten) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb)|\n| [Create video captions using Wav2Vec2](https://github.com/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb) | 如何使用Wav2Vec对任何视频的音频进行转录以创建YouTube字幕 | [Niklas Muennighoff](https://github.com/Muennighoff) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb) |\n| [Fine-tune the Vision Transformer on CIFAR-10 using PyTorch Lightning](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb) | 如何使用HuggingFace的Transformers、Datasets和PyTorch Lightning在CIFAR-10数据集上对Vision Transformer（ViT）进行微调 | [Niels Rogge](https://github.com/nielsrogge) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb) |\n| [Fine-tune the Vision Transformer on CIFAR-10 using the 🤗 Trainer](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb) | 如何使用HuggingFace的Transformers、Datasets和🤗 Trainer在CIFAR-10数据集上对Vision Transformer（ViT）进行微调| [Niels Rogge](https://github.com/nielsrogge) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb) |\n| [Evaluate LUKE on Open Entity, an entity typing dataset](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb) | 如何在开放实体数据集上评估*LukeForEntityClassification*| [Ikuya Yamada](https://github.com/ikuyamada) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb) |\n| [Evaluate LUKE on TACRED, a relation extraction dataset](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb) | 如何在 TACRED 数据集上评估*LukeForEntityPairClassification* | [Ikuya Yamada](https://github.com/ikuyamada) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb) |\n| [Evaluate LUKE on CoNLL-2003, an important NER benchmark](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb) | 如何在 CoNLL-2003 数据集上评估*LukeForEntitySpanClassification* | [Ikuya Yamada](https://github.com/ikuyamada) |[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb) |\n| [Evaluate BigBird-Pegasus on PubMed dataset](https://github.com/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb) | 如何在 PubMed 数据集上评估*BigBirdPegasusForConditionalGeneration*| [Vasudev Gupta](https://github.com/vasudevgupta7) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb) |\n| [Speech Emotion Classification with Wav2Vec2](https://github.com/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb) |如何利用预训练的 Wav2Vec2 模型在 MEGA 数据集上进行情绪分类| [Mehrdad Farahani](https://github.com/m3hrdadfi) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb) |\n| [Detect objects in an image with DETR](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_(with_DetrFeatureExtractor).ipynb) | 如何使用经过训练的*DetrForObjectDetection*模型检测图像中的物体并可视化注意力 | [Niels Rogge](https://github.com/NielsRogge) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_(with_DetrFeatureExtractor).ipynb) |\n| [Fine-tune DETR on a custom object detection dataset](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb) | 如何在自定义对象检测数据集上微调*DetrForObjectDetection* | [Niels Rogge](https://github.com/NielsRogge) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb) |\n| [Finetune T5 for Named Entity Recognition](https://github.com/ToluClassics/Notebooks/blob/main/T5_Ner_Finetuning.ipynb) | 如何在命名实体识别任务中微调*T5*| [Ogundepo Odunayo](https://github.com/ToluClassics) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1obr78FY_cBmWY5ODViCmzdY6O1KB65Vc?usp=sharing) |\n| [Fine-Tuning Open-Source LLM using QLoRA with MLflow and PEFT](https://github.com/mlflow/mlflow/blob/master/docs/source/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb) | 如何使用[QLoRA](https://github.com/artidoro/qlora) 和[PEFT](https://huggingface.co/docs/peft/en/index)以内存高效的方式微调大型语言模型（LLM），同时使用 [MLflow](https://mlflow.org/docs/latest/llms/transformers/index.html)进行实验跟踪| [Yuki Watanabe](https://github.com/B-Step62) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mlflow/mlflow/blob/master/docs/source/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb) |",
    "931": "一级标题：为 🤗 Transformers 做贡献\n二级标题：无\n内容：\n欢迎所有人为 🤗 Transformers 做出贡献，我们重视每个人的贡献。代码贡献并不是帮助社区的唯一途径。回答问题、帮助他人和改进文档也非常有价值。\n\n宣传 🤗 Transformers 也会帮助我们！比如在博客文章里介绍一下这个库是如何帮助你完成了很棒的项目，每次它帮助你时都在 Twitter 上大声宣传，或者给这个代码仓库点⭐️来表示感谢。\n\n无论你选择以哪种方式做出贡献，请注意并尊重我们的[行为准则](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)。\n\n**本指南的灵感来源于 [scikit-learn贡献指南](https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md) ，它令人印象深刻.**",
    "932": "一级标题：为 🤗 Transformers 做贡献\n二级标题：做贡献的方法\n内容：\n有多种方法可以为 🤗 Transformers 做贡献：\n\n* 修复现有代码中尚未解决的问题。\n* 提交与 bug 或所需新功能相关的 issue。\n* 实现新的模型。\n* 为示例或文档做贡献。\n\n如果你不知道从哪里开始，有一个特别的 [Good First Issue](https://github.com/huggingface/transformers/contribute) 列表。它会列出一些适合初学者的开放的 issues，并帮助你开始为开源项目做贡献。只需要在你想要处理的 issue 下发表评论就行。\n\n如果想要稍微更有挑战性的内容，你也可以查看 [Good Second Issue](https://github.com/huggingface/transformers/labels/Good%20Second%20Issue) 列表。总的来说，如果你觉得自己知道该怎么做，就去做吧，我们会帮助你达到目标的！🚀\n\n> 所有的贡献对社区来说都同样宝贵。🥰",
    "933": "一级标题：为 🤗 Transformers 做贡献\n二级标题：修复尚未解决的问题\n内容：\n如果你发现现有代码中存在问题，并且已经想到了解决方法，请随时[开始贡献](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md/#create-a-pull-request) 并创建一个 Pull Request！",
    "934": "一级标题：为 🤗 Transformers 做贡献\n二级标题：提交与 bug 相关的 issue 或功能请求\n内容：\n在提交与错误相关的 issue 或功能请求时，请尽量遵循下面的指南。这能让我们更容易迅速回复你，并提供良好的反馈意见。\n\n### 你发现了 bug 吗？\n\n🤗 Transformers 之所以强大可靠，要感谢用户报告了他们遇到的问题。\n\n在提出issue之前，请你**确认该 bug 尚未被报告**（使用 GitHub 的 Issues 下面的搜索栏）。issue 也应该是与库本身的 bug 有关，而不是与你的代码有关。如果不确定 bug 是在你的代码中还是在库中，请先在[论坛](https://discuss.huggingface.co/)中询问。这有助于我们更快地解决与库相关的问题。\n\n一旦你确认该 bug 尚未被报告，请在你的 issue 中包含以下信息，以便我们快速解决：\n\n* 使用的**操作系统类型和版本**，以及 **Python**、**PyTorch** 和 **TensorFlow** 的版本。\n* 一个简短、独立的代码片段，可以让我们在不到30秒内重现这个问题。\n* 如果发生异常，请提供*完整的* traceback。\n* 附上你认为可能有帮助的任何其他附加信息，如屏幕截图。\n\n想要自动获取操作系统和软件版本，请运行以下命令：\n\n```bash\ntransformers env\n```\n\n你也可以从代码仓库的根目录下运行相同的命令：\n\n```bash\npython src/transformers/commands/transformers_cli.py env\n```\n\n### 你想要新功能吗？\n\n如果你希望在 🤗 Transformers 中看到新功能，请提出一个 issue 并包含以下内容：\n\n1. 这个新功能的*动机*是什么呢？是因为使用这个库时遇到了问题或者感到了某种不满吗？是因为你的项目需要这个功能吗？或者是你自己开发了某项内容，并且认为它可能会对社区有所帮助？\n\n   不管是什么，我们都很想听！\n\n2. 请尽可能详细地描述你想要的功能。你告诉我们的越多，我们就能更好地帮助你。\n3. 请提供一个*代码片段*，演示该功能的使用方法。\n4. 如果这个功能与某篇论文相关，请包含链接。\n\n如果你描述得足够清晰，那么在你创建 issue 时，我们已经完成了80%的工作。\n\n我们已经添加了[模板](https://github.com/huggingface/transformers/tree/main/templates)，可能有助于你提出 issue。",
    "935": "一级标题：为 🤗 Transformers 做贡献\n二级标题：你想要实现一个新模型吗？\n内容：\n我们会持续发布新模型，如果你想要实现一个新模型，请提供以下信息:\n\n* 模型的简要描述和论文链接。\n* 如果实现是开源的，请提供实现的链接。\n* 如果模型权重可用，请提供模型权重的链接。\n\n如果你想亲自贡献模型，请告诉我们。让我们帮你把它添加到 🤗 Transformers！\n\n我们还有一个更技术性的指南，告诉你[如何将模型添加到 🤗 Transformers](https://huggingface.co/docs/transformers/add_new_model)。",
    "936": "一级标题：为 🤗 Transformers 做贡献\n二级标题：你想要添加文档吗？\n内容：\n我们始终在寻求改进文档，使其更清晰准确。请告诉我们如何改进文档，比如拼写错误以及任何缺失、不清楚或不准确的内容。我们非常乐意进行修改，如果你有兴趣，我们也可以帮助你做出贡献！\n\n有关如何生成、构建和编写文档的更多详细信息，请查看文档 [README](https://github.com/huggingface/transformers/tree/main/docs)。",
    "937": "一级标题：为 🤗 Transformers 做贡献\n二级标题：创建 Pull Request\n内容：\n在开始编写任何代码之前，我们强烈建议你先搜索现有的 PR(Pull Request) 或 issue，以确保没有其他人已经在做同样的事情。如果你不确定，提出 issue 来获取反馈意见是一个好办法。\n\n要为 🤗 Transformers 做贡献，你需要基本的 `git` 使用技能。虽然 `git` 不是一个很容易使用的工具，但它提供了非常全面的手册，在命令行中输入 `git --help` 并享受吧！如果你更喜欢书籍，[Pro Git](https://git-scm.com/book/en/v2)是一本很好的参考书。\n\n要为 🤗 Transformers 做贡献，你需要 **[Python 3.9](https://github.com/huggingface/transformers/blob/main/setup.py#L426)** 或更高版本。请按照以下步骤开始贡献：\n\n1. 点击[仓库](https://github.com/huggingface/transformers)页面上的 **[Fork](https://github.com/huggingface/transformers/fork)** 按钮，这会在你的 GitHub 账号下拷贝一份代码。\n\n2. 把派生仓库克隆到本地磁盘，并将基础仓库添加为远程仓库：\n\n   ```bash\n   git clone git@github.com:<your Github handle>/transformers.git\n   cd transformers\n   git remote add upstream https://github.com/huggingface/transformers.git\n   ```\n\n3. 创建一个新的分支来保存你的更改：\n\n   ```bash\n   git checkout -b a-descriptive-name-for-my-changes\n   ```\n\n   🚨 **不要**在 `main` 分支工作!\n\n4. 在虚拟环境中运行以下命令来设置开发环境：\n\n   ```bash\n   pip install -e \".[dev]\"\n   ```\n\n   如果在虚拟环境中已经安装了 🤗 Transformers，请先使用 `pip uninstall transformers` 卸载它，然后再用 `-e` 参数以可编辑模式重新安装。\n\n   根据你的操作系统，以及 Transformers 的可选依赖项数量的增加，可能会在执行此命令时出现失败。如果出现这种情况，请确保已经安装了你想使用的深度学习框架（PyTorch, TensorFlow 和 Flax），然后执行以下操作：\n\n   ```bash\n   pip install -e \".[quality]\"\n   ```\n\n   大多数情况下，这些应该够用了。\n\n5. 在你的分支上开发相关功能。\n\n   在编写代码时，请确保测试套件通过。用下面的方式运行受你的更改影响的测试：\n\n   ```bash\n   pytest tests/<TEST_TO_RUN>.py\n   ```\n\n   想了解更多关于测试的信息，请阅读[测试](https://huggingface.co/docs/transformers/testing)指南。\n\n   🤗 Transformers 使用 `black` 和 `ruff` 来保持代码风格的一致性。进行更改后，使用以下命令自动执行格式更正和代码验证：\n\n   ```bash\n   make fixup\n   ```\n\n   它已经被优化为仅适用于你创建的 PR 所修改过的文件。\n\n   如果想要逐个运行检查，可以使用以下命令：\n\n   ```bash\n   make style\n   ```\n\n   🤗 Transformers 还使用了 `ruff` 和一些自定义脚本来检查编码错误。虽然质量管理是通过 CI 进行的，但你也可以使用以下命令来运行相同的检查：\n\n   ```bash\n   make quality\n   ```\n\n   最后，我们有许多脚本来确保在添加新模型时不会忘记更新某些文件。你可以使用以下命令运行这些脚本：\n\n   ```bash\n   make repo-consistency\n   ```\n\n   想要了解有关这些检查及如何解决相关问题的更多信息，请阅读 [检查 Pull Request](https://huggingface.co/docs/transformers/pr_checks) 指南。\n\n   如果你修改了 `docs/source` 目录下的文档，请确保文档仍然能够被构建。这个检查也会在你创建 PR 时在 CI 中运行。如果要进行本地检查，请确保安装了文档构建工具：\n\n   ```bash\n   pip install \".[docs]\"\n   ```\n\n   在仓库的根目录下运行以下命令：\n\n   ```bash\n   doc-builder build transformers docs/source/en --build_dir ~/tmp/test-build\n   ```\n\n   这将会在 `~/tmp/test-build` 文件夹中构建文档，你可以使用自己喜欢的编辑器查看生成的 Markdown 文件。当你创建 PR 时，也可以在GitHub上预览文档。\n\n   当你对修改满意后，使用 `git add` 把修改的文件添加到暂存区，然后使用 `git commit` 在本地记录你的更改:\n\n   ```bash\n   git add modified_file.py\n   git commit\n   ```\n\n   请记得写一个[好的提交信息](https://chris.beams.io/posts/git-commit/)来清晰地传达你所做的更改！\n\n   为了保持你的代码副本与原始仓库的最新状态一致，在你创建 PR *之前*或者在管理员要求的情况下，把你的分支在 `upstream/branch` 上进行 rebase：\n\n   ```bash\n   git fetch upstream\n   git rebase upstream/main\n   ```\n\n   把你的更改推送到你的分支：\n\n   ```bash\n   git push -u origin a-descriptive-name-for-my-changes\n   ```\n\n   如果你已经创建了一个 PR，你需要使用 `--force` 参数进行强制推送。如果 PR 还没有被创建，你可以正常推送你的更改。\n\n6. 现在你可以转到 GitHub 上你的账号下的派生仓库，点击 **Pull Request** 来创建一个 PR。 请确保勾选我们 [checklist](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md/#pull-request-checklist) 下的所有项目。准备好这些后，可以将你的更改发送给项目管理员进行审查。\n\n7. 如果管理员要求你进行更改，别气馁，我们的核心贡献者也会经历相同的事情！请在你的本地分支上进行工作，并将更改推送到派生仓库，以便于每个人都可以在 PR 中看到你的更改。这样它们会自动出现在 PR 中。\n\n### Pull request 的检查清单\n\n☐ Pull request 的标题应该总结你的贡献内容。<br>\n☐ 如果你的 Pull request 解决了一个issue，请在 Pull request 描述中提及该 issue 的编号，以确保它们被关联起来（这样查看 issue 的人就知道你正在处理它）。<br>\n☐ 如果是正在进行中的工作，请在标题前加上 [WIP]。这有助于避免重复工作和区分哪些 PR 可以合并。<br>\n☐ 确保可以通过现有的测试。<br>\n☐ 如果添加了新功能，请同时添加对应的测试。<br>\n   - 如果添加一个新模型，请使用 `ModelTester.all_model_classes = (MyModel, MyModelWithLMHead,...)` 来触发通用测试。\n   - 如果你正在添加新的 `@slow` 测试，请确保通过以下检查：`RUN_SLOW=1 python -m pytest tests/models/my_new_model/test_my_new_model.py`\n   - 如果你正在添加一个新的分词器，请编写测试并确保通过以下检查：`RUN_SLOW=1 python -m pytest tests/models/{your_model_name}/test_tokenization_{your_model_name}.py`\n   - CircleCI 不会运行时间较长的测试，但 GitHub Actions 每晚会运行所有测试！<br>\n\n☐ 所有公共 method 必须具有信息文档（比如 [`modeling_bert.py`](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py)）。<br>\n☐ 由于代码仓库的体积正在迅速增长，请避免添加图像、视频和其他非文本文件，它们会增加仓库的负担。请使用 [`hf-internal-testing`](https://huggingface.co/hf-internal-testing) 等 Hub 仓库来托管这些文件，并通过 URL 引用它们。我们建议将与文档相关的图片放置在以下仓库中：[huggingface/documentation-images](https://huggingface.co/datasets/huggingface/documentation-images)。你可以在这个数据集仓库上创建一个 PR，并请求 Hugging Face 成员进行合并。\n\n要了解更多有关在 Pull request 上运行的检查的信息，请查看我们的 [检查 Pull Request](https://huggingface.co/docs/transformers/pr_checks) 指南。\n\n### 测试\n\n包含了广泛的测试套件来测试库的行为和一些示例。库测试可以在 [tests](https://github.com/huggingface/transformers/tree/main/tests) 文件夹中找到，示例测试可以在 [examples](https://github.com/huggingface/transformers/tree/main/examples) 文件夹中找到。\n\n我们喜欢使用 `pytest` 和 `pytest-xdist`，因为它运行更快。在仓库的根目录，指定一个*子文件夹的路径或测试文件*来运行测试：\n\n```bash\npython -m pytest -n auto --dist=loadfile -s -v ./tests/models/my_new_model\n```\n\n同样地，在 `examples` 目录，指定一个*子文件夹的路径或测试文件* 来运行测试。例如，以下命令会测试 PyTorch `examples` 目录中的文本分类子文件夹：\n\n```bash\npip install -r examples/xxx/requirements.txt  # 仅在第一次需要\npython -m pytest -n auto --dist=loadfile -s -v ./examples/pytorch/text-classification\n```\n\n实际上这就是我们的 `make test` 和 `make test-examples` 命令的实现方式（不包括 `pip install`）！\n\n你也可以指定一个较小的测试集来仅测试特定功能。\n\n默认情况下，会跳过时间较长的测试，但你可以将 `RUN_SLOW` 环境变量设置为 `yes` 来运行它们。这将下载以 GB 为单位的模型文件，所以确保你有足够的磁盘空间、良好的网络连接和足够的耐心！\n\n<Tip warning={true}>\n\n记得指定一个*子文件夹的路径或测试文件*来运行测试。否则你将会运行 `tests` 或 `examples` 文件夹中的所有测试，它会花费很长时间！\n\n</Tip>\n\n```bash\nRUN_SLOW=yes python -m pytest -n auto --dist=loadfile -s -v ./tests/models/my_new_model\nRUN_SLOW=yes python -m pytest -n auto --dist=loadfile -s -v ./examples/pytorch/text-classification\n```\n\n和时间较长的测试一样，还有其他环境变量在测试过程中，在默认情况下是未启用的：\n- `RUN_CUSTOM_TOKENIZERS`: 启用自定义分词器的测试。\n\n更多环境变量和额外信息可以在 [testing_utils.py](src/transformers/testing_utils.py) 中找到。\n\n🤗 Transformers 只是使用 `pytest` 作为测试运行程序，但测试套件本身没用任何与 `pytest` 相关的功能。\n\n这意味着完全支持 `unittest` 。以下是如何使用 `unittest` 运行测试的方法：\n\n```bash\npython -m unittest discover -s tests -t . -v\npython -m unittest discover -s examples -t examples -v\n```\n\n### 风格指南\n\n🤗 Transformers 的文档遵循 [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html)。请查看我们的 [文档编写指南](https://github.com/huggingface/transformers/tree/main/docs#writing-documentation---specification) 来获取更多信息。\n\n### 在 Windows 上开发\n\n在 Windows 上（除非你正在使用 [Windows Subsystem for Linux](https://learn.microsoft.com/en-us/windows/wsl/) 或 WSL），你需要配置 git 将 Windows 的 `CRLF` 行结束符转换为 Linux 的 `LF` 行结束符：\n\n```bash\ngit config core.autocrlf input\n```\n\n在 Windows 上有一种方法可以运行 `make` 命令，那就是使用 MSYS2：\n\n1. [下载 MSYS2](https://www.msys2.org/)，假设已经安装在 `C:\\msys64`。\n2. 从命令行打开 `C:\\msys64\\msys2.exe` （可以在 **开始** 菜单中找到）。\n3. 在 shell 中运行： `pacman -Syu` ，并使用 `pacman -S make` 安装 `make`。\n4. 把 `C:\\msys64\\usr\\bin` 添加到你的 PATH 环境变量中。\n\n现在你可以在任何终端（PowerShell、cmd.exe 等）中使用 `make` 命令了！ 🎉\n\n### 将派生仓库与上游主仓库（Hugging Face 仓库）同步\n\n更新派生仓库的主分支时，请按照以下步骤操作。这是为了避免向每个上游 PR 添加参考注释，同时避免向参与这些 PR 的开发人员发送不必要的通知。\n\n1. 可以的话，请避免使用派生仓库上的分支和 PR 来与上游进行同步，而是直接合并到派生仓库的主分支。\n2. 如果确实需要一个 PR，在检查你的分支后，请按照以下步骤操作：\n\n   ```bash\n   git checkout -b your-branch-for-syncing\n   git pull --squash --no-commit upstream main\n   git commit -m '<your message without GitHub references>'\n   git push --set-upstream origin your-branch-for-syncing\n   ```",
    "938": "一级标题：创建自定义架构\n二级标题：无\n内容：\n[`AutoClass`](model_doc/auto) 自动推断模型架构并下载预训练的配置和权重。一般来说，我们建议使用 `AutoClass` 生成与检查点（checkpoint）无关的代码。希望对特定模型参数有更多控制的用户，可以仅从几个基类创建自定义的 🤗 Transformers 模型。这对于任何有兴趣学习、训练或试验 🤗 Transformers 模型的人可能特别有用。通过本指南，深入了解如何不通过 `AutoClass` 创建自定义模型。了解如何：\n\n- 加载并自定义模型配置。\n- 创建模型架构。\n- 为文本创建慢速和快速分词器。\n- 为视觉任务创建图像处理器。\n- 为音频任务创建特征提取器。\n- 为多模态任务创建处理器。",
    "939": "一级标题：创建自定义架构\n二级标题：配置\n内容：\n[配置](main_classes/configuration) 涉及到模型的具体属性。每个模型配置都有不同的属性；例如，所有 NLP 模型都共享 `hidden_size`、`num_attention_heads`、 `num_hidden_layers` 和 `vocab_size` 属性。这些属性用于指定构建模型时的注意力头数量或隐藏层层数。\n\n访问 [`DistilBertConfig`] 以更近一步了解 [DistilBERT](model_doc/distilbert)，检查它的属性：\n\n```py\n>>> from transformers import DistilBertConfig\n\n>>> config = DistilBertConfig()\n>>> print(config)\nDistilBertConfig {\n  \"activation\": \"gelu\",\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"transformers_version\": \"4.16.2\",\n  \"vocab_size\": 30522\n}\n```\n\n[`DistilBertConfig`] 显示了构建基础 [`DistilBertModel`] 所使用的所有默认属性。所有属性都可以进行自定义，为实验创造了空间。例如，您可以将默认模型自定义为：\n\n- 使用 `activation` 参数尝试不同的激活函数。\n- 使用 `attention_dropout` 参数为 attention probabilities 使用更高的 dropout ratio。\n\n```py\n>>> my_config = DistilBertConfig(activation=\"relu\", attention_dropout=0.4)\n>>> print(my_config)\nDistilBertConfig {\n  \"activation\": \"relu\",\n  \"attention_dropout\": 0.4,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"hidden_dim\": 3072,\n  \"initializer_range\": 0.02,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"transformers_version\": \"4.16.2\",\n  \"vocab_size\": 30522\n}\n```\n\n预训练模型的属性可以在 [`~PretrainedConfig.from_pretrained`] 函数中进行修改：\n\n```py\n>>> my_config = DistilBertConfig.from_pretrained(\"distilbert/distilbert-base-uncased\", activation=\"relu\", attention_dropout=0.4)\n```\n\n当你对模型配置满意时，可以使用 [`~PretrainedConfig.save_pretrained`] 来保存配置。你的配置文件将以 JSON 文件的形式存储在指定的保存目录中：\n\n```py\n>>> my_config.save_pretrained(save_directory=\"./your_model_save_path\")\n```\n\n要重用配置文件，请使用 [`~PretrainedConfig.from_pretrained`] 进行加载：\n\n```py\n>>> my_config = DistilBertConfig.from_pretrained(\"./your_model_save_path/config.json\")\n```\n\n<Tip>\n\n你还可以将配置文件保存为字典，甚至只保存自定义配置属性与默认配置属性之间的差异！有关更多详细信息，请参阅 [配置](main_classes/configuration) 文档。\n\n</Tip>",
    "940": "一级标题：创建自定义架构\n二级标题：模型\n内容：\n接下来，创建一个[模型](main_classes/models)。模型，也可泛指架构，定义了每一层网络的行为以及进行的操作。配置中的 `num_hidden_layers` 等属性用于定义架构。每个模型都共享基类 [`PreTrainedModel`] 和一些常用方法，例如调整输入嵌入的大小和修剪自注意力头。此外，所有模型都是 [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)、[`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) 或 [`flax.linen.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html) 的子类。这意味着模型与各自框架的用法兼容。\n\n<frameworkcontent>\n<pt>\n将自定义配置属性加载到模型中：\n\n```py\n>>> from transformers import DistilBertModel\n\n>>> my_config = DistilBertConfig.from_pretrained(\"./your_model_save_path/config.json\")\n>>> model = DistilBertModel(my_config)\n```\n\n这段代码创建了一个具有随机参数而不是预训练权重的模型。在训练该模型之前，您还无法将该模型用于任何用途。训练是一项昂贵且耗时的过程。通常来说，最好使用预训练模型来更快地获得更好的结果，同时仅使用训练所需资源的一小部分。\n\n使用 [`~PreTrainedModel.from_pretrained`] 创建预训练模型：\n\n```py\n>>> model = DistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n```\n\n当加载预训练权重时，如果模型是由 🤗 Transformers 提供的，将自动加载默认模型配置。然而，如果你愿意，仍然可以将默认模型配置的某些或者所有属性替换成你自己的配置：\n\n```py\n>>> model = DistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\", config=my_config)\n```\n</pt>\n<tf>\n将自定义配置属性加载到模型中：\n\n```py\n>>> from transformers import TFDistilBertModel\n\n>>> my_config = DistilBertConfig.from_pretrained(\"./your_model_save_path/my_config.json\")\n>>> tf_model = TFDistilBertModel(my_config)\n```\n\n这段代码创建了一个具有随机参数而不是预训练权重的模型。在训练该模型之前，您还无法将该模型用于任何用途。训练是一项昂贵且耗时的过程。通常来说，最好使用预训练模型来更快地获得更好的结果，同时仅使用训练所需资源的一小部分。\n\n使用 [`~TFPreTrainedModel.from_pretrained`] 创建预训练模型：\n\n```py\n>>> tf_model = TFDistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n```\n\n当加载预训练权重时，如果模型是由 🤗 Transformers 提供的，将自动加载默认模型配置。然而，如果你愿意，仍然可以将默认模型配置的某些或者所有属性替换成自己的配置：\n\n```py\n>>> tf_model = TFDistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\", config=my_config)\n```\n</tf>\n</frameworkcontent>\n\n### 模型头（Model heads）\n\n此时，你已经有了一个输出*隐藏状态*的基础 DistilBERT 模型。隐藏状态作为输入传递到模型头以生成最终输出。🤗 Transformers 为每个任务提供不同的模型头，只要模型支持该任务（即，您不能使用 DistilBERT 来执行像翻译这样的序列到序列任务）。\n\n<frameworkcontent>\n<pt>\n例如，[`DistilBertForSequenceClassification`] 是一个带有序列分类头（sequence classification head）的基础 DistilBERT 模型。序列分类头是池化输出之上的线性层。\n\n```py\n>>> from transformers import DistilBertForSequenceClassification\n\n>>> model = DistilBertForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n```\n\n通过切换到不同的模型头，可以轻松地将此检查点重复用于其他任务。对于问答任务，你可以使用 [`DistilBertForQuestionAnswering`] 模型头。问答头（question answering head）与序列分类头类似，不同点在于它是隐藏状态输出之上的线性层。\n\n```py\n>>> from transformers import DistilBertForQuestionAnswering\n\n>>> model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")\n```\n</pt>\n<tf>\n例如，[`TFDistilBertForSequenceClassification`] 是一个带有序列分类头（sequence classification head）的基础 DistilBERT 模型。序列分类头是池化输出之上的线性层。\n\n```py\n>>> from transformers import TFDistilBertForSequenceClassification\n\n>>> tf_model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n```\n\n通过切换到不同的模型头,可以轻松地将此检查点重复用于其他任务。对于问答任务，你可以使用 [`TFDistilBertForQuestionAnswering`] 模型头。问答头（question answering head）与序列分类头类似，不同点在于它是隐藏状态输出之上的线性层。\n\n```py\n>>> from transformers import TFDistilBertForQuestionAnswering\n\n>>> tf_model = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")\n```\n</tf>\n</frameworkcontent>",
    "941": "一级标题：创建自定义架构\n二级标题：分词器\n内容：\n在将模型用于文本数据之前，你需要的最后一个基类是 [tokenizer](main_classes/tokenizer)，它用于将原始文本转换为张量。🤗 Transformers 支持两种类型的分词器：\n\n- [`PreTrainedTokenizer`]：分词器的Python实现\n- [`PreTrainedTokenizerFast`]：来自我们基于 Rust 的 [🤗 Tokenizer](https://huggingface.co/docs/tokenizers/python/latest/) 库的分词器。因为其使用了 Rust 实现，这种分词器类型的速度要快得多，尤其是在批量分词（batch tokenization）的时候。快速分词器还提供其他的方法，例如*偏移映射（offset mapping）*，它将标记（token）映射到其原始单词或字符。\n\n这两种分词器都支持常用的方法，如编码和解码、添加新标记以及管理特殊标记。\n\n<Tip warning={true}>\n\n并非每个模型都支持快速分词器。参照这张 [表格](index#supported-frameworks) 查看模型是否支持快速分词器。\n\n</Tip>\n\n如果您训练了自己的分词器，则可以从*词表*文件创建一个分词器：\n\n```py\n>>> from transformers import DistilBertTokenizer\n\n>>> my_tokenizer = DistilBertTokenizer(vocab_file=\"my_vocab_file.txt\", do_lower_case=False, padding_side=\"left\")\n```\n\n请务必记住，自定义分词器生成的词表与预训练模型分词器生成的词表是不同的。如果使用预训练模型，则需要使用预训练模型的词表，否则输入将没有意义。 使用 [`DistilBertTokenizer`] 类创建具有预训练模型词表的分词器：\n\n```py\n>>> from transformers import DistilBertTokenizer\n\n>>> slow_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n```\n\n使用 [`DistilBertTokenizerFast`] 类创建快速分词器：\n\n```py\n>>> from transformers import DistilBertTokenizerFast\n\n>>> fast_tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert/distilbert-base-uncased\")\n```\n\n<Tip>\n\n默认情况下，[`AutoTokenizer`] 将尝试加载快速标记生成器。你可以通过在 `from_pretrained` 中设置 `use_fast=False` 以禁用此行为。\n\n</Tip>",
    "942": "一级标题：创建自定义架构\n二级标题：图像处理器\n内容：\n图像处理器用于处理视觉输入。它继承自 [`~image_processing_utils.ImageProcessingMixin`] 基类。\n\n要使用它，需要创建一个与你使用的模型关联的图像处理器。例如，如果你使用 [ViT](model_doc/vit) 进行图像分类，可以创建一个默认的 [`ViTImageProcessor`]：\n\n```py\n>>> from transformers import ViTImageProcessor\n\n>>> vit_extractor = ViTImageProcessor()\n>>> print(vit_extractor)\nViTImageProcessor {\n  \"do_normalize\": true,\n  \"do_resize\": true,\n  \"image_processor_type\": \"ViTImageProcessor\",\n  \"image_mean\": [\n    0.5,\n    0.5,\n    0.5\n  ],\n  \"image_std\": [\n    0.5,\n    0.5,\n    0.5\n  ],\n  \"resample\": 2,\n  \"size\": 224\n}\n```\n\n<Tip>\n\n如果您不需要进行任何自定义，只需使用 `from_pretrained` 方法加载模型的默认图像处理器参数。\n\n</Tip>\n\n修改任何 [`ViTImageProcessor`] 参数以创建自定义图像处理器：\n\n```py\n>>> from transformers import ViTImageProcessor\n\n>>> my_vit_extractor = ViTImageProcessor(resample=\"PIL.Image.BOX\", do_normalize=False, image_mean=[0.3, 0.3, 0.3])\n>>> print(my_vit_extractor)\nViTImageProcessor {\n  \"do_normalize\": false,\n  \"do_resize\": true,\n  \"image_processor_type\": \"ViTImageProcessor\",\n  \"image_mean\": [\n    0.3,\n    0.3,\n    0.3\n  ],\n  \"image_std\": [\n    0.5,\n    0.5,\n    0.5\n  ],\n  \"resample\": \"PIL.Image.BOX\",\n  \"size\": 224\n}\n```",
    "943": "一级标题：创建自定义架构\n二级标题：特征提取器\n内容：\n特征提取器用于处理音频输入。它继承自 [`~feature_extraction_utils.FeatureExtractionMixin`] 基类，亦可继承 [`SequenceFeatureExtractor`] 类来处理音频输入。\n\n要使用它，创建一个与你使用的模型关联的特征提取器。例如，如果你使用 [Wav2Vec2](model_doc/wav2vec2) 进行音频分类，可以创建一个默认的 [`Wav2Vec2FeatureExtractor`]：\n\n```py\n>>> from transformers import Wav2Vec2FeatureExtractor\n\n>>> w2v2_extractor = Wav2Vec2FeatureExtractor()\n>>> print(w2v2_extractor)\nWav2Vec2FeatureExtractor {\n  \"do_normalize\": true,\n  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n  \"feature_size\": 1,\n  \"padding_side\": \"right\",\n  \"padding_value\": 0.0,\n  \"return_attention_mask\": false,\n  \"sampling_rate\": 16000\n}\n```\n\n<Tip>\n\n如果您不需要进行任何自定义，只需使用 `from_pretrained` 方法加载模型的默认特征提取器参数。\n\n</Tip>\n\n修改任何 [`Wav2Vec2FeatureExtractor`] 参数以创建自定义特征提取器：\n\n```py\n>>> from transformers import Wav2Vec2FeatureExtractor\n\n>>> w2v2_extractor = Wav2Vec2FeatureExtractor(sampling_rate=8000, do_normalize=False)\n>>> print(w2v2_extractor)\nWav2Vec2FeatureExtractor {\n  \"do_normalize\": false,\n  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n  \"feature_size\": 1,\n  \"padding_side\": \"right\",\n  \"padding_value\": 0.0,\n  \"return_attention_mask\": false,\n  \"sampling_rate\": 8000\n}\n```",
    "944": "一级标题：创建自定义架构\n二级标题：处理器\n内容：\n对于支持多模式任务的模型，🤗 Transformers 提供了一个处理器类，可以方便地将特征提取器和分词器等处理类包装到单个对象中。例如，让我们使用 [`Wav2Vec2Processor`] 来执行自动语音识别任务 (ASR)。 ASR 将音频转录为文本，因此您将需要一个特征提取器和一个分词器。\n\n创建一个特征提取器来处理音频输入：\n\n```py\n>>> from transformers import Wav2Vec2FeatureExtractor\n\n>>> feature_extractor = Wav2Vec2FeatureExtractor(padding_value=1.0, do_normalize=True)\n```\n\n创建一个分词器来处理文本输入：\n\n```py\n>>> from transformers import Wav2Vec2CTCTokenizer\n\n>>> tokenizer = Wav2Vec2CTCTokenizer(vocab_file=\"my_vocab_file.txt\")\n```\n\n将特征提取器和分词器合并到 [`Wav2Vec2Processor`] 中：\n\n```py\n>>> from transformers import Wav2Vec2Processor\n\n>>> processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n```\n\n通过两个基类 - 配置类和模型类 - 以及一个附加的预处理类（分词器、图像处理器、特征提取器或处理器），你可以创建 🤗 Transformers 支持的任何模型。 每个基类都是可配置的，允许你使用所需的特定属性。 你可以轻松设置模型进行训练或修改现有的预训练模型进行微调。",
    "945": "一级标题：共享自定义模型\n二级标题：无\n内容：\n🤗 Transformers 库设计得易于扩展。每个模型的代码都在仓库给定的子文件夹中，没有进行抽象，因此你可以轻松复制模型代码文件并根据需要进行调整。\n\n如果你要编写全新的模型，从头开始可能更容易。在本教程中，我们将向你展示如何编写自定义模型及其配置，以便可以在 Transformers 中使用它；以及如何与社区共享它（及其依赖的代码），以便任何人都可以使用，即使它不在 🤗 Transformers 库中。\n\n我们将以 ResNet 模型为例，通过将 [timm 库](https://github.com/rwightman/pytorch-image-models) 的 ResNet 类封装到 [`PreTrainedModel`] 中来进行说明。",
    "946": "一级标题：共享自定义模型\n二级标题：编写自定义配置\n内容：\n在深入研究模型之前，让我们首先编写其配置。模型的配置是一个对象，其中包含构建模型所需的所有信息。我们将在下一节中看到，模型只能接受一个 `config` 来进行初始化，因此我们很需要使该对象尽可能完整。\n\n我们将采用一些我们可能想要调整的 ResNet 类的参数举例。不同的配置将为我们提供不同类型可能的 ResNet 模型。在确认其中一些参数的有效性后，我们只需存储这些参数。\n\n```python\nfrom transformers import PretrainedConfig\nfrom typing import List\n\n\nclass ResnetConfig(PretrainedConfig):\n    model_type = \"resnet\"\n\n    def __init__(\n        self,\n        block_type=\"bottleneck\",\n        layers: list[int] = [3, 4, 6, 3],\n        num_classes: int = 1000,\n        input_channels: int = 3,\n        cardinality: int = 1,\n        base_width: int = 64,\n        stem_width: int = 64,\n        stem_type: str = \"\",\n        avg_down: bool = False,\n        **kwargs,\n    ):\n        if block_type not in [\"basic\", \"bottleneck\"]:\n            raise ValueError(f\"`block_type` must be 'basic' or bottleneck', got {block_type}.\")\n        if stem_type not in [\"\", \"deep\", \"deep-tiered\"]:\n            raise ValueError(f\"`stem_type` must be '', 'deep' or 'deep-tiered', got {stem_type}.\")\n\n        self.block_type = block_type\n        self.layers = layers\n        self.num_classes = num_classes\n        self.input_channels = input_channels\n        self.cardinality = cardinality\n        self.base_width = base_width\n        self.stem_width = stem_width\n        self.stem_type = stem_type\n        self.avg_down = avg_down\n        super().__init__(**kwargs)\n```\n\n编写自定义配置时需要记住的三个重要事项如下：\n- 必须继承自 `PretrainedConfig`，\n- `PretrainedConfig` 的 `__init__` 方法必须接受任何 kwargs，\n- 这些 `kwargs` 需要传递给超类的 `__init__` 方法。\n\n继承是为了确保你获得来自 🤗 Transformers 库的所有功能，而另外两个约束源于 `PretrainedConfig` 的字段比你设置的字段多。在使用 `from_pretrained` 方法重新加载配置时，这些字段需要被你的配置接受，然后传递给超类。\n\n为你的配置定义 `model_type`（此处为 `model_type=\"resnet\"`）不是必须的，除非你想使用自动类注册你的模型（请参阅最后一节）。\n\n做完这些以后，就可以像使用库里任何其他模型配置一样，轻松地创建和保存配置。以下代码展示了如何创建并保存 resnet50d 配置：\n\n```py\nresnet50d_config = ResnetConfig(block_type=\"bottleneck\", stem_width=32, stem_type=\"deep\", avg_down=True)\nresnet50d_config.save_pretrained(\"custom-resnet\")\n```\n\n这行代码将在 `custom-resnet` 文件夹内保存一个名为 `config.json` 的文件。然后，你可以使用 `from_pretrained` 方法重新加载配置：\n\n```py\nresnet50d_config = ResnetConfig.from_pretrained(\"custom-resnet\")\n```\n\n你还可以使用 [`PretrainedConfig`] 类的任何其他方法，例如 [`~PretrainedConfig.push_to_hub`]，直接将配置上传到 Hub。",
    "947": "一级标题：共享自定义模型\n二级标题：编写自定义模型\n内容：\n有了 ResNet 配置后，就可以继续编写模型了。实际上，我们将编写两个模型：一个模型用于从一批图像中提取隐藏特征（类似于 [`BertModel`]），另一个模型适用于图像分类（类似于 [`BertForSequenceClassification`]）。\n\n正如之前提到的，我们只会编写一个松散的模型包装，以使示例保持简洁。在编写此类之前，只需要建立起块类型（block types）与实际块类（block classes）之间的映射。然后，通过将所有内容传递给ResNet类，从配置中定义模型：\n\n```py\nfrom transformers import PreTrainedModel\nfrom timm.models.resnet import BasicBlock, Bottleneck, ResNet\nfrom .configuration_resnet import ResnetConfig\n\n\nBLOCK_MAPPING = {\"basic\": BasicBlock, \"bottleneck\": Bottleneck}\n\n\nclass ResnetModel(PreTrainedModel):\n    config_class = ResnetConfig\n\n    def __init__(self, config):\n        super().__init__(config)\n        block_layer = BLOCK_MAPPING[config.block_type]\n        self.model = ResNet(\n            block_layer,\n            config.layers,\n            num_classes=config.num_classes,\n            in_chans=config.input_channels,\n            cardinality=config.cardinality,\n            base_width=config.base_width,\n            stem_width=config.stem_width,\n            stem_type=config.stem_type,\n            avg_down=config.avg_down,\n        )\n\n    def forward(self, tensor):\n        return self.model.forward_features(tensor)\n```\n\n对用于进行图像分类的模型，我们只需更改前向方法：\n\n```py\nimport torch\n\n\nclass ResnetModelForImageClassification(PreTrainedModel):\n    config_class = ResnetConfig\n\n    def __init__(self, config):\n        super().__init__(config)\n        block_layer = BLOCK_MAPPING[config.block_type]\n        self.model = ResNet(\n            block_layer,\n            config.layers,\n            num_classes=config.num_classes,\n            in_chans=config.input_channels,\n            cardinality=config.cardinality,\n            base_width=config.base_width,\n            stem_width=config.stem_width,\n            stem_type=config.stem_type,\n            avg_down=config.avg_down,\n        )\n\n    def forward(self, tensor, labels=None):\n        logits = self.model(tensor)\n        if labels is not None:\n            loss = torch.nn.functional.cross_entropy(logits, labels)\n            return {\"loss\": loss, \"logits\": logits}\n        return {\"logits\": logits}\n```\n\n在这两种情况下，请注意我们如何继承 `PreTrainedModel` 并使用 `config` 调用了超类的初始化（有点像编写常规的torch.nn.Module）。设置 `config_class` 的那行代码不是必须的，除非你想使用自动类注册你的模型（请参阅最后一节）。\n\n<Tip>\n\n如果你的模型与库中的某个模型非常相似，你可以重用与该模型相同的配置。\n\n</Tip>\n\n你可以让模型返回任何你想要的内容，但是像我们为 `ResnetModelForImageClassification` 做的那样返回一个字典，并在传递标签时包含loss，可以使你的模型能够在 [`Trainer`] 类中直接使用。只要你计划使用自己的训练循环或其他库进行训练，也可以使用其他输出格式。\n\n现在我们已经有了模型类，让我们创建一个：\n\n```py\nresnet50d = ResnetModelForImageClassification(resnet50d_config)\n```\n\n同样的，你可以使用 [`PreTrainedModel`] 的任何方法，比如 [`~PreTrainedModel.save_pretrained`] 或者 [`~PreTrainedModel.push_to_hub`]。我们将在下一节中使用第二种方法，并了解如何如何使用我们的模型的代码推送模型权重。但首先，让我们在模型内加载一些预训练权重。\n\n在你自己的用例中，你可能会在自己的数据上训练自定义模型。为了快速完成本教程，我们将使用 resnet50d 的预训练版本。由于我们的模型只是它的包装，转移这些权重将会很容易：\n\n```py\nimport timm\n\npretrained_model = timm.create_model(\"resnet50d\", pretrained=True)\nresnet50d.model.load_state_dict(pretrained_model.state_dict())\n```\n\n现在让我们看看，如何确保在执行 [`~PreTrainedModel.save_pretrained`] 或 [`~PreTrainedModel.push_to_hub`] 时，模型的代码被保存。",
    "948": "一级标题：共享自定义模型\n二级标题：将代码发送到 Hub\n内容：\n<Tip warning={true}>\n\n此 API 是实验性的，未来的发布中可能会有一些轻微的不兼容更改。\n\n</Tip>\n\n首先，确保你的模型在一个 `.py` 文件中完全定义。只要所有文件都位于同一目录中，它就可以依赖于某些其他文件的相对导入（目前我们还不为子模块支持此功能）。对于我们的示例，我们将在当前工作目录中名为 `resnet_model` 的文件夹中定义一个 `modeling_resnet.py` 文件和一个 `configuration_resnet.py` 文件。 配置文件包含 `ResnetConfig` 的代码，模型文件包含 `ResnetModel` 和 `ResnetModelForImageClassification` 的代码。\n\n```\n.\n└── resnet_model\n    ├── __init__.py\n    ├── configuration_resnet.py\n    └── modeling_resnet.py\n```\n\n`__init__.py` 可以为空，它的存在只是为了让 Python 检测到 `resnet_model` 可以用作模块。\n\n<Tip warning={true}>\n\n如果从库中复制模型文件，你需要将文件顶部的所有相对导入替换为从 `transformers` 包中的导入。\n\n</Tip>\n\n请注意，你可以重用（或子类化）现有的配置/模型。\n\n要与社区共享您的模型，请参照以下步骤：首先从新创建的文件中导入ResNet模型和配置：\n\n```py\nfrom resnet_model.configuration_resnet import ResnetConfig\nfrom resnet_model.modeling_resnet import ResnetModel, ResnetModelForImageClassification\n```\n\n接下来，你需要告诉库，当使用 `save_pretrained` 方法时，你希望复制这些对象的代码文件，并将它们正确注册到给定的 Auto 类（特别是对于模型），只需要运行以下代码：\n\n```py\nResnetConfig.register_for_auto_class()\nResnetModel.register_for_auto_class(\"AutoModel\")\nResnetModelForImageClassification.register_for_auto_class(\"AutoModelForImageClassification\")\n```\n\n请注意，对于配置（只有一个自动类 [`AutoConfig`]），不需要指定自动类，但对于模型来说情况不同。 你的自定义模型可能适用于许多不同的任务，因此你必须指定哪一个自动类适合你的模型。\n\n接下来，让我们像之前一样创建配置和模型：\n\n```py\nresnet50d_config = ResnetConfig(block_type=\"bottleneck\", stem_width=32, stem_type=\"deep\", avg_down=True)\nresnet50d = ResnetModelForImageClassification(resnet50d_config)\n\npretrained_model = timm.create_model(\"resnet50d\", pretrained=True)\nresnet50d.model.load_state_dict(pretrained_model.state_dict())\n```\n\n现在要将模型推送到集线器，请确保你已登录。你看可以在终端中运行以下命令：\n\n```bash\nhf auth login\n```\n\n或者在笔记本中运行以下代码：\n\n```py\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n```\n\n然后，可以这样将模型推送到自己的命名空间（或你所属的组织）：\n\n```py\nresnet50d.push_to_hub(\"custom-resnet50d\")\n```\n\n除了模型权重和 JSON 格式的配置外，这行代码也会复制 `custom-resnet50d` 文件夹内的模型以及配置的 `.py` 文件并将结果上传至 Hub。你可以在此[模型仓库](https://huggingface.co/sgugger/custom-resnet50d)中查看结果。\n\n有关推推送至 Hub 方法的更多信息，请参阅[共享教程](model_sharing)。",
    "949": "一级标题：共享自定义模型\n二级标题：使用带有自定义代码的模型\n内容：\n可以使用自动类（auto-classes）和 `from_pretrained` 方法，使用模型仓库里带有自定义代码的配置、模型或分词器文件。所有上传到 Hub 的文件和代码都会进行恶意软件扫描（有关更多信息，请参阅 [Hub 安全](https://huggingface.co/docs/hub/security#malware-scanning) 文档）, 但你仍应查看模型代码和作者，以避免在你的计算机上执行恶意代码。 设置 `trust_remote_code=True` 以使用带有自定义代码的模型：\n\n```py\nfrom transformers import AutoModelForImageClassification\n\nmodel = AutoModelForImageClassification.from_pretrained(\"sgugger/custom-resnet50d\", trust_remote_code=True)\n```\n\n我们强烈建议为 `revision` 参数传递提交哈希（commit hash），以确保模型的作者没有使用一些恶意的代码行更新了代码（除非您完全信任模型的作者）。\n\n```py\ncommit_hash = \"ed94a7c6247d8aedce4647f00f20de6875b5b292\"\nmodel = AutoModelForImageClassification.from_pretrained(\n    \"sgugger/custom-resnet50d\", trust_remote_code=True, revision=commit_hash\n)\n```\n\n在 Hub 上浏览模型仓库的提交历史时，有一个按钮可以轻松复制任何提交的提交哈希。",
    "950": "一级标题：共享自定义模型\n二级标题：将自定义代码的模型注册到自动类\n内容：\n如果你在编写一个扩展 🤗 Transformers 的库，你可能想要扩展自动类以包含您自己的模型。这与将代码推送到 Hub 不同，因为用户需要导入你的库才能获取自定义模型（与从 Hub 自动下载模型代码相反）。\n\n只要你的配置 `model_type` 属性与现有模型类型不同，并且你的模型类有正确的 `config_class` 属性，你可以像这样将它们添加到自动类中：\n\n```py\nfrom transformers import AutoConfig, AutoModel, AutoModelForImageClassification\n\nAutoConfig.register(\"resnet\", ResnetConfig)\nAutoModel.register(ResnetConfig, ResnetModel)\nAutoModelForImageClassification.register(ResnetConfig, ResnetModelForImageClassification)\n```\n\n请注意，将自定义配置注册到 [`AutoConfig`] 时，使用的第一个参数需要与自定义配置的 `model_type` 匹配；而将自定义模型注册到任何自动模型类时，使用的第一个参数需要与 `config_class` 匹配。",
    "951": "一级标题：调试\n二级标题：无\n内容：",
    "952": "一级标题：调试\n二级标题：多GPU网络问题调试\n内容：\n当使用`DistributedDataParallel`和多个GPU进行训练或推理时，如果遇到进程和（或）节点之间的互联问题，您可以使用以下脚本来诊断网络问题。\n\n```bash\nwget https://raw.githubusercontent.com/huggingface/transformers/main/scripts/distributed/torch-distributed-gpu-test.py\n```\n\n例如，要测试两个GPU之间的互联，请执行以下操作：\n\n```bash\npython -m torch.distributed.run --nproc_per_node 2 --nnodes 1 torch-distributed-gpu-test.py\n```\n\n如果两个进程能够相互通信并分配GPU内存，它们各自将打印出 \"OK\" 状态。\n\n对于更多的GPU或节点，可以根据脚本中的参数进行调整。\n\n在诊断脚本内部，您将找到更多详细信息，甚至有关如何在SLURM环境中运行它的说明。\n\n另一种级别的调试是添加 `NCCL_DEBUG=INFO` 环境变量，如下所示：\n\n\n```bash\nNCCL_DEBUG=INFO python -m torch.distributed.run --nproc_per_node 2 --nnodes 1 torch-distributed-gpu-test.py\n```\n\n这将产生大量与NCCL相关的调试信息，如果发现有问题报告，您可以在线搜索以获取相关信息。或者，如果您不确定如何解释输出，可以在`issue`中分享日志文件。",
    "953": "一级标题：调试\n二级标题：下溢和上溢检测\n内容：\n<Tip>\n\n目前，此功能仅适用于PyTorch。\n\n</Tip>\n\n<Tip>\n\n对于多GPU训练，它需要使用DDP（`torch.distributed.launch`）。\n\n</Tip>\n\n<Tip>\n\n此功能可以与任何基于`nn.Module`的模型一起使用。\n\n</Tip>\n\n如果您开始发现`loss=NaN`或模型因激活值或权重中的`inf`或`nan`而出现一些异常行为，就需要发现第一个下溢或上溢发生的地方以及导致它的原因。幸运的是，您可以通过激活一个特殊模块来自动进行检测。\n\n如果您正在使用[`Trainer`]，只需把以下内容：\n\n\n```bash\n--debug underflow_overflow\n```\n\n添加到常规命令行参数中，或在创建[`TrainingArguments`]对象时传递 `debug=\"underflow_overflow\"`。\n\n如果您正在使用自己的训练循环或其他Trainer，您可以通过以下方式实现相同的功能：\n\n```python\nfrom transformers.debug_utils import DebugUnderflowOverflow\n\ndebug_overflow = DebugUnderflowOverflow(model)\n```\n\n[`debug_utils.DebugUnderflowOverflow`] 将`hooks`插入模型，紧跟在每次前向调用之后，进而测试输入和输出变量，以及相应模块的权重。一旦在激活值或权重的至少一个元素中检测到`inf`或`nan`，程序将执行`assert`并打印报告，就像这样（这是在`google/mt5-small`下使用fp16混合精度捕获的）：\n\n```\nDetected inf/nan during batch_number=0\nLast 21 forward frames:\nabs min  abs max  metadata\n                  encoder.block.1.layer.1.DenseReluDense.dropout Dropout\n0.00e+00 2.57e+02 input[0]\n0.00e+00 2.85e+02 output\n[...]\n                  encoder.block.2.layer.0 T5LayerSelfAttention\n6.78e-04 3.15e+03 input[0]\n2.65e-04 3.42e+03 output[0]\n             None output[1]\n2.25e-01 1.00e+04 output[2]\n                  encoder.block.2.layer.1.layer_norm T5LayerNorm\n8.69e-02 4.18e-01 weight\n2.65e-04 3.42e+03 input[0]\n1.79e-06 4.65e+00 output\n                  encoder.block.2.layer.1.DenseReluDense.wi_0 Linear\n2.17e-07 4.50e+00 weight\n1.79e-06 4.65e+00 input[0]\n2.68e-06 3.70e+01 output\n                  encoder.block.2.layer.1.DenseReluDense.wi_1 Linear\n8.08e-07 2.66e+01 weight\n1.79e-06 4.65e+00 input[0]\n1.27e-04 2.37e+02 output\n                  encoder.block.2.layer.1.DenseReluDense.dropout Dropout\n0.00e+00 8.76e+03 input[0]\n0.00e+00 9.74e+03 output\n                  encoder.block.2.layer.1.DenseReluDense.wo Linear\n1.01e-06 6.44e+00 weight\n0.00e+00 9.74e+03 input[0]\n3.18e-04 6.27e+04 output\n                  encoder.block.2.layer.1.DenseReluDense T5DenseGatedGeluDense\n1.79e-06 4.65e+00 input[0]\n3.18e-04 6.27e+04 output\n                  encoder.block.2.layer.1.dropout Dropout\n3.18e-04 6.27e+04 input[0]\n0.00e+00      inf output\n```\n\n由于篇幅原因，示例输出中间的部分已经被缩减。\n\n第二列显示了绝对最大元素的值，因此，如果您仔细查看最后`frame`，输入和输出都在`1e4`的范围内。因此，在使用fp16混合精度进行训练时，最后一步发生了溢出（因为在`fp16`下，在`inf`之前的最大数字是`64e3`）。为了避免在`fp16`下发生溢出，激活值必须保持低于`1e4`，因为`1e4 * 1e4 = 1e8`，因此任何具有大激活值的矩阵乘法都会导致数值溢出。\n\n在跟踪的开始处，您可以发现问题发生在哪个批次（这里的`Detected inf/nan during batch_number=0`表示问题发生在第一个批次）。\n\n每个报告的`frame`都以声明相应模块的层信息为开头，说明这一`frame`是为哪个模块报告的。如果只看这个`frame`：\n\n```\n                  encoder.block.2.layer.1.layer_norm T5LayerNorm\n8.69e-02 4.18e-01 weight\n2.65e-04 3.42e+03 input[0]\n1.79e-06 4.65e+00 output\n```\n\n在这里，`encoder.block.2.layer.1.layer_norm` 表示它是编码器的第二个块中第一层的`layer norm`。而 `forward` 的具体调用是 `T5LayerNorm`。\n\n让我们看看该报告的最后几个`frame`：\n\n```\nDetected inf/nan during batch_number=0\nLast 21 forward frames:\nabs min  abs max  metadata\n[...]\n                  encoder.block.2.layer.1.DenseReluDense.wi_0 Linear\n2.17e-07 4.50e+00 weight\n1.79e-06 4.65e+00 input[0]\n2.68e-06 3.70e+01 output\n                  encoder.block.2.layer.1.DenseReluDense.wi_1 Linear\n8.08e-07 2.66e+01 weight\n1.79e-06 4.65e+00 input[0]\n1.27e-04 2.37e+02 output\n                  encoder.block.2.layer.1.DenseReluDense.wo Linear\n1.01e-06 6.44e+00 weight\n0.00e+00 9.74e+03 input[0]\n3.18e-04 6.27e+04 output\n                  encoder.block.2.layer.1.DenseReluDense T5DenseGatedGeluDense\n1.79e-06 4.65e+00 input[0]\n3.18e-04 6.27e+04 output\n                  encoder.block.2.layer.1.dropout Dropout\n3.18e-04 6.27e+04 input[0]\n0.00e+00      inf output\n```\n\n最后一个`frame`报告了`Dropout.forward`函数，第一个条目是唯一的输入，第二个条目是唯一的输出。您可以看到，它是从`DenseReluDense`类内的属性`dropout`中调用的。我们可以看到它发生在第2个块的第1层，也就是在第一个批次期间。最后，绝对最大的输入元素值为`6.27e+04`，输出也是`inf`。\n\n您可以在这里看到，`T5DenseGatedGeluDense.forward`产生了输出激活值，其绝对最大值约为62.7K，非常接近fp16的上限64K。在下一个`frame`中，我们有`Dropout`对权重进行重新归一化，之后将某些元素归零，将绝对最大值推到了64K以上，导致溢出（`inf`）。\n\n正如你所看到的，我们需要查看前面的`frame`, 从那里fp16数字开始变得非常大。\n\n让我们将报告与`models/t5/modeling_t5.py`中的代码匹配：\n\n```python\nclass T5DenseGatedGeluDense(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.wi_0 = nn.Linear(config.d_model, config.d_ff, bias=False)\n        self.wi_1 = nn.Linear(config.d_model, config.d_ff, bias=False)\n        self.wo = nn.Linear(config.d_ff, config.d_model, bias=False)\n        self.dropout = nn.Dropout(config.dropout_rate)\n        self.gelu_act = ACT2FN[\"gelu_new\"]\n\n    def forward(self, hidden_states):\n        hidden_gelu = self.gelu_act(self.wi_0(hidden_states))\n        hidden_linear = self.wi_1(hidden_states)\n        hidden_states = hidden_gelu * hidden_linear\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.wo(hidden_states)\n        return hidden_states\n```\n\n现在很容易看到`dropout`调用，以及所有之前的调用。\n\n由于检测是在前向`hook`中进行的，这些报告将立即在每个`forward`返回后打印出来。\n\n回到完整的报告，要采取措施并解决问题，我们需要往回看几个`frame`，在那里数字开始上升，并且最有可能切换到fp32模式以便在乘法或求和时数字不会溢出。当然，可能还有其他解决方案。例如，如果启用了`amp`，我们可以在将原始`forward`移到`helper wrapper`中后，暂时关闭它，如下所示：\n\n```python\ndef _forward(self, hidden_states):\n    hidden_gelu = self.gelu_act(self.wi_0(hidden_states))\n    hidden_linear = self.wi_1(hidden_states)\n    hidden_states = hidden_gelu * hidden_linear\n    hidden_states = self.dropout(hidden_states)\n    hidden_states = self.wo(hidden_states)\n    return hidden_states\n\n\nimport torch\n\n\ndef forward(self, hidden_states):\n    if torch.is_autocast_enabled():\n        with torch.cuda.amp.autocast(enabled=False):\n            return self._forward(hidden_states)\n    else:\n        return self._forward(hidden_states)\n```\n\n由于自动检测器仅报告完整`frame`的输入和输出，一旦知道在哪里查找，您可能还希望分析特定`forward`函数的中间阶段。在这种情况下，您可以使用`detect_overflow`辅助函数将检测器放到希望的位置，例如：\n\n```python\nfrom debug_utils import detect_overflow\n\n\nclass T5LayerFF(nn.Module):\n    [...]\n\n    def forward(self, hidden_states):\n        forwarded_states = self.layer_norm(hidden_states)\n        detect_overflow(forwarded_states, \"after layer_norm\")\n        forwarded_states = self.DenseReluDense(forwarded_states)\n        detect_overflow(forwarded_states, \"after DenseReluDense\")\n        return hidden_states + self.dropout(forwarded_states)\n```\n\n可以看到，我们添加了2个检测器，现在我们可以跟踪是否在`forwarded_states`中间的某个地方检测到了`inf`或`nan`。\n\n实际上，检测器已经报告了这些，因为上面示例中的每个调用都是一个`nn.Module`，但假设如果您有一些本地的直接计算，这就是您将如何执行的方式。\n\n此外，如果您在自己的代码中实例化调试器，您可以调整从其默认打印的`frame`数，例如：\n\n```python\nfrom transformers.debug_utils import DebugUnderflowOverflow\n\ndebug_overflow = DebugUnderflowOverflow(model, max_frames_to_save=100)\n```\n\n### 特定批次的绝对最小值和最大值跟踪\n\n当关闭下溢/上溢检测功能, 同样的调试类可以用于批处理跟踪。\n\n假设您想要监视给定批次的每个`forward`调用的所有成分的绝对最小值和最大值，并且仅对批次1和3执行此操作，您可以这样实例化这个类：\n\n```python\ndebug_overflow = DebugUnderflowOverflow(model, trace_batch_nums=[1, 3])\n```\n\n现在，完整的批次1和3将以与下溢/上溢检测器相同的格式进行跟踪。\n\n批次从0开始计数。\n\n如果您知道程序在某个批次编号之后开始出现问题，那么您可以直接快进到该区域。以下是一个截取的配置示例输出：\n\n```\n                  *** Starting batch number=1 ***\nabs min  abs max  metadata\n                  shared Embedding\n1.01e-06 7.92e+02 weight\n0.00e+00 2.47e+04 input[0]\n5.36e-05 7.92e+02 output\n[...]\n                  decoder.dropout Dropout\n1.60e-07 2.27e+01 input[0]\n0.00e+00 2.52e+01 output\n                  decoder T5Stack\n     not a tensor output\n                  lm_head Linear\n1.01e-06 7.92e+02 weight\n0.00e+00 1.11e+00 input[0]\n6.06e-02 8.39e+01 output\n                   T5ForConditionalGeneration\n     not a tensor output\n\n                  *** Starting batch number=3 ***\nabs min  abs max  metadata\n                  shared Embedding\n1.01e-06 7.92e+02 weight\n0.00e+00 2.78e+04 input[0]\n5.36e-05 7.92e+02 output\n[...]\n```\n\n在这里，您将获得大量的`frame`被`dump` - 与您的模型中的前向调用一样多，它有可能符合也可能不符合您的要求，但有时对于调试目的来说，它可能比正常的调试器更容易使用。例如，如果问题开始发生在批次号150上，您可以`dump`批次149和150的跟踪，并比较数字开始发散的地方。\n\n你还可以使用以下命令指定停止训练的批次号：\n\n```python\ndebug_overflow = DebugUnderflowOverflow(model, trace_batch_nums=[1, 3], abort_after_batch_num=3)\n```",
    "954": "一级标题：使用 🤗 Tokenizers 中的分词器\n二级标题：无\n内容：\n[`PreTrainedTokenizerFast`] 依赖于 [🤗 Tokenizers](https://huggingface.co/docs/tokenizers) 库。从 🤗 Tokenizers 库获得的分词器可以被轻松地加载到 🤗 Transformers 中。\n\n在了解具体内容之前，让我们先用几行代码创建一个虚拟的分词器：\n\n```python\n>>> from tokenizers import Tokenizer\n>>> from tokenizers.models import BPE\n>>> from tokenizers.trainers import BpeTrainer\n>>> from tokenizers.pre_tokenizers import Whitespace\n\n>>> tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n>>> trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n\n>>> tokenizer.pre_tokenizer = Whitespace()\n>>> files = [...]\n>>> tokenizer.train(files, trainer)\n```\n\n现在，我们拥有了一个针对我们定义的文件进行训练的分词器。我们可以在当前运行时中继续使用它，或者将其保存到一个 JSON 文件以供将来重复使用。",
    "955": "一级标题：使用 🤗 Tokenizers 中的分词器\n二级标题：直接从分词器对象加载\n内容：\n让我们看看如何利用 🤗 Transformers 库中的这个分词器对象。[`PreTrainedTokenizerFast`] 类允许通过接受已实例化的 *tokenizer* 对象作为参数，进行轻松实例化：\n\n```python\n>>> from transformers import PreTrainedTokenizerFast\n\n>>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n```\n\n现在可以使用这个对象，使用 🤗 Transformers 分词器共享的所有方法！前往[分词器页面](main_classes/tokenizer)了解更多信息。",
    "956": "一级标题：使用 🤗 Tokenizers 中的分词器\n二级标题：从 JSON 文件加载\n内容：\n为了从 JSON 文件中加载分词器，让我们先保存我们的分词器：\n\n```python\n>>> tokenizer.save(\"tokenizer.json\")\n```\n\n我们保存此文件的路径可以通过 `tokenizer_file` 参数传递给 [`PreTrainedTokenizerFast`] 初始化方法：\n\n```python\n>>> from transformers import PreTrainedTokenizerFast\n\n>>> fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer.json\")\n```\n\n现在可以使用这个对象，使用 🤗 Transformers 分词器共享的所有方法！前往[分词器页面](main_classes/tokenizer)了解更多信息。",
    "957": "一级标题：完全分片数据并行\n二级标题：无\n内容：\n[完全分片数据并行（FSDP）](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)是一种数据并行方法，\n它将模型的参数、梯度和优化器状态在可用 GPU（也称为 Worker 或 *rank*）的数量上进行分片。\n与[分布式数据并行（DDP）](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)不同，\nFSDP 减少了内存使用量，因为模型在每个 GPU 上都被复制了一次。这就提高了 GPU 内存效率，\n使您能够用较少的 GPU 训练更大的模型。FSDP 已经集成到 Accelerate 中，\n这是一个用于在分布式环境中轻松管理训练的库，这意味着可以从 [`Trainer`] 类中调用这个库。\n\n在开始之前，请确保已安装 Accelerate，并且至少使用 PyTorch 2.1.0 或更高版本。\n\n```bash\npip install accelerate\n```",
    "958": "一级标题：完全分片数据并行\n二级标题：FSDP 配置\n内容：\n首先，运行 [`accelerate config`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-config)\n命令为您的训练环境创建一个配置文件。Accelerate 使用此配置文件根据您在 `accelerate config`\n中选择的训练选项来自动搭建正确的训练环境。\n\n```bash\naccelerate config\n```\n\n运行 `accelerate config` 时，您将被提示一系列选项来配置训练环境。\n本节涵盖了一些最重要的 FSDP 选项。要了解有关其他可用的 FSDP 选项的更多信息，\n请查阅 [fsdp_config](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.fsdp_config) 参数。\n\n### 分片策略\n\nFSDP 提供了多种可选择的分片策略：\n\n- `FULL_SHARD` - 将模型参数、梯度和优化器状态跨 Worker 进行分片；为此选项选择 `1`\n- `SHARD_GRAD_OP`- 将梯度和优化器状态跨 Worker 进行分片；为此选项选择 `2`\n- `NO_SHARD` - 不分片任何内容（这等同于 DDP）；为此选项选择 `3`\n- `HYBRID_SHARD` - 在每个 Worker 中分片模型参数、梯度和优化器状态，其中每个 Worker 也有完整副本；为此选项选择 `4`\n- `HYBRID_SHARD_ZERO2` - 在每个 Worker 中分片梯度和优化器状态，其中每个 Worker 也有完整副本；为此选项选择 `5`\n\n这由 `fsdp_sharding_strategy` 标志启用。\n\n### CPU 卸载\n\n当参数和梯度在不使用时可以卸载到 CPU 上，以节省更多 GPU 内存并帮助您适应即使 FSDP 也不足以容纳大型模型的情况。\n在运行 `accelerate config` 时，通过设置 `fsdp_offload_params: true` 来启用此功能。\n\n### 包装策略\n\nFSDP 是通过包装网络中的每个层来应用的。通常，包装是以嵌套方式应用的，其中完整的权重在每次前向传递后被丢弃，\n以便在下一层使用内存。**自动包装**策略是实现这一点的最简单方法，您不需要更改任何代码。\n您应该选择 `fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP` 来包装一个 Transformer 层，\n并且 `fsdp_transformer_layer_cls_to_wrap` 来指定要包装的层（例如 `BertLayer`）。\n\n否则，您可以选择基于大小的包装策略，其中如果一层的参数超过一定数量，则应用 FSDP。通过设置\n`fsdp_wrap_policy: SIZE_BASED_WRAP` 和 `min_num_param` 来启用此功能，将参数设置为所需的大小阈值。\n\n### 检查点\n\n应该使用 `fsdp_state_dict_type: SHARDED_STATE_DICT` 来保存中间检查点，\n因为在排名 0 上保存完整状态字典需要很长时间，通常会导致 `NCCL Timeout` 错误，因为在广播过程中会无限期挂起。\n您可以使用 [`~accelerate.Accelerator.load_state`] 方法加载分片状态字典以恢复训练。\n\n```py\n# 包含检查点的目录\naccelerator.load_state(\"ckpt\")\n```\n\n然而，当训练结束时，您希望保存完整状态字典，因为分片状态字典仅与 FSDP 兼容。\n\n```py\nif trainer.is_fsdp_enabled:\n    trainer.accelerator.state.fsdp_plugin.set_state_dict_type(\"FULL_STATE_DICT\")\n\ntrainer.save_model(script_args.output_dir)\n```\n\n### TPU\n\n[PyTorch XLA](https://pytorch.org/xla/release/2.1/index.html) 支持用于 TPUs 的 FSDP 训练，\n可以通过修改由 `accelerate config` 生成的 FSDP 配置文件来启用。除了上面指定的分片策略和包装选项外，\n您还可以将以下参数添加到文件中。\n\n```yaml\nxla: True # 必须设置为 True 以启用 PyTorch/XLA\nxla_fsdp_settings: # XLA 特定的 FSDP 参数\nxla_fsdp_grad_ckpt: True # 使用梯度检查点\n```\n\n[`xla_fsdp_settings`](https://github.com/pytorch/xla/blob/2e6e183e0724818f137c8135b34ef273dea33318/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py#L128)\n允许您配置用于 FSDP 的额外 XLA 特定参数。",
    "959": "一级标题：完全分片数据并行\n二级标题：启动训练\n内容：\nFSDP 配置文件示例如下所示：\n\n```yaml\ncompute_environment: LOCAL_MACHINE\ndebug: false\ndistributed_type: FSDP\ndowncast_bf16: \"no\"\nfsdp_config:\n  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n  fsdp_backward_prefetch_policy: BACKWARD_PRE\n  fsdp_cpu_ram_efficient_loading: true\n  fsdp_forward_prefetch: false\n  fsdp_offload_params: true\n  fsdp_sharding_strategy: 1\n  fsdp_state_dict_type: SHARDED_STATE_DICT\n  fsdp_sync_module_states: true\n  fsdp_transformer_layer_cls_to_wrap: BertLayer\n  fsdp_use_orig_params: true\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: bf16\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n```\n\n要启动训练，请运行 [`accelerate launch`](https://huggingface.co/docs/accelerate/package_reference/cli#accelerate-launch)\n命令，它将自动使用您之前使用 `accelerate config` 创建的配置文件。\n\n```bash\naccelerate launch my-trainer-script.py\n```\n\n```bash\naccelerate launch --fsdp=\"full shard\" --fsdp_config=\"path/to/fsdp_config/ my-trainer-script.py\n```",
    "960": "一级标题：完全分片数据并行\n二级标题：下一步\n内容：\nFSDP 在大规模模型训练方面是一个强大的工具，您可以使用多个 GPU 或 TPU。\n通过分片模型参数、优化器和梯度状态，甚至在它们不活动时将其卸载到 CPU 上，\nFSDP 可以减少大规模训练的高成本。如果您希望了解更多信息，下面的内容可能会有所帮助：\n\n- 深入参考 Accelerate 指南，了解有关\n  [FSDP](https://huggingface.co/docs/accelerate/usage_guides/fsdp)的更多信息。\n- 阅读[介绍 PyTorch 完全分片数据并行（FSDP）API](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) 博文。\n- 阅读[使用 FSDP 在云 TPU 上扩展 PyTorch 模型](https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/)博文。",
    "961": "一级标题：GGUF 和 Transformers 的交互\n二级标题：无\n内容：\nGGUF文件格式用于存储模型，以便通过[GGML](https://github.com/ggerganov/ggml)和其他依赖它的库进行推理，例如非常流行的[llama.cpp](https://github.com/ggerganov/llama.cpp)或[whisper.cpp](https://github.com/ggerganov/whisper.cpp)。\n\n该文件格式[由抱抱脸支持](https://huggingface.co/docs/hub/en/gguf)，可用于快速检查文件中张量和元数据。\n\n该文件格式是一种“单文件格式”，通常单个文件就包含了配置属性、分词器词汇表和其他属性，同时还有模型中要加载的所有张量。这些文件根据文件的量化类型有不同的格式。我们在[这里](https://huggingface.co/docs/hub/en/gguf#quantization-types)进行了简要介绍。",
    "962": "一级标题：GGUF 和 Transformers 的交互\n二级标题：在 Transformers 中的支持\n内容：\n我们在 transformers 中添加了加载 gguf 文件的功能，这样可以对 GGUF 模型进行进一步的训练或微调，然后再将模型转换回 GGUF 格式，以便在 ggml 生态系统中使用。加载模型时，我们首先将其反量化为 FP32，然后再加载权重以在 PyTorch 中使用。\n\n>    [!注意]\n>    目前这个功能还处于探索阶段，欢迎大家贡献力量，以便在不同量化类型和模型架构之间更好地完善这一功能。\n\n目前，支持的模型架构和量化类型如下：\n\n### 支持的量化类型\n\n根据分享在 Hub 上的较为热门的量化文件，初步支持以下量化类型：\n\n- F32\n- F16\n- BF16\n- Q4_0\n- Q4_1\n- Q5_0\n- Q5_1\n- Q8_0\n- Q2_K\n- Q3_K\n- Q4_K\n- Q5_K\n- Q6_K\n- IQ1_S\n- IQ1_M\n- IQ2_XXS\n- IQ2_XS\n- IQ2_S\n- IQ3_XXS\n- IQ3_S\n- IQ4_XS\n- IQ4_NL\n\n>    [!注意]\n>    为了支持 gguf 反量化，需要安装 `gguf>=0.10.0`。\n\n### 支持的模型架构\n\n目前支持以下在 Hub 上非常热门的模型架构：\n\n- LLaMa\n- Mistral\n- Qwen2\n- Qwen2Moe\n- Phi3\n- Bloom\n- Falcon\n- StableLM\n- GPT2\n- Starcoder2",
    "963": "一级标题：GGUF 和 Transformers 的交互\n二级标题：使用示例\n内容：\n为了在`transformers`中加载`gguf`文件，你需要在 `from_pretrained`方法中为分词器和模型指定 `gguf_file`参数。下面是从同一个文件中加载分词器和模型的示例：\n\n```py\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_id = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\nfilename = \"tinyllama-1.1b-chat-v1.0.Q6_K.gguf\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)\n```\n\n现在，你就已经可以结合 PyTorch 生态系统中的一系列其他工具，来使用完整的、未量化的模型了。\n\n为了将模型转换回`gguf`文件，我们建议使用`llama.cpp`中的[`convert-hf-to-gguf.py`文件](https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py)。\n\n以下是如何补充上面的脚本，以保存模型并将其导出回 `gguf`的示例：\n\n```py\ntokenizer.save_pretrained('directory')\nmodel.save_pretrained('directory')\n\n!python ${path_to_llama_cpp}/convert-hf-to-gguf.py ${directory}\n```",
    "964": "一级标题：使用Trainer API进行超参数搜索\n二级标题：无\n内容：\n🤗 Transformers库提供了一个优化过的[`Trainer`]类，用于训练🤗 Transformers模型，相比于手动编写自己的训练循环，这更容易开始训练。[`Trainer`]提供了超参数搜索的API。本文档展示了如何在示例中启用它。",
    "965": "一级标题：使用Trainer API进行超参数搜索\n二级标题：超参数搜索后端\n内容：\n[`Trainer`] 目前支持四种超参数搜索后端：[optuna](https://optuna.org/)，[sigopt](https://sigopt.com/)，[raytune](https://docs.ray.io/en/latest/tune/index.html)，[wandb](https://wandb.ai/site/sweeps)\n\n在使用它们之前，您应该先安装它们作为超参数搜索后端。\n\n```bash\npip install optuna/sigopt/wandb/ray[tune]\n```",
    "966": "一级标题：使用Trainer API进行超参数搜索\n二级标题：如何在示例中启用超参数搜索\n内容：\n定义超参数搜索空间，不同的后端需要不同的格式。\n\n对于sigopt，请参阅sigopt [object_parameter](https://docs.sigopt.com/ai-module-api-references/api_reference/objects/object_parameter)，它类似于以下内容：\n\n```py\n>>> def sigopt_hp_space(trial):\n...     return [\n...         {\"bounds\": {\"min\": 1e-6, \"max\": 1e-4}, \"name\": \"learning_rate\", \"type\": \"double\"},\n...         {\n...             \"categorical_values\": [\"16\", \"32\", \"64\", \"128\"],\n...             \"name\": \"per_device_train_batch_size\",\n...             \"type\": \"categorical\",\n...         },\n...     ]\n```\n\n对于optuna，请参阅optuna [object_parameter](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html#sphx-glr-tutorial-10-key-features-002-configurations-py)，它类似于以下内容：\n\n```py\n>>> def optuna_hp_space(trial):\n...     return {\n...         \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n...         \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64, 128]),\n...     }\n```\n\nOptuna提供了多目标HPO。您可以在`hyperparameter_search`中传递`direction`参数，并定义自己的`compute_objective`以返回多个目标值。在`hyperparameter_search`中将返回Pareto Front（`list[BestRun]`），您应该参考[test_trainer](https://github.com/huggingface/transformers/blob/main/tests/trainer/test_trainer.py)中的测试用例`TrainerHyperParameterMultiObjectOptunaIntegrationTest`。它类似于以下内容：\n\n```py\n>>> best_trials = trainer.hyperparameter_search(\n...     direction=[\"minimize\", \"maximize\"],\n...     backend=\"optuna\",\n...     hp_space=optuna_hp_space,\n...     n_trials=20,\n...     compute_objective=compute_objective,\n... )\n```\n\n对于raytune，可以参考raytune的[object_parameter](https://docs.ray.io/en/latest/tune/api/search_space.html)，它类似于以下内容：\n\n```py\n>>> def ray_hp_space(trial):\n...     return {\n...         \"learning_rate\": tune.loguniform(1e-6, 1e-4),\n...         \"per_device_train_batch_size\": tune.choice([16, 32, 64, 128]),\n...     }\n```\n\n对于wandb，可以参考wandb的[object_parameter](https://docs.wandb.ai/guides/sweeps/configuration)，它类似于以下内容：\n\n```py\n>>> def wandb_hp_space(trial):\n...     return {\n...         \"method\": \"random\",\n...         \"metric\": {\"name\": \"objective\", \"goal\": \"minimize\"},\n...         \"parameters\": {\n...             \"learning_rate\": {\"distribution\": \"uniform\", \"min\": 1e-6, \"max\": 1e-4},\n...             \"per_device_train_batch_size\": {\"values\": [16, 32, 64, 128]},\n...         },\n...     }\n```\n\n定义一个`model_init`函数并将其传递给[Trainer]，作为示例：\n\n```py\n>>> def model_init(trial):\n...     return AutoModelForSequenceClassification.from_pretrained(\n...         model_args.model_name_or_path,\n...         from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n...         config=config,\n...         cache_dir=model_args.cache_dir,\n...         revision=model_args.model_revision,\n...         use_auth_token=True if model_args.use_auth_token else None,\n...     )\n```\n\n使用你的`model_init`函数、训练参数、训练和测试数据集以及评估函数创建一个[`Trainer`]。\n\n```py\n>>> trainer = Trainer(\n...     model=None,\n...     args=training_args,\n...     train_dataset=small_train_dataset,\n...     eval_dataset=small_eval_dataset,\n...     compute_metrics=compute_metrics,\n...     processing_class=tokenizer,\n...     model_init=model_init,\n...     data_collator=data_collator,\n... )\n```\n\n调用超参数搜索，获取最佳试验参数，后端可以是`\"optuna\"`/`\"sigopt\"`/`\"wandb\"`/`\"ray\"`。方向可以是`\"minimize\"`或`\"maximize\"`，表示是否优化更大或更低的目标。\n\n您可以定义自己的compute_objective函数，如果没有定义，将调用默认的compute_objective，并将评估指标（如f1）之和作为目标值返回。\n\n```py\n>>> best_trial = trainer.hyperparameter_search(\n...     direction=\"maximize\",\n...     backend=\"optuna\",\n...     hp_space=optuna_hp_space,\n...     n_trials=20,\n...     compute_objective=compute_objective,\n... )\n```",
    "967": "一级标题：使用Trainer API进行超参数搜索\n二级标题：针对DDP微调的超参数搜索\n内容：\n目前，Optuna和Sigopt已启用针对DDP的超参数搜索。只有rank-zero进程会进行超参数搜索并将参数传递给其他进程。",
    "968": "一级标题：🤗 Transformers简介\n二级标题：无\n内容：\n为 [PyTorch](https://pytorch.org/)、[TensorFlow](https://www.tensorflow.org/) 和 [JAX](https://jax.readthedocs.io/en/latest/) 打造的先进的机器学习工具.\n\n🤗 Transformers 提供了可以轻松地下载并且训练先进的预训练模型的 API 和工具。使用预训练模型可以减少计算消耗和碳排放，并且节省从头训练所需要的时间和资源。这些模型支持不同模态中的常见任务，比如：\n\n📝 **自然语言处理**：文本分类、命名实体识别、问答、语言建模、摘要、翻译、多项选择和文本生成。<br>\n🖼️ **机器视觉**：图像分类、目标检测和语义分割。<br>\n🗣️ **音频**：自动语音识别和音频分类。<br>\n🐙 **多模态**：表格问答、光学字符识别、从扫描文档提取信息、视频分类和视觉问答。\n\n🤗 Transformers 支持在 PyTorch、TensorFlow 和 JAX 上的互操作性. 这给在模型的每个阶段使用不同的框架带来了灵活性；在一个框架中使用几行代码训练一个模型，然后在另一个框架中加载它并进行推理。模型也可以被导出为 ONNX 和 TorchScript 格式，用于在生产环境中部署。\n\n马上加入在 [Hub](https://huggingface.co/models)、[论坛](https://discuss.huggingface.co/) 或者 [Discord](https://discord.com/invite/JfAtkvEtRb) 上正在快速发展的社区吧！",
    "969": "一级标题：🤗 Transformers简介\n二级标题：如果你需要来自 Hugging Face 团队的个性化支持\n内容：\n<a target=\"_blank\" href=\"https://huggingface.co/support\">\n    <img alt=\"HuggingFace Expert Acceleration Program\" src=\"https://cdn-media.huggingface.co/marketing/transformers/new-support-improved.png\" style=\"width: 100%; max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);\">\n</a>",
    "970": "一级标题：🤗 Transformers简介\n二级标题：目录\n内容：\n这篇文档由以下 5 个章节组成：\n\n- **开始使用** 包含了库的快速上手和安装说明，便于配置和运行。\n- **教程** 是一个初学者开始的好地方。本章节将帮助你获得你会用到的使用这个库的基本技能。\n- **操作指南** 向你展示如何实现一个特定目标，比如为语言建模微调一个预训练模型或者如何创造并分享个性化模型。\n- **概念指南** 对 🤗 Transformers 的模型，任务和设计理念背后的基本概念和思想做了更多的讨论和解释。\n- **API 介绍** 描述了所有的类和函数：\n\n  - **主要类别** 详述了配置（configuration）、模型（model）、分词器（tokenizer）和流水线（pipeline）这几个最重要的类。\n  - **模型** 详述了在这个库中和每个模型实现有关的类和函数。\n  - **内部帮助** 详述了内部使用的工具类和函数。\n\n### 支持的模型和框架\n\n下表展示了库中对每个模型的支持情况，如是否具有 Python 分词器（表中的“Tokenizer slow”）、是否具有由 🤗 Tokenizers 库支持的快速分词器（表中的“Tokenizer fast”）、是否支持 Jax（通过 Flax）、PyTorch 与 TensorFlow。\n\n<!--This table is updated automatically from the auto modules with _make fix-copies_. Do not update manually!-->\n\n|                                  模型                                   | PyTorch 支持 | TensorFlow 支持 | Flax 支持 |\n|:------------------------------------------------------------------------:|:---------------:|:------------------:|:------------:|\n|                        [ALBERT](../en/model_doc/albert)                        |       ✅        |         ✅         |      ✅      |\n|                         [ALIGN](../en/model_doc/align)                         |       ✅        |         ❌         |      ❌      |\n|                       [AltCLIP](../en/model_doc/altclip)                       |       ✅        |         ❌         |      ❌      |\n| [Audio Spectrogram Transformer](../en/model_doc/audio-spectrogram-transformer) |       ✅        |         ❌         |      ❌      |\n|                    [Autoformer](../en/model_doc/autoformer)                    |       ✅        |         ❌         |      ❌      |\n|                          [Bark](../en/model_doc/bark)                          |       ✅        |         ❌         |      ❌      |\n|                          [BART](../en/model_doc/bart)                          |       ✅        |         ✅         |      ✅      |\n|                       [BARThez](../en/model_doc/barthez)                       |       ✅        |         ✅         |      ✅      |\n|                       [BARTpho](../en/model_doc/bartpho)                       |       ✅        |         ✅         |      ✅      |\n|                          [BEiT](../en/model_doc/beit)                          |       ✅        |         ❌         |      ✅      |\n|                          [BERT](../en/model_doc/bert)                          |       ✅        |         ✅         |      ✅      |\n|               [Bert Generation](../en/model_doc/bert-generation)               |       ✅        |         ❌         |      ❌      |\n|                 [BertJapanese](../en/model_doc/bert-japanese)                  |       ✅        |         ✅         |      ✅      |\n|                      [BERTweet](../en/model_doc/bertweet)                      |       ✅        |         ✅         |      ✅      |\n|                      [BigBird](../en/model_doc/big_bird)                       |       ✅        |         ❌         |      ✅      |\n|               [BigBird-Pegasus](../en/model_doc/bigbird_pegasus)               |       ✅        |         ❌         |      ❌      |\n|                        [BioGpt](../en/model_doc/biogpt)                        |       ✅        |         ❌         |      ❌      |\n|                           [BiT](../en/model_doc/bit)                           |       ✅        |         ❌         |      ❌      |\n|                    [Blenderbot](../en/model_doc/blenderbot)                    |       ✅        |         ✅         |      ✅      |\n|              [BlenderbotSmall](../en/model_doc/blenderbot-small)               |       ✅        |         ✅         |      ✅      |\n|                          [BLIP](../en/model_doc/blip)                          |       ✅        |         ✅         |      ❌      |\n|                        [BLIP-2](../en/model_doc/blip-2)                        |       ✅        |         ❌         |      ❌      |\n|                         [BLOOM](../en/model_doc/bloom)                         |       ✅        |         ❌         |      ✅      |\n|                          [BORT](../en/model_doc/bort)                          |       ✅        |         ✅         |      ✅      |\n|                   [BridgeTower](../en/model_doc/bridgetower)                   |       ✅        |         ❌         |      ❌      |\n|                          [BROS](../en/model_doc/bros)                          |       ✅        |         ❌         |      ❌      |\n|                          [ByT5](../en/model_doc/byt5)                          |       ✅        |         ✅         |      ✅      |\n|                     [CamemBERT](../en/model_doc/camembert)                     |       ✅        |         ✅         |      ❌      |\n|                        [CANINE](../en/model_doc/canine)                        |       ✅        |         ❌         |      ❌      |\n|                  [Chinese-CLIP](../en/model_doc/chinese_clip)                  |       ✅        |         ❌         |      ❌      |\n|                          [CLAP](../en/model_doc/clap)                          |       ✅        |         ❌         |      ❌      |\n|                          [CLIP](../en/model_doc/clip)                          |       ✅        |         ✅         |      ✅      |\n|                       [CLIPSeg](../en/model_doc/clipseg)                       |       ✅        |         ❌         |      ❌      |\n|                          [CLVP](../en/model_doc/clvp)                          |       ✅        |         ❌         |      ❌      |\n|                       [CodeGen](../en/model_doc/codegen)                       |       ✅        |         ❌         |      ❌      |\n|                    [CodeLlama](../en/model_doc/code_llama)                     |       ✅        |         ❌         |      ✅      |\n|              [Conditional DETR](../en/model_doc/conditional_detr)              |       ✅        |         ❌         |      ❌      |\n|                      [ConvBERT](../en/model_doc/convbert)                      |       ✅        |         ✅         |      ❌      |\n|                      [ConvNeXT](../en/model_doc/convnext)                      |       ✅        |         ✅         |      ❌      |\n|                    [ConvNeXTV2](../en/model_doc/convnextv2)                    |       ✅        |         ✅         |      ❌      |\n|                           [CPM](../en/model_doc/cpm)                           |       ✅        |         ✅         |      ✅      |\n|                       [CPM-Ant](../en/model_doc/cpmant)                        |       ✅        |         ❌         |      ❌      |\n|                          [CTRL](../en/model_doc/ctrl)                          |       ✅        |         ✅         |      ❌      |\n|                           [CvT](../en/model_doc/cvt)                           |       ✅        |         ✅         |      ❌      |\n|                   [Data2VecAudio](../en/model_doc/data2vec)                    |       ✅        |         ❌         |      ❌      |\n|                    [Data2VecText](../en/model_doc/data2vec)                    |       ✅        |         ❌         |      ❌      |\n|                   [Data2VecVision](../en/model_doc/data2vec)                   |       ✅        |         ✅         |      ❌      |\n|                       [DeBERTa](../en/model_doc/deberta)                       |       ✅        |         ✅         |      ❌      |\n|                    [DeBERTa-v2](../en/model_doc/deberta-v2)                    |       ✅        |         ✅         |      ❌      |\n|          [Decision Transformer](../en/model_doc/decision_transformer)          |       ✅        |         ❌         |      ❌      |\n|               [Deformable DETR](../en/model_doc/deformable_detr)               |       ✅        |         ❌         |      ❌      |\n|                          [DeiT](../en/model_doc/deit)                          |       ✅        |         ✅         |      ❌      |\n|                        [DePlot](../en/model_doc/deplot)                        |       ✅        |         ❌         |      ❌      |\n|                [Depth Anything](../en/model_doc/depth_anything)                |       ✅        |         ❌         |      ❌      |\n|                          [DETA](../en/model_doc/deta)                          |       ✅        |         ❌         |      ❌      |\n|                          [DETR](../en/model_doc/detr)                          |       ✅        |         ❌         |      ❌      |\n|                      [DialoGPT](../en/model_doc/dialogpt)                      |       ✅        |         ✅         |      ✅      |\n|                         [DiNAT](../en/model_doc/dinat)                         |       ✅        |         ❌         |      ❌      |\n|                        [DINOv2](../en/model_doc/dinov2)                        |       ✅        |         ❌         |      ❌      |\n|                    [DistilBERT](../en/model_doc/distilbert)                    |       ✅        |         ✅         |      ✅      |\n|                           [DiT](../en/model_doc/dit)                           |       ✅        |         ❌         |      ✅      |\n|                       [DonutSwin](../en/model_doc/donut)                       |       ✅        |         ❌         |      ❌      |\n|                           [DPR](../en/model_doc/dpr)                           |       ✅        |         ✅         |      ❌      |\n|                           [DPT](../en/model_doc/dpt)                           |       ✅        |         ❌         |      ❌      |\n|               [EfficientFormer](../en/model_doc/efficientformer)               |       ✅        |         ✅         |      ❌      |\n|                  [EfficientNet](../en/model_doc/efficientnet)                  |       ✅        |         ❌         |      ❌      |\n|                       [ELECTRA](../en/model_doc/electra)                       |       ✅        |         ✅         |      ✅      |\n|                       [EnCodec](../en/model_doc/encodec)                       |       ✅        |         ❌         |      ❌      |\n|               [Encoder decoder](../en/model_doc/encoder-decoder)               |       ✅        |         ✅         |      ✅      |\n|                         [ERNIE](../en/model_doc/ernie)                         |       ✅        |         ❌         |      ❌      |\n|                       [ErnieM](../en/model_doc/ernie_m)                        |       ✅        |         ❌         |      ❌      |\n|                           [ESM](../en/model_doc/esm)                           |       ✅        |         ✅         |      ❌      |\n|              [FairSeq Machine-Translation](../en/model_doc/fsmt)               |       ✅        |         ❌         |      ❌      |\n|                        [Falcon](../en/model_doc/falcon)                        |       ✅        |         ❌         |      ❌      |\n|         [FastSpeech2Conformer](../en/model_doc/fastspeech2_conformer)          |       ✅        |         ❌         |      ❌      |\n|                       [FLAN-T5](../en/model_doc/flan-t5)                       |       ✅        |         ✅         |      ✅      |\n|                      [FLAN-UL2](../en/model_doc/flan-ul2)                      |       ✅        |         ✅         |      ✅      |\n|                      [FlauBERT](../en/model_doc/flaubert)                      |       ✅        |         ✅         |      ❌      |\n|                         [FLAVA](../en/model_doc/flava)                         |       ✅        |         ❌         |      ❌      |\n|                          [FNet](../en/model_doc/fnet)                          |       ✅        |         ❌         |      ❌      |\n|                      [FocalNet](../en/model_doc/focalnet)                      |       ✅        |         ❌         |      ❌      |\n|                  [Funnel Transformer](../en/model_doc/funnel)                  |       ✅        |         ✅         |      ❌      |\n|                          [Fuyu](../en/model_doc/fuyu)                          |       ✅        |         ❌         |      ❌      |\n|                         [Gemma](../en/model_doc/gemma)                         |       ✅        |         ❌         |      ✅      |\n|                           [GIT](../en/model_doc/git)                           |       ✅        |         ❌         |      ❌      |\n|                          [GLPN](../en/model_doc/glpn)                          |       ✅        |         ❌         |      ❌      |\n|                       [GPT Neo](../en/model_doc/gpt_neo)                       |       ✅        |         ❌         |      ✅      |\n|                      [GPT NeoX](../en/model_doc/gpt_neox)                      |       ✅        |         ❌         |      ❌      |\n|             [GPT NeoX Japanese](../en/model_doc/gpt_neox_japanese)             |       ✅        |         ❌         |      ❌      |\n|                         [GPT-J](../en/model_doc/gptj)                          |       ✅        |         ✅         |      ✅      |\n|                       [GPT-Sw3](../en/model_doc/gpt-sw3)                       |       ✅        |         ✅         |      ✅      |\n|                   [GPTBigCode](../en/model_doc/gpt_bigcode)                    |       ✅        |         ❌         |      ❌      |\n|               [GPTSAN-japanese](../en/model_doc/gptsan-japanese)               |       ✅        |         ❌         |      ❌      |\n|                    [Graphormer](../en/model_doc/graphormer)                    |       ✅        |         ❌         |      ❌      |\n|                      [GroupViT](../en/model_doc/groupvit)                      |       ✅        |         ✅         |      ❌      |\n|                       [HerBERT](../en/model_doc/herbert)                       |       ✅        |         ✅         |      ✅      |\n|                        [Hubert](../en/model_doc/hubert)                        |       ✅        |         ✅         |      ❌      |\n|                        [I-BERT](../en/model_doc/ibert)                         |       ✅        |         ❌         |      ❌      |\n|                       [IDEFICS](../en/model_doc/idefics)                       |       ✅        |         ❌         |      ❌      |\n|                      [ImageGPT](../en/model_doc/imagegpt)                      |       ✅        |         ❌         |      ❌      |\n|                      [Informer](../en/model_doc/informer)                      |       ✅        |         ❌         |      ❌      |\n|                  [InstructBLIP](../en/model_doc/instructblip)                  |       ✅        |         ❌         |      ❌      |\n|                       [Jukebox](../en/model_doc/jukebox)                       |       ✅        |         ❌         |      ❌      |\n|                      [KOSMOS-2](../en/model_doc/kosmos-2)                      |       ✅        |         ❌         |      ❌      |\n|                      [LayoutLM](../en/model_doc/layoutlm)                      |       ✅        |         ✅         |      ❌      |\n|                    [LayoutLMv2](../en/model_doc/layoutlmv2)                    |       ✅        |         ❌         |      ❌      |\n|                    [LayoutLMv3](../en/model_doc/layoutlmv3)                    |       ✅        |         ✅         |      ❌      |\n|                     [LayoutXLM](../en/model_doc/layoutxlm)                     |       ✅        |         ❌         |      ❌      |\n|                           [LED](../en/model_doc/led)                           |       ✅        |         ✅         |      ❌      |\n|                         [LeViT](../en/model_doc/levit)                         |       ✅        |         ❌         |      ❌      |\n|                          [LiLT](../en/model_doc/lilt)                          |       ✅        |         ❌         |      ❌      |\n|                         [LLaMA](../en/model_doc/llama)                         |       ✅        |         ❌         |      ✅      |\n|                        [Llama2](../en/model_doc/llama2)                        |       ✅        |         ❌         |      ✅      |\n|                         [LLaVa](../en/model_doc/llava)                         |       ✅        |         ❌         |      ❌      |\n|                    [Longformer](../en/model_doc/longformer)                    |       ✅        |         ✅         |      ❌      |\n|                        [LongT5](../en/model_doc/longt5)                        |       ✅        |         ❌         |      ✅      |\n|                          [LUKE](../en/model_doc/luke)                          |       ✅        |         ❌         |      ❌      |\n|                        [LXMERT](../en/model_doc/lxmert)                        |       ✅        |         ✅         |      ❌      |\n|                        [M-CTC-T](../en/model_doc/mctct)                        |       ✅        |         ❌         |      ❌      |\n|                       [M2M100](../en/model_doc/m2m_100)                        |       ✅        |         ❌         |      ❌      |\n|                    [MADLAD-400](../en/model_doc/madlad-400)                    |       ✅        |         ✅         |      ✅      |\n|                        [Marian](../en/model_doc/marian)                        |       ✅        |         ✅         |      ✅      |\n|                      [MarkupLM](../en/model_doc/markuplm)                      |       ✅        |         ❌         |      ❌      |\n|                   [Mask2Former](../en/model_doc/mask2former)                   |       ✅        |         ❌         |      ❌      |\n|                    [MaskFormer](../en/model_doc/maskformer)                    |       ✅        |         ❌         |      ❌      |\n|                        [MatCha](../en/model_doc/matcha)                        |       ✅        |         ❌         |      ❌      |\n|                         [mBART](../en/model_doc/mbart)                         |       ✅        |         ✅         |      ✅      |\n|                      [mBART-50](../en/model_doc/mbart50)                       |       ✅        |         ✅         |      ✅      |\n|                          [MEGA](../en/model_doc/mega)                          |       ✅        |         ❌         |      ❌      |\n|                 [Megatron-BERT](../en/model_doc/megatron-bert)                 |       ✅        |         ❌         |      ❌      |\n|                 [Megatron-GPT2](../en/model_doc/megatron_gpt2)                 |       ✅        |         ✅         |      ✅      |\n|                       [MGP-STR](../en/model_doc/mgp-str)                       |       ✅        |         ❌         |      ❌      |\n|                       [Mistral](../en/model_doc/mistral)                       |       ✅        |         ❌         |      ✅      |\n|                       [Mixtral](../en/model_doc/mixtral)                       |       ✅        |         ❌         |      ❌      |\n|                         [mLUKE](../en/model_doc/mluke)                         |       ✅        |         ❌         |      ❌      |\n|                           [MMS](../en/model_doc/mms)                           |       ✅        |         ✅         |      ✅      |\n|                    [MobileBERT](../en/model_doc/mobilebert)                    |       ✅        |         ✅         |      ❌      |\n|                  [MobileNetV1](../en/model_doc/mobilenet_v1)                   |       ✅        |         ❌         |      ❌      |\n|                  [MobileNetV2](../en/model_doc/mobilenet_v2)                   |       ✅        |         ❌         |      ❌      |\n|                     [MobileViT](../en/model_doc/mobilevit)                     |       ✅        |         ✅         |      ❌      |\n|                   [MobileViTV2](../en/model_doc/mobilevitv2)                   |       ✅        |         ❌         |      ❌      |\n|                         [MPNet](../en/model_doc/mpnet)                         |       ✅        |         ✅         |      ❌      |\n|                           [MPT](../en/model_doc/mpt)                           |       ✅        |         ❌         |      ❌      |\n|                           [MRA](../en/model_doc/mra)                           |       ✅        |         ❌         |      ❌      |\n|                           [MT5](../en/model_doc/mt5)                           |       ✅        |         ✅         |      ✅      |\n|                      [MusicGen](../en/model_doc/musicgen)                      |       ✅        |         ❌         |      ❌      |\n|                           [MVP](../en/model_doc/mvp)                           |       ✅        |         ❌         |      ❌      |\n|                           [NAT](../en/model_doc/nat)                           |       ✅        |         ❌         |      ❌      |\n|                         [Nezha](../en/model_doc/nezha)                         |       ✅        |         ❌         |      ❌      |\n|                          [NLLB](../en/model_doc/nllb)                          |       ✅        |         ❌         |      ❌      |\n|                      [NLLB-MOE](../en/model_doc/nllb-moe)                      |       ✅        |         ❌         |      ❌      |\n|                        [Nougat](../en/model_doc/nougat)                        |       ✅        |         ✅         |      ✅      |\n|                 [Nyströmformer](../en/model_doc/nystromformer)                 |       ✅        |         ❌         |      ❌      |\n|                     [OneFormer](../en/model_doc/oneformer)                     |       ✅        |         ❌         |      ❌      |\n|                    [OpenAI GPT](../en/model_doc/openai-gpt)                    |       ✅        |         ✅         |      ❌      |\n|                      [OpenAI GPT-2](../en/model_doc/gpt2)                      |       ✅        |         ✅         |      ✅      |\n|                    [OpenLlama](../en/model_doc/open-llama)                     |       ✅        |         ❌         |      ❌      |\n|                           [OPT](../en/model_doc/opt)                           |       ✅        |         ✅         |      ✅      |\n|                       [OWL-ViT](../en/model_doc/owlvit)                        |       ✅        |         ❌         |      ❌      |\n|                         [OWLv2](../en/model_doc/owlv2)                         |       ✅        |         ❌         |      ❌      |\n|                  [PatchTSMixer](../en/model_doc/patchtsmixer)                  |       ✅        |         ❌         |      ❌      |\n|                      [PatchTST](../en/model_doc/patchtst)                      |       ✅        |         ❌         |      ❌      |\n|                       [Pegasus](../en/model_doc/pegasus)                       |       ✅        |         ✅         |      ✅      |\n|                     [PEGASUS-X](../en/model_doc/pegasus_x)                     |       ✅        |         ❌         |      ❌      |\n|                     [Perceiver](../en/model_doc/perceiver)                     |       ✅        |         ❌         |      ❌      |\n|                     [Persimmon](../en/model_doc/persimmon)                     |       ✅        |         ❌         |      ❌      |\n|                           [Phi](../en/model_doc/phi)                           |       ✅        |         ❌         |      ❌      |\n|                       [PhoBERT](../en/model_doc/phobert)                       |       ✅        |         ✅         |      ✅      |\n|                    [Pix2Struct](../en/model_doc/pix2struct)                    |       ✅        |         ❌         |      ❌      |\n|                        [PLBart](../en/model_doc/plbart)                        |       ✅        |         ❌         |      ❌      |\n|                    [PoolFormer](../en/model_doc/poolformer)                    |       ✅        |         ❌         |      ❌      |\n|                     [Pop2Piano](../en/model_doc/pop2piano)                     |       ✅        |         ❌         |      ❌      |\n|                    [ProphetNet](../en/model_doc/prophetnet)                    |       ✅        |         ❌         |      ❌      |\n|                           [PVT](../en/model_doc/pvt)                           |       ✅        |         ❌         |      ❌      |\n|                       [QDQBert](../en/model_doc/qdqbert)                       |       ✅        |         ❌         |      ❌      |\n|                         [Qwen2](../en/model_doc/qwen2)                         |       ✅        |         ❌         |      ❌      |\n|                           [RAG](../en/model_doc/rag)                           |       ✅        |         ✅         |      ❌      |\n|                         [REALM](../en/model_doc/realm)                         |       ✅        |         ❌         |      ❌      |\n|                      [Reformer](../en/model_doc/reformer)                      |       ✅        |         ❌         |      ❌      |\n|                        [RegNet](../en/model_doc/regnet)                        |       ✅        |         ✅         |      ✅      |\n|                       [RemBERT](../en/model_doc/rembert)                       |       ✅        |         ✅         |      ❌      |\n|                        [ResNet](../en/model_doc/resnet)                        |       ✅        |         ✅         |      ✅      |\n|                     [RetriBERT](../en/model_doc/retribert)                     |       ✅        |         ❌         |      ❌      |\n|                       [RoBERTa](../en/model_doc/roberta)                       |       ✅        |         ✅         |      ✅      |\n|          [RoBERTa-PreLayerNorm](../en/model_doc/roberta-prelayernorm)          |       ✅        |         ✅         |      ✅      |\n|                      [RoCBert](../en/model_doc/roc_bert)                       |       ✅        |         ❌         |      ❌      |\n|                      [RoFormer](../en/model_doc/roformer)                      |       ✅        |         ✅         |      ✅      |\n|                          [RWKV](../en/model_doc/rwkv)                          |       ✅        |         ❌         |      ❌      |\n|                           [SAM](../en/model_doc/sam)                           |       ✅        |         ✅         |      ❌      |\n|                  [SeamlessM4T](../en/model_doc/seamless_m4t)                   |       ✅        |         ❌         |      ❌      |\n|                [SeamlessM4Tv2](../en/model_doc/seamless_m4t_v2)                |       ✅        |         ❌         |      ❌      |\n|                     [SegFormer](../en/model_doc/segformer)                     |       ✅        |         ✅         |      ❌      |\n|                        [SegGPT](../en/model_doc/seggpt)                        |       ✅        |         ❌         |      ❌      |\n|                           [SEW](../en/model_doc/sew)                           |       ✅        |         ❌         |      ❌      |\n|                         [SEW-D](../en/model_doc/sew-d)                         |       ✅        |         ❌         |      ❌      |\n|                        [SigLIP](../en/model_doc/siglip)                        |       ✅        |         ❌         |      ❌      |\n|        [Speech Encoder decoder](../en/model_doc/speech-encoder-decoder)        |       ✅        |         ❌         |      ✅      |\n|                 [Speech2Text](../en/model_doc/speech_to_text)                  |       ✅        |         ✅         |      ❌      |\n|                      [SpeechT5](../en/model_doc/speecht5)                      |       ✅        |         ❌         |      ❌      |\n|                      [Splinter](../en/model_doc/splinter)                      |       ✅        |         ❌         |      ❌      |\n|                   [SqueezeBERT](../en/model_doc/squeezebert)                   |       ✅        |         ❌         |      ❌      |\n|                      [StableLm](../en/model_doc/stablelm)                      |       ✅        |         ❌         |      ❌      |\n|                    [Starcoder2](../en/model_doc/starcoder2)                    |       ✅        |         ❌         |      ❌      |\n|                   [SwiftFormer](../en/model_doc/swiftformer)                   |       ✅        |         ❌         |      ❌      |\n|                    [Swin Transformer](../en/model_doc/swin)                    |       ✅        |         ✅         |      ❌      |\n|                 [Swin Transformer V2](../en/model_doc/swinv2)                  |       ✅        |         ❌         |      ❌      |\n|                       [Swin2SR](../en/model_doc/swin2sr)                       |       ✅        |         ❌         |      ❌      |\n|           [SwitchTransformers](../en/model_doc/switch_transformers)            |       ✅        |         ❌         |      ❌      |\n|                            [T5](../en/model_doc/t5)                            |       ✅        |         ✅         |      ✅      |\n|                        [T5v1.1](../en/model_doc/t5v1.1)                        |       ✅        |         ✅         |      ✅      |\n|             [Table Transformer](../en/model_doc/table-transformer)             |       ✅        |         ❌         |      ❌      |\n|                         [TAPAS](../en/model_doc/tapas)                         |       ✅        |         ✅         |      ❌      |\n|                         [TAPEX](../en/model_doc/tapex)                         |       ✅        |         ✅         |      ✅      |\n|       [Time Series Transformer](../en/model_doc/time_series_transformer)       |       ✅        |         ❌         |      ❌      |\n|                   [TimeSformer](../en/model_doc/timesformer)                   |       ✅        |         ❌         |      ❌      |\n|        [Trajectory Transformer](../en/model_doc/trajectory_transformer)        |       ✅        |         ❌         |      ❌      |\n|                  [Transformer-XL](../en/model_doc/transfo-xl)                  |       ✅        |         ✅         |      ❌      |\n|                         [TrOCR](../en/model_doc/trocr)                         |       ✅        |         ❌         |      ❌      |\n|                          [TVLT](../en/model_doc/tvlt)                          |       ✅        |         ❌         |      ❌      |\n|                           [TVP](../en/model_doc/tvp)                           |       ✅        |         ❌         |      ❌      |\n|                           [UL2](../en/model_doc/ul2)                           |       ✅        |         ✅         |      ✅      |\n|                          [UMT5](../en/model_doc/umt5)                          |       ✅        |         ❌         |      ❌      |\n|                     [UniSpeech](../en/model_doc/unispeech)                     |       ✅        |         ❌         |      ❌      |\n|                 [UniSpeechSat](../en/model_doc/unispeech-sat)                  |       ✅        |         ❌         |      ❌      |\n|                       [UnivNet](../en/model_doc/univnet)                       |       ✅        |         ❌         |      ❌      |\n|                       [UPerNet](../en/model_doc/upernet)                       |       ✅        |         ❌         |      ❌      |\n|                           [VAN](../en/model_doc/van)                           |       ✅        |         ❌         |      ❌      |\n|                      [VideoMAE](../en/model_doc/videomae)                      |       ✅        |         ❌         |      ❌      |\n|                          [ViLT](../en/model_doc/vilt)                          |       ✅        |         ❌         |      ❌      |\n|                      [VipLlava](../en/model_doc/vipllava)                      |       ✅        |         ❌         |      ❌      |\n|        [Vision Encoder decoder](../en/model_doc/vision-encoder-decoder)        |       ✅        |         ✅         |      ✅      |\n|       [VisionTextDualEncoder](../en/model_doc/vision-text-dual-encoder)        |       ✅        |         ✅         |      ✅      |\n|                   [VisualBERT](../en/model_doc/visual_bert)                    |       ✅        |         ❌         |      ❌      |\n|                           [ViT](../en/model_doc/vit)                           |       ✅        |         ✅         |      ✅      |\n|                    [ViT Hybrid](../en/model_doc/vit_hybrid)                    |       ✅        |         ❌         |      ❌      |\n|                        [VitDet](../en/model_doc/vitdet)                        |       ✅        |         ❌         |      ❌      |\n|                       [ViTMAE](../en/model_doc/vit_mae)                        |       ✅        |         ✅         |      ❌      |\n|                      [ViTMatte](../en/model_doc/vitmatte)                      |       ✅        |         ❌         |      ❌      |\n|                       [ViTMSN](../en/model_doc/vit_msn)                        |       ✅        |         ❌         |      ❌      |\n|                          [VITS](../en/model_doc/vits)                          |       ✅        |         ❌         |      ❌      |\n|                         [ViViT](../en/model_doc/vivit)                         |       ✅        |         ❌         |      ❌      |\n|                      [Wav2Vec2](../en/model_doc/wav2vec2)                      |       ✅        |         ✅         |      ✅      |\n|                 [Wav2Vec2-BERT](../en/model_doc/wav2vec2-bert)                 |       ✅        |         ❌         |      ❌      |\n|            [Wav2Vec2-Conformer](../en/model_doc/wav2vec2-conformer)            |       ✅        |         ❌         |      ❌      |\n|              [Wav2Vec2Phoneme](../en/model_doc/wav2vec2_phoneme)               |       ✅        |         ✅         |      ✅      |\n|                         [WavLM](../en/model_doc/wavlm)                         |       ✅        |         ❌         |      ❌      |\n|                       [Whisper](../en/model_doc/whisper)                       |       ✅        |         ✅         |      ✅      |\n|                        [X-CLIP](../en/model_doc/xclip)                         |       ✅        |         ❌         |      ❌      |\n|                         [X-MOD](../en/model_doc/xmod)                          |       ✅        |         ❌         |      ❌      |\n|                          [XGLM](../en/model_doc/xglm)                          |       ✅        |         ✅         |      ✅      |\n|                           [XLM](../en/model_doc/xlm)                           |       ✅        |         ✅         |      ❌      |\n|                [XLM-ProphetNet](../en/model_doc/xlm-prophetnet)                |       ✅        |         ❌         |      ❌      |\n|                   [XLM-RoBERTa](../en/model_doc/xlm-roberta)                   |       ✅        |         ✅         |      ✅      |\n|                [XLM-RoBERTa-XL](../en/model_doc/xlm-roberta-xl)                |       ✅        |         ❌         |      ❌      |\n|                         [XLM-V](../en/model_doc/xlm-v)                         |       ✅        |         ✅         |      ✅      |\n|                         [XLNet](../en/model_doc/xlnet)                         |       ✅        |         ✅         |      ❌      |\n|                         [XLS-R](../en/model_doc/xls_r)                         |       ✅        |         ✅         |      ✅      |\n|                 [XLSR-Wav2Vec2](../en/model_doc/xlsr_wav2vec2)                 |       ✅        |         ✅         |      ✅      |\n|                         [YOLOS](../en/model_doc/yolos)                         |       ✅        |         ❌         |      ❌      |\n|                          [YOSO](../en/model_doc/yoso)                          |       ✅        |         ❌         |      ❌      |\n\n<!-- End table-->",
    "971": "一级标题：安装\n二级标题：无\n内容：\n为你正在使用的深度学习框架安装 🤗 Transformers、设置缓存，并选择性配置 🤗 Transformers 以离线运行。\n\n🤗 Transformers 已在 Python 3.6+、PyTorch 1.1.0+、TensorFlow 2.0+ 以及 Flax 上进行测试。针对你使用的深度学习框架，请参照以下安装说明进行安装：\n\n* [PyTorch](https://pytorch.org/get-started/locally/) 安装说明。\n* [TensorFlow 2.0](https://www.tensorflow.org/install/pip) 安装说明。\n* [Flax](https://flax.readthedocs.io/en/latest/) 安装说明。",
    "972": "一级标题：安装\n二级标题：使用 pip 安装\n内容：\n你应该使用 [虚拟环境](https://docs.python.org/3/library/venv.html) 安装 🤗 Transformers。如果你不熟悉 Python 虚拟环境，请查看此 [教程](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)。使用虚拟环境，你可以轻松管理不同项目，避免不同依赖项之间的兼容性问题。\n\n首先，在项目目录中创建虚拟环境：\n\n```bash\npython -m venv .env\n```\n\n在 Linux 和 MacOs 系统中激活虚拟环境：\n\n```bash\nsource .env/bin/activate\n```\n在 Windows 系统中激活虚拟环境：\n\n```bash\n.env/Scripts/activate\n```\n\n现在你可以使用以下命令安装 🤗 Transformers：\n\n```bash\npip install transformers\n```\n\n若仅需 CPU 支持，可以使用单行命令方便地安装 🤗 Transformers 和深度学习库。例如，使用以下命令安装 🤗 Transformers 和 PyTorch：\n\n```bash\npip install 'transformers[torch]'\n```\n\n🤗 Transformers 和 TensorFlow 2.0：\n\n```bash\npip install 'transformers[tf-cpu]'\n```\n\n<Tip warning={true}>\n\nM1 / ARM用户\n\n在安装 TensorFlow 2.0 前，你需要安装以下库：\n```bash\nbrew install cmake\nbrew install pkg-config\n```\n\n</Tip>\n\n🤗 Transformers 和 Flax:\n\n```bash\npip install 'transformers[flax]'\n```\n\n最后，运行以下命令以检查 🤗 Transformers 是否已被正确安装。该命令将下载一个预训练模型：\n\n```bash\npython -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\"\n```\n\n然后打印标签以及分数：\n\n```bash\n[{'label': 'POSITIVE', 'score': 0.9998704791069031}]\n```",
    "973": "一级标题：安装\n二级标题：源码安装\n内容：\n使用以下命令从源码安装 🤗 Transformers：\n\n```bash\npip install git+https://github.com/huggingface/transformers\n```\n\n此命令下载的是最新的前沿 `main` 版本而不是最新的 `stable` 版本。`main` 版本适用于跟最新开发保持一致。例如，上次正式版发布带来的 bug 被修复了，但新版本尚未被推出。但是，这也说明 `main` 版本并不一定总是稳定的。我们努力保持 `main` 版本的可操作性，大多数问题通常在几个小时或一天以内就能被解决。如果你遇到问题，请提个 [Issue](https://github.com/huggingface/transformers/issues) 以便我们能更快修复。\n\n运行以下命令以检查 🤗 Transformers 是否已被正确安装：\n\n```bash\npython -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))\"\n```",
    "974": "一级标题：安装\n二级标题：可编辑安装\n内容：\n如果你有下列需求，需要进行可编辑安装：\n\n* 使用源码的 `main` 版本。\n* 为 🤗 Transformers 贡献代码，需要测试代码中的更改。\n\n使用以下命令克隆仓库并安装 🤗 Transformers：\n\n```bash\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\npip install -e .\n```\n\n这些命令将会链接你克隆的仓库以及你的 Python 库路径。现在，Python 不仅会在正常的库路径中搜索库，也会在你克隆到的文件夹中进行查找。例如，如果你的 Python 包通常本应安装在 `~/anaconda3/envs/main/lib/python3.7/site-packages/` 目录中，在这种情况下 Python 也会搜索你克隆到的文件夹：`~/transformers/`。\n\n<Tip warning={true}>\n\n如果你想继续使用这个库，必须保留 `transformers` 文件夹。\n\n</Tip>\n\n现在，你可以使用以下命令，将你克隆的 🤗 Transformers 库轻松更新至最新版本：\n\n```bash\ncd ~/transformers/\ngit pull\n```\n\n你的 Python 环境将在下次运行时找到 `main` 版本的 🤗 Transformers。",
    "975": "一级标题：安装\n二级标题：使用 conda 安装\n内容：\n从 conda 的 `conda-forge` 频道安装：\n\n```bash\nconda install conda-forge::transformers\n```",
    "976": "一级标题：安装\n二级标题：缓存设置\n内容：\n预训练模型会被下载并本地缓存到 `~/.cache/huggingface/hub`。这是由环境变量 `TRANSFORMERS_CACHE` 指定的默认目录。在 Windows 上，默认目录为 `C:\\Users\\username\\.cache\\huggingface\\hub`。你可以按照不同优先级改变下述环境变量，以指定不同的缓存目录。\n\n1. 环境变量（默认）: `HF_HUB_CACHE` 或 `TRANSFORMERS_CACHE`。\n2. 环境变量 `HF_HOME`。\n3. 环境变量 `XDG_CACHE_HOME` + `/huggingface`。\n\n<Tip>\n\n除非你明确指定了环境变量 `TRANSFORMERS_CACHE`，🤗 Transformers 将可能会使用较早版本设置的环境变量 `PYTORCH_TRANSFORMERS_CACHE` 或 `PYTORCH_PRETRAINED_BERT_CACHE`。\n\n</Tip>",
    "977": "一级标题：安装\n二级标题：离线模式\n内容：\n🤗 Transformers 可以仅使用本地文件在防火墙或离线环境中运行。设置环境变量 `HF_HUB_OFFLINE=1` 以启用该行为。\n\n<Tip>\n\n通过设置环境变量 `HF_DATASETS_OFFLINE=1` 将 [🤗 Datasets](https://huggingface.co/docs/datasets/) 添加至你的离线训练工作流程中。\n\n</Tip>\n\n例如，你通常会使用以下命令对外部实例进行防火墙保护的的普通网络上运行程序：\n\n```bash\npython examples/pytorch/translation/run_translation.py --model_name_or_path google-t5/t5-small --dataset_name wmt16 --dataset_config ro-en ...\n```\n\n在离线环境中运行相同的程序：\n\n```bash\nHF_DATASETS_OFFLINE=1 HF_HUB_OFFLINE=1 \\\npython examples/pytorch/translation/run_translation.py --model_name_or_path google-t5/t5-small --dataset_name wmt16 --dataset_config ro-en ...\n```\n\n现在脚本可以应该正常运行，而无需挂起或等待超时，因为它知道只应查找本地文件。\n\n### 获取离线时使用的模型和分词器\n\n另一种离线时使用 🤗 Transformers 的方法是预先下载好文件，然后在需要离线使用时指向它们的离线路径。有三种实现的方法：\n\n* 单击 [Model Hub](https://huggingface.co/models) 用户界面上的 ↓ 图标下载文件。\n\n    ![下载图标](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/download-icon.png)\n\n* 使用 [`PreTrainedModel.from_pretrained`] 和 [`PreTrainedModel.save_pretrained`] 工作流程：\n\n    1. 预先使用 [`PreTrainedModel.from_pretrained`] 下载文件：\n\n    ```py\n    >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n    >>> tokenizer = AutoTokenizer.from_pretrained(\"bigscience/T0_3B\")\n    >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0_3B\")\n    ```\n\n    2. 使用 [`PreTrainedModel.save_pretrained`] 将文件保存至指定目录：\n\n    ```py\n    >>> tokenizer.save_pretrained(\"./your/path/bigscience_t0\")\n    >>> model.save_pretrained(\"./your/path/bigscience_t0\")\n    ```\n\n    3. 现在，你可以在离线时从指定目录使用 [`PreTrainedModel.from_pretrained`] 重新加载你的文件：\n\n    ```py\n    >>> tokenizer = AutoTokenizer.from_pretrained(\"./your/path/bigscience_t0\")\n    >>> model = AutoModel.from_pretrained(\"./your/path/bigscience_t0\")\n    ```\n\n* 使用代码用 [huggingface_hub](https://github.com/huggingface/huggingface_hub/tree/main/src/huggingface_hub) 库下载文件：\n\n    1. 在你的虚拟环境中安装 `huggingface_hub` 库：\n\n    ```bash\n    python -m pip install huggingface_hub\n    ```\n\n    2. 使用 [`hf_hub_download`](https://huggingface.co/docs/hub/adding-a-library#download-files-from-the-hub) 函数将文件下载到指定路径。例如，以下命令将 `config.json` 文件从 [T0](https://huggingface.co/bigscience/T0_3B) 模型下载至你想要的路径：\n\n    ```py\n    >>> from huggingface_hub import hf_hub_download\n\n    >>> hf_hub_download(repo_id=\"bigscience/T0_3B\", filename=\"config.json\", cache_dir=\"./your/path/bigscience_t0\")\n    ```\n\n下载完文件并在本地缓存后，指定其本地路径以加载和使用该模型：\n\n```py\n>>> from transformers import AutoConfig\n\n>>> config = AutoConfig.from_pretrained(\"./your/path/bigscience_t0/config.json\")\n```\n\n<Tip>\n\n请参阅 [如何从 Hub 下载文件](https://huggingface.co/docs/hub/how-to-downstream) 部分，获取有关下载存储在 Hub 上文件的更多详细信息。\n\n</Tip>",
    "978": "一级标题：`FeatureExtractors`的工具\n二级标题：无\n内容：\n此页面列出了音频 [`FeatureExtractor`] 可以使用的所有实用函数，以便使用常见的算法（如 *Short Time Fourier Transform* 或 *log mel spectrogram*）从原始音频中计算特殊特征。\n\n其中大多数仅在您研究库中音频processors的代码时有用。",
    "979": "一级标题：`FeatureExtractors`的工具\n二级标题：音频转换\n内容：\n[[autodoc]] audio_utils.hertz_to_mel\n\n[[autodoc]] audio_utils.mel_to_hertz\n\n[[autodoc]] audio_utils.mel_filter_bank\n\n[[autodoc]] audio_utils.optimal_fft_length\n\n[[autodoc]] audio_utils.window_function\n\n[[autodoc]] audio_utils.spectrogram\n\n[[autodoc]] audio_utils.power_to_db\n\n[[autodoc]] audio_utils.amplitude_to_db",
    "980": "一级标题：通用工具\n二级标题：无\n内容：\n此页面列出了在`utils.py`文件中找到的所有Transformers通用实用函数。\n\n其中大多数仅在您研究库中的通用代码时才有用。",
    "981": "一级标题：通用工具\n二级标题：Enums和namedtuples(命名元组)\n内容：\n[[autodoc]] utils.ExplicitEnum\n\n[[autodoc]] utils.PaddingStrategy\n\n[[autodoc]] utils.TensorType",
    "982": "一级标题：通用工具\n二级标题：特殊的装饰函数\n内容：\n[[autodoc]] utils.add_start_docstrings\n\n[[autodoc]] utils.add_start_docstrings_to_model_forward\n\n[[autodoc]] utils.add_end_docstrings\n\n[[autodoc]] utils.add_code_sample_docstrings\n\n[[autodoc]] utils.replace_return_docstrings",
    "983": "一级标题：通用工具\n二级标题：特殊的属性\n内容：\n[[autodoc]] utils.cached_property",
    "984": "一级标题：通用工具\n二级标题：其他实用程序\n内容：\n[[autodoc]] utils._LazyModule",
    "985": "一级标题：用于生成的工具\n二级标题：无\n内容：\n此页面列出了所有由 [`~generation.GenerationMixin.generate`]。",
    "986": "一级标题：用于生成的工具\n二级标题：生成输出\n内容：\n[`~generation.GenerationMixin.generate`] 的输出是 [`~utils.ModelOutput`] 的一个子类的实例。这个输出是一种包含 [`~generation.GenerationMixin.generate`] 返回的所有信息数据结构，但也可以作为元组或字典使用。\n这里是一个例子：\n\n\n```python\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\nmodel = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n\ninputs = tokenizer(\"Hello, my dog is cute and \", return_tensors=\"pt\")\ngeneration_output = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)\n```\n\n`generation_output` 的对象是 [`~generation.GenerateDecoderOnlyOutput`] 的一个实例，从该类的文档中我们可以看到，这意味着它具有以下属性：\n\n- `sequences`: 生成的tokens序列\n- `scores`（可选）: 每个生成步骤的语言建模头的预测分数\n- `hidden_states`（可选）: 每个生成步骤模型的hidden states\n- `attentions`（可选）: 每个生成步骤模型的注意力权重\n\n在这里，由于我们传递了 `output_scores=True`，我们具有 `scores` 属性。但我们没有 `hidden_states` 和 `attentions`，因为没有传递 `output_hidden_states=True` 或 `output_attentions=True`。\n\n您可以像通常一样访问每个属性，如果该属性未被模型返回，则将获得 `None`。例如，在这里 `generation_output.scores` 是语言建模头的所有生成预测分数，而 `generation_output.attentions` 为 `None`。\n\n当我们将 `generation_output` 对象用作元组时，它只保留非 `None` 值的属性。例如，在这里它有两个元素，`loss` 然后是 `logits`，所以\n\n\n```python\ngeneration_output[:2]\n```\n\n将返回元组`(generation_output.sequences, generation_output.scores)`。\n\n当我们将`generation_output`对象用作字典时，它只保留非`None`的属性。例如，它有两个键，分别是`sequences`和`scores`。\n\n我们在此记录所有输出类型。\n\n\n### PyTorch\n\n[[autodoc]] generation.GenerateDecoderOnlyOutput\n\n[[autodoc]] generation.GenerateEncoderDecoderOutput\n\n[[autodoc]] generation.GenerateBeamDecoderOnlyOutput\n\n[[autodoc]] generation.GenerateBeamEncoderDecoderOutput\n\n### TensorFlow\n\n[[autodoc]] generation.TFGreedySearchEncoderDecoderOutput\n\n[[autodoc]] generation.TFGreedySearchDecoderOnlyOutput\n\n[[autodoc]] generation.TFSampleEncoderDecoderOutput\n\n[[autodoc]] generation.TFSampleDecoderOnlyOutput\n\n[[autodoc]] generation.TFBeamSearchEncoderDecoderOutput\n\n[[autodoc]] generation.TFBeamSearchDecoderOnlyOutput\n\n[[autodoc]] generation.TFBeamSampleEncoderDecoderOutput\n\n[[autodoc]] generation.TFBeamSampleDecoderOnlyOutput\n\n[[autodoc]] generation.TFContrastiveSearchEncoderDecoderOutput\n\n[[autodoc]] generation.TFContrastiveSearchDecoderOnlyOutput\n\n### FLAX\n\n[[autodoc]] generation.FlaxSampleOutput\n\n[[autodoc]] generation.FlaxGreedySearchOutput\n\n[[autodoc]] generation.FlaxBeamSearchOutput",
    "987": "一级标题：用于生成的工具\n二级标题：LogitsProcessor\n内容：\n[`LogitsProcessor`] 可以用于修改语言模型头的预测分数以进行生成\n\n\n### PyTorch\n\n[[autodoc]] AlternatingCodebooksLogitsProcessor\n    - __call__\n\n[[autodoc]] ClassifierFreeGuidanceLogitsProcessor\n    - __call__\n\n[[autodoc]] EncoderNoRepeatNGramLogitsProcessor\n    - __call__\n\n[[autodoc]] EncoderRepetitionPenaltyLogitsProcessor\n    - __call__\n\n[[autodoc]] EpsilonLogitsWarper\n    - __call__\n\n[[autodoc]] EtaLogitsWarper\n    - __call__\n\n[[autodoc]] ExponentialDecayLengthPenalty\n    - __call__\n\n[[autodoc]] ForcedBOSTokenLogitsProcessor\n    - __call__\n\n[[autodoc]] ForcedEOSTokenLogitsProcessor\n    - __call__\n\n[[autodoc]] HammingDiversityLogitsProcessor\n    - __call__\n\n[[autodoc]] InfNanRemoveLogitsProcessor\n    - __call__\n\n[[autodoc]] LogitNormalization\n    - __call__\n\n[[autodoc]] LogitsProcessor\n    - __call__\n\n[[autodoc]] LogitsProcessorList\n    - __call__\n\n[[autodoc]] MinLengthLogitsProcessor\n    - __call__\n\n[[autodoc]] MinNewTokensLengthLogitsProcessor\n    - __call__\n\n[[autodoc]] NoBadWordsLogitsProcessor\n    - __call__\n\n[[autodoc]] NoRepeatNGramLogitsProcessor\n    - __call__\n\n[[autodoc]] PrefixConstrainedLogitsProcessor\n    - __call__\n\n[[autodoc]] RepetitionPenaltyLogitsProcessor\n    - __call__\n\n[[autodoc]] SequenceBiasLogitsProcessor\n    - __call__\n\n[[autodoc]] SuppressTokensAtBeginLogitsProcessor\n    - __call__\n\n[[autodoc]] SuppressTokensLogitsProcessor\n    - __call__\n\n[[autodoc]] TemperatureLogitsWarper\n    - __call__\n\n[[autodoc]] TopKLogitsWarper\n    - __call__\n\n[[autodoc]] TopPLogitsWarper\n    - __call__\n\n[[autodoc]] TypicalLogitsWarper\n    - __call__\n\n[[autodoc]] UnbatchedClassifierFreeGuidanceLogitsProcessor\n    - __call__\n\n[[autodoc]] WhisperTimeStampLogitsProcessor\n    - __call__\n\n### TensorFlow\n\n[[autodoc]] TFForcedBOSTokenLogitsProcessor\n    - __call__\n\n[[autodoc]] TFForcedEOSTokenLogitsProcessor\n    - __call__\n\n[[autodoc]] TFForceTokensLogitsProcessor\n    - __call__\n\n[[autodoc]] TFLogitsProcessor\n    - __call__\n\n[[autodoc]] TFLogitsProcessorList\n    - __call__\n\n[[autodoc]] TFLogitsWarper\n    - __call__\n\n[[autodoc]] TFMinLengthLogitsProcessor\n    - __call__\n\n[[autodoc]] TFNoBadWordsLogitsProcessor\n    - __call__\n\n[[autodoc]] TFNoRepeatNGramLogitsProcessor\n    - __call__\n\n[[autodoc]] TFRepetitionPenaltyLogitsProcessor\n    - __call__\n\n[[autodoc]] TFSuppressTokensAtBeginLogitsProcessor\n    - __call__\n\n[[autodoc]] TFSuppressTokensLogitsProcessor\n    - __call__\n\n[[autodoc]] TFTemperatureLogitsWarper\n    - __call__\n\n[[autodoc]] TFTopKLogitsWarper\n    - __call__\n\n[[autodoc]] TFTopPLogitsWarper\n    - __call__\n\n### FLAX\n\n[[autodoc]] FlaxForcedBOSTokenLogitsProcessor\n    - __call__\n\n[[autodoc]] FlaxForcedEOSTokenLogitsProcessor\n    - __call__\n\n[[autodoc]] FlaxForceTokensLogitsProcessor\n    - __call__\n\n[[autodoc]] FlaxLogitsProcessor\n    - __call__\n\n[[autodoc]] FlaxLogitsProcessorList\n    - __call__\n\n[[autodoc]] FlaxLogitsWarper\n    - __call__\n\n[[autodoc]] FlaxMinLengthLogitsProcessor\n    - __call__\n\n[[autodoc]] FlaxSuppressTokensAtBeginLogitsProcessor\n    - __call__\n\n[[autodoc]] FlaxSuppressTokensLogitsProcessor\n    - __call__\n\n[[autodoc]] FlaxTemperatureLogitsWarper\n    - __call__\n\n[[autodoc]] FlaxTopKLogitsWarper\n    - __call__\n\n[[autodoc]] FlaxTopPLogitsWarper\n    - __call__\n\n[[autodoc]] FlaxWhisperTimeStampLogitsProcessor\n    - __call__",
    "988": "一级标题：用于生成的工具\n二级标题：StoppingCriteria\n内容：\n可以使用[`StoppingCriteria`]来更改停止生成的时间（除了EOS token以外的方法）。请注意，这仅适用于我们的PyTorch实现。\n\n\n[[autodoc]] StoppingCriteria\n    - __call__\n\n[[autodoc]] StoppingCriteriaList\n    - __call__\n\n[[autodoc]] MaxLengthCriteria\n    - __call__\n\n[[autodoc]] MaxTimeCriteria\n    - __call__",
    "989": "一级标题：用于生成的工具\n二级标题：Constraints\n内容：\n可以使用[`Constraint`]来强制生成结果包含输出中的特定tokens或序列。请注意，这仅适用于我们的PyTorch实现。\n\n[[autodoc]] Constraint\n\n[[autodoc]] PhrasalConstraint\n\n[[autodoc]] DisjunctiveConstraint\n\n[[autodoc]] ConstraintListState",
    "990": "一级标题：用于生成的工具\n二级标题：BeamSearch\n内容：\n[[autodoc]] BeamScorer\n    - process\n    - finalize\n\n[[autodoc]] BeamSearchScorer\n    - process\n    - finalize\n\n[[autodoc]] ConstrainedBeamSearchScorer\n    - process\n    - finalize",
    "991": "一级标题：用于生成的工具\n二级标题：Streamers\n内容：\n[[autodoc]] TextStreamer\n\n[[autodoc]] TextIteratorStreamer",
    "992": "一级标题：Image Processors的工具\n二级标题：无\n内容：\n此页面列出了image processors使用的所有实用函数功能，主要是用于处理图像的功能变换。\n\n其中大多数仅在您研究库中image processors的代码时有用。",
    "993": "一级标题：Image Processors的工具\n二级标题：图像转换\n内容：\n[[autodoc]] image_transforms.center_crop\n\n[[autodoc]] image_transforms.center_to_corners_format\n\n[[autodoc]] image_transforms.corners_to_center_format\n\n[[autodoc]] image_transforms.id_to_rgb\n\n[[autodoc]] image_transforms.normalize\n\n[[autodoc]] image_transforms.pad\n\n[[autodoc]] image_transforms.rgb_to_id\n\n[[autodoc]] image_transforms.rescale\n\n[[autodoc]] image_transforms.resize\n\n[[autodoc]] image_transforms.to_pil_image",
    "994": "一级标题：Image Processors的工具\n二级标题：ImageProcessingMixin\n内容：\n[[autodoc]] image_processing_utils.ImageProcessingMixin",
    "995": "一级标题：自定义层和工具\n二级标题：无\n内容：\n此页面列出了库使用的所有自定义层，以及它为模型提供的实用函数。\n\n其中大多数只有在您研究库中模型的代码时才有用。",
    "996": "一级标题：自定义层和工具\n二级标题：Pytorch自定义模块\n内容：\n[[autodoc]] pytorch_utils.Conv1D",
    "997": "一级标题：自定义层和工具\n二级标题：PyTorch帮助函数\n内容：\n[[autodoc]] pytorch_utils.apply_chunking_to_forward\n\n[[autodoc]] pytorch_utils.find_pruneable_heads_and_indices\n\n[[autodoc]] pytorch_utils.prune_layer\n\n[[autodoc]] pytorch_utils.prune_conv1d_layer\n\n[[autodoc]] pytorch_utils.prune_linear_layer",
    "998": "一级标题：自定义层和工具\n二级标题：TensorFlow自定义层\n内容：\n[[autodoc]] modeling_tf_utils.TFConv1D\n\n[[autodoc]] modeling_tf_utils.TFSequenceSummary",
    "999": "一级标题：自定义层和工具\n二级标题：TensorFlow loss 函数\n内容：\n[[autodoc]] modeling_tf_utils.TFCausalLanguageModelingLoss\n\n[[autodoc]] modeling_tf_utils.TFMaskedLanguageModelingLoss\n\n[[autodoc]] modeling_tf_utils.TFMultipleChoiceLoss\n\n[[autodoc]] modeling_tf_utils.TFQuestionAnsweringLoss\n\n[[autodoc]] modeling_tf_utils.TFSequenceClassificationLoss\n\n[[autodoc]] modeling_tf_utils.TFTokenClassificationLoss",
    "1000": "一级标题：自定义层和工具\n二级标题：TensorFlow帮助函数\n内容：\n[[autodoc]] modeling_tf_utils.get_initializer\n\n[[autodoc]] modeling_tf_utils.keras_serializable\n\n[[autodoc]] modeling_tf_utils.shape_list",
    "1001": "一级标题：pipelines的工具\n二级标题：无\n内容：\n此页面列出了库为pipelines提供的所有实用程序功能。\n\n其中大多数只有在您研究库中模型的代码时才有用。",
    "1002": "一级标题：pipelines的工具\n二级标题：参数处理\n内容：\n[[autodoc]] pipelines.ArgumentHandler\n\n[[autodoc]] pipelines.ZeroShotClassificationArgumentHandler\n\n[[autodoc]] pipelines.QuestionAnsweringArgumentHandler",
    "1003": "一级标题：pipelines的工具\n二级标题：数据格式\n内容：\n[[autodoc]] pipelines.PipelineDataFormat\n\n[[autodoc]] pipelines.CsvPipelineDataFormat\n\n[[autodoc]] pipelines.JsonPipelineDataFormat\n\n[[autodoc]] pipelines.PipedPipelineDataFormat",
    "1004": "一级标题：pipelines的工具\n二级标题：实用函数\n内容：\n[[autodoc]] pipelines.PipelineException",
    "1005": "一级标题：时间序列工具\n二级标题：无\n内容：\n此页面列出了可用于时间序列类模型的所有实用函数和类。\n\n其中大多数仅在您研究时间序列模型的代码，或希望添加到分布输出类集合时有用。",
    "1006": "一级标题：时间序列工具\n二级标题：输出分布\n内容：\n[[autodoc]] time_series_utils.NormalOutput\n\n[[autodoc]] time_series_utils.StudentTOutput\n\n[[autodoc]] time_series_utils.NegativeBinomialOutput",
    "1007": "一级标题：Tokenizers的工具\n二级标题：无\n内容：\n并保留格式：此页面列出了tokenizers使用的所有实用函数，主要是类\n[`~tokenization_utils_base.PreTrained TokenizerBase`] 实现了常用方法之间的\n[`PreTrained Tokenizer`] 和 [`PreTrained TokenizerFast`] 以及混合类\n[`~tokenization_utils_base.SpecialTokens Mixin`]。\n\n其中大多数只有在您研究库中tokenizers的代码时才有用。",
    "1008": "一级标题：Tokenizers的工具\n二级标题：PreTrainedTokenizerBase\n内容：\n[[autodoc]] tokenization_utils_base.PreTrainedTokenizerBase\n    - __call__\n    - all",
    "1009": "一级标题：Tokenizers的工具\n二级标题：SpecialTokensMixin\n内容：\n[[autodoc]] tokenization_utils_base.SpecialTokensMixin",
    "1010": "一级标题：Tokenizers的工具\n二级标题：Enums和namedtuples(命名元组)\n内容：\n[[autodoc]] tokenization_utils_base.TruncationStrategy\n\n[[autodoc]] tokenization_utils_base.CharSpan\n\n[[autodoc]] tokenization_utils_base.TokenSpan",
    "1011": "一级标题：Trainer的工具\n二级标题：无\n内容：\n此页面列出了 [`Trainer`] 使用的所有实用函数。\n\n其中大多数仅在您研究库中Trainer的代码时有用。",
    "1012": "一级标题：Trainer的工具\n二级标题：工具\n内容：\n[[autodoc]] EvalPrediction\n\n[[autodoc]] IntervalStrategy\n\n[[autodoc]] enable_full_determinism\n\n[[autodoc]] set_seed\n\n[[autodoc]] torch_distributed_zero_first",
    "1013": "一级标题：Trainer的工具\n二级标题：Callbacks内部机制\n内容：\n[[autodoc]] trainer_callback.CallbackHandler",
    "1014": "一级标题：Trainer的工具\n二级标题：分布式评估\n内容：\n[[autodoc]] trainer_pt_utils.DistributedTensorGatherer",
    "1015": "一级标题：Trainer的工具\n二级标题：Trainer参数解析\n内容：\n[[autodoc]] HfArgumentParser",
    "1016": "一级标题：Trainer的工具\n二级标题：Debug工具\n内容：\n[[autodoc]] debug_utils.DebugUnderflowOverflow",
    "1017": "一级标题：Callbacks\n二级标题：无\n内容：\nCallbacks可以用来自定义PyTorch [Trainer]中训练循环行为的对象（此功能尚未在TensorFlow中实现），该对象可以检查训练循环状态（用于进度报告、在TensorBoard或其他ML平台上记录日志等），并做出决策（例如提前停止）。\n\nCallbacks是“只读”的代码片段，除了它们返回的[TrainerControl]对象外，它们不能更改训练循环中的任何内容。对于需要更改训练循环的自定义，您应该继承[Trainer]并重载您需要的方法（有关示例，请参见[trainer](trainer)）。\n\n默认情况下，`TrainingArguments.report_to` 设置为\"all\"，然后[Trainer]将使用以下callbacks。\n\n\n- [`DefaultFlowCallback`]，它处理默认的日志记录、保存和评估行为\n- [`PrinterCallback`] 或 [`ProgressCallback`]，用于显示进度和打印日志（如果通过[`TrainingArguments`]停用tqdm，则使用第一个函数；否则使用第二个）。\n- [`~integrations.TensorBoardCallback`]，如果TensorBoard可访问（通过PyTorch版本 >= 1.4 或者 tensorboardX）。\n- [`~integrations.WandbCallback`]，如果安装了[wandb](https://www.wandb.com/)。\n- [`~integrations.CometCallback`]，如果安装了[comet_ml](https://www.comet.com/site/)。\n- [`~integrations.MLflowCallback`]，如果安装了[mlflow](https://www.mlflow.org/)。\n- [`~integrations.NeptuneCallback`]，如果安装了[neptune](https://neptune.ai/)。\n- [`~integrations.AzureMLCallback`]，如果安装了[azureml-sdk](https://pypi.org/project/azureml-sdk/)。\n- [`~integrations.CodeCarbonCallback`]，如果安装了[codecarbon](https://pypi.org/project/codecarbon/)。\n- [`~integrations.ClearMLCallback`]，如果安装了[clearml](https://github.com/allegroai/clearml)。\n- [`~integrations.DagsHubCallback`]，如果安装了[dagshub](https://dagshub.com/)。\n- [`~integrations.FlyteCallback`]，如果安装了[flyte](https://flyte.org/)。\n- [`~integrations.DVCLiveCallback`]，如果安装了[dvclive](https://dvc.org/doc/dvclive)。\n- [`~integrations.SwanLabCallback`]，如果安装了[swanlab](http://swanlab.cn/)。\n\n如果安装了一个软件包，但您不希望使用相关的集成，您可以将 `TrainingArguments.report_to` 更改为仅包含您想要使用的集成的列表（例如 `[\"azure_ml\", \"wandb\"]`）。\n\n实现callbacks的主要类是[`TrainerCallback`]。它获取用于实例化[`Trainer`]的[`TrainingArguments`]，可以通过[`TrainerState`]访问该Trainer的内部状态，并可以通过[`TrainerControl`]对训练循环执行一些操作。",
    "1018": "一级标题：Callbacks\n二级标题：可用的Callbacks\n内容：\n这里是库里可用[`TrainerCallback`]的列表：\n\n[[autodoc]] integrations.CometCallback\n    - setup\n\n[[autodoc]] DefaultFlowCallback\n\n[[autodoc]] PrinterCallback\n\n[[autodoc]] ProgressCallback\n\n[[autodoc]] EarlyStoppingCallback\n\n[[autodoc]] integrations.TensorBoardCallback\n\n[[autodoc]] integrations.WandbCallback\n    - setup\n\n[[autodoc]] integrations.MLflowCallback\n    - setup\n\n[[autodoc]] integrations.AzureMLCallback\n\n[[autodoc]] integrations.CodeCarbonCallback\n\n[[autodoc]] integrations.NeptuneCallback\n\n[[autodoc]] integrations.ClearMLCallback\n\n[[autodoc]] integrations.DagsHubCallback\n\n[[autodoc]] integrations.FlyteCallback\n\n[[autodoc]] integrations.DVCLiveCallback\n    - setup\n\n[[autodoc]] integrations.SwanLabCallback\n    - setup",
    "1019": "一级标题：Callbacks\n二级标题：TrainerCallback\n内容：\n[[autodoc]] TrainerCallback\n\n以下是如何使用PyTorch注册自定义callback的示例：\n\n[`Trainer`]:\n\n```python\nclass MyCallback(TrainerCallback):\n    \"A callback that prints a message at the beginning of training\"\n\n    def on_train_begin(self, args, state, control, **kwargs):\n        print(\"Starting training\")\n\n\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    callbacks=[MyCallback],  # We can either pass the callback class this way or an instance of it (MyCallback())\n)\n```\n\n注册callback的另一种方式是调用 `trainer.add_callback()`，如下所示：\n\n\n```python\ntrainer = Trainer(...)\ntrainer.add_callback(MyCallback)\n# Alternatively, we can pass an instance of the callback class\ntrainer.add_callback(MyCallback())\n```",
    "1020": "一级标题：Callbacks\n二级标题：TrainerState\n内容：\n[[autodoc]] TrainerState",
    "1021": "一级标题：Callbacks\n二级标题：TrainerControl\n内容：\n[[autodoc]] TrainerControl",
    "1022": "一级标题：Configuration\n二级标题：无\n内容：\n基类[`PretrainedConfig`]实现了从本地文件或目录加载/保存配置的常见方法，或下载库提供的预训练模型配置（从HuggingFace的AWS S3库中下载）。\n\n每个派生的配置类都实现了特定于模型的属性。所有配置类中共同存在的属性有：`hidden_size`、`num_attention_heads` 和 `num_hidden_layers`。文本模型进一步添加了 `vocab_size`。",
    "1023": "一级标题：Configuration\n二级标题：PretrainedConfig\n内容：\n[[autodoc]] PretrainedConfig\n    - push_to_hub\n    - all",
    "1024": "一级标题：Data Collator\n二级标题：无\n内容：\nData collators是一个对象，通过使用数据集元素列表作为输入来形成一个批次。这些元素与 `train_dataset` 或 `eval_dataset` 的元素类型相同。\n\n为了能够构建批次，Data collators可能会应用一些预处理（比如填充）。其中一些（比如[`DataCollatorForLanguageModeling`]）还会在形成的批次上应用一些随机数据增强（比如随机掩码）。\n\n在[示例脚本](../examples)或[示例notebooks](../notebooks)中可以找到使用的示例。",
    "1025": "一级标题：Data Collator\n二级标题：Default data collator\n内容：\n[[autodoc]] data.data_collator.default_data_collator",
    "1026": "一级标题：Data Collator\n二级标题：DefaultDataCollator\n内容：\n[[autodoc]] data.data_collator.DefaultDataCollator",
    "1027": "一级标题：Data Collator\n二级标题：DataCollatorWithPadding\n内容：\n[[autodoc]] data.data_collator.DataCollatorWithPadding",
    "1028": "一级标题：Data Collator\n二级标题：DataCollatorForTokenClassification\n内容：\n[[autodoc]] data.data_collator.DataCollatorForTokenClassification",
    "1029": "一级标题：Data Collator\n二级标题：DataCollatorForSeq2Seq\n内容：\n[[autodoc]] data.data_collator.DataCollatorForSeq2Seq",
    "1030": "一级标题：Data Collator\n二级标题：DataCollatorForLanguageModeling\n内容：\n[[autodoc]] data.data_collator.DataCollatorForLanguageModeling\n    - numpy_mask_tokens\n    - tf_mask_tokens\n    - torch_mask_tokens",
    "1031": "一级标题：Data Collator\n二级标题：DataCollatorForWholeWordMask\n内容：\n[[autodoc]] data.data_collator.DataCollatorForWholeWordMask\n    - numpy_mask_tokens\n    - tf_mask_tokens\n    - torch_mask_tokens",
    "1032": "一级标题：Data Collator\n二级标题：DataCollatorForPermutationLanguageModeling\n内容：\n[[autodoc]] data.data_collator.DataCollatorForPermutationLanguageModeling\n    - numpy_mask_tokens\n    - tf_mask_tokens\n    - torch_mask_tokens",
    "1033": "一级标题：DeepSpeed集成\n二级标题：无\n内容：\n[DeepSpeed](https://github.com/deepspeedai/DeepSpeed)实现了[ZeRO论文](https://huggingface.co/papers/1910.02054)中描述的所有内容。目前，它提供对以下功能的全面支持：\n\n1. 优化器状态分区（ZeRO stage 1）\n2. 梯度分区（ZeRO stage 2）\n3. 参数分区（ZeRO stage 3）\n4. 自定义混合精度训练处理\n5. 一系列基于CUDA扩展的快速优化器\n6. ZeRO-Offload 到 CPU 和 NVMe\n\nZeRO-Offload有其自己的专门论文：[ZeRO-Offload: Democratizing Billion-Scale Model Training](https://huggingface.co/papers/2101.06840)。而NVMe支持在论文[ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://huggingface.co/papers/2104.07857)中进行了描述。\n\nDeepSpeed ZeRO-2主要用于训练，因为它的特性对推理没有用处。\n\nDeepSpeed ZeRO-3也可以用于推理，因为它允许将单个GPU无法加载的大模型加载到多个GPU上。\n\n🤗 Transformers通过以下两种方式集成了[DeepSpeed](https://github.com/deepspeedai/DeepSpeed)：\n\n1. 通过[`Trainer`]集成核心的DeepSpeed功能。这是一种“为您完成一切”式的集成 - 您只需提供自定义配置文件或使用我们的模板配置文件。本文档的大部分内容都集中在这个功能上。\n2. 如果您不使用[`Trainer`]并希望在自己的Trainer中集成DeepSpeed，那么像`from_pretrained`和`from_config`这样的核心功能函数将包括ZeRO stage 3及以上的DeepSpeed的基础部分，如`zero.Init`。要利用此功能，请阅读有关[非Trainer DeepSpeed集成](#nontrainer-deepspeed-integration)的文档。\n\n集成的内容：\n\n训练：\n\n1. DeepSpeed ZeRO训练支持完整的ZeRO stages 1、2和3，以及ZeRO-Infinity（CPU和NVMe offload）。\n\n推理：\n\n1. DeepSpeed ZeRO推理支持ZeRO stage 3和ZeRO-Infinity。它使用与训练相同的ZeRO协议，但不使用优化器和学习率调度器，只有stage 3与推理相关。更多详细信息请参阅：[zero-inference](#zero-inference)。\n\n此外还有DeepSpeed推理 - 这是一种完全不同的技术，它使用张量并行而不是ZeRO（即将推出）。\n\n\n<a id='deepspeed-trainer-integration'></a>",
    "1034": "一级标题：DeepSpeed集成\n二级标题：Trainer DeepSpeed 集成\n内容：\n<a id='deepspeed-installation'></a>\n\n### 安装\n\n通过pypi安装库：\n\n\n```bash\npip install deepspeed\n```\n\n或通过 `transformers` 的 `extras`安装：\n\n```bash\npip install transformers[deepspeed]\n```\n\n或在 [DeepSpeed 的 GitHub 页面](https://github.com/deepspeedai/DeepSpeed#installation) 和\n[高级安装](https://www.deepspeed.ai/tutorials/advanced-install/) 中查找更多详细信息。\n\n如果构建过程中仍然遇到问题，请首先确保阅读 [CUDA 扩展安装注意事项](trainer#cuda-extension-installation-notes)。\n\n如果您没有预先构建扩展而是在运行时构建它们，而且您尝试了以上所有解决方案都无效，下一步可以尝试在安装之前预先构建扩展。\n\n进行 DeepSpeed 的本地构建：\n\n\n```bash\ngit clone https://github.com/deepspeedai/DeepSpeed/\ncd DeepSpeed\nrm -rf build\nTORCH_CUDA_ARCH_LIST=\"8.6\" DS_BUILD_CPU_ADAM=1 DS_BUILD_UTILS=1 pip install . \\\n--global-option=\"build_ext\" --global-option=\"-j8\" --no-cache -v \\\n--disable-pip-version-check 2>&1 | tee build.log\n```\n\n如果您打算使用 NVMe offload，您还需要在上述说明中添加 `DS_BUILD_AIO=1`（并且还需要在系统范围内安装 *libaio-dev*）。\n\n编辑 `TORCH_CUDA_ARCH_LIST` 以插入您打算使用的 GPU 卡的架构代码。假设您的所有卡都是相同的，您可以通过以下方式获取架构：\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python -c \"import torch; print(torch.cuda.get_device_capability())\"\n```\n\n因此，如果您得到 `8, 6`，则使用 `TORCH_CUDA_ARCH_LIST=\"8.6\"`。如果您有多个不同的卡，您可以像这样列出所有卡 `TORCH_CUDA_ARCH_LIST=\"6.1;8.6\"`。\n\n如果您需要在多台机器上使用相同的设置，请创建一个二进制 wheel：\n\n\n```bash\ngit clone https://github.com/deepspeedai/DeepSpeed/\ncd DeepSpeed\nrm -rf build\nTORCH_CUDA_ARCH_LIST=\"8.6\" DS_BUILD_CPU_ADAM=1 DS_BUILD_UTILS=1 \\\npython setup.py build_ext -j8 bdist_wheel\n```\n\n它将生成类似于 `dist/deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl` 的文件，现在您可以在本地或任何其他机器上安装它，如 `pip install deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl`。\n\n再次提醒确保调整 `TORCH_CUDA_ARCH_LIST` 以匹配目标架构。\n\n您可以在[这里](https://developer.nvidia.com/cuda-gpus)找到完整的 NVIDIA GPU 列表及其对应的 **计算能力**（与此上下文中的架构相同）。\n\n您可以使用以下命令检查 PyTorch 构建时使用的架构：\n\n\n```bash\npython -c \"import torch; print(torch.cuda.get_arch_list())\"\n```\n\n以下是如何查找已安装 GPU 中的一张卡的架构。例如，对于 GPU 0：\n\n```bash\nCUDA_VISIBLE_DEVICES=0 python -c \"import torch; \\\nprint(torch.cuda.get_device_properties(torch.device('cuda')))\"\n```\n\n如果输出结果如下：\n\n```bash\n_CudaDeviceProperties(name='GeForce RTX 3090', major=8, minor=6, total_memory=24268MB, multi_processor_count=82)\n```\n\n然后您就知道这张卡的架构是 `8.6`。\n\n您也可以完全省略 `TORCH_CUDA_ARCH_LIST`，然后构建程序将自动查询构建所在的 GPU 的架构。这可能与目标机器上的 GPU 不匹配，因此最好明确指定所需的架构。\n\n如果尝试了所有建议的方法仍然遇到构建问题，请继续在 [Deepspeed](https://github.com/deepspeedai/DeepSpeed/issues)的 GitHub Issue 上提交问题。\n\n\n<a id='deepspeed-multi-gpu'></a>\n\n### 多GPU启用\n\n为了启用DeepSpeed 集成，调整 [`Trainer`] 的命令行参数，添加一个新的参数 `--deepspeed ds_config.json`，其中 `ds_config.json` 是 DeepSpeed 配置文件，如文档 [这里](https://www.deepspeed.ai/docs/config-json/) 所述。文件命名由您决定。\n建议使用 DeepSpeed 的 `add_config_arguments` 程序将必要的命令行参数添加到您的代码中。\n有关更多信息，请参阅 [DeepSpeed 的参数解析](https://deepspeed.readthedocs.io/en/latest/initialize.html#argument-parsing) 文档。\n\n在这里，您可以使用您喜欢的启动器。您可以继续使用 PyTorch 启动器：\n\n\n```bash\ntorch.distributed.run --nproc_per_node=2 your_program.py <normal cl args> --deepspeed ds_config.json\n```\n\n或使用由 `deepspeed` 提供的启动器：\n\n\n```bash\ndeepspeed --num_gpus=2 your_program.py <normal cl args> --deepspeed ds_config.json\n```\n\n\n正如您所见，这两个启动器的参数不同，但对于大多数需求，任何一个都可以满足工作需求。有关如何配置各个节点和 GPU 的完整详细信息，请查看 [此处](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node)。\n\n当您使用 `deepspeed` 启动器并且希望使用所有可用的 GPU 时，您可以简单地省略 `--num_gpus` 标志。\n\n以下是在 DeepSpeed 中启用使用所有可用 GPU情况下， 运行 `run_translation.py` 的示例：\n\n\n```bash\ndeepspeed examples/pytorch/translation/run_translation.py \\\n--deepspeed tests/deepspeed/ds_config_zero3.json \\\n--model_name_or_path google-t5/t5-small --per_device_train_batch_size 1 \\\n--output_dir output_dir --overwrite_output_dir --fp16 \\\n--do_train --max_train_samples 500 --num_train_epochs 1 \\\n--dataset_name wmt16 --dataset_config \"ro-en\" \\\n--source_lang en --target_lang ro\n```\n\n请注意，在 DeepSpeed 文档中，您可能会看到 `--deepspeed --deepspeed_config ds_config.json` - 即两个与 DeepSpeed 相关的参数，但为简单起见，并且因为已经有很多参数要处理，我们将两者合并为一个单一参数。\n\n有关一些实际使用示例，请参阅 [此帖](https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400)。\n\n\n\n<a id='deepspeed-one-gpu'></a>\n\n### 单GPU启用\n\n要使用一张 GPU 启用 DeepSpeed，调整 [`Trainer`] 的命令行参数如下：\n\n\n```bash\ndeepspeed --num_gpus=1 examples/pytorch/translation/run_translation.py \\\n--deepspeed tests/deepspeed/ds_config_zero2.json \\\n--model_name_or_path google-t5/t5-small --per_device_train_batch_size 1 \\\n--output_dir output_dir --overwrite_output_dir --fp16 \\\n--do_train --max_train_samples 500 --num_train_epochs 1 \\\n--dataset_name wmt16 --dataset_config \"ro-en\" \\\n--source_lang en --target_lang ro\n```\n\n这与多 GPU 的情况几乎相同，但在这里我们通过 `--num_gpus=1` 明确告诉 DeepSpeed 仅使用一张 GPU。默认情况下，DeepSpeed 启用给定节点上可以看到的所有 GPU。如果您一开始只有一张 GPU，那么您不需要这个参数。以下 [文档](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node) 讨论了启动器的选项。\n\n为什么要在仅使用一张 GPU 的情况下使用 DeepSpeed 呢？\n\n1. 它具有 ZeRO-offload 功能，可以将一些计算和内存委托给主机的 CPU 和 内存，从而为模型的需求保留更多 GPU 资源 - 例如更大的批处理大小，或启用正常情况下无法容纳的非常大模型。\n2. 它提供了智能的 GPU 内存管理系统，最小化内存碎片，这再次允许您容纳更大的模型和数据批次。\n\n虽然接下来我们将详细讨论配置，但在单个 GPU 上通过 DeepSpeed 实现巨大性能提升的关键是在配置文件中至少有以下配置：\n\n\n```json\n{\n  \"zero_optimization\": {\n     \"stage\": 2,\n     \"offload_optimizer\": {\n         \"device\": \"cpu\",\n         \"pin_memory\": true\n     },\n     \"allgather_partitions\": true,\n     \"allgather_bucket_size\": 2e8,\n     \"reduce_scatter\": true,\n     \"reduce_bucket_size\": 2e8,\n     \"overlap_comm\": true,\n     \"contiguous_gradients\": true\n  }\n}\n```\n\n这会启用`optimizer offload `和一些其他重要功能。您可以尝试不同的buffer大小，有关详细信息，请参见下面的讨论。\n\n关于这种启用类型的实际使用示例，请参阅 [此帖](https://github.com/huggingface/transformers/issues/8771#issuecomment-759176685)。\n\n您还可以尝试使用本文后面进一步解释的支持`CPU 和 NVMe offload`功能的ZeRO-3 。\n\n\n<!--- TODO: Benchmark whether we can get better performance out of ZeRO-3 vs. ZeRO-2 on a single GPU, and then\nrecommend ZeRO-3 config as starting one. -->\n\n注意：\n\n- 如果您需要在特定的 GPU 上运行，而不是 GPU 0，则无法使用 `CUDA_VISIBLE_DEVICES` 来限制可用 GPU 的可见范围。相反，您必须使用以下语法：\n\n  ```bash\n  deepspeed --include localhost:1 examples/pytorch/translation/run_translation.py ...\n  ```\n\n  在这个例子中，我们告诉 DeepSpeed 使用 GPU 1（第二个 GPU）。\n\n\n\n<a id='deepspeed-multi-node'></a>\n\n### 多节点启用\n\n这一部分的信息不仅适用于 DeepSpeed 集成，也适用于任何多节点程序。但 DeepSpeed 提供了一个比其他启动器更易于使用的 `deepspeed` 启动器，除非您在 SLURM 环境中。\n\n在本节，让我们假设您有两个节点，每个节点有 8 张 GPU。您可以通过 `ssh hostname1` 访问第一个节点，通过 `ssh hostname2` 访问第二个节点，两者必须能够在本地通过 ssh 无密码方式相互访问。当然，您需要将这些主机（节点）名称重命名为您实际使用的主机名称。\n\n\n#### torch.distributed.run启动器\n\n\n例如，要使用 `torch.distributed.run`，您可以执行以下操作：\n\n```bash\npython -m torch.distributed.run --nproc_per_node=8 --nnode=2 --node_rank=0 --master_addr=hostname1 \\\n--master_port=9901 your_program.py <normal cl args> --deepspeed ds_config.json\n```\n\n您必须 ssh 到每个节点，并在每个节点上运行相同的命令！不用担心，启动器会等待两个节点同步完成。\n\n有关更多信息，请参阅 [torchrun](https://pytorch.org/docs/stable/elastic/run.html)。顺便说一下，这也是替代了几个 PyTorch 版本前的 `torch.distributed.launch` 的启动器。\n\n\n#### deepspeed启动器\n\n要改用 `deepspeed` 启动器，首先需要创建一个 `hostfile` 文件：\n\n```\nhostname1 slots=8\nhostname2 slots=8\n```\n然后，您可以这样启动：\n\n```bash\ndeepspeed --num_gpus 8 --num_nodes 2 --hostfile hostfile --master_addr hostname1 --master_port=9901 \\\nyour_program.py <normal cl args> --deepspeed ds_config.json\n```\n\n与 `torch.distributed.run` 启动器不同，`deepspeed` 将自动在两个节点上启动此命令！\n\n更多信息，请参阅[资源配置（多节点）](https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node)。\n\n\n#### 在 SLURM 环境中启动\n\n在 SLURM 环境中，可以采用以下方法。以下是一个 SLURM 脚本 `launch.slurm`，您需要根据您的具体 SLURM 环境进行调整。\n\n```bash\n#SBATCH --job-name=test-nodes        # name\n#SBATCH --nodes=2                    # nodes\n#SBATCH --ntasks-per-node=1          # crucial - only 1 task per dist per node!\n#SBATCH --cpus-per-task=10           # number of cores per tasks\n#SBATCH --gres=gpu:8                 # number of gpus\n#SBATCH --time 20:00:00              # maximum execution time (HH:MM:SS)\n#SBATCH --output=%x-%j.out           # output file name\n\nexport GPUS_PER_NODE=8\nexport MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\nexport MASTER_PORT=9901\n\nsrun --jobid $SLURM_JOBID bash -c 'python -m torch.distributed.run \\\n --nproc_per_node $GPUS_PER_NODE --nnodes $SLURM_NNODES --node_rank $SLURM_PROCID \\\n --master_addr $MASTER_ADDR --master_port $MASTER_PORT \\\nyour_program.py <normal cl args> --deepspeed ds_config.json'\n```\n\n剩下的就是运行它：\n\n```bash\nsbatch launch.slurm\n```\n\n`srun` 将负责在所有节点上同时启动程序。\n\n\n#### 使用非共享文件系统\n\n默认情况下，DeepSpeed 假定多节点环境使用共享存储。如果不是这种情况，每个节点只能看到本地文件系统，你需要调整配置文件，包含一个 [`checkpoint` 部分](https://www.deepspeed.ai/docs/config-json/#checkpoint-options)并设置如下选项：\n\n```json\n{\n  \"checkpoint\": {\n    \"use_node_local_storage\": true\n  }\n}\n```\n\n或者，你还可以使用 [`Trainer`] 的 `--save_on_each_node` 参数，上述配置将自动添加。\n\n\n<a id='deepspeed-notebook'></a>\n\n### 在Notebooks启用\n\n在将`notebook cells`作为脚本运行的情况下，问题在于没有正常的 `deepspeed` 启动器可依赖，因此在某些设置下，我们必须仿真运行它。\n\n如果您只使用一个 GPU，以下是如何调整notebook中的训练代码以使用 DeepSpeed。\n\n```python\n# DeepSpeed requires a distributed environment even when only one process is used.\n# This emulates a launcher in the notebook\nimport os\n\nos.environ[\"MASTER_ADDR\"] = \"localhost\"\nos.environ[\"MASTER_PORT\"] = \"9994\"  # modify if RuntimeError: Address already in use\nos.environ[\"RANK\"] = \"0\"\nos.environ[\"LOCAL_RANK\"] = \"0\"\nos.environ[\"WORLD_SIZE\"] = \"1\"\n\n# Now proceed as normal, plus pass the deepspeed config file\ntraining_args = TrainingArguments(..., deepspeed=\"ds_config_zero3.json\")\ntrainer = Trainer(...)\ntrainer.train()\n```\n\n注意：`...` 代表您传递给函数的正常参数。\n\n如果要使用多于一个 GPU，您必须在 DeepSpeed 中使用多进程环境。也就是说，您必须使用专门的启动器来实现这一目的，而不能通过仿真本节开头呈现的分布式环境来完成。\n\n如果想要在notebook中动态创建配置文件并保存在当前目录，您可以在一个专用的cell中使用：\n\n```python no-style\n%%bash\ncat <<'EOT' > ds_config_zero3.json\n{\n    \"fp16\": {\n        \"enabled\": \"auto\",\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"betas\": \"auto\",\n            \"eps\": \"auto\",\n            \"weight_decay\": \"auto\"\n        }\n    },\n\n    \"scheduler\": {\n        \"type\": \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\"\n        }\n    },\n\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"offload_param\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"overlap_comm\": true,\n        \"contiguous_gradients\": true,\n        \"sub_group_size\": 1e9,\n        \"reduce_bucket_size\": \"auto\",\n        \"stage3_prefetch_bucket_size\": \"auto\",\n        \"stage3_param_persistence_threshold\": \"auto\",\n        \"stage3_max_live_parameters\": 1e9,\n        \"stage3_max_reuse_distance\": 1e9,\n        \"stage3_gather_16bit_weights_on_model_save\": true\n    },\n\n    \"gradient_accumulation_steps\": \"auto\",\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 2000,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"wall_clock_breakdown\": false\n}\nEOT\n```\n\n如果训练脚本在一个普通文件中而不是在notebook cells中，您可以通过笔记本中的 shell 正常启动 `deepspeed`。例如，要使用 `run_translation.py`，您可以这样启动：\n\n```python no-style\n!git clone https://github.com/huggingface/transformers\n!cd transformers; deepspeed examples/pytorch/translation/run_translation.py ...\n```\n\n或者使用 `%%bash` 魔术命令，您可以编写多行代码，用于运行 shell 程序：\n\n```python no-style\n%%bash\n\ngit clone https://github.com/huggingface/transformers\ncd transformers\ndeepspeed examples/pytorch/translation/run_translation.py ...\n```\n\n在这种情况下，您不需要本节开头呈现的任何代码。\n\n注意：虽然 `%%bash` 魔术命令很方便，但目前它会缓冲输出，因此在进程完成之前您看不到日志。\n\n\n<a id='deepspeed-config'></a>\n\n### 配置\n\n有关可以在 DeepSpeed 配置文件中使用的完整配置选项的详细指南，请参阅[以下文档](https://www.deepspeed.ai/docs/config-json/)。\n\n您可以在 [DeepSpeedExamples 仓库](https://github.com/deepspeedai/DeepSpeedExamples)中找到解决各种实际需求的数十个 DeepSpeed 配置示例。\n\n```bash\ngit clone https://github.com/deepspeedai/DeepSpeedExamples\ncd DeepSpeedExamples\nfind . -name '*json'\n```\n\n延续上面的代码，假设您要配置 Lamb 优化器。那么您可以通过以下方式在示例的 `.json` 文件中进行搜索：\n\n```bash\ngrep -i Lamb $(find . -name '*json')\n```\n\n还可以在[主仓](https://github.com/deepspeedai/DeepSpeed)中找到更多示例。\n\n在使用 DeepSpeed 时，您总是需要提供一个 DeepSpeed 配置文件，但是一些配置参数必须通过命令行进行配置。您将在本指南的剩余章节找到这些细微差别。\n\n为了了解 DeepSpeed 配置文件，这里有一个激活 ZeRO stage 2 功能的示例，包括优化器状态的 CPU offload，使用 `AdamW` 优化器和 `WarmupLR`  调度器，并且如果传递了 `--fp16` 参数将启用混合精度训练：\n\n```json\n{\n    \"fp16\": {\n        \"enabled\": \"auto\",\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"betas\": \"auto\",\n            \"eps\": \"auto\",\n            \"weight_decay\": \"auto\"\n        }\n    },\n\n    \"scheduler\": {\n        \"type\": \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\"\n        }\n    },\n\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"allgather_partitions\": true,\n        \"allgather_bucket_size\": 2e8,\n        \"overlap_comm\": true,\n        \"reduce_scatter\": true,\n        \"reduce_bucket_size\": 2e8,\n        \"contiguous_gradients\": true\n    },\n\n    \"gradient_accumulation_steps\": \"auto\",\n    \"gradient_clipping\": \"auto\",\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n}\n```\n\n当您执行程序时，DeepSpeed 将把它从 [`Trainer`] 收到的配置日志输出到console，因此您可以看到传递给它的最终配置。\n\n\n\n<a id='deepspeed-config-passing'></a>\n\n### 传递配置\n\n正如本文档讨论的那样，通常将 DeepSpeed 配置作为指向 JSON 文件的路径传递，但如果您没有使用命令行界面配置训练，而是通过 [`TrainingArguments`] 实例化 [`Trainer`]，那么对于 `deepspeed` 参数，你可以传递一个嵌套的 `dict`。这使您能够即时创建配置，而无需在将其传递给 [`TrainingArguments`] 之前将其写入文件系统。\n\n总结起来，您可以这样做：\n\n```python\nTrainingArguments(..., deepspeed=\"/path/to/ds_config.json\")\n```\n\n或者:\n\n```python\nds_config_dict = dict(scheduler=scheduler_params, optimizer=optimizer_params)\nTrainingArguments(..., deepspeed=ds_config_dict)\n```\n\n<a id='deepspeed-config-shared'></a>\n\n### 共享配置\n\n\n<Tip warning={true}>\n\n这一部分是必读的。\n\n</Tip>\n\n一些配置值对于 [`Trainer`] 和 DeepSpeed 正常运行都是必需的，因此，为了防止定义冲突及导致的难以检测的错误，我们选择通过 [`Trainer`] 命令行参数配置这些值。\n\n此外，一些配置值是基于模型的配置自动派生的，因此，与其记住手动调整多个值，最好让 [`Trainer`] 为您做大部分配置。\n\n因此，在本指南的其余部分，您将找到一个特殊的配置值：`auto`，当设置时将自动将参数替换为正确或最有效的值。请随意选择忽略此建议或显式设置该值，在这种情况下，请务必确保 [`Trainer`] 参数和 DeepSpeed 配置保持一致。例如，您是否使用相同的学习率、批量大小或梯度累积设置？如果这些不匹配，训练可能以非常难以检测的方式失败。请重视该警告。\n\n还有一些参数是仅适用于 DeepSpeed 的，并且这些参数必须手动设置以适应您的需求。\n\n在您自己的程序中，如果您想要作为主动修改 DeepSpeed 配置并以此配置 [`TrainingArguments`]，您还可以使用以下方法。步骤如下：\n\n1. 创建或加载要用作主配置的 DeepSpeed 配置\n2. 根据这些参数值创建 [`TrainingArguments`] 对象\n\n请注意，一些值，比如 `scheduler.params.total_num_steps`，是在 [`Trainer`] 的 `train` 过程中计算的，但当然您也可以自己计算这些值。\n\n\n<a id='deepspeed-zero'></a>\n\n### ZeRO\n\n[Zero Redundancy Optimizer (ZeRO)](https://www.deepspeed.ai/tutorials/zero/) 是 DeepSpeed 的工作核心。它支持3个不同级别（stages）的优化。Stage 1 对于扩展性来说不是很有趣，因此本文档重点关注Stage 2和Stage 3。Stage 3通过最新的 ZeRO-Infinity 进一步改进。你可以在 DeepSpeed 文档中找到更详细的信息。\n\n配置文件的 `zero_optimization` 部分是最重要的部分（[文档](https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training)），因为在这里您定义了要启用哪些 ZeRO stages 以及如何配置它们。您可以在 DeepSpeed 文档中找到每个参数的解释。\n\n这一部分必须通过 DeepSpeed 配置文件单独配置 - [`Trainer`] 不提供相应的命令行参数。\n\n注意：目前 DeepSpeed 不验证参数名称，因此如果您拼错了任何参数，它将使用拼写错误的参数的默认设置。您可以观察 DeepSpeed 引擎启动日志消息，看看它将使用哪些值。\n\n<a id='deepspeed-zero2-config'></a>\n\n#### ZeRO-2 配置\n\n以下是 ZeRO stage 2 的配置示例：\n\n```json\n{\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"allgather_partitions\": true,\n        \"allgather_bucket_size\": 5e8,\n        \"overlap_comm\": true,\n        \"reduce_scatter\": true,\n        \"reduce_bucket_size\": 5e8,\n        \"contiguous_gradients\": true\n    }\n}\n```\n\n**性能调优：**\n\n- 启用 `offload_optimizer` 应该减少 GPU 内存使用（需要 `\"stage\": 2`）。\n- `\"overlap_comm\": true` 通过增加 GPU 内存使用来降低all-reduce 的延迟。 `overlap_comm` 使用了 `allgather_bucket_size` 和 `reduce_bucket_size` 值的4.5倍。因此，如果它们设置为 `5e8`，这将需要一个9GB的内存占用（`5e8 x 2Bytes x 2 x 4.5`）。因此，如果您的 GPU 内存为8GB或更小，为了避免出现OOM错误，您需要将这些参数减小到约 `2e8`，这将需要3.6GB。如果您的 GPU 容量更大，当您开始遇到OOM时，你可能也需要这样做。\n- 当减小这些buffers时，您以更慢的通信速度来换取更多的 GPU 内存。buffers大小越小，通信速度越慢，GPU 可用于其他任务的内存就越多。因此，如果更大的批处理大小很重要，那么稍微减慢训练时间可能是一个很好的权衡。\n\n此外，`deepspeed==0.4.4` 添加了一个新选项 `round_robin_gradients`，您可以通过以下方式启用：\n\n```json\n{\n    \"zero_optimization\": {\n        \"round_robin_gradients\": true\n    }\n}\n```\n这是一个用于 CPU offloading 的stage 2优化，通过细粒度梯度分区在 ranks 之间并行复制到 CPU 内存，从而实现了性能的提升。性能优势随着梯度累积步骤（在优化器步骤之间进行更多复制）或 GPU 数量（增加并行性）增加而增加。\n\n<a id='deepspeed-zero3-config'></a>\n\n#### ZeRO-3 配置\n\n以下是 ZeRO stage 3的配置示例：\n\n```json\n{\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"offload_param\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"overlap_comm\": true,\n        \"contiguous_gradients\": true,\n        \"sub_group_size\": 1e9,\n        \"reduce_bucket_size\": \"auto\",\n        \"stage3_prefetch_bucket_size\": \"auto\",\n        \"stage3_param_persistence_threshold\": \"auto\",\n        \"stage3_max_live_parameters\": 1e9,\n        \"stage3_max_reuse_distance\": 1e9,\n        \"stage3_gather_16bit_weights_on_model_save\": true\n    }\n}\n```\n\n如果您因为你的模型或激活值超过 GPU 内存而遇到OOM问题，并且您有未使用的 CPU 内存，可以通股票使用 `\"device\": \"cpu\"` 将优化器状态和参数卸载到 CPU 内存中，来解决这个限制。如果您不想卸载到 CPU 内存，可以在 `device` 条目中使用 `none` 代替 `cpu`。将优化器状态卸载到 NVMe 上会在后面进一步讨论。\n\n通过将 `pin_memory` 设置为 `true` 启用固定内存。此功能会以减少可用于其他进程的内存为代价来提高吞吐量。固定内存被分配给特定请求它的进程，通常比普通 CPU 内存访问速度更快。\n\n**性能调优：**\n\n- `stage3_max_live_parameters`: `1e9`\n- `stage3_max_reuse_distance`: `1e9`\n\n如果遇到OOM问题，请减小 `stage3_max_live_parameters` 和 `stage3_max_reuse_distance`。它们对性能的影响应该很小，除非您正在进行激活值checkpointing。`1e9` 大约会消耗 ~2GB。内存由 `stage3_max_live_parameters` 和 `stage3_max_reuse_distance` 共享，所以它不是叠加的，而是总共2GB。\n\n`stage3_max_live_parameters` 是在任何给定时间要在 GPU 上保留多少个完整参数的上限。\"reuse distance\" 是我们用来确定参数在将来何时会再次使用的度量标准，我们使用 `stage3_max_reuse_distance` 来决定是丢弃参数还是保留参数。如果一个参数在不久的将来（小于 `stage3_max_reuse_distance`）将被再次使用，那么我们将其保留以减少通信开销。这在启用激活值checkpoing时非常有用，其中我们以单层粒度进行前向重计算和反向传播，并希望在反向传播期间保留前向重计算中的参数。\n\n以下配置值取决于模型的隐藏大小：\n\n- `reduce_bucket_size`: `hidden_size*hidden_size`\n- `stage3_prefetch_bucket_size`: `0.9 * hidden_size * hidden_size`\n- `stage3_param_persistence_threshold`: `10 * hidden_size`\n\n因此，将这些值设置为 `auto`，[`Trainer`] 将自动分配推荐的参数值。当然，如果您愿意，也可以显式设置这些值。\n\n`stage3_gather_16bit_weights_on_model_save` 在模型保存时启用模型的 fp16 权重整合。对于大模型和多个 GPU，无论是在内存还是速度方面，这都是一项昂贵的操作。目前如果计划恢复训练，这是必需的。请注意未来的更新可能会删除此限制并让使用更加灵活。\n\n如果您从 ZeRO-2 配置迁移，请注意 `allgather_partitions`、`allgather_bucket_size` 和 `reduce_scatter` 配置参数在 ZeRO-3 中不被使用。如果保留这些配置文件，它们将被忽略。\n\n- `sub_group_size`: `1e9`\n\n`sub_group_size` 控制在优化器步骤期间更新参数的粒度。参数被分组到大小为 `sub_group_size` 的桶中，每个桶逐个更新。在 ZeRO-Infinity 中与 NVMe offload一起使用时，`sub_group_size` 控制了在优化器步骤期间在 NVMe 和 CPU 内存之间移动模型状态的粒度。这可以防止非常大的模型耗尽 CPU 内存。\n\n当不使用 NVMe offload时，可以将 `sub_group_size` 保留为其默认值 *1e9*。在以下情况下，您可能需要更改其默认值：\n\n1. 在优化器步骤中遇到OOM：减小 `sub_group_size` 以减少临时buffers的内存利用\n2. 优化器步骤花费很长时间：增加 `sub_group_size` 以提高由于增加的数据buffers而导致的带宽利用率。\n\n\n#### ZeRO-0 配置\n\n请注意，我们将 Stage 0 和 1 放在最后，因为它们很少使用。\n\nStage 0 禁用了所有类型的分片，只是将 DeepSpeed 作为 DDP 使用。您可以通过以下方式启用：\n\n```json\n{\n    \"zero_optimization\": {\n        \"stage\": 0\n    }\n}\n```\n\n这将实质上禁用 ZeRO，而无需更改其他任何内容。\n\n\n#### ZeRO-1 配置\n\n\nStage 1 等同于 Stage 2 减去梯度分片。您可以尝试使用以下配置，仅对优化器状态进行分片，以稍微加速：\n\n\n```json\n{\n    \"zero_optimization\": {\n        \"stage\": 1\n    }\n}\n```\n\n\n\n<a id='deepspeed-nvme'></a>\n\n### NVMe 支持\n\nZeRO-Infinity 通过使用 NVMe 内存扩展 GPU 和 CPU 内存，从而允许训练非常大的模型。由于智能分区和平铺算法，在offload期间每个 GPU 需要发送和接收非常小量的数据，因此 NVMe 被证明适用于训练过程中提供更大的总内存池。ZeRO-Infinity 需要启用 ZeRO-3。\n\n以下配置示例启用 NVMe 来offload优化器状态和参数：\n\n```json\n{\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\n            \"device\": \"nvme\",\n            \"nvme_path\": \"/local_nvme\",\n            \"pin_memory\": true,\n            \"buffer_count\": 4,\n            \"fast_init\": false\n        },\n        \"offload_param\": {\n            \"device\": \"nvme\",\n            \"nvme_path\": \"/local_nvme\",\n            \"pin_memory\": true,\n            \"buffer_count\": 5,\n            \"buffer_size\": 1e8,\n            \"max_in_cpu\": 1e9\n        },\n        \"aio\": {\n            \"block_size\": 262144,\n            \"queue_depth\": 32,\n            \"thread_count\": 1,\n            \"single_submit\": false,\n            \"overlap_events\": true\n        },\n        \"overlap_comm\": true,\n        \"contiguous_gradients\": true,\n        \"sub_group_size\": 1e9,\n        \"reduce_bucket_size\": \"auto\",\n        \"stage3_prefetch_bucket_size\": \"auto\",\n        \"stage3_param_persistence_threshold\": \"auto\",\n        \"stage3_max_live_parameters\": 1e9,\n        \"stage3_max_reuse_distance\": 1e9,\n        \"stage3_gather_16bit_weights_on_model_save\": true\n    },\n}\n```\n\n您可以选择将优化器状态和参数都卸载到 NVMe，也可以只选择其中一个，或者都不选择。例如，如果您有大量的 CPU 内存可用，只卸载到 CPU 内存训练速度会更快（提示：\"device\": \"cpu\"）。\n\n这是有关卸载 [优化器状态](https://www.deepspeed.ai/docs/config-json/#optimizer-offloading) 和 [参数](https://www.deepspeed.ai/docs/config-json/#parameter-offloading) 的完整文档。\n\n确保您的 `nvme_path` 实际上是一个 NVMe，因为它与普通硬盘或 SSD 一起工作，但速度会慢得多。快速可扩展的训练是根据现代 NVMe 传输速度设计的（截至本文撰写时，可以达到 ~3.5GB/s 读取，~3GB/s 写入的峰值速度）。\n\n为了找出最佳的 `aio` 配置块，您必须在目标设置上运行一个基准测试，具体操作请参见[说明](https://github.com/deepspeedai/DeepSpeed/issues/998)。\n\n\n\n<a id='deepspeed-zero2-zero3-performance'></a>\n\n#### ZeRO-2 和 ZeRO-3 性能对比\n\n如果其他一切都配置相同，ZeRO-3 可能比 ZeRO-2 慢，因为前者除了 ZeRO-2 的操作外，还必须收集模型权重。如果 ZeRO-2 满足您的需求，而且您不需要扩展到几个 GPU 以上，那么您可以选择继续使用它。重要的是要理解，ZeRO-3 以速度为代价实现了更高的可扩展性。\n\n可以调整 ZeRO-3 配置使其性能接近 ZeRO-2：\n\n- 将 `stage3_param_persistence_threshold` 设置为一个非常大的数字 - 大于最大的参数，例如 `6 * hidden_size * hidden_size`。这将保留参数在 GPU 上。\n- 关闭 `offload_params`，因为 ZeRO-2 没有这个选项。\n\n即使不更改 `stage3_param_persistence_threshold`，仅将 `offload_params` 关闭，性能可能会显著提高。当然，这些更改将影响您可以训练的模型的大小。因此，这些更改可根据需求帮助您在可扩展性和速度之间进行权衡。\n\n\n\n<a id='deepspeed-zero2-example'></a>\n\n#### ZeRO-2 示例\n\n这是一个完整的 ZeRO-2 自动配置文件 `ds_config_zero2.json`：\n\n```json\n{\n    \"fp16\": {\n        \"enabled\": \"auto\",\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"betas\": \"auto\",\n            \"eps\": \"auto\",\n            \"weight_decay\": \"auto\"\n        }\n    },\n\n    \"scheduler\": {\n        \"type\": \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\"\n        }\n    },\n\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"allgather_partitions\": true,\n        \"allgather_bucket_size\": 2e8,\n        \"overlap_comm\": true,\n        \"reduce_scatter\": true,\n        \"reduce_bucket_size\": 2e8,\n        \"contiguous_gradients\": true\n    },\n\n    \"gradient_accumulation_steps\": \"auto\",\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 2000,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"wall_clock_breakdown\": false\n}\n```\n\n这是一个完整的手动设置的启用所有功能的 ZeRO-2 配置文件。主要是为了让您看到典型的参数值是什么样的，但我们强烈建议使用其中包含多个 `auto` 设置的配置文件。\n\n```json\n{\n    \"fp16\": {\n        \"enabled\": true,\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": 3e-5,\n            \"betas\": [0.8, 0.999],\n            \"eps\": 1e-8,\n            \"weight_decay\": 3e-7\n        }\n    },\n\n    \"scheduler\": {\n        \"type\": \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": 0,\n            \"warmup_max_lr\": 3e-5,\n            \"warmup_num_steps\": 500\n        }\n    },\n\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"allgather_partitions\": true,\n        \"allgather_bucket_size\": 2e8,\n        \"overlap_comm\": true,\n        \"reduce_scatter\": true,\n        \"reduce_bucket_size\": 2e8,\n        \"contiguous_gradients\": true\n    },\n\n    \"steps_per_print\": 2000,\n    \"wall_clock_breakdown\": false\n}\n```\n\n<a id='deepspeed-zero3-example'></a>\n\n#### ZeRO-3 示例\n\n这是一个完整的 ZeRO-3 自动配置文件 `ds_config_zero3.json`：\n\n```json\n{\n    \"fp16\": {\n        \"enabled\": \"auto\",\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"betas\": \"auto\",\n            \"eps\": \"auto\",\n            \"weight_decay\": \"auto\"\n        }\n    },\n\n    \"scheduler\": {\n        \"type\": \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\"\n        }\n    },\n\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"offload_param\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"overlap_comm\": true,\n        \"contiguous_gradients\": true,\n        \"sub_group_size\": 1e9,\n        \"reduce_bucket_size\": \"auto\",\n        \"stage3_prefetch_bucket_size\": \"auto\",\n        \"stage3_param_persistence_threshold\": \"auto\",\n        \"stage3_max_live_parameters\": 1e9,\n        \"stage3_max_reuse_distance\": 1e9,\n        \"stage3_gather_16bit_weights_on_model_save\": true\n    },\n\n    \"gradient_accumulation_steps\": \"auto\",\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 2000,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"wall_clock_breakdown\": false\n}\n```\n\n这是一个完整的 手动设置的启用所有功能的ZeRO-3 配置文件。主要是为了让您看到典型的参数值是什么样的，但我们强烈建议使用其中包含多个 `auto` 设置的配置文件。\n\n```json\n{\n    \"fp16\": {\n        \"enabled\": true,\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": 3e-5,\n            \"betas\": [0.8, 0.999],\n            \"eps\": 1e-8,\n            \"weight_decay\": 3e-7\n        }\n    },\n\n    \"scheduler\": {\n        \"type\": \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": 0,\n            \"warmup_max_lr\": 3e-5,\n            \"warmup_num_steps\": 500\n        }\n    },\n\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"offload_param\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"overlap_comm\": true,\n        \"contiguous_gradients\": true,\n        \"sub_group_size\": 1e9,\n        \"reduce_bucket_size\": 1e6,\n        \"stage3_prefetch_bucket_size\": 0.94e6,\n        \"stage3_param_persistence_threshold\": 1e4,\n        \"stage3_max_live_parameters\": 1e9,\n        \"stage3_max_reuse_distance\": 1e9,\n        \"stage3_gather_16bit_weights_on_model_save\": true\n    },\n\n    \"steps_per_print\": 2000,\n    \"wall_clock_breakdown\": false\n}\n```\n\n#### 如何选择最佳性能的ZeRO Stage和 offloads\n\n了解了这些不同stages后，现在您需要决定使用哪个stage。本节将尝试回答这个问题。\n\n通常，以下规则适用：\n\n- 速度方面（左边比右边快）\n\n  stage 0（DDP） > stage 1 > stage 2 > stage 2 + offload  > stage 3 > stage3 + offload\n\n- GPU内存使用方面（右边比左边更节省GPU内存）\n\n  stage 0（DDP） < stage 1 < stage 2 < stage 2 + offload < stage 3 < stage 3 + offload\n\n所以，当您希望在尽量使用较少数量的GPU的同时获得最快的执行速度时，可以按照以下步骤进行。我们从最快的方法开始，如果遇到GPU内存溢出，然后切换到下一个速度较慢但使用的GPU内存更少的方法。以此类推。\n\n首先，将批量大小设置为1（您始终可以使用梯度累积来获得任何所需的有效批量大小）。\n\n\n1. 启用 `--gradient_checkpointing 1`（HF Trainer）或直接 `model.gradient_checkpointing_enable()` - 如果发生OOM（Out of Memory），则执行以下步骤。\n2. 首先尝试 ZeRO stage 2。如果发生OOM，则执行以下步骤。\n3. 尝试 ZeRO stage 2 + `offload_optimizer` - 如果发生OOM，则执行以下步骤。\n4. 切换到 ZeRO stage 3 - 如果发生OOM，则执行以下步骤。\n5. 启用 `offload_param` 到 `cpu` - 如果发生OOM，则执行以下步骤。\n6. 启用 `offload_optimizer` 到 `cpu` - 如果发生OOM，则执行以下步骤。\n7. 如果仍然无法适应批量大小为1，请首先检查各种默认值并尽可能降低它们。例如，如果使用 `generate` 并且不使用宽搜索束，将其缩小，因为它会占用大量内存。\n8. 绝对要使用混合半精度而非fp32 - 在Ampere及更高的GPU上使用bf16，在旧的GPU体系结构上使用fp16。\n9. 如果仍然发生OOM，可以添加更多硬件或启用ZeRO-Infinity - 即切换 `offload_param` 和 `offload_optimizer` 到 `nvme`。您需要确保它是非常快的NVMe。作为趣闻，我曾经能够在一个小型GPU上使用BLOOM-176B进行推理，使用了ZeRO-Infinity，尽管速度非常慢。但它奏效了！\n\n当然，您也可以按相反的顺序进行这些步骤，从最节省GPU内存的配置开始，然后逐步反向进行，或者尝试进行二分法。\n\n一旦您的批量大小为1不会导致OOM，就测量您的有效吞吐量。\n\n接下来尝试将批量大小增加到尽可能大，因为批量大小越大，GPU的效率越高，特别是在它们乘法运算的矩阵很大时。\n\n现在性能优化游戏开始了。您可以关闭一些offload特性，或者降低ZeRO stage，并增加/减少批量大小，再次测量有效吞吐量。反复尝试，直到满意为止。\n\n不要花费太多时间，但如果您即将开始一个为期3个月的训练 - 请花几天时间找到吞吐量方面最有效的设置。这样您的训练成本将最低，而且您会更快地完成训练。在当前快节奏的机器学习世界中，如果您花费一个额外的月份来训练某样东西，你很可能会错过一个黄金机会。当然，这只是我分享的一种观察，我并不是在催促你。在开始训练BLOOM-176B之前，我花了2天时间进行这个过程，成功将吞吐量从90 TFLOPs提高到150 TFLOPs！这一努力为我们节省了一个多月的训练时间。\n\n这些注释主要是为训练模式编写的，但它们在推理中也应该大部分适用。例如，在推理中，Gradient Checkpointing 是无用的，因为它只在训练过程中有用。此外，我们发现，如果你正在进行多GPU推理并且不使用 [DeepSpeed-Inference](https://www.deepspeed.ai/tutorials/inference-tutorial/)，[Accelerate](https://huggingface.co/blog/bloom-inference-pytorch-scripts) 应该提供更优越的性能。\n\n其他与性能相关的快速注释：\n- 如果您从头开始训练某个模型，请尽量确保张量的形状可以被16整除（例如隐藏层大小）。对于批量大小，至少尝试可被2整除。如果您想从GPU中挤取更高性能，还有一些硬件特定的[wave和tile量化](https://developer.nvidia.com/blog/optimizing-gpu-performance-tensor-cores/)的可整除性。\n\n\n\n### Activation Checkpointing 或 Gradient Checkpointing\n\nActivation Checkpointing和Gradient Checkpointing是指相同方法的两个不同术语。这确实让人感到困惑，但事实就是这样。\n\nGradient Checkpointing允许通过牺牲速度来换取GPU内存，这要么使您能够克服GPU内存溢出，要么增加批量大小来获得更好的性能。\n\nHF Transformers 模型对DeepSpeed的Activation Checkpointing一无所知，因此如果尝试在DeepSpeed配置文件中启用该功能，什么都不会发生。\n\n因此，您有两种方法可以利用这个非常有益的功能：\n\n1. 如果您想使用 HF Transformers 模型，你可以使用 `model.gradient_checkpointing_enable()` 或在 HF Trainer 中使用 `--gradient_checkpointing`，它会自动为您启用这个功能。在这里使用了 `torch.utils.checkpoint`。\n2. 如果您编写自己的模型并希望使用DeepSpeed的Activation Checkpointing，可以使用[规定的API](https://deepspeed.readthedocs.io/en/latest/activation-checkpointing.html)。您还可以使用 HF Transformers 的模型代码，将 `torch.utils.checkpoint` 替换为 DeepSpeed 的API。后者更灵活，因为它允许您将前向激活值卸载到CPU内存，而不是重新计算它们。\n\n\n### Optimizer 和 Scheduler\n\n只要你不启用 `offload_optimizer`，您可以混合使用DeepSpeed和HuggingFace的调度器和优化器，但有一个例外，即不要使用HuggingFace调度器和DeepSpeed优化器的组合：\n\n\n| Combos       | HF Scheduler | DS Scheduler |\n|:-------------|:-------------|:-------------|\n| HF Optimizer | Yes          | Yes          |\n| DS Optimizer | No           | Yes          |\n\n在启用 `offload_optimizer` 的情况下，可以使用非DeepSpeed优化器，只要该优化器具有CPU和GPU的实现（除了LAMB）。\n\n<a id='deepspeed-optimizer'></a>\n\n#### Optimizer\n\nDeepSpeed的主要优化器包括Adam、AdamW、OneBitAdam和Lamb。这些优化器已经与ZeRO进行了彻底的测试，因此建议使用它们。然而，也可以导入`torch`中的其他优化器。完整的文档在[这里](https://www.deepspeed.ai/docs/config-json/#optimizer-parameters)。\n\n如果在配置文件中不配置`optimizer`条目，[`Trainer`] 将自动将其设置为 `AdamW`，并使用提供的值或以下命令行参数的默认值：`--learning_rate`、`--adam_beta1`、`--adam_beta2`、`--adam_epsilon` 和 `--weight_decay`。\n\n以下是`AdamW` 的自动配置示例：\n\n```json\n{\n   \"optimizer\": {\n       \"type\": \"AdamW\",\n       \"params\": {\n         \"lr\": \"auto\",\n         \"betas\": \"auto\",\n         \"eps\": \"auto\",\n         \"weight_decay\": \"auto\"\n       }\n   }\n}\n```\n\n请注意，命令行参数将设置配置文件中的值。这是为了有一个明确的值来源，并避免在不同地方设置学习率等值时难以找到的错误。命令行参数配置高于其他。被覆盖的值包括：\n\n- `lr` 的值为 `--learning_rate`\n- `betas` 的值为 `--adam_beta1 --adam_beta2`\n- `eps` 的值为 `--adam_epsilon`\n- `weight_decay` 的值为 `--weight_decay`\n\n因此，请记住在命令行上调整共享的超参数。\n\n您也可以显式地设置这些值：\n\n```json\n{\n   \"optimizer\": {\n       \"type\": \"AdamW\",\n       \"params\": {\n         \"lr\": 0.001,\n         \"betas\": [0.8, 0.999],\n         \"eps\": 1e-8,\n         \"weight_decay\": 3e-7\n       }\n   }\n}\n```\n\n但在这种情况下，您需要自己同步[`Trainer`]命令行参数和DeepSpeed配置。\n\n如果您想使用上面未列出的其他优化器，您将不得不将其添加到顶层配置中。\n\n```json\n{\n   \"zero_allow_untested_optimizer\": true\n}\n```\n\n类似于 `AdamW`，您可以配置其他官方支持的优化器。只是记住这些可能有不同的配置值。例如，对于Adam，您可能需要将 `weight_decay` 设置在 `0.01` 左右。\n\n此外，当与DeepSpeed的CPU Adam优化器一起使用时，offload的效果最好。如果您想在offload时使用不同的优化器，自 `deepspeed==0.8.3` 起，您还需要添加：\n\n\n```json\n{\n   \"zero_force_ds_cpu_optimizer\": false\n}\n```\n到顶层配置中。\n\n\n\n<a id='deepspeed-scheduler'></a>\n\n#### Scheduler\n\nDeepSpeed支持`LRRangeTest`、`OneCycle`、`WarmupLR`和`WarmupDecayLR`学习率调度器。完整文档在[这里](https://www.deepspeed.ai/docs/config-json/#scheduler-parameters)。\n\n以下是🤗 Transformers 和 DeepSpeed 之间的调度器重叠部分：\n\n- 通过 `--lr_scheduler_type constant_with_warmup` 实现 `WarmupLR`\n- 通过 `--lr_scheduler_type linear` 实现 `WarmupDecayLR`。这也是 `--lr_scheduler_type` 的默认值，因此，如果不配置调度器，这将是默认配置的调度器。\n\n如果在配置文件中不配置 `scheduler` 条目，[`Trainer`] 将使用 `--lr_scheduler_type`、`--learning_rate` 和 `--warmup_steps` 或 `--warmup_ratio` 的值来配置其🤗 Transformers 版本。\n\n以下是 `WarmupLR` 的自动配置示例：\n\n```json\n{\n   \"scheduler\": {\n         \"type\": \"WarmupLR\",\n         \"params\": {\n             \"warmup_min_lr\": \"auto\",\n             \"warmup_max_lr\": \"auto\",\n             \"warmup_num_steps\": \"auto\"\n         }\n     }\n}\n```\n\n由于使用了 *\"auto\"*，[`Trainer`] 的参数将在配置文件中设置正确的值。这是为了有一个明确的值来源，并避免在不同地方设置学习率等值时难以找到的错误。命令行配置高于其他。被设置的值包括：\n\n- `warmup_min_lr` 的值为 `0`。\n- `warmup_max_lr` 的值为 `--learning_rate`。\n- `warmup_num_steps` 的值为 `--warmup_steps`（如果提供）。否则，将使用 `--warmup_ratio` 乘以训练步骤的数量，并四舍五入。\n- `total_num_steps` 的值为 `--max_steps` 或者如果没有提供，将在运行时根据环境、数据集的大小和其他命令行参数（对于 `WarmupDecayLR` 来说需要）自动推导。\n\n当然，您可以接管任何或所有的配置值，并自行设置这些值：\n\n```json\n{\n   \"scheduler\": {\n         \"type\": \"WarmupLR\",\n         \"params\": {\n             \"warmup_min_lr\": 0,\n             \"warmup_max_lr\": 0.001,\n             \"warmup_num_steps\": 1000\n         }\n     }\n}\n```\n\n但在这种情况下，您需要自己同步[`Trainer`]命令行参数和DeepSpeed配置。\n\n例如，对于 `WarmupDecayLR`，您可以使用以下条目：\n\n```json\n{\n   \"scheduler\": {\n         \"type\": \"WarmupDecayLR\",\n         \"params\": {\n             \"last_batch_iteration\": -1,\n             \"total_num_steps\": \"auto\",\n             \"warmup_min_lr\": \"auto\",\n             \"warmup_max_lr\": \"auto\",\n             \"warmup_num_steps\": \"auto\"\n         }\n     }\n}\n```\n\n然后，`total_num_steps`、`warmup_max_lr`、`warmup_num_steps` 和 `total_num_steps` 将在加载时设置。\n\n\n<a id='deepspeed-fp32'></a>\n\n### fp32精度\n\nDeepSpeed支持完整的fp32和fp16混合精度。\n\n由于fp16混合精度具有更小的内存需求和更快的速度，唯一不使用它的时候是当您使用的模型在这种训练模式下表现不佳时。通常，当模型没有在fp16混合精度下进行预训练时（例如，bf16预训练模型经常出现这种情况），会出现这种情况。这样的模型可能会发生溢出或下溢，导致 `NaN` 损失。如果是这种情况，那么您将希望使用完整的fp32模式，通过显式禁用默认启用的fp16混合精度模式：\n\n```json\n{\n    \"fp16\": {\n        \"enabled\": false,\n    }\n}\n```\n\n如果您使用基于Ampere架构的GPU，PyTorch版本1.7及更高版本将自动切换到使用更高效的tf32格式进行一些操作，但结果仍将以fp32格式呈现。有关详细信息和基准测试，请参见[TensorFloat-32(TF32) on Ampere devices](https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)。如果出于某种原因您不希望使用它，该文档包括有关如何禁用此自动转换的说明。\n\n在🤗 Trainer中，你可以使用 `--tf32` 来启用它，或使用 `--tf32 0` 或 `--no_tf32` 来禁用它。默认情况下，使用PyTorch的默认设置。\n\n\n\n<a id='deepspeed-amp'></a>\n\n### 自动混合精度\n\n您可以使用自动混合精度，可以选择使用类似 PyTorch AMP 的方式，也可以选择使用类似 Apex 的方式：\n\n### fp16\n\n要配置PyTorch AMP-like 的 fp16（float16） 模式，请设置：\n\n```json\n{\n    \"fp16\": {\n        \"enabled\": \"auto\",\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    }\n}\n```\n\n并且，[`Trainer`]将根据`args.fp16_backend`的值自动启用或禁用它。其余的配置值由您决定。\n\n当传递`--fp16 --fp16_backend amp`或`--fp16_full_eval`命令行参数时，此模式将被启用。\n\n您也可以显式地启用/禁用此模式：\n\n```json\n{\n    \"fp16\": {\n        \"enabled\": true,\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    }\n}\n```\n\n但是之后您需要自己同步[`Trainer`]命令行参数和DeepSpeed配置。\n\n以下是[相关文档](https://www.deepspeed.ai/docs/config-json/#fp16-training-options)\n\n\n### bf16\n\n如果需要使用bfloat16而不是fp16，那么可以使用以下配置部分：\n\n```json\n{\n    \"bf16\": {\n        \"enabled\": \"auto\"\n    }\n}\n```\n\nbf16具有与fp32相同的动态范围，因此不需要损失缩放。\n\n当传递`--bf16`或`--bf16_full_eval`命令行参数时，启用此模式。\n\n您还可以显式地启用/禁用此模式：\n\n```json\n{\n    \"bf16\": {\n        \"enabled\": true\n    }\n}\n```\n\n<Tip>\n\n在`deepspeed==0.6.0`版本中，bf16支持是新的实验性功能。\n\n如果您启用了bf16来进行[梯度累积](#gradient-accumulation)，您需要意识到它会以bf16累积梯度，这可能不是您想要的，因为这种格式的低精度可能会导致lossy accumulation。\n\n修复这个问题的工作正在努力进行，同时提供了使用更高精度的`dtype`（fp16或fp32）的选项。\n\n</Tip>\n\n\n### NCCL集合\n\n在训练过程中，有两种数据类型：`dtype`和用于通信收集操作的`dtype`，如各种归约和收集/分散操作。\n\n所有的gather/scatter操作都是在数据相同的`dtype`中执行的，所以如果您正在使用bf16的训练模式，那么它将在bf16中进行gather操作 - gather操作是非损失性的。\n\n各种reduce操作可能会是非常损失性的，例如当梯度在多个gpu上平均时，如果通信是在fp16或bf16中进行的，那么结果可能是有损失性的 - 因为当在一个低精度中添加多个数字时，结果可能不是精确的。更糟糕的是，bf16比fp16具有更低的精度。通常，当平均梯度时，损失最小，这些梯度通常非常小。因此，对于半精度训练，默认情况下，fp16被用作reduction操作的默认值。但是，您可以完全控制这个功能，如果你选择的话，您可以添加一个小的开销，并确保reductions将使用fp32作为累积数据类型，只有当结果准备好时，它才会降级到您在训练中使用的半精度`dtype`。\n\n要覆盖默认设置，您只需添加一个新的配置条目：\n\n```json\n{\n    \"communication_data_type\": \"fp32\"\n}\n```\n\n根据这个信息，有效的值包括\"fp16\"、\"bfp16\"和\"fp32\"。\n\n注意：在stage zero 3中，bf16通信数据类型存在一个bug，该问题已在`deepspeed==0.8.1`版本中得到修复。\n\n\n### apex\n\n配置apex AMP-like模式：\n\n```json\n\"amp\": {\n    \"enabled\": \"auto\",\n    \"opt_level\": \"auto\"\n}\n```\n\n并且，[`Trainer`]将根据`args.fp16_backend`和`args.fp16_opt_level`的值自动配置它。\n\n当传递`--fp16 --fp16_backend apex --fp16_opt_level 01`命令行参数时，此模式将被启用。\n\n您还可以显式配置此模式：\n\n```json\n{\n    \"amp\": {\n        \"enabled\": true,\n        \"opt_level\": \"O1\"\n    }\n}\n```\n\n但是，您需要自己同步[`Trainer`]命令行参数和DeepSpeed配置。\n\n这里是[文档](https://www.deepspeed.ai/docs/config-json/#automatic-mixed-precision-amp-training-options)\n\n\n<a id='deepspeed-bs'></a>\n\n### Batch Size\n\n配置batch size可以使用如下参数:\n\n```json\n{\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\"\n}\n```\n\n并且，[`Trainer`]将自动将`train_micro_batch_size_per_gpu`设置为`args.per_device_train_batch_size`的值，并将`train_batch_size`设置为`args.world_size * args.per_device_train_batch_size * args.gradient_accumulation_steps`。\n\n您也可以显式设置这些值：\n\n```json\n{\n    \"train_batch_size\": 12,\n    \"train_micro_batch_size_per_gpu\": 4\n}\n```\n\n但是，您需要自己同步[`Trainer`]命令行参数和DeepSpeed配置。\n\n\n<a id='deepspeed-grad-acc'></a>\n\n### Gradient Accumulation\n\n配置gradient accumulation设置如下:\n\n```json\n{\n    \"gradient_accumulation_steps\": \"auto\"\n}\n```\n\n并且，[`Trainer`]将自动将其设置为`args.gradient_accumulation_steps`的值。\n\n您也可以显式设置这个值：\n\n```json\n{\n    \"gradient_accumulation_steps\": 3\n}\n```\n\n但是，您需要自己同步[`Trainer`]命令行参数和DeepSpeed配置。\n\n\n<a id='deepspeed-grad-clip'></a>\n\n### Gradient Clipping\n\n配置gradient clipping如下:\n\n```json\n{\n    \"gradient_clipping\": \"auto\"\n}\n```\n\n并且，[`Trainer`]将自动将其设置为`args.max_grad_norm`的值。\n\n您也可以显式设置这个值：\n\n```json\n{\n    \"gradient_clipping\": 1.0\n}\n```\n\n但是，您需要自己同步[`Trainer`]命令行参数和DeepSpeed配置。\n\n\n\n<a id='deepspeed-weight-extraction'></a>\n\n### 获取模型权重\n\n只要您继续使用DeepSpeed进行训练和恢复，您就不需要担心任何事情。DeepSpeed在其自定义检查点优化器文件中存储fp32主权重，这些文件是`global_step*/*optim_states.pt`（这是glob模式），并保存在正常的checkpoint下。\n\n**FP16权重：**\n\n当模型保存在ZeRO-2下时，您最终会得到一个包含模型权重的普通`pytorch_model.bin`文件，但它们只是权重的fp16版本。\n\n在ZeRO-3下，事情要复杂得多，因为模型权重分布在多个GPU上，因此需要`\"stage3_gather_16bit_weights_on_model_save\": true`才能让`Trainer`保存fp16版本的权重。如果这个设置是`False`，`pytorch_model.bin`将不会被创建。这是因为默认情况下，DeepSpeed的`state_dict`包含一个占位符而不是实际的权重。如果我们保存这个`state_dict`，就无法再加载它了。\n\n\n```json\n{\n    \"zero_optimization\": {\n        \"stage3_gather_16bit_weights_on_model_save\": true\n    }\n}\n```\n\n**FP32权重：**\n\n虽然fp16权重适合恢复训练，但如果您完成了模型的微调并希望将其上传到[models hub](https://huggingface.co/models)或传递给其他人，您很可能想要获取fp32权重。这最好不要在训练期间完成，因为这需要大量内存，因此最好在训练完成后离线进行。但是，如果需要并且有充足的空闲CPU内存，可以在相同的训练脚本中完成。以下部分将讨论这两种方法。\n\n**实时FP32权重恢复：**\n\n如果您的模型很大，并且在训练结束时几乎没有剩余的空闲CPU内存，这种方法可能不起作用。\n\n如果您至少保存了一个检查点，并且想要使用最新的一个，可以按照以下步骤操作：\n\n```python\nfrom transformers.trainer_utils import get_last_checkpoint\nfrom deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint\n\ncheckpoint_dir = get_last_checkpoint(trainer.args.output_dir)\nfp32_model = load_state_dict_from_zero_checkpoint(trainer.model, checkpoint_dir)\n```\n\n如果您在使用`--load_best_model_at_end`类：*~transformers.TrainingArguments*参数（用于跟踪最佳\n检查点），那么你可以首先显式地保存最终模型，然后再执行相同的操作：\n\n```python\nfrom deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint\n\ncheckpoint_dir = os.path.join(trainer.args.output_dir, \"checkpoint-final\")\ntrainer.deepspeed.save_checkpoint(checkpoint_dir)\nfp32_model = load_state_dict_from_zero_checkpoint(trainer.model, checkpoint_dir)\n```\n\n<Tip>\n\n注意，一旦运行了`load_state_dict_from_zero_checkpoint`，该模型将不再可以在相同的应用程序的DeepSpeed上下文中使用。也就是说，您需要重新初始化deepspeed引擎，因为`model.load_state_dict(state_dict)`会从其中移除所有的DeepSpeed相关点。所以您只能训练结束时这样做。\n\n</Tip>\n\n当然，您不必使用类：*~transformers.Trainer*，您可以根据你的需求调整上面的示例。\n\n如果您出于某种原因想要更多的优化，您也可以提取权重的fp32 `state_dict`并按照以下示例进行操作：\n\n```python\nfrom deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint\n\nstate_dict = get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir)  # already on cpu\nmodel = model.cpu()\nmodel.load_state_dict(state_dict)\n```\n\n**离线FP32权重恢复：**\n\nDeepSpeed会创建一个特殊的转换脚本`zero_to_fp32.py`，并将其放置在checkpoint文件夹的顶层。使用此脚本，您可以在任何时候提取权重。该脚本是独立的，您不再需要配置文件或`Trainer`来执行提取操作。\n\n假设您的checkpoint文件夹如下所示：\n\n```bash\n$ ls -l output_dir/checkpoint-1/\n-rw-rw-r-- 1 stas stas 1.4K Mar 27 20:42 config.json\ndrwxrwxr-x 2 stas stas 4.0K Mar 25 19:52 global_step1/\n-rw-rw-r-- 1 stas stas   12 Mar 27 13:16 latest\n-rw-rw-r-- 1 stas stas 827K Mar 27 20:42 optimizer.pt\n-rw-rw-r-- 1 stas stas 231M Mar 27 20:42 pytorch_model.bin\n-rw-rw-r-- 1 stas stas  623 Mar 27 20:42 scheduler.pt\n-rw-rw-r-- 1 stas stas 1.8K Mar 27 20:42 special_tokens_map.json\n-rw-rw-r-- 1 stas stas 774K Mar 27 20:42 spiece.model\n-rw-rw-r-- 1 stas stas 1.9K Mar 27 20:42 tokenizer_config.json\n-rw-rw-r-- 1 stas stas  339 Mar 27 20:42 trainer_state.json\n-rw-rw-r-- 1 stas stas 2.3K Mar 27 20:42 training_args.bin\n-rwxrw-r-- 1 stas stas 5.5K Mar 27 13:16 zero_to_fp32.py*\n```\n\n在这个例子中，只有一个DeepSpeed检查点子文件夹*global_step1*。因此，要重构fp32权重，只需运行：\n\n```bash\npython zero_to_fp32.py . pytorch_model.bin\n```\n\n这就是它。`pytorch_model.bin`现在将包含从多个GPUs合并的完整的fp32模型权重。\n\n该脚本将自动能够处理ZeRO-2或ZeRO-3 checkpoint。\n\n`python zero_to_fp32.py -h`将为您提供使用细节。\n\n该脚本将通过文件`latest`的内容自动发现deepspeed子文件夹，在当前示例中，它将包含`global_step1`。\n\n注意：目前该脚本需要2倍于最终fp32模型权重的通用内存。\n\n\n### ZeRO-3 和 Infinity Nuances\n\nZeRO-3与ZeRO-2有很大的不同，主要是因为它的参数分片功能。\n\nZeRO-Infinity进一步扩展了ZeRO-3，以支持NVMe内存和其他速度和可扩展性改进。\n\n尽管所有努力都是为了在不需要对模型进行任何特殊更改的情况下就能正常运行，但在某些情况下，您可能需要以下信息。\n\n\n#### 构建大模型\n\nDeepSpeed/ZeRO-3可以处理参数量达到数万亿的模型，这些模型可能无法适应现有的内存。在这种情况下，如果您还是希望初始化更快地发生，可以使用*deepspeed.zero.Init()*上下文管理器（也是一个函数装饰器）来初始化模型，如下所示：\n\n```python\nfrom transformers import T5ForConditionalGeneration, T5Config\nimport deepspeed\n\nwith deepspeed.zero.Init():\n    config = T5Config.from_pretrained(\"google-t5/t5-small\")\n    model = T5ForConditionalGeneration(config)\n```\n\n如您所见，这会为您随机初始化一个模型。\n\n如果您想使用预训练模型，`model_class.from_pretrained`将在`is_deepspeed_zero3_enabled()`返回`True`的情况下激活此功能，目前这是通过传递的DeepSpeed配置文件中的ZeRO-3配置部分设置的。因此，在调用`from_pretrained`之前，您必须创建**TrainingArguments**对象。以下是可能的顺序示例：\n\n```python\nfrom transformers import AutoModel, Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(..., deepspeed=ds_config)\nmodel = AutoModel.from_pretrained(\"google-t5/t5-small\")\ntrainer = Trainer(model=model, args=training_args, ...)\n```\n\n如果您使用的是官方示例脚本，并且命令行参数中包含`--deepspeed ds_config.json`且启用了ZeRO-3配置，那么一切都已经为您准备好了，因为这是示例脚本的编写方式。\n\n注意：如果模型的fp16权重无法适应单个GPU的内存，则必须使用此功能。\n\n有关此方法和其他相关功能的完整详细信息，请参阅[构建大模型](https://deepspeed.readthedocs.io/en/latest/zero3.html#constructing-massive-models)。\n\n此外，在加载fp16预训练模型时，您希望`from_pretrained`使用`torch_dtype=torch.float16`。详情请参见[from_pretrained-torch-dtype](#from_pretrained-torch-dtype)。\n\n\n#### 参数收集\n\n在多个GPU上使用ZeRO-3时，没有一个GPU拥有所有参数，除非它是当前执行层的参数。因此，如果您需要一次访问所有层的所有参数，有一个特定的方法可以实现。\n您可能不需要它，但如果您需要，请参考[参数收集](https://deepspeed.readthedocs.io/en/latest/zero3.html#manual-parameter-coordination)。\n\n然而，我们在多个地方确实使用了它，其中一个例子是在`from_pretrained`中加载预训练模型权重。我们一次加载一层，然后立即将其分区到所有参与的GPU上，因为对于非常大的模型，无法在一个GPU上一次性加载并将其分布到多个GPU上，因为内存限制。\n\n此外，在ZeRO-3下，如果您编写自己的代码并遇到看起来像这样的模型参数权重：\n\n```python\ntensor([1.0], device=\"cuda:0\", dtype=torch.float16, requires_grad=True)\n```\n\n强调`tensor([1.])`，或者如果您遇到一个错误，它说参数的大小是`1`，而不是某个更大的多维形状，这意味着参数被划分了，你看到的是一个ZeRO-3占位符。\n\n\n\n<a id='deepspeed-zero-inference'></a>\n\n\n### ZeRO 推理\n\n\"ZeRO 推断\" 使用与 \"ZeRO-3 训练\" 相同的配置。您只需要去掉优化器和调度器部分。实际上，如果您希望与训练共享相同的配置文件，您可以将它们保留在配置文件中，它们只会被忽略。\n\n您只需要传递通常的[`TrainingArguments`]参数。例如：\n\n```bash\ndeepspeed --num_gpus=2 your_program.py <normal cl args> --do_eval --deepspeed ds_config.json\n```\n\n唯一的重要事情是您需要使用ZeRO-3配置，因为ZeRO-2对于推理没有任何优势，因为只有ZeRO-3才对参数进行分片，而ZeRO-1则对梯度和优化器状态进行分片。\n\n以下是在DeepSpeed下运行`run_translation.py`启用所有可用GPU的示例：\n\n```bash\ndeepspeed examples/pytorch/translation/run_translation.py \\\n--deepspeed tests/deepspeed/ds_config_zero3.json \\\n--model_name_or_path google-t5/t5-small --output_dir output_dir \\\n--do_eval --max_eval_samples 50 --warmup_steps 50  \\\n--max_source_length 128 --val_max_target_length 128 \\\n--overwrite_output_dir --per_device_eval_batch_size 4 \\\n--predict_with_generate --dataset_config \"ro-en\" --fp16 \\\n--source_lang en --target_lang ro --dataset_name wmt16 \\\n--source_prefix \"translate English to Romanian: \"\n```\n\n由于在推理阶段，优化器状态和梯度不需要额外的大量内存，您应该能够将更大的批次和/或序列长度放到相同的硬件上。\n\n此外，DeepSpeed目前正在开发一个名为Deepspeed-Inference的相关产品，它与ZeRO技术无关，而是使用张量并行来扩展无法适应单个GPU的模型。这是一个正在进行的工作，一旦该产品完成，我们将提供集成。\n\n\n### 内存要求\n\n由于 DeepSpeed ZeRO 可以将内存卸载到 CPU（和 NVMe），该框架提供了一些工具，允许根据使用的 GPU 数量告知将需要多少 CPU 和 GPU 内存。\n\n让我们估计在单个GPU上微调\"bigscience/T0_3B\"所需的内存：\n\n```bash\n$ python -c 'from transformers import AutoModel; \\\nfrom deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live; \\\nmodel = AutoModel.from_pretrained(\"bigscience/T0_3B\"); \\\nestimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=1, num_nodes=1)'\n[...]\nEstimated memory needed for params, optim states and gradients for a:\nHW: Setup with 1 node, 1 GPU per node.\nSW: Model with 2783M total params, 65M largest layer params.\n  per CPU  |  per GPU |   Options\n   70.00GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=1\n   70.00GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=0\n   62.23GB |   5.43GB | offload_param=none, offload_optimizer=cpu , zero_init=1\n   62.23GB |   5.43GB | offload_param=none, offload_optimizer=cpu , zero_init=0\n    0.37GB |  46.91GB | offload_param=none, offload_optimizer=none, zero_init=1\n   15.56GB |  46.91GB | offload_param=none, offload_optimizer=none, zero_init=0\n```\n\n因此，您可以将模型拟合在单个80GB的GPU上，不进行CPU offload，或者使用微小的8GB GPU，但需要约60GB的CPU内存。（请注意，这仅是参数、优化器状态和梯度所需的内存 - 您还需要为CUDA内核、激活值和临时变量分配更多的内存。）\n\n然后，这是成本与速度的权衡。购买/租用较小的 GPU（或较少的 GPU，因为您可以使用多个 GPU 进行 Deepspeed ZeRO）。但这样会更慢，因此即使您不关心完成某项任务的速度，减速也直接影响 GPU 使用的持续时间，从而导致更大的成本。因此，请进行实验并比较哪种方法效果最好。\n\n如果您有足够的GPU内存，请确保禁用CPU/NVMe卸载，因为这会使所有操作更快。\n\n例如，让我们重复相同的操作，使用2个GPU：\n\n```bash\n$ python -c 'from transformers import AutoModel; \\\nfrom deepspeed.runtime.zero.stage3 import estimate_zero3_model_states_mem_needs_all_live; \\\nmodel = AutoModel.from_pretrained(\"bigscience/T0_3B\"); \\\nestimate_zero3_model_states_mem_needs_all_live(model, num_gpus_per_node=2, num_nodes=1)'\n[...]\nEstimated memory needed for params, optim states and gradients for a:\nHW: Setup with 1 node, 2 GPUs per node.\nSW: Model with 2783M total params, 65M largest layer params.\n  per CPU  |  per GPU |   Options\n   70.00GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=1\n   70.00GB |   0.25GB | offload_param=cpu , offload_optimizer=cpu , zero_init=0\n   62.23GB |   2.84GB | offload_param=none, offload_optimizer=cpu , zero_init=1\n   62.23GB |   2.84GB | offload_param=none, offload_optimizer=cpu , zero_init=0\n    0.74GB |  23.58GB | offload_param=none, offload_optimizer=none, zero_init=1\n   31.11GB |  23.58GB | offload_param=none, offload_optimizer=none, zero_init=0\n\n```\n\n所以，您需要2个32GB或更高的GPU，且不进行CPU卸载。\n\n如需了解更多信息，请参阅[内存估算器](https://deepspeed.readthedocs.io/en/latest/memory.html)。\n\n\n\n### 归档Issues\n\n请按照以下步骤提交问题，以便我们能够迅速找到问题并帮助您解除工作阻塞。\n\n在您的报告中，请始终包括以下内容：\n\n1. 完整的Deepspeed配置文件\n2. 如果使用了[`Trainer`]，则包括命令行参数；如果自己编写了Trainer设置，则包括[`TrainingArguments`]参数。请不要导出[`TrainingArguments`]，因为它有几十个与问题无关的条目。\n3. 输出：\n\n    ```bash\n    python -c 'import torch; print(f\"torch: {torch.__version__}\")'\n    python -c 'import transformers; print(f\"transformers: {transformers.__version__}\")'\n    python -c 'import deepspeed; print(f\"deepspeed: {deepspeed.__version__}\")'\n    ```\n\n4. 如果可能，请包含一个Google Colab notebook链接，我们可以使用它来重现问题。您可以使用这个[notebook](https://github.com/stas00/porting/blob/master/transformers/deepspeed/DeepSpeed_on_colab_CLI.ipynb)作为起点。\n5. 除非不可能，否则请始终使用标准数据集，而不是自定义数据集。\n6. 如果可能，尝试使用现有[示例](https://github.com/huggingface/transformers/tree/main/examples/pytorch)之一来重现问题。\n\n需要考虑的因素：\n\n- Deepspeed通常不是问题的原因。\n\n  一些已提交的问题被证明与Deepspeed无关。也就是说，一旦将Deepspeed从设置中移除，问题仍然存在。\n\n  因此，如果问题明显与DeepSpeed相关，例如您可以看到有一个异常并且可以看到DeepSpeed模块涉及其中，请先重新测试没有DeepSpeed的设置。只有当问题仍然存在时，才向Deepspeed提供所有必需的细节。\n\n- 如果您明确问题是在Deepspeed核心中而不是集成部分，请直接向[Deepspeed](https://github.com/deepspeedai/DeepSpeed/)提交问题。如果您不确定，请不要担心，无论使用哪个issue跟踪问题都可以，一旦您发布问题，我们会弄清楚并将其重定向到另一个issue跟踪（如果需要的话）。\n\n\n\n### Troubleshooting\n\n#### 启动时`deepspeed`进程被终止，没有回溯\n\n如果启动时`deepspeed`进程被终止，没有回溯，这通常意味着程序尝试分配的CPU内存超过了系统的限制或进程被允许分配的内存，操作系统内核杀死了该进程。这是因为您的配置文件很可能将`offload_optimizer`或`offload_param`或两者都配置为卸载到`cpu`。如果您有NVMe，可以尝试在ZeRO-3下卸载到NVMe。这里是如何[估计特定模型所需的内存](https://deepspeed.readthedocs.io/en/latest/memory.html)。\n\n#### 训练和/或评估/预测loss为`NaN`\n\n这种情况通常发生在使用bf16混合精度模式预训练的模型试图在fp16（带或不带混合精度）下使用时。大多数在TPU上训练的模型以及由谷歌发布的模型都属于这个类别（例如，几乎所有基于t5的模型）。在这种情况下，解决方案是要么使用fp32，要么在支持的情况下使用bf16（如TPU、Ampere GPU或更新的版本）。\n\n另一个问题可能与使用fp16有关。当您配置此部分时：\n\n```json\n{\n    \"fp16\": {\n        \"enabled\": \"auto\",\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    }\n}\n```\n\n并且您在日志中看到Deepspeed报告`OVERFLOW`如下\n\n```\n0%|                                                                                                                             | 0/189 [00:00<?, ?it/s]\n [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 262144\n  1%|▌                                                                                                                    | 1/189 [00:00<01:26,  2.17it/s]\n [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072.0\n  1%|█▏\n [...]\n [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 1\n 14%|████████████████▌                                                                                                   | 27/189 [00:14<01:13,  2.21it/s]\n [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 1\n 15%|█████████████████▏                                                                                                  | 28/189 [00:14<01:13,  2.18it/s]\n [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 1\n 15%|█████████████████▊                                                                                                  | 29/189 [00:15<01:13,  2.18it/s]\n [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1, reducing to 1\n[...]\n```\n\n这意味着Deepspeed损失缩放器无法找到一个克服损失溢出的缩放系数。\n\n在这种情况下，通常需要提高`initial_scale_power`的值。将其设置为`\"initial_scale_power\": 32`通常会解决问题。\n\n\n\n### 注意事项\n\n- 尽管 DeepSpeed 有一个可安装的 PyPI 包，但强烈建议从源代码安装它，以最好地匹配您的硬件，如果您需要启用某些功能，如 1-bit Adam，这些功能在 pypi 发行版中不可用。\n- 您不必使用🤗  Transformers的 [`Trainer`] 来使用 DeepSpeed   - 您可以使用任何模型与自己的训练器，您还需要根据 [DeepSpeed 集成说明](https://www.deepspeed.ai/getting-started/#writing-deepspeed-models) 调整后者。",
    "1035": "一级标题：DeepSpeed集成\n二级标题：Non-Trainer Deepspeed集成\n内容：\n当`Trainer`没有被使用时，`~integrations.HfDeepSpeedConfig`被用来将Deepspeed集成到huggingface的Transformers核心功能中。它唯一做的事情就是在`from_pretrained`调用期间处理Deepspeed ZeRO-3参数收集和将模型自动分割到多个GPU上。除此之外，您需要自己完成其他所有工作。\n\n当使用`Trainer`时，所有事情都自动得到了处理。\n\n当不使用`Trainer`时，为了高效地部署Deepspeed ZeRO-3，您必须在实例化模型之前实例化`~integrations.HfDeepSpeedConfig`对象并保持该对象活跃。\n\n如果您正在使用Deepspeed ZeRO-1或ZeRO-2，您根本不需要使用`HfDeepSpeedConfig`。\n\n以预训练模型为例:\n\n```python\nfrom transformers.integrations import HfDeepSpeedConfig\nfrom transformers import AutoModel\nimport deepspeed\n\nds_config = {...}  # deepspeed config object or path to the file\n# must run before instantiating the model to detect zero 3\ndschf = HfDeepSpeedConfig(ds_config)  # keep this object alive\nmodel = AutoModel.from_pretrained(\"openai-community/gpt2\")\nengine = deepspeed.initialize(model=model, config_params=ds_config, ...)\n```\n\n或者以非预训练模型为例：\n\n```python\nfrom transformers.integrations import HfDeepSpeedConfig\nfrom transformers import AutoModel, AutoConfig\nimport deepspeed\n\nds_config = {...}  # deepspeed config object or path to the file\n# must run before instantiating the model to detect zero 3\ndschf = HfDeepSpeedConfig(ds_config)  # keep this object alive\nconfig = AutoConfig.from_pretrained(\"openai-community/gpt2\")\nmodel = AutoModel.from_config(config)\nengine = deepspeed.initialize(model=model, config_params=ds_config, ...)\n```\n\n请注意，如果您没有使用[`Trainer`]集成，您完全需要自己动手。基本上遵循[Deepspeed](https://www.deepspeed.ai/)网站上的文档。同时，您必须显式配置配置文件 - 不能使用`\"auto\"`值，而必须放入实际值。",
    "1036": "一级标题：DeepSpeed集成\n二级标题：HfDeepSpeedConfig\n内容：\n[[autodoc]] integrations.HfDeepSpeedConfig\n    - all\n\n### 自定义DeepSpeed ZeRO推理\n\n以下是一个示例，演示了在无法将模型放入单个 GPU 时如果不使用[Trainer]进行 DeepSpeed ZeRO 推理 。该解决方案包括使用额外的 GPU 或/和将 GPU 内存卸载到 CPU 内存。\n\n这里要理解的重要细微差别是，ZeRO的设计方式可以让您在不同的GPU上并行处理不同的输入。\n\n这个例子有很多注释，并且是自文档化的。\n\n请确保：\n\n1. 如果您有足够的GPU内存（因为这会减慢速度），禁用CPU offload。\n2. 如果您拥有Ampere架构或更新的GPU，启用bf16以加快速度。如果您没有这种硬件，只要不使用任何在bf16混合精度下预训练的模型（如大多数t5模型），就可以启用fp16。否则这些模型通常在fp16中溢出，您会看到输出无效结果。\n\n```python\n#!/usr/bin/env python\n\n# This script demonstrates how to use Deepspeed ZeRO in an inference mode when one can't fit a model\n# into a single GPU\n#\n# 1. Use 1 GPU with CPU offload\n# 2. Or use multiple GPUs instead\n#\n# First you need to install deepspeed: pip install deepspeed\n#\n# Here we use a 3B \"bigscience/T0_3B\" model which needs about 15GB GPU RAM - so 1 largish or 2\n# small GPUs can handle it. or 1 small GPU and a lot of CPU memory.\n#\n# To use a larger model like \"bigscience/T0\" which needs about 50GB, unless you have an 80GB GPU -\n# you will need 2-4 gpus. And then you can adapt the script to handle more gpus if you want to\n# process multiple inputs at once.\n#\n# The provided deepspeed config also activates CPU memory offloading, so chances are that if you\n# have a lot of available CPU memory and you don't mind a slowdown you should be able to load a\n# model that doesn't normally fit into a single GPU. If you have enough GPU memory the program will\n# run faster if you don't want offload to CPU - so disable that section then.\n#\n# To deploy on 1 gpu:\n#\n# deepspeed --num_gpus 1 t0.py\n# or:\n# python -m torch.distributed.run --nproc_per_node=1 t0.py\n#\n# To deploy on 2 gpus:\n#\n# deepspeed --num_gpus 2 t0.py\n# or:\n# python -m torch.distributed.run --nproc_per_node=2 t0.py\n\n\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM\nfrom transformers.integrations import HfDeepSpeedConfig\nimport deepspeed\nimport os\nimport torch\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # To avoid warnings about parallelism in tokenizers\n\n# distributed setup\nlocal_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\nworld_size = int(os.getenv(\"WORLD_SIZE\", \"1\"))\ntorch.cuda.set_device(local_rank)\ndeepspeed.init_distributed()\n\nmodel_name = \"bigscience/T0_3B\"\n\nconfig = AutoConfig.from_pretrained(model_name)\nmodel_hidden_size = config.d_model\n\n# batch size has to be divisible by world_size, but can be bigger than world_size\ntrain_batch_size = 1 * world_size\n\n# ds_config notes\n#\n# - enable bf16 if you use Ampere or higher GPU - this will run in mixed precision and will be\n# faster.\n#\n# - for older GPUs you can enable fp16, but it'll only work for non-bf16 pretrained models - e.g.\n# all official t5 models are bf16-pretrained\n#\n# - set offload_param.device to \"none\" or completely remove the `offload_param` section if you don't\n# - want CPU offload\n#\n# - if using `offload_param` you can manually finetune stage3_param_persistence_threshold to control\n# - which params should remain on gpus - the larger the value the smaller the offload size\n#\n# For in-depth info on Deepspeed config see\n# https://huggingface.co/docs/transformers/main/main_classes/deepspeed\n\n# keeping the same format as json for consistency, except it uses lower case for true/false\n# fmt: off\nds_config = {\n    \"fp16\": {\n        \"enabled\": False\n    },\n    \"bf16\": {\n        \"enabled\": False\n    },\n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_param\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": True\n        },\n        \"overlap_comm\": True,\n        \"contiguous_gradients\": True,\n        \"reduce_bucket_size\": model_hidden_size * model_hidden_size,\n        \"stage3_prefetch_bucket_size\": 0.9 * model_hidden_size * model_hidden_size,\n        \"stage3_param_persistence_threshold\": 10 * model_hidden_size\n    },\n    \"steps_per_print\": 2000,\n    \"train_batch_size\": train_batch_size,\n    \"train_micro_batch_size_per_gpu\": 1,\n    \"wall_clock_breakdown\": False\n}\n# fmt: on\n\n# next line instructs transformers to partition the model directly over multiple gpus using\n# deepspeed.zero.Init when model's `from_pretrained` method is called.\n#\n# **it has to be run before loading the model AutoModelForSeq2SeqLM.from_pretrained(model_name)**\n#\n# otherwise the model will first be loaded normally and only partitioned at forward time which is\n# less efficient and when there is little CPU RAM may fail\ndschf = HfDeepSpeedConfig(ds_config)  # keep this object alive\n\n# now a model can be loaded.\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n# initialise Deepspeed ZeRO and store only the engine object\nds_engine = deepspeed.initialize(model=model, config_params=ds_config)[0]\nds_engine.module.eval()  # inference\n\n# Deepspeed ZeRO can process unrelated inputs on each GPU. So for 2 gpus you process 2 inputs at once.\n# If you use more GPUs adjust for more.\n# And of course if you have just one input to process you then need to pass the same string to both gpus\n# If you use only one GPU, then you will have only rank 0.\nrank = torch.distributed.get_rank()\nif rank == 0:\n    text_in = \"Is this review positive or negative? Review: this is the best cast iron skillet you will ever buy\"\nelif rank == 1:\n    text_in = \"Is this review positive or negative? Review: this is the worst restaurant ever\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ninputs = tokenizer.encode(text_in, return_tensors=\"pt\").to(device=local_rank)\nwith torch.no_grad():\n    outputs = ds_engine.module.generate(inputs, synced_gpus=True)\ntext_out = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(f\"rank{rank}:\\n   in={text_in}\\n  out={text_out}\")\n```\n\n让我们保存它为 `t0.py`并运行：\n```bash\n$ deepspeed --num_gpus 2 t0.py\nrank0:\n   in=Is this review positive or negative? Review: this is the best cast iron skillet you will ever buy\n  out=Positive\nrank1:\n   in=Is this review positive or negative? Review: this is the worst restaurant ever\n  out=negative\n```\n\n这是一个非常基本的例子，您需要根据自己的需求进行修改。\n\n### `generate` 的差异\n\n在使用ZeRO stage 3的多GPU时，需要通过调用`generate(..., synced_gpus=True)`来同步GPU。如果一个GPU在其它GPU之前完成生成，整个系统将挂起，因为其他GPU无法从停止生成的GPU接收权重分片。\n\n从`transformers>=4.28`开始，如果没有明确指定`synced_gpus`，检测到这些条件后它将自动设置为`True`。但如果您需要覆盖`synced_gpus`的值，仍然可以这样做。",
    "1037": "一级标题：DeepSpeed集成\n二级标题：测试 DeepSpeed 集成\n内容：\n如果您提交了一个涉及DeepSpeed集成的PR，请注意我们的CircleCI PR CI设置没有GPU，因此我们只在另一个CI夜间运行需要GPU的测试。因此，如果您在PR中获得绿色的CI报告，并不意味着DeepSpeed测试通过。\n\n要运行DeepSpeed测试，请至少运行以下命令：\n\n```bash\nRUN_SLOW=1 pytest tests/deepspeed/test_deepspeed.py\n```\n\n如果你更改了任何模型或PyTorch示例代码，请同时运行多模型测试。以下将运行所有DeepSpeed测试：\n\n```bash\nRUN_SLOW=1 pytest tests/deepspeed\n```",
    "1038": "一级标题：DeepSpeed集成\n二级标题：主要的DeepSpeed资源\n内容：\n- [项目GitHub](https://github.com/deepspeedai/DeepSpeed)\n- [使用文档](https://www.deepspeed.ai/getting-started/)\n- [API文档](https://deepspeed.readthedocs.io/en/latest/index.html)\n- [博客文章](https://www.microsoft.com/en-us/research/search/?q=deepspeed)\n\n论文:\n\n- [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://huggingface.co/papers/1910.02054)\n- [ZeRO-Offload: Democratizing Billion-Scale Model Training](https://huggingface.co/papers/2101.06840)\n- [ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://huggingface.co/papers/2104.07857)\n\n最后，请记住，HuggingFace [`Trainer`]仅集成了DeepSpeed，因此如果您在使用DeepSpeed时遇到任何问题或疑问，请在[DeepSpeed GitHub](https://github.com/deepspeedai/DeepSpeed/issues)上提交一个issue。",
    "1039": "一级标题：Feature Extractor\n二级标题：无\n内容：\nFeature Extractor负责为音频或视觉模型准备输入特征。这包括从序列中提取特征，例如，对音频文件进行预处理以生成Log-Mel频谱特征，以及从图像中提取特征，例如，裁剪图像文件，同时还包括填充、归一化和转换为NumPy、PyTorch和TensorFlow张量。",
    "1040": "一级标题：Feature Extractor\n二级标题：FeatureExtractionMixin\n内容：\n[[autodoc]] feature_extraction_utils.FeatureExtractionMixin\n    - from_pretrained\n    - save_pretrained",
    "1041": "一级标题：Feature Extractor\n二级标题：SequenceFeatureExtractor\n内容：\n[[autodoc]] SequenceFeatureExtractor\n    - pad",
    "1042": "一级标题：Feature Extractor\n二级标题：BatchFeature\n内容：\n[[autodoc]] BatchFeature",
    "1043": "一级标题：Feature Extractor\n二级标题：ImageFeatureExtractionMixin\n内容：\n[[autodoc]] image_utils.ImageFeatureExtractionMixin",
    "1044": "一级标题：Image Processor\n二级标题：无\n内容：\nImage processor负责为视觉模型准备输入特征并后期处理处理它们的输出。这包括诸如调整大小、归一化和转换为PyTorch、TensorFlow、Flax和NumPy张量等转换。它还可能包括特定于模型的后期处理，例如将logits转换为分割掩码。",
    "1045": "一级标题：Image Processor\n二级标题：ImageProcessingMixin\n内容：\n[[autodoc]] image_processing_utils.ImageProcessingMixin\n    - from_pretrained\n    - save_pretrained",
    "1046": "一级标题：Image Processor\n二级标题：BatchFeature\n内容：\n[[autodoc]] BatchFeature",
    "1047": "一级标题：Image Processor\n二级标题：BaseImageProcessor\n内容：\n[[autodoc]] image_processing_utils.BaseImageProcessor",
    "1048": "一级标题：Keras callbacks\n二级标题：无\n内容：\n在Keras中训练Transformers模型时，有一些库特定的callbacks函数可用于自动执行常见任务：",
    "1049": "一级标题：Keras callbacks\n二级标题：KerasMetricCallback\n内容：\n[[autodoc]] KerasMetricCallback",
    "1050": "一级标题：Keras callbacks\n二级标题：PushToHubCallback\n内容：\n[[autodoc]] PushToHubCallback",
    "1051": "一级标题：Logging\n二级标题：无\n内容：\n🤗 Transformers拥有一个集中式的日志系统，因此您可以轻松设置库输出的日志详细程度。\n\n当前库的默认日志详细程度为`WARNING`。\n\n要更改日志详细程度，只需使用其中一个直接的setter。例如，以下是如何将日志详细程度更改为INFO级别的方法：\n\n```python\nimport transformers\n\ntransformers.logging.set_verbosity_info()\n```\n\n您还可以使用环境变量`TRANSFORMERS_VERBOSITY`来覆盖默认的日志详细程度。您可以将其设置为以下级别之一：`debug`、`info`、`warning`、`error`、`critical`。例如：\n\n```bash\nTRANSFORMERS_VERBOSITY=error ./myprogram.py\n```\n\n此外，通过将环境变量`TRANSFORMERS_NO_ADVISORY_WARNINGS`设置为`true`（如*1*），可以禁用一些`warnings`。这将禁用[`logger.warning_advice`]记录的任何警告。例如：\n\n```bash\nTRANSFORMERS_NO_ADVISORY_WARNINGS=1 ./myprogram.py\n```\n\n以下是如何在您自己的模块或脚本中使用与库相同的logger的示例：\n\n```python\nfrom transformers.utils import logging\n\nlogging.set_verbosity_info()\nlogger = logging.get_logger(\"transformers\")\nlogger.info(\"INFO\")\nlogger.warning(\"WARN\")\n```\n\n\n此日志模块的所有方法都在下面进行了记录，主要的方法包括 [`logging.get_verbosity`] 用于获取logger当前输出日志详细程度的级别和 [`logging.set_verbosity`] 用于将详细程度设置为您选择的级别。按照顺序（从最不详细到最详细），这些级别（及其相应的整数值）为：\n\n- `transformers.logging.CRITICAL` 或 `transformers.logging.FATAL`（整数值，50）：仅报告最关键的errors。\n- `transformers.logging.ERROR`（整数值，40）：仅报告errors。\n- `transformers.logging.WARNING` 或 `transformers.logging.WARN`（整数值，30）：仅报告error和warnings。这是库使用的默认级别。\n- `transformers.logging.INFO`（整数值，20）：报告error、warnings和基本信息。\n- `transformers.logging.DEBUG`（整数值，10）：报告所有信息。\n\n默认情况下，将在模型下载期间显示`tqdm`进度条。[`logging.disable_progress_bar`] 和 [`logging.enable_progress_bar`] 可用于禁止或启用此行为。",
    "1052": "一级标题：Logging\n二级标题：`logging` vs `warnings`\n内容：\nPython有两个经常一起使用的日志系统：如上所述的`logging`，和对特定buckets中的警告进行进一步分类的`warnings`，例如，`FutureWarning`用于输出已经被弃用的功能或路径，`DeprecationWarning`用于指示即将被弃用的内容。\n\n我们在`transformers`库中同时使用这两个系统。我们利用并调整了`logging`的`captureWarning`方法，以便通过上面的详细程度setters来管理这些警告消息。\n\n对于库的开发人员，这意味着什么呢？我们应该遵循以下启发法则：\n- 库的开发人员和依赖于`transformers`的库应优先使用`warnings`\n- `logging`应该用于在日常项目中经常使用它的用户\n\n以下是`captureWarnings`方法的参考。\n\n[[autodoc]] logging.captureWarnings",
    "1053": "一级标题：Logging\n二级标题：Base setters\n内容：\n[[autodoc]] logging.set_verbosity_error\n\n[[autodoc]] logging.set_verbosity_warning\n\n[[autodoc]] logging.set_verbosity_info\n\n[[autodoc]] logging.set_verbosity_debug",
    "1054": "一级标题：Logging\n二级标题：Other functions\n内容：\n[[autodoc]] logging.get_verbosity\n\n[[autodoc]] logging.set_verbosity\n\n[[autodoc]] logging.get_logger\n\n[[autodoc]] logging.enable_default_handler\n\n[[autodoc]] logging.disable_default_handler\n\n[[autodoc]] logging.enable_explicit_format\n\n[[autodoc]] logging.reset_format\n\n[[autodoc]] logging.enable_progress_bar\n\n[[autodoc]] logging.disable_progress_bar",
    "1055": "一级标题：模型\n二级标题：无\n内容：\n基类 [`PreTrainedModel`]、[`TFPreTrainedModel`] 和 [`FlaxPreTrainedModel`] 实现了从本地文件或目录加载/保存模型的常用方法，或者从库上提供的预训练模型配置（从 HuggingFace 的 AWS S3 存储库下载）加载模型。\n\n[`PreTrainedModel`] 和 [`TFPreTrainedModel`] 还实现了一些所有模型共有的方法：\n\n- 在向量词嵌入增加新词汇时调整输入标记（token）的大小\n- 对模型的注意力头进行修剪。\n\n其他的通用方法在 [`~modeling_utils.ModuleUtilsMixin`]（用于 PyTorch 模型）和 [`~modeling_tf_utils.TFModuleUtilsMixin`]（用于 TensorFlow 模型）中定义；文本生成方面的方法则定义在 [`~generation.GenerationMixin`]（用于 PyTorch 模型）、[`~generation.TFGenerationMixin`]（用于 TensorFlow 模型）和 [`~generation.FlaxGenerationMixin`]（用于 Flax/JAX 模型）中。",
    "1056": "一级标题：模型\n二级标题：PreTrainedModel\n内容：\n[[autodoc]] PreTrainedModel\n    - push_to_hub\n    - all\n\n<a id='from_pretrained-torch-dtype'></a>\n\n### 大模型加载\n\n在 Transformers 4.20.0 中，[`~PreTrainedModel.from_pretrained`] 方法已重新设计，以适应使用 [Accelerate](https://huggingface.co/docs/accelerate/big_modeling) 加载大型模型的场景。这需要您使用的 Accelerate 和 PyTorch 版本满足： Accelerate >= 0.9.0， PyTorch >= 1.9.0。除了创建完整模型，然后在其中加载预训练权重（这会占用两倍于模型大小的内存空间，一个用于随机初始化模型，一个用于预训练权重），我们提供了一种选项，将模型创建为空壳，然后只有在加载预训练权重时才实例化其参数。\n\n此外，如果内存不足以放下加载整个模型（目前仅适用于推理），您可以直接将模型放置在不同的设备上。使用 `device_map=\"auto\"`，Accelerate 将确定将每一层放置在哪个设备上，以最大化使用最快的设备（GPU），并将其余部分卸载到 CPU，甚至硬盘上（如果您没有足够的 GPU 内存 或 CPU 内存）。即使模型分布在几个设备上，它也将像您通常期望的那样运行。\n\n```python\nfrom transformers import AutoModelForSeq2SeqLM\n\nt0pp = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", device_map=\"auto\")\n```\n\n您可以通过 `hf_device_map` 属性来查看模型是如何在设备上分割的：\n\n```python\nt0pp.hf_device_map\n{'shared': 0,\n 'decoder.embed_tokens': 0,\n 'encoder': 0,\n 'decoder.block.0': 0,\n 'decoder.block.1': 1,\n 'decoder.block.2': 1,\n 'decoder.block.3': 1,\n 'decoder.block.4': 1,\n 'decoder.block.5': 1,\n 'decoder.block.6': 1,\n 'decoder.block.7': 1,\n 'decoder.block.8': 1,\n 'decoder.block.9': 1,\n 'decoder.block.10': 1,\n 'decoder.block.11': 1,\n 'decoder.block.12': 1,\n 'decoder.block.13': 1,\n 'decoder.block.14': 1,\n 'decoder.block.15': 1,\n 'decoder.block.16': 1,\n 'decoder.block.17': 1,\n 'decoder.block.18': 1,\n 'decoder.block.19': 1,\n 'decoder.block.20': 1,\n 'decoder.block.21': 1,\n 'decoder.block.22': 'cpu',\n 'decoder.block.23': 'cpu',\n 'decoder.final_layer_norm': 'cpu',\n 'decoder.dropout': 'cpu',\n 'lm_head': 'cpu'}\n```\n\n您还可以按照相同的格式（一个层名称到设备的映射关系的字典）编写自己的设备映射规则。它应该将模型的所有参数映射到给定的设备上，如果该层的所有子模块都在同一设备上，您不必详细说明其中所有子模块的位置。例如，以下设备映射对于 T0pp 将正常工作（只要您有 GPU 内存）：\n\n```python\ndevice_map = {\"shared\": 0, \"encoder\": 0, \"decoder\": 1, \"lm_head\": 1}\n```\n\n另一种减少模型内存影响的方法是以较低精度的 dtype（例如 `torch.float16`）实例化它，或者使用下面介绍的直接量化技术。\n\n### 模型实例化 dtype\n\n在 PyTorch 下，模型通常以 `torch.float32` 格式实例化。如果尝试加载权重为 fp16 的模型，这可能会导致问题，因为它将需要两倍的内存。为了克服此限制，您可以使用 `torch_dtype` 参数显式传递所需的 `dtype`：\n\n```python\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5\", torch_dtype=torch.float16)\n```\n或者，如果您希望模型始终以最优的内存模式加载，则可以使用特殊值 `\"auto\"`，然后 `dtype` 将自动从模型的权重中推导出：\n```python\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5\", torch_dtype=\"auto\")\n```\n\n也可以通过以下方式告知从头开始实例化的模型要使用哪种 `dtype`：\n\n```python\nconfig = T5Config.from_pretrained(\"t5\")\nmodel = AutoModel.from_config(config)\n```\n\n由于 PyTorch 的设计，此功能仅适用于浮点类型。",
    "1057": "一级标题：模型\n二级标题：ModuleUtilsMixin\n内容：\n[[autodoc]] modeling_utils.ModuleUtilsMixin\n\nTFPreTrainedModel\n[[autodoc]] TFPreTrainedModel\n    - push_to_hub\n    - all",
    "1058": "一级标题：模型\n二级标题：TFModelUtilsMixin\n内容：\n[[autodoc]] modeling_tf_utils.TFModelUtilsMixin\n\nFlaxPreTrainedModel\n[[autodoc]] FlaxPreTrainedModel\n    - push_to_hub\n    - all",
    "1059": "一级标题：模型\n二级标题：推送到 Hub\n内容：\n[[autodoc]] utils.PushToHubMixin",
    "1060": "一级标题：模型\n二级标题：分片检查点\n内容：\n[[autodoc]] modeling_utils.load_sharded_checkpoint",
    "1061": "一级标题：导出 🤗 Transformers 模型到 ONNX\n二级标题：无\n内容：\n🤗 Transformers提供了一个`transformers.onnx`包，通过利用配置对象，您可以将模型checkpoints转换为ONNX图。\n\n有关更多详细信息，请参阅导出 🤗 Transformers 模型的[指南](../serialization)。",
    "1062": "一级标题：导出 🤗 Transformers 模型到 ONNX\n二级标题：ONNX Configurations\n内容：\n我们提供了三个抽象类，取决于您希望导出的模型架构类型：\n\n* 基于编码器的模型继承 [`~onnx.config.OnnxConfig`]\n* 基于解码器的模型继承 [`~onnx.config.OnnxConfigWithPast`]\n* 编码器-解码器模型继承 [`~onnx.config.OnnxSeq2SeqConfigWithPast`]\n\n### OnnxConfig\n\n[[autodoc]] onnx.config.OnnxConfig\n\n### OnnxConfigWithPast\n\n[[autodoc]] onnx.config.OnnxConfigWithPast\n\n### OnnxSeq2SeqConfigWithPast\n\n[[autodoc]] onnx.config.OnnxSeq2SeqConfigWithPast",
    "1063": "一级标题：导出 🤗 Transformers 模型到 ONNX\n二级标题：ONNX Features\n内容：\n每个ONNX配置与一组 _特性_ 相关联，使您能够为不同类型的拓扑结构或任务导出模型。\n\n### FeaturesManager\n\n[[autodoc]] onnx.features.FeaturesManager",
    "1064": "一级标题：Optimization\n二级标题：无\n内容：\n`.optimization` 模块提供了：\n\n- 一个带有固定权重衰减的优化器，可用于微调模型\n- 继承自 `_LRSchedule` 多个调度器：\n- 一个梯度累积类，用于累积多个批次的梯度",
    "1065": "一级标题：Optimization\n二级标题：AdaFactor (PyTorch)\n内容：\n[[autodoc]] Adafactor",
    "1066": "一级标题：Optimization\n二级标题：AdamWeightDecay (TensorFlow)\n内容：\n[[autodoc]] AdamWeightDecay\n\n[[autodoc]] create_optimizer",
    "1067": "一级标题：Optimization\n二级标题：Schedules\n内容：\n### Learning Rate Schedules (Pytorch)\n\n[[autodoc]] SchedulerType\n\n[[autodoc]] get_scheduler\n\n[[autodoc]] get_constant_schedule\n\n[[autodoc]] get_constant_schedule_with_warmup\n\n<img alt=\"\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_constant_schedule.png\"/>\n\n[[autodoc]] get_cosine_schedule_with_warmup\n\n<img alt=\"\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_cosine_schedule.png\"/>\n\n[[autodoc]] get_cosine_with_hard_restarts_schedule_with_warmup\n\n<img alt=\"\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_cosine_hard_restarts_schedule.png\"/>\n\n[[autodoc]] get_linear_schedule_with_warmup\n\n<img alt=\"\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_linear_schedule.png\"/>\n\n[[autodoc]] get_polynomial_decay_schedule_with_warmup\n\n[[autodoc]] get_inverse_sqrt_schedule\n\n### Warmup (TensorFlow)\n\n[[autodoc]] WarmUp",
    "1068": "一级标题：Optimization\n二级标题：Gradient Strategies\n内容：\n### GradientAccumulator (TensorFlow)\n\n[[autodoc]] GradientAccumulator",
    "1069": "一级标题：模型输出\n二级标题：无\n内容：\n所有模型的输出都是 [`~utils.ModelOutput`] 的子类的实例。这些是包含模型返回的所有信息的数据结构，但也可以用作元组或字典。\n\n让我们看一个例子：\n\n```python\nfrom transformers import BertTokenizer, BertForSequenceClassification\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\nmodel = BertForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\")\n\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\nlabels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\noutputs = model(**inputs, labels=labels)\n```\n\n`outputs` 对象是 [`~modeling_outputs.SequenceClassifierOutput`]，如下面该类的文档中所示，它表示它有一个可选的 `loss`，一个 `logits`，一个可选的 `hidden_states` 和一个可选的 `attentions` 属性。在这里，我们有 `loss`，因为我们传递了 `labels`，但我们没有 `hidden_states` 和 `attentions`，因为我们没有传递 `output_hidden_states=True` 或 `output_attentions=True`。\n\n<Tip>\n\n当传递 `output_hidden_states=True` 时，您可能希望 `outputs.hidden_states[-1]` 与 `outputs.last_hidden_states` 完全匹配。然而，这并不总是成立。一些模型在返回最后的 hidden state时对其应用归一化或其他后续处理。\n\n</Tip>\n\n\n您可以像往常一样访问每个属性，如果模型未返回该属性，您将得到 `None`。在这里，例如，`outputs.loss` 是模型计算的损失，而 `outputs.attentions` 是 `None`。\n\n当将我们的 `outputs` 对象视为元组时，它仅考虑那些没有 `None` 值的属性。例如这里它有两个元素，`loss` 和 `logits`，所以\n\n```python\noutputs[:2]\n```\n\n将返回元组 `(outputs.loss, outputs.logits)`。\n\n将我们的 `outputs` 对象视为字典时，它仅考虑那些没有 `None` 值的属性。例如在这里它有两个键，分别是 `loss` 和 `logits`。\n\n我们在这里记录了被多个类型模型使用的通用模型输出。特定输出类型在其相应的模型页面上有文档。",
    "1070": "一级标题：模型输出\n二级标题：ModelOutput\n内容：\n[[autodoc]] utils.ModelOutput\n    - to_tuple",
    "1071": "一级标题：模型输出\n二级标题：BaseModelOutput\n内容：\n[[autodoc]] modeling_outputs.BaseModelOutput",
    "1072": "一级标题：模型输出\n二级标题：BaseModelOutputWithPooling\n内容：\n[[autodoc]] modeling_outputs.BaseModelOutputWithPooling",
    "1073": "一级标题：模型输出\n二级标题：BaseModelOutputWithCrossAttentions\n内容：\n[[autodoc]] modeling_outputs.BaseModelOutputWithCrossAttentions",
    "1074": "一级标题：模型输出\n二级标题：BaseModelOutputWithPoolingAndCrossAttentions\n内容：\n[[autodoc]] modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions",
    "1075": "一级标题：模型输出\n二级标题：BaseModelOutputWithPast\n内容：\n[[autodoc]] modeling_outputs.BaseModelOutputWithPast",
    "1076": "一级标题：模型输出\n二级标题：BaseModelOutputWithPastAndCrossAttentions\n内容：\n[[autodoc]] modeling_outputs.BaseModelOutputWithPastAndCrossAttentions",
    "1077": "一级标题：模型输出\n二级标题：Seq2SeqModelOutput\n内容：\n[[autodoc]] modeling_outputs.Seq2SeqModelOutput",
    "1078": "一级标题：模型输出\n二级标题：CausalLMOutput\n内容：\n[[autodoc]] modeling_outputs.CausalLMOutput",
    "1079": "一级标题：模型输出\n二级标题：CausalLMOutputWithCrossAttentions\n内容：\n[[autodoc]] modeling_outputs.CausalLMOutputWithCrossAttentions",
    "1080": "一级标题：模型输出\n二级标题：CausalLMOutputWithPast\n内容：\n[[autodoc]] modeling_outputs.CausalLMOutputWithPast",
    "1081": "一级标题：模型输出\n二级标题：MaskedLMOutput\n内容：\n[[autodoc]] modeling_outputs.MaskedLMOutput",
    "1082": "一级标题：模型输出\n二级标题：Seq2SeqLMOutput\n内容：\n[[autodoc]] modeling_outputs.Seq2SeqLMOutput",
    "1083": "一级标题：模型输出\n二级标题：NextSentencePredictorOutput\n内容：\n[[autodoc]] modeling_outputs.NextSentencePredictorOutput",
    "1084": "一级标题：模型输出\n二级标题：SequenceClassifierOutput\n内容：\n[[autodoc]] modeling_outputs.SequenceClassifierOutput",
    "1085": "一级标题：模型输出\n二级标题：Seq2SeqSequenceClassifierOutput\n内容：\n[[autodoc]] modeling_outputs.Seq2SeqSequenceClassifierOutput",
    "1086": "一级标题：模型输出\n二级标题：MultipleChoiceModelOutput\n内容：\n[[autodoc]] modeling_outputs.MultipleChoiceModelOutput",
    "1087": "一级标题：模型输出\n二级标题：TokenClassifierOutput\n内容：\n[[autodoc]] modeling_outputs.TokenClassifierOutput",
    "1088": "一级标题：模型输出\n二级标题：QuestionAnsweringModelOutput\n内容：\n[[autodoc]] modeling_outputs.QuestionAnsweringModelOutput",
    "1089": "一级标题：模型输出\n二级标题：Seq2SeqQuestionAnsweringModelOutput\n内容：\n[[autodoc]] modeling_outputs.Seq2SeqQuestionAnsweringModelOutput",
    "1090": "一级标题：模型输出\n二级标题：Seq2SeqSpectrogramOutput\n内容：\n[[autodoc]] modeling_outputs.Seq2SeqSpectrogramOutput",
    "1091": "一级标题：模型输出\n二级标题：SemanticSegmenterOutput\n内容：\n[[autodoc]] modeling_outputs.SemanticSegmenterOutput",
    "1092": "一级标题：模型输出\n二级标题：ImageClassifierOutput\n内容：\n[[autodoc]] modeling_outputs.ImageClassifierOutput",
    "1093": "一级标题：模型输出\n二级标题：ImageClassifierOutputWithNoAttention\n内容：\n[[autodoc]] modeling_outputs.ImageClassifierOutputWithNoAttention",
    "1094": "一级标题：模型输出\n二级标题：DepthEstimatorOutput\n内容：\n[[autodoc]] modeling_outputs.DepthEstimatorOutput",
    "1095": "一级标题：模型输出\n二级标题：Wav2Vec2BaseModelOutput\n内容：\n[[autodoc]] modeling_outputs.Wav2Vec2BaseModelOutput",
    "1096": "一级标题：模型输出\n二级标题：XVectorOutput\n内容：\n[[autodoc]] modeling_outputs.XVectorOutput",
    "1097": "一级标题：模型输出\n二级标题：Seq2SeqTSModelOutput\n内容：\n[[autodoc]] modeling_outputs.Seq2SeqTSModelOutput",
    "1098": "一级标题：模型输出\n二级标题：Seq2SeqTSPredictionOutput\n内容：\n[[autodoc]] modeling_outputs.Seq2SeqTSPredictionOutput",
    "1099": "一级标题：模型输出\n二级标题：SampleTSPredictionOutput\n内容：\n[[autodoc]] modeling_outputs.SampleTSPredictionOutput",
    "1100": "一级标题：模型输出\n二级标题：TFBaseModelOutput\n内容：\n[[autodoc]] modeling_tf_outputs.TFBaseModelOutput",
    "1101": "一级标题：模型输出\n二级标题：TFBaseModelOutputWithPooling\n内容：\n[[autodoc]] modeling_tf_outputs.TFBaseModelOutputWithPooling",
    "1102": "一级标题：模型输出\n二级标题：TFBaseModelOutputWithPoolingAndCrossAttentions\n内容：\n[[autodoc]] modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions",
    "1103": "一级标题：模型输出\n二级标题：TFBaseModelOutputWithPast\n内容：\n[[autodoc]] modeling_tf_outputs.TFBaseModelOutputWithPast",
    "1104": "一级标题：模型输出\n二级标题：TFBaseModelOutputWithPastAndCrossAttentions\n内容：\n[[autodoc]] modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions",
    "1105": "一级标题：模型输出\n二级标题：TFSeq2SeqModelOutput\n内容：\n[[autodoc]] modeling_tf_outputs.TFSeq2SeqModelOutput",
    "1106": "一级标题：模型输出\n二级标题：TFCausalLMOutput\n内容：\n[[autodoc]] modeling_tf_outputs.TFCausalLMOutput",
    "1107": "一级标题：模型输出\n二级标题：TFCausalLMOutputWithCrossAttentions\n内容：\n[[autodoc]] modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions",
    "1108": "一级标题：模型输出\n二级标题：TFCausalLMOutputWithPast\n内容：\n[[autodoc]] modeling_tf_outputs.TFCausalLMOutputWithPast",
    "1109": "一级标题：模型输出\n二级标题：TFMaskedLMOutput\n内容：\n[[autodoc]] modeling_tf_outputs.TFMaskedLMOutput",
    "1110": "一级标题：模型输出\n二级标题：TFSeq2SeqLMOutput\n内容：\n[[autodoc]] modeling_tf_outputs.TFSeq2SeqLMOutput",
    "1111": "一级标题：模型输出\n二级标题：TFNextSentencePredictorOutput\n内容：\n[[autodoc]] modeling_tf_outputs.TFNextSentencePredictorOutput",
    "1112": "一级标题：模型输出\n二级标题：TFSequenceClassifierOutput\n内容：\n[[autodoc]] modeling_tf_outputs.TFSequenceClassifierOutput",
    "1113": "一级标题：模型输出\n二级标题：TFSeq2SeqSequenceClassifierOutput\n内容：\n[[autodoc]] modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput",
    "1114": "一级标题：模型输出\n二级标题：TFMultipleChoiceModelOutput\n内容：\n[[autodoc]] modeling_tf_outputs.TFMultipleChoiceModelOutput",
    "1115": "一级标题：模型输出\n二级标题：TFTokenClassifierOutput\n内容：\n[[autodoc]] modeling_tf_outputs.TFTokenClassifierOutput",
    "1116": "一级标题：模型输出\n二级标题：TFQuestionAnsweringModelOutput\n内容：\n[[autodoc]] modeling_tf_outputs.TFQuestionAnsweringModelOutput",
    "1117": "一级标题：模型输出\n二级标题：TFSeq2SeqQuestionAnsweringModelOutput\n内容：\n[[autodoc]] modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput",
    "1118": "一级标题：模型输出\n二级标题：FlaxBaseModelOutput\n内容：\n[[autodoc]] modeling_flax_outputs.FlaxBaseModelOutput",
    "1119": "一级标题：模型输出\n二级标题：FlaxBaseModelOutputWithPast\n内容：\n[[autodoc]] modeling_flax_outputs.FlaxBaseModelOutputWithPast",
    "1120": "一级标题：模型输出\n二级标题：FlaxBaseModelOutputWithPooling\n内容：\n[[autodoc]] modeling_flax_outputs.FlaxBaseModelOutputWithPooling",
    "1121": "一级标题：模型输出\n二级标题：FlaxBaseModelOutputWithPastAndCrossAttentions\n内容：\n[[autodoc]] modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions",
    "1122": "一级标题：模型输出\n二级标题：FlaxSeq2SeqModelOutput\n内容：\n[[autodoc]] modeling_flax_outputs.FlaxSeq2SeqModelOutput",
    "1123": "一级标题：模型输出\n二级标题：FlaxCausalLMOutputWithCrossAttentions\n内容：\n[[autodoc]] modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions",
    "1124": "一级标题：模型输出\n二级标题：FlaxMaskedLMOutput\n内容：\n[[autodoc]] modeling_flax_outputs.FlaxMaskedLMOutput",
    "1125": "一级标题：模型输出\n二级标题：FlaxSeq2SeqLMOutput\n内容：\n[[autodoc]] modeling_flax_outputs.FlaxSeq2SeqLMOutput",
    "1126": "一级标题：模型输出\n二级标题：FlaxNextSentencePredictorOutput\n内容：\n[[autodoc]] modeling_flax_outputs.FlaxNextSentencePredictorOutput",
    "1127": "一级标题：模型输出\n二级标题：FlaxSequenceClassifierOutput\n内容：\n[[autodoc]] modeling_flax_outputs.FlaxSequenceClassifierOutput",
    "1128": "一级标题：模型输出\n二级标题：FlaxSeq2SeqSequenceClassifierOutput\n内容：\n[[autodoc]] modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput",
    "1129": "一级标题：模型输出\n二级标题：FlaxMultipleChoiceModelOutput\n内容：\n[[autodoc]] modeling_flax_outputs.FlaxMultipleChoiceModelOutput",
    "1130": "一级标题：模型输出\n二级标题：FlaxTokenClassifierOutput\n内容：\n[[autodoc]] modeling_flax_outputs.FlaxTokenClassifierOutput",
    "1131": "一级标题：模型输出\n二级标题：FlaxQuestionAnsweringModelOutput\n内容：\n[[autodoc]] modeling_flax_outputs.FlaxQuestionAnsweringModelOutput",
    "1132": "一级标题：模型输出\n二级标题：FlaxSeq2SeqQuestionAnsweringModelOutput\n内容：\n[[autodoc]] modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput",
    "1133": "一级标题：Pipelines\n二级标题：无\n内容：\npipelines是使用模型进行推理的一种简单方法。这些pipelines是抽象了库中大部分复杂代码的对象，提供了一个专用于多个任务的简单API，包括专名识别、掩码语言建模、情感分析、特征提取和问答等。请参阅[任务摘要](../task_summary)以获取使用示例。\n\n有两种pipelines抽象类需要注意：\n\n- [`pipeline`]，它是封装所有其他pipelines的最强大的对象。\n- 针对特定任务pipelines，适用于[音频](#audio)、[计算机视觉](#computer-vision)、[自然语言处理](#natural-language-processing)和[多模态](#multimodal)任务。",
    "1134": "一级标题：Pipelines\n二级标题：pipeline抽象类\n内容：\n*pipeline*抽象类是对所有其他可用pipeline的封装。它可以像任何其他pipeline一样实例化，但进一步提供额外的便利性。\n\n简单调用一个项目：\n\n\n```python\n>>> pipe = pipeline(\"text-classification\")\n>>> pipe(\"This restaurant is awesome\")\n[{'label': 'POSITIVE', 'score': 0.9998743534088135}]\n```\n\n如果您想使用 [hub](https://huggingface.co) 上的特定模型，可以忽略任务，如果hub上的模型已经定义了该任务：\n\n```python\n>>> pipe = pipeline(model=\"FacebookAI/roberta-large-mnli\")\n>>> pipe(\"This restaurant is awesome\")\n[{'label': 'NEUTRAL', 'score': 0.7313136458396912}]\n```\n\n要在多个项目上调用pipeline，可以使用*列表*调用它。\n\n```python\n>>> pipe = pipeline(\"text-classification\")\n>>> pipe([\"This restaurant is awesome\", \"This restaurant is awful\"])\n[{'label': 'POSITIVE', 'score': 0.9998743534088135},\n {'label': 'NEGATIVE', 'score': 0.9996669292449951}]\n```\n\n为了遍历整个数据集，建议直接使用 `dataset`。这意味着您不需要一次性分配整个数据集，也不需要自己进行批处理。这应该与GPU上的自定义循环一样快。如果不是，请随时提出issue。\n\n```python\nimport datasets\nfrom transformers import pipeline\nfrom transformers.pipelines.pt_utils import KeyDataset\nfrom tqdm.auto import tqdm\n\npipe = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\", device=0)\ndataset = datasets.load_dataset(\"superb\", name=\"asr\", split=\"test\")\n\n# KeyDataset (only *pt*) will simply return the item in the dict returned by the dataset item\n# as we're not interested in the *target* part of the dataset. For sentence pair use KeyPairDataset\nfor out in tqdm(pipe(KeyDataset(dataset, \"file\"))):\n    print(out)\n    # {\"text\": \"NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND\"}\n    # {\"text\": ....}\n    # ....\n```\n\n为了方便使用，也可以使用生成器：\n\n\n```python\nfrom transformers import pipeline\n\npipe = pipeline(\"text-classification\")\n\n\ndef data():\n    while True:\n        # This could come from a dataset, a database, a queue or HTTP request\n        # in a server\n        # Caveat: because this is iterative, you cannot use `num_workers > 1` variable\n        # to use multiple threads to preprocess data. You can still have 1 thread that\n        # does the preprocessing while the main runs the big inference\n        yield \"This is a test\"\n\n\nfor out in pipe(data()):\n    print(out)\n    # {\"text\": \"NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND\"}\n    # {\"text\": ....}\n    # ....\n```\n\n[[autodoc]] pipeline",
    "1135": "一级标题：Pipelines\n二级标题：Pipeline batching\n内容：\n所有pipeline都可以使用批处理。这将在pipeline使用其流处理功能时起作用（即传递列表或 `Dataset` 或 `generator` 时）。\n\n```python\nfrom transformers import pipeline\nfrom transformers.pipelines.pt_utils import KeyDataset\nimport datasets\n\ndataset = datasets.load_dataset(\"imdb\", name=\"plain_text\", split=\"unsupervised\")\npipe = pipeline(\"text-classification\", device=0)\nfor out in pipe(KeyDataset(dataset, \"text\"), batch_size=8, truncation=\"only_first\"):\n    print(out)\n    # [{'label': 'POSITIVE', 'score': 0.9998743534088135}]\n    # Exactly the same output as before, but the content are passed\n    # as batches to the model\n```\n\n<Tip warning={true}>\n\n然而，这并不自动意味着性能提升。它可能是一个10倍的加速或5倍的减速，具体取决于硬件、数据和实际使用的模型。\n\n主要是加速的示例：\n\n</Tip>\n\n```python\nfrom transformers import pipeline\nfrom torch.utils.data import Dataset\nfrom tqdm.auto import tqdm\n\npipe = pipeline(\"text-classification\", device=0)\n\n\nclass MyDataset(Dataset):\n    def __len__(self):\n        return 5000\n\n    def __getitem__(self, i):\n        return \"This is a test\"\n\n\ndataset = MyDataset()\n\nfor batch_size in [1, 8, 64, 256]:\n    print(\"-\" * 30)\n    print(f\"Streaming batch_size={batch_size}\")\n    for out in tqdm(pipe(dataset, batch_size=batch_size), total=len(dataset)):\n        pass\n```\n\n```\n# On GTX 970\n------------------------------\nStreaming no batching\n100%|██████████████████████████████████████████████████████████████████████| 5000/5000 [00:26<00:00, 187.52it/s]\n------------------------------\nStreaming batch_size=8\n100%|█████████████████████████████████████████████████████████████████████| 5000/5000 [00:04<00:00, 1205.95it/s]\n------------------------------\nStreaming batch_size=64\n100%|█████████████████████████████████████████████████████████████████████| 5000/5000 [00:02<00:00, 2478.24it/s]\n------------------------------\nStreaming batch_size=256\n100%|█████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 2554.43it/s]\n(diminishing returns, saturated the GPU)\n```\n\n主要是减速的示例：\n\n```python\nclass MyDataset(Dataset):\n    def __len__(self):\n        return 5000\n\n    def __getitem__(self, i):\n        if i % 64 == 0:\n            n = 100\n        else:\n            n = 1\n        return \"This is a test\" * n\n```\n\n与其他句子相比，这是一个非常长的句子。在这种情况下，**整个**批次将需要400个tokens的长度，因此整个批次将是 [64, 400] 而不是 [64, 4]，从而导致较大的减速。更糟糕的是，在更大的批次上，程序会崩溃。\n\n```\n------------------------------\nStreaming no batching\n100%|█████████████████████████████████████████████████████████████████████| 1000/1000 [00:05<00:00, 183.69it/s]\n------------------------------\nStreaming batch_size=8\n100%|█████████████████████████████████████████████████████████████████████| 1000/1000 [00:03<00:00, 265.74it/s]\n------------------------------\nStreaming batch_size=64\n100%|██████████████████████████████████████████████████████████████████████| 1000/1000 [00:26<00:00, 37.80it/s]\n------------------------------\nStreaming batch_size=256\n  0%|                                                                                 | 0/1000 [00:00<?, ?it/s]\nTraceback (most recent call last):\n  File \"/home/nicolas/src/transformers/test.py\", line 42, in <module>\n    for out in tqdm(pipe(dataset, batch_size=256), total=len(dataset)):\n....\n    q = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, dim_per_head)\nRuntimeError: CUDA out of memory. Tried to allocate 376.00 MiB (GPU 0; 3.95 GiB total capacity; 1.72 GiB already allocated; 354.88 MiB free; 2.46 GiB reserved in total by PyTorch)\n```\n\n对于这个问题，没有好的（通用）解决方案，效果可能因您的用例而异。经验法则如下：\n\n对于用户，一个经验法则是：\n\n- **使用硬件测量负载性能。测量、测量、再测量。真实的数字是唯一的方法。**\n- 如果受到延迟的限制（进行推理的实时产品），不要进行批处理。\n- 如果使用CPU，不要进行批处理。\n- 如果您在GPU上处理的是吞吐量（您希望在大量静态数据上运行模型），则：\n  - 如果对序列长度的大小没有概念（\"自然\"数据），默认情况下不要进行批处理，进行测试并尝试逐渐添加，添加OOM检查以在失败时恢复（如果您不能控制序列长度，它将在某些时候失败）。\n  - 如果您的序列长度非常规律，那么批处理更有可能非常有趣，进行测试并推动它，直到出现OOM。\n  - GPU越大，批处理越有可能变得更有趣\n- 一旦启用批处理，确保能够很好地处理OOM。",
    "1136": "一级标题：Pipelines\n二级标题：Pipeline chunk batching\n内容：\n`zero-shot-classification` 和 `question-answering` 在某种意义上稍微特殊，因为单个输入可能会导致模型的多次前向传递。在正常情况下，这将导致 `batch_size` 参数的问题。\n\n为了规避这个问题，这两个pipeline都有点特殊，它们是 `ChunkPipeline` 而不是常规的 `Pipeline`。简而言之：\n\n\n```python\npreprocessed = pipe.preprocess(inputs)\nmodel_outputs = pipe.forward(preprocessed)\noutputs = pipe.postprocess(model_outputs)\n```\n\n现在变成：\n\n\n```python\nall_model_outputs = []\nfor preprocessed in pipe.preprocess(inputs):\n    model_outputs = pipe.forward(preprocessed)\n    all_model_outputs.append(model_outputs)\noutputs = pipe.postprocess(all_model_outputs)\n```\n\n这对您的代码应该是非常直观的，因为pipeline的使用方式是相同的。\n\n这是一个简化的视图，因为Pipeline可以自动处理批次！这意味着您不必担心您的输入实际上会触发多少次前向传递，您可以独立于输入优化 `batch_size`。前面部分的注意事项仍然适用。",
    "1137": "一级标题：Pipelines\n二级标题：Pipeline自定义\n内容：\n如果您想要重载特定的pipeline。\n\n请随时为您手头的任务创建一个issue，Pipeline的目标是易于使用并支持大多数情况，因此 `transformers` 可能支持您的用例。\n\n如果您想简单地尝试一下，可以：\n\n- 继承您选择的pipeline\n\n```python\nclass MyPipeline(TextClassificationPipeline):\n    def postprocess():\n        # Your code goes here\n        scores = scores * 100\n        # And here\n\n\nmy_pipeline = MyPipeline(model=model, tokenizer=tokenizer, ...)\n# or if you use *pipeline* function, then:\nmy_pipeline = pipeline(model=\"xxxx\", pipeline_class=MyPipeline)\n```\n\n这样就可以让您编写所有想要的自定义代码。",
    "1138": "一级标题：Pipelines\n二级标题：实现一个pipeline\n内容：\n[实现一个新的pipeline](../add_new_pipeline)",
    "1139": "一级标题：Pipelines\n二级标题：音频\n内容：\n可用于音频任务的pipeline包括以下几种。\n\n### AudioClassificationPipeline\n\n[[autodoc]] AudioClassificationPipeline\n    - __call__\n    - all\n\n### AutomaticSpeechRecognitionPipeline\n\n[[autodoc]] AutomaticSpeechRecognitionPipeline\n    - __call__\n    - all\n\n### TextToAudioPipeline\n\n[[autodoc]] TextToAudioPipeline\n    - __call__\n    - all\n\n\n### ZeroShotAudioClassificationPipeline\n\n[[autodoc]] ZeroShotAudioClassificationPipeline\n    - __call__\n    - all",
    "1140": "一级标题：Pipelines\n二级标题：计算机视觉\n内容：\n可用于计算机视觉任务的pipeline包括以下几种。\n\n### DepthEstimationPipeline\n[[autodoc]] DepthEstimationPipeline\n    - __call__\n    - all\n\n### ImageClassificationPipeline\n\n[[autodoc]] ImageClassificationPipeline\n    - __call__\n    - all\n\n### ImageSegmentationPipeline\n\n[[autodoc]] ImageSegmentationPipeline\n    - __call__\n    - all\n\n### ImageToImagePipeline\n\n[[autodoc]] ImageToImagePipeline\n    - __call__\n    - all\n\n### ObjectDetectionPipeline\n\n[[autodoc]] ObjectDetectionPipeline\n    - __call__\n    - all\n\n### VideoClassificationPipeline\n\n[[autodoc]] VideoClassificationPipeline\n    - __call__\n    - all\n\n### ZeroShotImageClassificationPipeline\n\n[[autodoc]] ZeroShotImageClassificationPipeline\n    - __call__\n    - all\n\n### ZeroShotObjectDetectionPipeline\n\n[[autodoc]] ZeroShotObjectDetectionPipeline\n    - __call__\n    - all",
    "1141": "一级标题：Pipelines\n二级标题：自然语言处理\n内容：\n可用于自然语言处理任务的pipeline包括以下几种。\n\n### FillMaskPipeline\n\n[[autodoc]] FillMaskPipeline\n    - __call__\n    - all\n\n### NerPipeline\n\n[[autodoc]] NerPipeline\n\nSee [`TokenClassificationPipeline`] for all details.\n\n### QuestionAnsweringPipeline\n\n[[autodoc]] QuestionAnsweringPipeline\n    - __call__\n    - all\n\n### SummarizationPipeline\n\n[[autodoc]] SummarizationPipeline\n    - __call__\n    - all\n\n### TableQuestionAnsweringPipeline\n\n[[autodoc]] TableQuestionAnsweringPipeline\n    - __call__\n\n### TextClassificationPipeline\n\n[[autodoc]] TextClassificationPipeline\n    - __call__\n    - all\n\n### TextGenerationPipeline\n\n[[autodoc]] TextGenerationPipeline\n    - __call__\n    - all\n\n### Text2TextGenerationPipeline\n\n[[autodoc]] Text2TextGenerationPipeline\n    - __call__\n    - all\n\n### TokenClassificationPipeline\n\n[[autodoc]] TokenClassificationPipeline\n    - __call__\n    - all\n\n### TranslationPipeline\n\n[[autodoc]] TranslationPipeline\n    - __call__\n    - all\n\n### ZeroShotClassificationPipeline\n\n[[autodoc]] ZeroShotClassificationPipeline\n    - __call__\n    - all",
    "1142": "一级标题：Pipelines\n二级标题：多模态\n内容：\n可用于多模态任务的pipeline包括以下几种。\n\n### DocumentQuestionAnsweringPipeline\n\n[[autodoc]] DocumentQuestionAnsweringPipeline\n    - __call__\n    - all\n\n### FeatureExtractionPipeline\n\n[[autodoc]] FeatureExtractionPipeline\n    - __call__\n    - all\n\n### ImageFeatureExtractionPipeline\n\n[[autodoc]] ImageFeatureExtractionPipeline\n    - __call__\n    - all\n\n### ImageToTextPipeline\n\n[[autodoc]] ImageToTextPipeline\n    - __call__\n    - all\n\n### ImageTextToTextPipeline\n\n[[autodoc]] ImageTextToTextPipeline\n    - __call__\n    - all\n\n### MaskGenerationPipeline\n\n[[autodoc]] MaskGenerationPipeline\n    - __call__\n    - all\n\n### VisualQuestionAnsweringPipeline\n\n[[autodoc]] VisualQuestionAnsweringPipeline\n    - __call__\n    - all",
    "1143": "一级标题：Pipelines\n二级标题：Parent class: `Pipeline`\n内容：\n[[autodoc]] Pipeline",
    "1144": "一级标题：Processors\n二级标题：无\n内容：\n在 Transformers 库中，processors可以有两种不同的含义：\n- 为多模态模型，例如[Wav2Vec2](../model_doc/wav2vec2)（语音和文本）或[CLIP](../model_doc/clip)（文本和视觉）预处理输入的对象\n- 在库的旧版本中用于预处理GLUE或SQUAD数据的已弃用对象。",
    "1145": "一级标题：Processors\n二级标题：多模态processors\n内容：\n任何多模态模型都需要一个对象来编码或解码将多个模态（包括文本、视觉和音频）组合在一起的数据。这由称为processors的对象处理，这些processors将两个或多个处理对象组合在一起，例如tokenizers（用于文本模态），image processors（用于视觉）和feature extractors（用于音频）。\n\n这些processors继承自以下实现保存和加载功能的基类：\n\n\n[[autodoc]] ProcessorMixin",
    "1146": "一级标题：Processors\n二级标题：已弃用的processors\n内容：\n所有processor都遵循与 [`~data.processors.utils.DataProcessor`] 相同的架构。processor返回一个 [`~data.processors.utils.InputExample`] 列表。这些 [`~data.processors.utils.InputExample`] 可以转换为 [`~data.processors.utils.InputFeatures`] 以供输送到模型。\n\n[[autodoc]] data.processors.utils.DataProcessor\n\n[[autodoc]] data.processors.utils.InputExample\n\n[[autodoc]] data.processors.utils.InputFeatures",
    "1147": "一级标题：Processors\n二级标题：GLUE\n内容：\n[General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) 是一个基准测试，评估模型在各种现有的自然语言理解任务上的性能。它与论文 [GLUE: A multi-task benchmark and analysis platform for natural language understanding](https://openreview.net/pdf?id=rJ4km2R5t7) 一同发布。\n\n该库为以下任务提供了总共10个processor：MRPC、MNLI、MNLI（mismatched）、CoLA、SST2、STSB、QQP、QNLI、RTE 和 WNLI。\n\n这些processor是：\n\n- [`~data.processors.utils.MrpcProcessor`]\n- [`~data.processors.utils.MnliProcessor`]\n- [`~data.processors.utils.MnliMismatchedProcessor`]\n- [`~data.processors.utils.Sst2Processor`]\n- [`~data.processors.utils.StsbProcessor`]\n- [`~data.processors.utils.QqpProcessor`]\n- [`~data.processors.utils.QnliProcessor`]\n- [`~data.processors.utils.RteProcessor`]\n- [`~data.processors.utils.WnliProcessor`]\n\n此外，还可以使用以下方法从数据文件加载值并将其转换为 [`~data.processors.utils.InputExample`] 列表。\n\n[[autodoc]] data.processors.glue.glue_convert_examples_to_features",
    "1148": "一级标题：Processors\n二级标题：XNLI\n内容：\n[跨语言NLI语料库（XNLI）](https://www.nyu.edu/projects/bowman/xnli/) 是一个评估跨语言文本表示质量的基准测试。XNLI是一个基于[*MultiNLI*](http://www.nyu.edu/projects/bowman/multinli/)的众包数据集：”文本对“被标记为包含15种不同语言（包括英语等高资源语言和斯瓦希里语等低资源语言）的文本蕴涵注释。\n\n它与论文 [XNLI: Evaluating Cross-lingual Sentence Representations](https://huggingface.co/papers/1809.05053) 一同发布。\n\n该库提供了加载XNLI数据的processor：\n\n- [`~data.processors.utils.XnliProcessor`]\n\n请注意，由于测试集上有“gold”标签，因此评估是在测试集上进行的。\n\n使用这些processor的示例在 [run_xnli.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification/run_xnli.py) 脚本中提供。",
    "1149": "一级标题：Processors\n二级标题：SQuAD\n内容：\n[斯坦福问答数据集（SQuAD）](https://rajpurkar.github.io/SQuAD-explorer//) 是一个评估模型在问答上性能的基准测试。有两个版本，v1.1 和 v2.0。第一个版本（v1.1）与论文 [SQuAD: 100,000+ Questions for Machine Comprehension of Text](https://huggingface.co/papers/1606.05250) 一同发布。第二个版本（v2.0）与论文 [Know What You Don't Know: Unanswerable Questions for SQuAD](https://huggingface.co/papers/1806.03822) 一同发布。\n\n该库为两个版本各自提供了一个processor：\n\n### Processors\n\n这两个processor是：\n\n- [`~data.processors.utils.SquadV1Processor`]\n- [`~data.processors.utils.SquadV2Processor`]\n\n它们都继承自抽象类 [`~data.processors.utils.SquadProcessor`]。\n\n[[autodoc]] data.processors.squad.SquadProcessor\n    - all\n\n此外，可以使用以下方法将 SQuAD 示例转换为可用作模型输入的 [`~data.processors.utils.SquadFeatures`]。\n\n[[autodoc]] data.processors.squad.squad_convert_examples_to_features\n\n\n这些processor以及前面提到的方法可以与包含数据的文件以及tensorflow_datasets包一起使用。下面给出了示例。\n\n\n### Example使用\n\n以下是使用processor以及使用数据文件的转换方法的示例：\n\n```python\n# Loading a V2 processor\nprocessor = SquadV2Processor()\nexamples = processor.get_dev_examples(squad_v2_data_dir)\n\n# Loading a V1 processor\nprocessor = SquadV1Processor()\nexamples = processor.get_dev_examples(squad_v1_data_dir)\n\nfeatures = squad_convert_examples_to_features(\n    examples=examples,\n    tokenizer=tokenizer,\n    max_seq_length=max_seq_length,\n    doc_stride=args.doc_stride,\n    max_query_length=max_query_length,\n    is_training=not evaluate,\n)\n```\n\n使用 *tensorflow_datasets* 就像使用数据文件一样简单：\n\n```python\n# tensorflow_datasets only handle Squad V1.\ntfds_examples = tfds.load(\"squad\")\nexamples = SquadV1Processor().get_examples_from_dataset(tfds_examples, evaluate=evaluate)\n\nfeatures = squad_convert_examples_to_features(\n    examples=examples,\n    tokenizer=tokenizer,\n    max_seq_length=max_seq_length,\n    doc_stride=args.doc_stride,\n    max_query_length=max_query_length,\n    is_training=not evaluate,\n)\n```\n\n另一个使用这些processor的示例在 [run_squad.py](https://github.com/huggingface/transformers/tree/main/examples/legacy/question-answering/run_squad.py) 脚本中提供。",
    "1150": "一级标题：量化 🤗 Transformers 模型\n二级标题：无\n内容：",
    "1151": "一级标题：量化 🤗 Transformers 模型\n二级标题：AWQ集成\n内容：\nAWQ方法已经在[*AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration*论文](https://huggingface.co/papers/2306.00978)中引入。通过AWQ，您可以以4位精度运行模型，同时保留其原始性能（即没有性能降级），并具有比下面介绍的其他量化方法更出色的吞吐量 - 达到与纯`float16`推理相似的吞吐量。\n\n我们现在支持使用任何AWQ模型进行推理，这意味着任何人都可以加载和使用在Hub上推送或本地保存的AWQ权重。请注意，使用AWQ需要访问NVIDIA GPU。目前不支持CPU推理。\n\n\n### 量化一个模型\n\n我们建议用户查看生态系统中不同的现有工具，以使用AWQ算法对其模型进行量化，例如：\n\n- [`llm-awq`](https://github.com/mit-han-lab/llm-awq)，来自MIT Han Lab\n- [`autoawq`](https://github.com/casper-hansen/AutoAWQ)，来自[`casper-hansen`](https://github.com/casper-hansen)\n- Intel neural compressor，来自Intel - 通过[`optimum-intel`](https://huggingface.co/docs/optimum/main/en/intel/optimization_inc)使用\n\n生态系统中可能存在许多其他工具，请随时提出PR将它们添加到列表中。\n目前与🤗 Transformers的集成仅适用于使用`autoawq`和`llm-awq`量化后的模型。大多数使用`auto-awq`量化的模型可以在🤗 Hub的[`TheBloke`](https://huggingface.co/TheBloke)命名空间下找到，要使用`llm-awq`对模型进行量化，请参阅[`llm-awq`](https://github.com/mit-han-lab/llm-awq/)的示例文件夹中的[`convert_to_hf.py`](https://github.com/mit-han-lab/llm-awq/blob/main/examples/convert_to_hf.py)脚本。\n\n\n### 加载一个量化的模型\n\n您可以使用`from_pretrained`方法从Hub加载一个量化模型。通过检查模型配置文件（`configuration.json`）中是否存在`quantization_config`属性，来进行确认推送的权重是量化的。您可以通过检查字段`quantization_config.quant_method`来确认模型是否以AWQ格式进行量化，该字段应该设置为`\"awq\"`。请注意，为了性能原因，默认情况下加载模型将设置其他权重为`float16`。如果您想更改这种设置，可以通过将`torch_dtype`参数设置为`torch.float32`或`torch.bfloat16`。在下面的部分中，您可以找到一些示例片段和notebook。",
    "1152": "一级标题：量化 🤗 Transformers 模型\n二级标题：示例使用\n内容：\n首先，您需要安装[`autoawq`](https://github.com/casper-hansen/AutoAWQ)库\n\n```bash\npip install autoawq\n```\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"TheBloke/zephyr-7B-alpha-AWQ\"\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda:0\")\n```\n\n如果您首先将模型加载到CPU上，请确保在使用之前将其移动到GPU设备上。\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"TheBloke/zephyr-7B-alpha-AWQ\"\nmodel = AutoModelForCausalLM.from_pretrained(model_id).to(\"cuda:0\")\n```\n\n### 结合 AWQ 和 Flash Attention\n\n您可以将AWQ量化与Flash Attention结合起来，得到一个既被量化又更快速的模型。只需使用`from_pretrained`加载模型，并传递`attn_implementation=\"flash_attention_2\"`参数。\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"TheBloke/zephyr-7B-alpha-AWQ\", attn_implementation=\"flash_attention_2\", device_map=\"cuda:0\")\n```\n\n### 基准测试\n\n我们使用[`optimum-benchmark`](https://github.com/huggingface/optimum-benchmark)库进行了一些速度、吞吐量和延迟基准测试。\n\n请注意，在编写本文档部分时，可用的量化方法包括：`awq`、`gptq`和`bitsandbytes`。\n\n基准测试在一台NVIDIA-A100实例上运行，使用[`TheBloke/Mistral-7B-v0.1-AWQ`](https://huggingface.co/TheBloke/Mistral-7B-v0.1-AWQ)作为AWQ模型，[`TheBloke/Mistral-7B-v0.1-GPTQ`](https://huggingface.co/TheBloke/Mistral-7B-v0.1-GPTQ)作为GPTQ模型。我们还将其与`bitsandbytes`量化模型和`float16`模型进行了对比。以下是一些结果示例：\n\n\n<div style=\"text-align: center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/forward_memory_plot.png\">\n</div>\n\n<div style=\"text-align: center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/generate_memory_plot.png\">\n</div>\n\n<div style=\"text-align: center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/generate_throughput_plot.png\">\n</div>\n\n<div style=\"text-align: center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/forward_latency_plot.png\">\n</div>\n\n你可以在[此链接](https://github.com/huggingface/optimum-benchmark/tree/main/examples/running-mistrals)中找到完整的结果以及包版本。\n\n从结果来看，AWQ量化方法是推理、文本生成中最快的量化方法，并且在文本生成的峰值内存方面属于最低。然而，对于每批数据，AWQ似乎有最大的前向延迟。\n\n\n### Google colab 演示\n\n查看如何在[Google Colab演示](https://colab.research.google.com/drive/1HzZH89yAXJaZgwJDhQj9LqSBux932BvY)中使用此集成！\n\n\n### AwqConfig\n\n[[autodoc]] AwqConfig",
    "1153": "一级标题：量化 🤗 Transformers 模型\n二级标题：`AutoGPTQ` 集成\n内容：\n🤗 Transformers已经整合了`optimum` API，用于对语言模型执行GPTQ量化。您可以以8、4、3甚至2位加载和量化您的模型，而性能无明显下降，并且推理速度更快！这受到大多数GPU硬件的支持。\n\n要了解更多关于量化模型的信息，请查看：\n- [GPTQ](https://huggingface.co/papers/2210.17323)论文\n- `optimum`关于GPTQ量化的[指南](https://huggingface.co/docs/optimum/llm_quantization/usage_guides/quantization)\n- 用作后端的[`AutoGPTQ`](https://github.com/PanQiWei/AutoGPTQ)库\n\n\n### 要求\n\n为了运行下面的代码，您需要安装：\n\n- 安装最新版本的 `AutoGPTQ` 库\n`pip install auto-gptq`\n\n- 从源代码安装最新版本的`optimum`\n`pip install git+https://github.com/huggingface/optimum.git`\n\n- 从源代码安装最新版本的`transformers`\n`pip install git+https://github.com/huggingface/transformers.git`\n\n- 安装最新版本的`accelerate`库：\n`pip install --upgrade accelerate`\n\n请注意，目前GPTQ集成仅支持文本模型，对于视觉、语音或多模态模型可能会遇到预期以外结果。\n\n### 加载和量化模型\n\nGPTQ是一种在使用量化模型之前需要进行权重校准的量化方法。如果您想从头开始对transformers模型进行量化，生成量化模型可能需要一些时间（在Google Colab上对`facebook/opt-350m`模型量化约为5分钟）。\n\n因此，有两种不同的情况下您可能想使用GPTQ量化模型。第一种情况是加载已经由其他用户在Hub上量化的模型，第二种情况是从头开始对您的模型进行量化并保存或推送到Hub，以便其他用户也可以使用它。\n\n\n#### GPTQ 配置\n\n为了加载和量化一个模型，您需要创建一个[`GPTQConfig`]。您需要传递`bits`的数量，一个用于校准量化的`dataset`，以及模型的`tokenizer`以准备数据集。\n\n```python\nmodel_id = \"facebook/opt-125m\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ngptq_config = GPTQConfig(bits=4, dataset = \"c4\", tokenizer=tokenizer)\n```\n\n请注意，您可以将自己的数据集以字符串列表形式传递到模型。然而，强烈建议您使用GPTQ论文中提供的数据集。\n\n\n```python\ndataset = [\"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"]\nquantization = GPTQConfig(bits=4, dataset = dataset, tokenizer=tokenizer)\n```\n\n#### 量化\n\n您可以通过使用`from_pretrained`并设置`quantization_config`来对模型进行量化。\n\n```python\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=gptq_config)\n\n```\n\n请注意，您需要一个GPU来量化模型。我们将模型放在cpu中，并将模块来回移动到gpu中，以便对其进行量化。\n\n如果您想在使用 CPU 卸载的同时最大化 GPU 使用率，您可以设置 `device_map = \"auto\"`。\n\n\n```python\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", quantization_config=gptq_config)\n```\n\n请注意，不支持磁盘卸载。此外，如果由于数据集而内存不足，您可能需要在`from_pretrained`中设置`max_memory`。查看这个[指南](https://huggingface.co/docs/accelerate/usage_guides/big_modeling#designing-a-device-map)以了解有关`device_map`和`max_memory`的更多信息。\n\n\n<Tip warning={true}>\n目前，GPTQ量化仅适用于文本模型。此外，量化过程可能会花费很多时间，具体取决于硬件性能（175B模型在NVIDIA A100上需要4小时）。请在Hub上检查是否有模型的GPTQ量化版本。如果没有，您可以在GitHub上提交需求。\n</Tip>\n\n### 推送量化模型到 🤗 Hub\n\n您可以使用`push_to_hub`将量化模型像任何模型一样推送到Hub。量化配置将与模型一起保存和推送。\n\n```python\nquantized_model.push_to_hub(\"opt-125m-gptq\")\ntokenizer.push_to_hub(\"opt-125m-gptq\")\n```\n\n如果您想在本地计算机上保存量化模型，您也可以使用`save_pretrained`来完成：\n\n```python\nquantized_model.save_pretrained(\"opt-125m-gptq\")\ntokenizer.save_pretrained(\"opt-125m-gptq\")\n```\n\n请注意，如果您量化模型时想使用`device_map`，请确保在保存之前将整个模型移动到您的GPU或CPU之一。\n\n```python\nquantized_model.to(\"cpu\")\nquantized_model.save_pretrained(\"opt-125m-gptq\")\n```\n\n### 从 🤗 Hub 加载一个量化模型\n\n您可以使用`from_pretrained`从Hub加载量化模型。\n请确保推送权重是量化的，检查模型配置对象中是否存在`quantization_config`属性。\n\n\n```python\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\"{your_username}/opt-125m-gptq\")\n```\n\n如果您想更快地加载模型，并且不需要分配比实际需要内存更多的内存，量化模型也使用`device_map`参数。确保您已安装`accelerate`库。\n\n```python\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\"{your_username}/opt-125m-gptq\", device_map=\"auto\")\n```\n\n### Exllama内核加快推理速度\n\n保留格式：对于 4 位模型，您可以使用 exllama 内核来提高推理速度。默认情况下，它处于启用状态。您可以通过在 [`GPTQConfig`] 中传递 `use_exllama` 来更改此配置。这将覆盖存储在配置中的量化配置。请注意，您只能覆盖与内核相关的属性。此外，如果您想使用 exllama 内核，整个模型需要全部部署在 gpus 上。此外，您可以使用 版本 > 0.4.2 的 Auto-GPTQ 并传递 `device_map` = \"cpu\" 来执行 CPU 推理。对于 CPU 推理，您必须在 `GPTQConfig` 中传递 `use_exllama = False`。\n\n```py\nimport torch\ngptq_config = GPTQConfig(bits=4)\nmodel = AutoModelForCausalLM.from_pretrained(\"{your_username}/opt-125m-gptq\", device_map=\"auto\", quantization_config=gptq_config)\n```\n\n随着 exllamav2 内核的发布，与 exllama 内核相比，您可以获得更快的推理速度。您只需在 [`GPTQConfig`] 中传递 `exllama_config={\"version\": 2}`：\n\n```py\nimport torch\ngptq_config = GPTQConfig(bits=4, exllama_config={\"version\":2})\nmodel = AutoModelForCausalLM.from_pretrained(\"{your_username}/opt-125m-gptq\", device_map=\"auto\", quantization_config = gptq_config)\n```\n\n请注意，目前仅支持 4 位模型。此外，如果您正在使用 peft 对量化模型进行微调，建议禁用 exllama 内核。\n\n您可以在此找到这些内核的基准测试 [这里](https://github.com/huggingface/optimum/tree/main/tests/benchmark#gptq-benchmark)\n\n\n#### 微调一个量化模型\n\n在Hugging Face生态系统的官方支持下，您可以使用GPTQ进行量化后的模型进行微调。\n请查看`peft`库了解更多详情。\n\n### 示例演示\n\n请查看 Google Colab [notebook](https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94ilkUFu6ZX4ceb?usp=sharing)，了解如何使用GPTQ量化您的模型以及如何使用peft微调量化模型。\n\n### GPTQConfig\n\n[[autodoc]] GPTQConfig",
    "1154": "一级标题：量化 🤗 Transformers 模型\n二级标题：`bitsandbytes` 集成\n内容：\n🤗 Transformers 与 `bitsandbytes` 上最常用的模块紧密集成。您可以使用几行代码以 8 位精度加载您的模型。\n自bitsandbytes的0.37.0版本发布以来，大多数GPU硬件都支持这一点。\n\n在[LLM.int8()](https://huggingface.co/papers/2208.07339)论文中了解更多关于量化方法的信息，或者在[博客文章](https://huggingface.co/blog/hf-bitsandbytes-integration)中了解关于合作的更多信息。\n\n自其“0.39.0”版本发布以来，您可以使用FP4数据类型，通过4位量化加载任何支持“device_map”的模型。\n\n如果您想量化自己的 pytorch 模型，请查看 🤗 Accelerate 的[文档](https://huggingface.co/docs/accelerate/main/en/usage_guides/quantization)。\n\n以下是您可以使用“bitsandbytes”集成完成的事情\n\n### 通用用法\n\n只要您的模型支持使用 🤗 Accelerate 进行加载并包含 `torch.nn.Linear` 层，您可以在调用 [`~PreTrainedModel.from_pretrained`] 方法时使用 `load_in_8bit` 或 `load_in_4bit` 参数来量化模型。这也应该适用于任何模态。\n\n```python\nfrom transformers import AutoModelForCausalLM\n\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", load_in_8bit=True)\nmodel_4bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", load_in_4bit=True)\n```\n\n默认情况下，所有其他模块（例如 `torch.nn.LayerNorm`）将被转换为 `torch.float16` 类型。但如果您想更改它们的 `dtype`，可以重载 `torch_dtype` 参数：\n\n```python\n>>> import torch\n>>> from transformers import AutoModelForCausalLM\n\n>>> model_8bit = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", load_in_8bit=True, torch_dtype=torch.float32)\n>>> model_8bit.model.decoder.layers[-1].final_layer_norm.weight.dtype\ntorch.float32\n```\n\n\n### FP4 量化\n\n#### 要求\n\n确保在运行以下代码段之前已完成以下要求：\n\n- 最新版本 `bitsandbytes` 库\n`pip install bitsandbytes>=0.39.0`\n\n- 安装最新版本 `accelerate`\n`pip install --upgrade accelerate`\n\n- 安装最新版本 `transformers`\n`pip install --upgrade transformers`\n\n#### 提示和最佳实践\n\n\n- **高级用法：** 请参考 [此 Google Colab notebook](https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf) 以获取 4 位量化高级用法和所有可选选项。\n\n- **使用 `batch_size=1` 实现更快的推理：** 自 `bitsandbytes` 的 `0.40.0` 版本以来，设置 `batch_size=1`，您可以从快速推理中受益。请查看 [这些发布说明](https://github.com/TimDettmers/bitsandbytes/releases/tag/0.40.0) ，并确保使用大于 `0.40.0` 的版本以直接利用此功能。\n\n- **训练：** 根据 [QLoRA 论文](https://huggingface.co/papers/2305.14314)，对于4位基模型训练（使用 LoRA 适配器），应使用 `bnb_4bit_quant_type='nf4'`。\n\n- **推理：** 对于推理，`bnb_4bit_quant_type` 对性能影响不大。但是为了与模型的权重保持一致，请确保使用相同的 `bnb_4bit_compute_dtype` 和 `torch_dtype` 参数。\n\n#### 加载 4 位量化的大模型\n\n在调用 `.from_pretrained` 方法时使用 `load_in_4bit=True`，可以将您的内存使用量减少到大约原来的 1/4。\n\n```python\n# pip install transformers accelerate bitsandbytes\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"bigscience/bloom-1b7\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_4bit=True)\n```\n\n<Tip warning={true}>\n\n需要注意的是，一旦模型以 4 位量化方式加载，就无法将量化后的权重推送到 Hub 上。此外，您不能训练 4 位量化权重，因为目前尚不支持此功能。但是，您可以使用 4 位量化模型来训练额外参数，这将在下一部分中介绍。\n\n</Tip>\n\n### 加载 8 位量化的大模型\n\n您可以通过在调用 `.from_pretrained` 方法时使用 `load_in_8bit=True` 参数，将内存需求大致减半来加载模型\n\n\n```python\n# pip install transformers accelerate bitsandbytes\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nmodel_id = \"bigscience/bloom-1b7\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True))\n```\n\n然后，像通常使用 `PreTrainedModel` 一样使用您的模型。\n\n您可以使用 `get_memory_footprint` 方法检查模型的内存占用。\n\n\n```python\nprint(model.get_memory_footprint())\n```\n\n通过这种集成，我们能够在较小的设备上加载大模型并运行它们而没有任何问题。\n\n<Tip warning={true}>\n\n需要注意的是，一旦模型以 8 位量化方式加载，除了使用最新的 `transformers` 和 `bitsandbytes` 之外，目前尚无法将量化后的权重推送到 Hub 上。此外，您不能训练 8 位量化权重，因为目前尚不支持此功能。但是，您可以使用 8 位量化模型来训练额外参数，这将在下一部分中介绍。\n\n注意，`device_map` 是可选的，但设置 `device_map = 'auto'` 更适合用于推理，因为它将更有效地调度可用资源上的模型。\n\n\n</Tip>\n\n#### 高级用例\n\n在这里，我们将介绍使用 FP4 量化的一些高级用例。\n\n##### 更改计算数据类型\n\n计算数据类型用于改变在进行计算时使用的数据类型。例如，hidden states可以是 `float32`，但为了加速，计算时可以被设置为 `bf16`。默认情况下，计算数据类型被设置为 `float32`。\n\n\n```python\nimport torch\nfrom transformers import BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n```\n\n#### 使用 NF4（普通浮点数 4）数据类型\n\n您还可以使用 NF4 数据类型，这是一种针对使用正态分布初始化的权重而适应的新型 4 位数据类型。要运行：\n\n```python\nfrom transformers import BitsAndBytesConfig\n\nnf4_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n)\n\nmodel_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)\n```\n\n#### 使用嵌套量化进行更高效的内存推理\n\n我们还建议用户使用嵌套量化技术。从我们的经验观察来看，这种方法在不增加额外性能的情况下节省更多内存。这使得 llama-13b 模型能够在具有 1024 个序列长度、1 个批次大小和 4 个梯度累积步骤的 NVIDIA-T4 16GB 上进行 fine-tuning。\n\n```python\nfrom transformers import BitsAndBytesConfig\n\ndouble_quant_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel_double_quant = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=double_quant_config)\n```\n\n### 将量化模型推送到🤗 Hub\n\n您可以使用 `push_to_hub` 方法将量化模型推送到 Hub 上。这将首先推送量化配置文件，然后推送量化模型权重。\n请确保使用 `bitsandbytes>0.37.2`（在撰写本文时，我们使用的是 `bitsandbytes==0.38.0.post1`）才能使用此功能。\n\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nmodel = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-560m\", quantization_config=BitsAndBytesConfig(load_in_8bit=True))\ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n\nmodel.push_to_hub(\"bloom-560m-8bit\")\n```\n\n<Tip warning={true}>\n\n对大模型，强烈鼓励将 8 位量化模型推送到 Hub 上，以便让社区能够从内存占用减少和加载中受益，例如在 Google Colab 上加载大模型。\n\n</Tip>\n\n### 从🤗 Hub加载量化模型\n\n您可以使用 `from_pretrained` 方法从 Hub 加载量化模型。请确保推送的权重是量化的，检查模型配置对象中是否存在 `quantization_config` 属性。\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"{your_username}/bloom-560m-8bit\", device_map=\"auto\")\n```\n\n请注意，在这种情况下，您不需要指定 `load_in_8bit=True` 参数，但需要确保 `bitsandbytes` 和 `accelerate` 已安装。\n情注意，`device_map` 是可选的，但设置 `device_map = 'auto'` 更适合用于推理，因为它将更有效地调度可用资源上的模型。\n\n### 高级用例\n\n本节面向希望探索除了加载和运行 8 位模型之外还能做什么的进阶用户。\n\n#### 在 `cpu` 和 `gpu` 之间卸载\n\n此高级用例之一是能够加载模型并将权重分派到 `CPU` 和 `GPU` 之间。请注意，将在 CPU 上分派的权重 **不会** 转换为 8 位，因此会保留为 `float32`。此功能适用于想要适应非常大的模型并将模型分派到 GPU 和 CPU 之间的用户。\n\n首先，从 `transformers` 中加载一个 [`BitsAndBytesConfig`]，并将属性 `llm_int8_enable_fp32_cpu_offload` 设置为 `True`：\n\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nquantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n```\n\n假设您想加载 `bigscience/bloom-1b7` 模型，您的 GPU显存仅足够容纳除了`lm_head`外的整个模型。因此，您可以按照以下方式编写自定义的 device_map：\n\n```python\ndevice_map = {\n    \"transformer.word_embeddings\": 0,\n    \"transformer.word_embeddings_layernorm\": 0,\n    \"lm_head\": \"cpu\",\n    \"transformer.h\": 0,\n    \"transformer.ln_f\": 0,\n}\n```\n\n然后如下加载模型：\n\n```python\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\n    \"bigscience/bloom-1b7\",\n    device_map=device_map,\n    quantization_config=quantization_config,\n)\n```\n\n这就是全部内容！享受您的模型吧！\n\n#### 使用`llm_int8_threshold`\n\n您可以使用 `llm_int8_threshold` 参数来更改异常值的阈值。“异常值”是一个大于特定阈值的`hidden state`值。\n这对应于`LLM.int8()`论文中描述的异常检测的异常阈值。任何高于此阈值的`hidden state`值都将被视为异常值，对这些值的操作将在 fp16 中完成。值通常是正态分布的，也就是说，大多数值在 [-3.5, 3.5] 范围内，但有一些额外的系统异常值，对于大模型来说，它们的分布非常不同。这些异常值通常在区间 [-60, -6] 或 [6, 60] 内。Int8 量化对于幅度为 ~5 的值效果很好，但超出这个范围，性能就会明显下降。一个好的默认阈值是 6，但对于更不稳定的模型（小模型、微调）可能需要更低的阈值。\n这个参数会影响模型的推理速度。我们建议尝试这个参数，以找到最适合您的用例的参数。\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nmodel_id = \"bigscience/bloom-1b7\"\n\nquantization_config = BitsAndBytesConfig(\n    llm_int8_threshold=10,\n)\n\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=device_map,\n    quantization_config=quantization_config,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n```\n\n#### 跳过某些模块的转换\n\n一些模型有几个需要保持未转换状态以确保稳定性的模块。例如，Jukebox 模型有几个 `lm_head` 模块需要跳过。使用 `llm_int8_skip_modules` 参数进行相应操作。\n\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nmodel_id = \"bigscience/bloom-1b7\"\n\nquantization_config = BitsAndBytesConfig(\n    llm_int8_skip_modules=[\"lm_head\"],\n)\n\nmodel_8bit = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=device_map,\n    quantization_config=quantization_config,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n```\n\n#### 微调已加载为8位精度的模型\n\n借助Hugging Face生态系统中适配器（adapters）的官方支持，您可以在8位精度下微调模型。这使得可以在单个Google Colab中微调大模型，例如`flan-t5-large`或`facebook/opt-6.7b`。请查看[`peft`](https://github.com/huggingface/peft)库了解更多详情。\n\n注意，加载模型进行训练时无需传递`device_map`。它将自动将您的模型加载到GPU上。如果需要，您可以将设备映射为特定设备（例如`cuda:0`、`0`、`torch.device('cuda:0')`）。请注意，`device_map=auto`仅应用于推理。\n\n\n### BitsAndBytesConfig\n\n[[autodoc]] BitsAndBytesConfig",
    "1155": "一级标题：量化 🤗 Transformers 模型\n二级标题：使用 🤗 `optimum` 进行量化\n内容：\n请查看[Optimum 文档](https://huggingface.co/docs/optimum/index)以了解更多关于`optimum`支持的量化方法，并查看这些方法是否适用于您的用例。",
    "1156": "一级标题：Generation\n二级标题：无\n内容：\n每个框架都在它们各自的 `GenerationMixin` 类中实现了文本生成的 `generate` 方法：\n\n- PyTorch [`~generation.GenerationMixin.generate`] 在 [`~generation.GenerationMixin`] 中实现。\n- TensorFlow [`~generation.TFGenerationMixin.generate`] 在 [`~generation.TFGenerationMixin`] 中实现。\n- Flax/JAX [`~generation.FlaxGenerationMixin.generate`] 在 [`~generation.FlaxGenerationMixin`] 中实现。\n\n无论您选择哪个框架，都可以使用 [`~generation.GenerationConfig`] 类实例对 generate 方法进行参数化。有关生成方法的控制参数的完整列表，请参阅此类。\n\n要了解如何检查模型的生成配置、默认值是什么、如何临时更改参数以及如何创建和保存自定义生成配置，请参阅 [文本生成策略指南](../generation_strategies)。该指南还解释了如何使用相关功能，如token流。",
    "1157": "一级标题：Generation\n二级标题：GenerationConfig\n内容：\n[[autodoc]] generation.GenerationConfig\n\t- from_pretrained\n\t- from_model_config\n\t- save_pretrained",
    "1158": "一级标题：Generation\n二级标题：GenerationMixin\n内容：\n[[autodoc]] generation.GenerationMixin\n\t- generate\n\t- compute_transition_scores",
    "1159": "一级标题：Generation\n二级标题：TFGenerationMixin\n内容：\n[[autodoc]] generation.TFGenerationMixin\n\t- generate\n\t- compute_transition_scores",
    "1160": "一级标题：Generation\n二级标题：FlaxGenerationMixin\n内容：\n[[autodoc]] generation.FlaxGenerationMixin\n\t- generate",
    "1161": "一级标题：Tokenizer\n二级标题：无\n内容：\ntokenizer负责准备输入以供模型使用。该库包含所有模型的tokenizer。大多数tokenizer都有两种版本：一个是完全的 Python 实现，另一个是基于 Rust 库 [🤗 Tokenizers](https://github.com/huggingface/tokenizers) 的“Fast”实现。\"Fast\" 实现允许：\n\n1. 在批量分词时显著提速\n2. 在原始字符串（字符和单词）和token空间之间进行映射的其他方法（例如，获取包含给定字符的token的索引或与给定token对应的字符范围）。\n\n基类 [PreTrainedTokenizer] 和 [PreTrained TokenizerFast] 实现了在模型输入中编码字符串输入的常用方法（见下文），并从本地文件或目录或从库提供的预训练的 tokenizer（从 HuggingFace 的 AWS S3 存储库下载）实例化/保存 python 和“Fast” tokenizer。它们都依赖于包含常用方法的 [`~tokenization_utils_base.PreTrainedTokenizerBase`]和[`~tokenization_utils_base.SpecialTokensMixin`]。\n\n因此，[`PreTrainedTokenizer`] 和 [`PreTrainedTokenizerFast`] 实现了使用所有tokenizers的主要方法：\n\n- 分词（将字符串拆分为子词标记字符串），将tokens字符串转换为id并转换回来，以及编码/解码（即标记化并转换为整数）。\n- 以独立于底层结构（BPE、SentencePiece……）的方式向词汇表中添加新tokens。\n- 管理特殊tokens（如mask、句首等）：添加它们，将它们分配给tokenizer中的属性以便于访问，并确保它们在标记过程中不会被分割。\n\n[`BatchEncoding`] 包含 [`~tokenization_utils_base.PreTrainedTokenizerBase`] 的编码方法（`__call__`、`encode_plus` 和 `batch_encode_plus`）的输出，并且是从 Python 字典派生的。当tokenizer是纯 Python tokenizer时，此类的行为就像标准的 Python 字典一样，并保存这些方法计算的各种模型输入（`input_ids`、`attention_mask` 等）。当分词器是“Fast”分词器时（即由 HuggingFace 的 [tokenizers 库](https://github.com/huggingface/tokenizers) 支持），此类还提供了几种高级对齐方法，可用于在原始字符串（字符和单词）与token空间之间进行映射（例如，获取包含给定字符的token的索引或与给定token对应的字符范围）。",
    "1162": "一级标题：Tokenizer\n二级标题：PreTrainedTokenizer\n内容：\n[[autodoc]] PreTrainedTokenizer\n    - __call__\n    - add_tokens\n    - add_special_tokens\n    - apply_chat_template\n    - batch_decode\n    - decode\n    - encode\n    - push_to_hub\n    - all",
    "1163": "一级标题：Tokenizer\n二级标题：PreTrainedTokenizerFast\n内容：\n[`PreTrainedTokenizerFast`] 依赖于 [tokenizers](https://huggingface.co/docs/tokenizers) 库。可以非常简单地将从 🤗 tokenizers 库获取的tokenizers加载到 🤗 transformers 中。查看 [使用 🤗 tokenizers 的分词器](../fast_tokenizers) 页面以了解如何执行此操作。\n\n[[autodoc]] PreTrainedTokenizerFast\n    - __call__\n    - add_tokens\n    - add_special_tokens\n    - apply_chat_template\n    - batch_decode\n    - decode\n    - encode\n    - push_to_hub\n    - all",
    "1164": "一级标题：Tokenizer\n二级标题：BatchEncoding\n内容：\n[[autodoc]] BatchEncoding",
    "1165": "一级标题：Trainer\n二级标题：无\n内容：\n[`Trainer`] 类提供了一个 PyTorch 的 API，用于处理大多数标准用例的全功能训练。它在大多数[示例脚本](https://github.com/huggingface/transformers/tree/main/examples)中被使用。\n\n<Tip>\n\n如果你想要使用自回归技术在文本数据集上微调像 Llama-2 或 Mistral 这样的语言模型，考虑使用 [`trl`](https://github.com/huggingface/trl) 的 [`~trl.SFTTrainer`]。[`~trl.SFTTrainer`] 封装了 [`Trainer`]，专门针对这个特定任务进行了优化，并支持序列打包、LoRA、量化和 DeepSpeed，以有效扩展到任何模型大小。另一方面，[`Trainer`] 是一个更通用的选项，适用于更广泛的任务。\n\n</Tip>\n\n在实例化你的 [`Trainer`] 之前，创建一个 [`TrainingArguments`]，以便在训练期间访问所有定制点。\n\n这个 API 支持在多个 GPU/TPU 上进行分布式训练，支持 [NVIDIA Apex](https://github.com/NVIDIA/apex) 的混合精度和 PyTorch 的原生 AMP。\n\n[`Trainer`] 包含基本的训练循环，支持上述功能。如果需要自定义训练，你可以继承 `Trainer` 并覆盖以下方法：\n\n- **get_train_dataloader** -- 创建训练 DataLoader。\n- **get_eval_dataloader** -- 创建评估 DataLoader。\n- **get_test_dataloader** -- 创建测试 DataLoader。\n- **log** -- 记录观察训练的各种对象的信息。\n- **create_optimizer_and_scheduler** -- 如果它们没有在初始化时传递，请设置优化器和学习率调度器。请注意，你还可以单独继承或覆盖 `create_optimizer` 和 `create_scheduler` 方法。\n- **create_optimizer** -- 如果在初始化时没有传递，则设置优化器。\n- **create_scheduler** -- 如果在初始化时没有传递，则设置学习率调度器。\n- **compute_loss** - 计算单批训练输入的损失。\n- **training_step** -- 执行一步训练。\n- **prediction_step** -- 执行一步评估/测试。\n- **evaluate** -- 运行评估循环并返回指标。\n- **predict** -- 返回在测试集上的预测（如果有标签，则包括指标）。\n\n<Tip warning={true}>\n\n[`Trainer`] 类被优化用于 🤗 Transformers 模型，并在你在其他模型上使用时可能会有一些令人惊讶的结果。当在你自己的模型上使用时，请确保：\n\n- 你的模型始终返回元组或 [`~utils.ModelOutput`] 的子类。\n- 如果提供了 `labels` 参数，你的模型可以计算损失，并且损失作为元组的第一个元素返回（如果你的模型返回元组）。\n- 你的模型可以接受多个标签参数（在 [`TrainingArguments`] 中使用 `label_names` 将它们的名称指示给 [`Trainer`]），但它们中没有一个应该被命名为 `\"label\"`。\n\n</Tip>\n\n以下是如何自定义 [`Trainer`] 以使用加权损失的示例（在训练集不平衡时很有用）：\n\n```python\nfrom torch import nn\nfrom transformers import Trainer\n\n\nclass CustomTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        # forward pass\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        # compute custom loss (suppose one has 3 labels with different weights)\n        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0], device=model.device))\n        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n```\n\n在 PyTorch [`Trainer`] 中自定义训练循环行为的另一种方法是使用 [callbacks](callback)，这些回调可以检查训练循环状态（用于进度报告、在 TensorBoard 或其他 ML 平台上记录日志等）并做出决策（比如提前停止）。",
    "1166": "一级标题：Trainer\n二级标题：Trainer\n内容：\n[[autodoc]] Trainer - all",
    "1167": "一级标题：Trainer\n二级标题：Seq2SeqTrainer\n内容：\n[[autodoc]] Seq2SeqTrainer - evaluate - predict",
    "1168": "一级标题：Trainer\n二级标题：TrainingArguments\n内容：\n[[autodoc]] TrainingArguments - all",
    "1169": "一级标题：Trainer\n二级标题：Seq2SeqTrainingArguments\n内容：\n[[autodoc]] Seq2SeqTrainingArguments - all",
    "1170": "一级标题：Trainer\n二级标题：Checkpoints\n内容：\n默认情况下，[`Trainer`] 会将所有checkpoints保存在你使用的 [`TrainingArguments`] 中设置的 `output_dir` 中。这些checkpoints将位于名为 `checkpoint-xxx` 的子文件夹中，xxx 是训练的步骤。\n\n从checkpoints恢复训练可以通过调用 [`Trainer.train`] 时使用以下任一方式进行：\n\n- `resume_from_checkpoint=True`，这将从最新的checkpoint恢复训练。\n- `resume_from_checkpoint=checkpoint_dir`，这将从指定目录中的特定checkpoint恢复训练。\n\n此外，当使用 `push_to_hub=True` 时，你可以轻松将checkpoints保存在 Model Hub 中。默认情况下，保存在训练中间过程的checkpoints中的所有模型都保存在不同的提交中，但不包括优化器状态。你可以根据需要调整 [`TrainingArguments`] 的 `hub-strategy` 值：\n\n- `\"checkpoint\"`: 最新的checkpoint也被推送到一个名为 last-checkpoint 的子文件夹中，让你可以通过 `trainer.train(resume_from_checkpoint=\"output_dir/last-checkpoint\")` 轻松恢复训练。\n- `\"all_checkpoints\"`: 所有checkpoints都像它们出现在输出文件夹中一样被推送（因此你将在最终存储库中的每个文件夹中获得一个checkpoint文件夹）。",
    "1171": "一级标题：Trainer\n二级标题：Logging\n内容：\n默认情况下，[`Trainer`] 将对主进程使用 `logging.INFO`，对副本（如果有的话）使用 `logging.WARNING`。\n\n可以通过 [`TrainingArguments`] 的参数覆盖这些默认设置，使用其中的 5 个 `logging` 级别：\n\n- `log_level` - 用于主进程\n- `log_level_replica` - 用于副本\n\n此外，如果 [`TrainingArguments`] 的 `log_on_each_node` 设置为 `False`，则只有主节点将使用其主进程的日志级别设置，所有其他节点将使用副本的日志级别设置。\n\n请注意，[`Trainer`] 将在其 [`Trainer.__init__`] 中分别为每个节点设置 `transformers` 的日志级别。因此，如果在创建 [`Trainer`] 对象之前要调用其他 `transformers` 功能，可能需要更早地设置这一点（请参见下面的示例）。\n\n以下是如何在应用程序中使用的示例：\n\n```python\n[...]\nlogger = logging.getLogger(__name__)\n\n# Setup logging\nlogging.basicConfig(\n    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n    datefmt=\"%m/%d/%Y %H:%M:%S\",\n    handlers=[logging.StreamHandler(sys.stdout)],\n)\n\n# set the main code and the modules it uses to the same log-level according to the node\nlog_level = training_args.get_process_log_level()\nlogger.setLevel(log_level)\ndatasets.utils.logging.set_verbosity(log_level)\ntransformers.utils.logging.set_verbosity(log_level)\n\ntrainer = Trainer(...)\n```\n\n然后，如果你只想在主节点上看到警告，并且所有其他节点不打印任何可能重复的警告，可以这样运行：\n\n```bash\nmy_app.py ... --log_level warning --log_level_replica error\n```\n\n在多节点环境中，如果你也不希望每个节点的主进程的日志重复输出，你需要将上面的代码更改为：\n\n```bash\nmy_app.py ... --log_level warning --log_level_replica error --log_on_each_node 0\n```\n\n然后，只有第一个节点的主进程将以 \"warning\" 级别记录日志，主节点上的所有其他进程和其他节点上的所有进程将以 \"error\" 级别记录日志。\n\n如果你希望应用程序尽可能”安静“，可以执行以下操作：\n\n\n```bash\nmy_app.py ... --log_level error --log_level_replica error --log_on_each_node 0\n```\n\n(如果在多节点环境，添加 `--log_on_each_node 0`)",
    "1172": "一级标题：Trainer\n二级标题：随机性\n内容：\n当从 [`Trainer`] 生成的checkpoint恢复训练时，程序会尽一切努力将 _python_、_numpy_ 和 _pytorch_ 的 RNG（随机数生成器）状态恢复为保存检查点时的状态，这样可以使“停止和恢复”式训练尽可能接近“非停止式”训练。\n\n然而，由于各种默认的非确定性 PyTorch 设置，这可能无法完全实现。如果你想要完全确定性，请参阅[控制随机源](https://pytorch.org/docs/stable/notes/randomness)。正如文档中所解释的那样，使事物变得确定的一些设置（例如 `torch.backends.cudnn.deterministic`）可能会减慢速度，因此不能默认执行，但如果需要，你可以自行启用这些设置。",
    "1173": "一级标题：Trainer\n二级标题：特定GPU选择\n内容：\n让我们讨论一下如何告诉你的程序应该使用哪些 GPU 以及使用的顺序。\n\n当使用 [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html) 且仅使用部分 GPU 时，你只需指定要使用的 GPU 数量。例如，如果你有 4 个 GPU，但只想使用前 2 个，可以执行以下操作：\n\n\n```bash\npython -m torch.distributed.launch --nproc_per_node=2  trainer-program.py ...\n```\n\n如果你安装了 [`accelerate`](https://github.com/huggingface/accelerate) 或 [`deepspeed`](https://github.com/deepspeedai/DeepSpeed)，你还可以通过以下任一方法实现相同的效果：\n\n\n```bash\naccelerate launch --num_processes 2 trainer-program.py ...\n```\n\n```bash\ndeepspeed --num_gpus 2 trainer-program.py ...\n```\n\n你不需要使用 Accelerate 或 [Deepspeed 集成](Deepspeed) 功能来使用这些启动器。\n\n到目前为止，你已经能够告诉程序要使用多少个 GPU。现在让我们讨论如何选择特定的 GPU 并控制它们的顺序。\n\n以下环境变量可帮助你控制使用哪些 GPU 以及它们的顺序。\n\n\n**`CUDA_VISIBLE_DEVICES`**\n\n如果你有多个 GPU，想要仅使用其中的一个或几个 GPU，请将环境变量 `CUDA_VISIBLE_DEVICES` 设置为要使用的 GPU 列表。\n\n例如，假设你有 4 个 GPU：0、1、2 和 3。要仅在物理 GPU 0 和 2 上运行，你可以执行以下操作：\n\n\n```bash\nCUDA_VISIBLE_DEVICES=0,2 python -m torch.distributed.launch trainer-program.py ...\n```\n\n现在，PyTorch 将只看到 2 个 GPU，其中你的物理 GPU 0 和 2 分别映射到 `cuda:0` 和 `cuda:1`。\n\n你甚至可以改变它们的顺序：\n\n\n```bash\nCUDA_VISIBLE_DEVICES=2,0 python -m torch.distributed.launch trainer-program.py ...\n```\n\n这里，你的物理 GPU 0 和 2 分别映射到 `cuda:1` 和 `cuda:0`。\n\n上面的例子都是针对 `DistributedDataParallel` 使用模式的，但同样的方法也适用于 [`DataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html)：\n\n\n```bash\nCUDA_VISIBLE_DEVICES=2,0 python trainer-program.py ...\n```\n\n为了模拟没有 GPU 的环境，只需将此环境变量设置为空值，如下所示：\n\n```bash\nCUDA_VISIBLE_DEVICES= python trainer-program.py ...\n```\n\n与任何环境变量一样，你当然可以将其export到环境变量而不是将其添加到命令行，如下所示：\n\n\n```bash\nexport CUDA_VISIBLE_DEVICES=0,2\npython -m torch.distributed.launch trainer-program.py ...\n```\n\n这种方法可能会令人困惑，因为你可能会忘记之前设置了环境变量，进而不明白为什么会使用错误的 GPU。因此，在同一命令行中仅为特定运行设置环境变量是一种常见做法，正如本节大多数示例所示。\n\n\n**`CUDA_DEVICE_ORDER`**\n\n还有一个额外的环境变量 `CUDA_DEVICE_ORDER`，用于控制物理设备的排序方式。有两个选择：\n\n1. 按 PCIe 总线 ID 排序（与 nvidia-smi 的顺序相匹配）- 这是默认选项。\n\n\n```bash\nexport CUDA_DEVICE_ORDER=PCI_BUS_ID\n```\n\n2. 按 GPU 计算能力排序。\n\n```bash\nexport CUDA_DEVICE_ORDER=FASTEST_FIRST\n```\n\n大多数情况下，你不需要关心这个环境变量，但如果你的设置不均匀，那么这将非常有用，例如，您的旧 GPU 和新 GPU 物理上安装在一起，但让速度较慢的旧卡排在运行的第一位。解决这个问题的一种方法是交换卡的位置。但如果不能交换卡（例如，如果设备的散热受到影响），那么设置 `CUDA_DEVICE_ORDER=FASTEST_FIRST` 将始终将较新、更快的卡放在第一位。但这可能会有点混乱，因为 `nvidia-smi` 仍然会按照 PCIe 顺序报告它们。\n\n交换卡的顺序的另一种方法是使用：\n\n\n```bash\nexport CUDA_VISIBLE_DEVICES=1,0\n```\n\n在此示例中，我们只使用了 2 个 GPU，但是当然，对于计算机上有的任何数量的 GPU，都适用相同的方法。\n\n此外，如果你设置了这个环境变量，最好将其设置在 `~/.bashrc` 文件或其他启动配置文件中，然后就可以忘记它了。",
    "1174": "一级标题：Trainer\n二级标题：Trainer集成\n内容：\n[`Trainer`] 已经被扩展，以支持可能显著提高训练时间并适应更大模型的库。\n\n目前，它支持第三方解决方案 [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) 和 [PyTorch FSDP](https://pytorch.org/docs/stable/fsdp.html)，它们实现了论文 [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models, by Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He](https://huggingface.co/papers/1910.02054) 的部分内容。\n\n截至撰写本文，此提供的支持是新的且实验性的。尽管我们欢迎围绕 DeepSpeed 和 PyTorch FSDP 的issues，但我们不再支持 FairScale 集成，因为它已经集成到了 PyTorch 主线（参见 [PyTorch FSDP 集成](#pytorch-fully-sharded-data-parallel)）。\n\n\n<a id='zero-install-notes'></a>\n\n### CUDA拓展安装注意事项\n\n\n撰写时，Deepspeed 需要在使用之前编译 CUDA C++ 代码。\n\n虽然所有安装问题都应通过 [Deepspeed](https://github.com/deepspeedai/DeepSpeed/issues) 的 GitHub Issues处理，但在构建依赖CUDA 扩展的任何 PyTorch 扩展时，可能会遇到一些常见问题。\n\n因此，如果在执行以下操作时遇到与 CUDA 相关的构建问题：\n\n\n```bash\npip install deepspeed\n```\n\n请首先阅读以下说明。\n\n在这些说明中，我们提供了在 `pytorch` 使用 CUDA `10.2` 构建时应采取的操作示例。如果你的情况有所不同，请记得将版本号调整为您所需的版本。\n\n\n#### 可能的问题 #1\n\n尽管 PyTorch 自带了其自己的 CUDA 工具包，但要构建这两个项目，你必须在整个系统上安装相同版本的 CUDA。\n\n例如，如果你在 Python 环境中使用 `cudatoolkit==10.2` 安装了 `pytorch`，你还需要在整个系统上安装 CUDA `10.2`。\n\n确切的位置可能因系统而异，但在许多 Unix 系统上，`/usr/local/cuda-10.2` 是最常见的位置。当 CUDA 正确设置并添加到 `PATH` 环境变量时，可以通过执行以下命令找到安装位置：\n\n\n```bash\nwhich nvcc\n```\n\n如果你尚未在整个系统上安装 CUDA，请首先安装。你可以使用你喜欢的搜索引擎查找说明。例如，如果你使用的是 Ubuntu，你可能想搜索：[ubuntu cuda 10.2 install](https://www.google.com/search?q=ubuntu+cuda+10.2+install)。\n\n\n#### 可能的问题 #2\n\n另一个可能的常见问题是你可能在整个系统上安装了多个 CUDA 工具包。例如，你可能有：\n\n\n```bash\n/usr/local/cuda-10.2\n/usr/local/cuda-11.0\n```\n\n在这种情况下，你需要确保 `PATH` 和 `LD_LIBRARY_PATH` 环境变量包含所需 CUDA 版本的正确路径。通常，软件包安装程序将设置这些变量以包含最新安装的版本。如果遇到构建失败的问题，且是因为在整个系统安装但软件仍找不到正确的 CUDA 版本，这意味着你需要调整这两个环境变量。\n\n首先，你以查看它们的内容：\n\n\n```bash\necho $PATH\necho $LD_LIBRARY_PATH\n```\n\n因此，您可以了解其中的内容。\n\n`LD_LIBRARY_PATH` 可能是空的。\n\n`PATH` 列出了可以找到可执行文件的位置，而 `LD_LIBRARY_PATH` 用于查找共享库。在这两种情况下，较早的条目优先于较后的条目。 `:` 用于分隔多个条目。\n\n现在，为了告诉构建程序在哪里找到特定的 CUDA 工具包，请插入所需的路径，让其首先列出：\n\n\n```bash\nexport PATH=/usr/local/cuda-10.2/bin:$PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64:$LD_LIBRARY_PATH\n```\n\n请注意，我们没有覆盖现有值，而是在前面添加新的值。\n\n当然，根据需要调整版本号和完整路径。检查你分配的目录是否实际存在。`lib64` 子目录是各种 CUDA `.so` 对象（如 `libcudart.so`）的位置，这个名字可能在你的系统中是不同的，如果是，请调整以反映实际情况。\n\n\n#### 可能的问题 #3\n\n一些较旧的 CUDA 版本可能会拒绝使用更新的编译器。例如，你可能有 `gcc-9`，但 CUDA 可能需要 `gcc-7`。\n\n有各种方法可以解决这个问题。\n\n如果你可以安装最新的 CUDA 工具包，通常它应该支持更新的编译器。\n\n或者，你可以在已经拥有的编译器版本之外安装较低版本，或者你可能已经安装了它但它不是默认的编译器，因此构建系统无法找到它。如果你已经安装了 `gcc-7` 但构建系统找不到它，以下操作可能会解决问题：\n\n\n```bash\nsudo ln -s /usr/bin/gcc-7  /usr/local/cuda-10.2/bin/gcc\nsudo ln -s /usr/bin/g++-7  /usr/local/cuda-10.2/bin/g++\n```\n\n这里，我们正在从 `/usr/local/cuda-10.2/bin/gcc` 创建到 `gcc-7` 的软链接，由于 `/usr/local/cuda-10.2/bin/` 应该在 `PATH` 环境变量中（参见前一个问题的解决方案），它应该能够找到 `gcc-7`（和 `g++7`），然后构建将成功。\n\n与往常一样，请确保编辑示例中的路径以匹配你的情况。\n\n\n\n### PyTorch完全分片数据并行（FSDP)\n\n为了加速在更大批次大小上训练庞大模型，我们可以使用完全分片的数据并行模型。这种数据并行范例通过对优化器状态、梯度和参数进行分片，实现了在更多数据和更大模型上的训练。要了解更多信息以及其优势，请查看[完全分片的数据并行博客](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)。我们已经集成了最新的PyTorch完全分片的数据并行（FSDP）训练功能。您只需通过配置启用它。\n\n**FSDP支持所需的PyTorch版本**: PyTorch Nightly（或者如果你在发布后阅读这个，使用1.12.0版本，因为带有激活的FSDP的模型保存仅在最近的修复中可用。\n\n\n**用法**:\n\n- 如果你尚未使用过分布式启动器，确保你已经添加了它 `-m torch.distributed.launch --nproc_per_node=NUMBER_OF_GPUS_YOU_HAVE`。\n\n- **分片策略**：\n  - FULL_SHARD：在数据并行线程/GPU之间，对优化器状态、梯度和模型参数进行分片。\n    为此，请在命令行参数中添加 `--fsdp full_shard`。\n  - SHARD_GRAD_OP：在数据并行线程/GPU之间对优化器状态和梯度进行分片。\n    为此，请在命令行参数中添加 `--fsdp shard_grad_op`。\n  - NO_SHARD：不进行分片。为此，请在命令行参数中添加 `--fsdp no_shard`。\n- 要将参数和梯度卸载到CPU，添加 `--fsdp \"full_shard offload\"` 或 `--fsdp \"shard_grad_op offload\"` 到命令行参数中。\n- 要使用 `default_auto_wrap_policy` 自动递归地用FSDP包装层，请添加 `--fsdp \"full_shard auto_wrap\"` 或 `--fsdp \"shard_grad_op auto_wrap\"` 到命令行参数中。\n- 要同时启用CPU卸载和自动包装层工具，请添加 `--fsdp \"full_shard offload auto_wrap\"` 或 `--fsdp \"shard_grad_op offload auto_wrap\"` 到命令行参数中。\n- 其余的FSDP配置通过 `--fsdp_config <path_to_fsdp_config.json>` 传递。它可以是FSDP json配置文件的位置（例如，`fsdp_config.json`）或已加载的json文件作为 `dict`。\n  - 如果启用了自动包装，您可以使用基于transformer的自动包装策略或基于大小的自动包装策略。\n    - 对于基于transformer的自动包装策略，建议在配置文件中指定 `fsdp_transformer_layer_cls_to_wrap`。如果未指定，则默认值为 `model._no_split_modules`（如果可用）。这将指定要包装的transformer层类名（区分大小写），例如 [`BertLayer`]、[`GPTJBlock`]、[`T5Block`] 等。这很重要，因为共享权重的子模块（例如，embedding层）不应最终出现在不同的FSDP包装单元中。使用此策略，每个包装的块将包含多头注意力和后面的几个MLP层。剩余的层，包括共享的embedding层，都将被方便地包装在同一个最外层的FSDP单元中。因此，对于基于transformer的模型，请使用这个方法。\n    - 对于基于大小的自动包装策略，请在配置文件中添加 `fsdp_min_num_params`。它指定了FSDP进行自动包装的最小参数数量。\n  - 可以在配置文件中指定 `fsdp_backward_prefetch`。它控制何时预取下一组参数。`backward_pre` 和 `backward_pos` 是可用的选项。有关更多信息，请参阅 `torch.distributed.fsdp.fully_sharded_data_parallel.BackwardPrefetch`\n  - 可以在配置文件中指定 `fsdp_forward_prefetch`。它控制何时预取下一组参数。如果是`\"True\"`，在执行前向传递时，FSDP明确地预取下一次即将发生的全局聚集。\n  - 可以在配置文件中指定 `limit_all_gathers`。如果是`\"True\"`，FSDP明确地同步CPU线程，以防止太多的进行中的全局聚集。\n  - 可以在配置文件中指定 `activation_checkpointing`。如果是`\"True\"`，FSDP activation checkpoint是一种通过清除某些层的激活值并在反向传递期间重新计算它们来减少内存使用的技术。实际上，这以更多的计算时间为代价减少了内存使用。\n\n\n**需要注意几个注意事项**\n- 它与 `generate` 不兼容，因此与所有seq2seq/clm脚本（翻译/摘要/clm等）中的 `--predict_with_generate` 不兼容。请参阅issue[#21667](https://github.com/huggingface/transformers/issues/21667)。\n\n\n### PyTorch/XLA 完全分片数据并行\n\n对于所有TPU用户，有个好消息！PyTorch/XLA现在支持FSDP。所有最新的完全分片数据并行（FSDP）训练都受支持。有关更多信息，请参阅[在云端TPU上使用FSDP扩展PyTorch模型](https://pytorch.org/blog/scaling-pytorch-models-on-cloud-tpus-with-fsdp/)和[PyTorch/XLA FSDP的实现](https://github.com/pytorch/xla/tree/master/torch_xla/distributed/fsdp)。使用它只需通过配置启用。\n\n**需要的 PyTorch/XLA 版本以支持 FSDP**：>=2.0\n\n**用法**：\n\n传递 `--fsdp \"full shard\"`，同时对 `--fsdp_config <path_to_fsdp_config.json>` 进行以下更改：\n- `xla` 应设置为 `True` 以启用 PyTorch/XLA FSDP。\n- `xla_fsdp_settings` 的值是一个字典，存储 XLA FSDP 封装参数。完整的选项列表，请参见[此处](https://github.com/pytorch/xla/blob/master/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py)。\n- `xla_fsdp_grad_ckpt`。当 `True` 时，在每个嵌套的 XLA FSDP 封装层上使用梯度checkpoint。该设置只能在将 xla 标志设置为 true，并通过 `fsdp_min_num_params` 或 `fsdp_transformer_layer_cls_to_wrap` 指定自动包装策略时使用。\n- 您可以使用基于transformer的自动包装策略或基于大小的自动包装策略。\n  - 对于基于transformer的自动包装策略，建议在配置文件中指定 `fsdp_transformer_layer_cls_to_wrap`。如果未指定，默认值为 `model._no_split_modules`（如果可用）。这指定了要包装的transformer层类名列表（区分大小写），例如 [`BertLayer`]、[`GPTJBlock`]、[`T5Block`] 等。这很重要，因为共享权重的子模块（例如，embedding层）不应最终出现在不同的FSDP包装单元中。使用此策略，每个包装的块将包含多头注意力和后面的几个MLP层。剩余的层，包括共享的embedding层，都将被方便地包装在同一个最外层的FSDP单元中。因此，对于基于transformer的模型，请使用这个方法。\n  - 对于基于大小的自动包装策略，请在配置文件中添加 `fsdp_min_num_params`。它指定了自动包装的 FSDP 的最小参数数量。\n\n\n### 在 Mac 上使用 Trainer 进行加速的 PyTorch 训练\n\n随着 PyTorch v1.12 版本的发布，开发人员和研究人员可以利用 Apple Silicon GPU 进行显著更快的模型训练。这使得可以在 Mac 上本地执行原型设计和微调等机器学习工作流程。Apple 的 Metal Performance Shaders（MPS）作为 PyTorch 的后端实现了这一点，并且可以通过新的 `\"mps\"` 设备来使用。\n这将在 MPS 图形框架上映射计算图和神经图元，并使用 MPS 提供的优化内核。更多信息，请参阅官方文档 [Introducing Accelerated PyTorch Training on Mac](https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/) 和 [MPS BACKEND](https://pytorch.org/docs/stable/notes/mps.html)。\n\n\n<Tip warning={false}>\n\n我们强烈建议在你的 MacOS 机器上安装 PyTorch >= 1.13（在撰写本文时为最新版本）。对于基于 transformer 的模型， 它提供与模型正确性和性能改进相关的重大修复。有关更多详细信息，请参阅[pytorch/pytorch#82707](https://github.com/pytorch/pytorch/issues/82707)。\n\n</Tip>\n\n**使用 Apple Silicon 芯片进行训练和推理的好处**\n\n1. 使用户能够在本地训练更大的网络或批量数据。\n2. 由于统一内存架构，减少数据检索延迟，并为 GPU 提供对完整内存存储的直接访问。从而提高端到端性能。\n3. 降低与基于云的开发或需要额外本地 GPU 的成本。\n\n**先决条件**：要安装带有 mps 支持的 torch，请按照这篇精彩的 Medium 文章操作 [GPU-Acceleration Comes to PyTorch on M1 Macs](https://medium.com/towards-data-science/gpu-acceleration-comes-to-pytorch-on-m1-macs-195c399efcc1)。\n\n**用法**：\n如果可用，`mps` 设备将默认使用，类似于使用 `cuda` 设备的方式。因此，用户无需采取任何操作。例如，您可以使用以下命令在 Apple Silicon GPU 上运行官方的 Glue 文本分类任务（从根文件夹运行）：\n\n```bash\nexport TASK_NAME=mrpc\n\npython examples/pytorch/text-classification/run_glue.py \\\n  --model_name_or_path google-bert/bert-base-cased \\\n  --task_name $TASK_NAME \\\n  --do_train \\\n  --do_eval \\\n  --max_seq_length 128 \\\n  --per_device_train_batch_size 32 \\\n  --learning_rate 2e-5 \\\n  --num_train_epochs 3 \\\n  --output_dir /tmp/$TASK_NAME/ \\\n  --overwrite_output_dir\n```\n\n**需要注意的一些注意事项**\n\n1. 一些 PyTorch 操作尚未在 mps 中实现，将引发错误。解决此问题的一种方法是设置环境变量 `PYTORCH_ENABLE_MPS_FALLBACK=1`，它将把这些操作回退到 CPU 进行。然而，它仍然会抛出 UserWarning 信息。\n2. 分布式设置 `gloo` 和 `nccl` 在 `mps` 设备上不起作用。这意味着当前只能使用 `mps` 设备类型的单个 GPU。\n\n最后，请记住，🤗 `Trainer` 仅集成了 MPS 后端，因此如果你在使用 MPS 后端时遇到任何问题或有疑问，请在 [PyTorch GitHub](https://github.com/pytorch/pytorch/issues) 上提交问题。",
    "1175": "一级标题：Trainer\n二级标题：通过 Accelerate Launcher 使用 Trainer\n内容：\nAccelerate 现在支持 Trainer。用户可以期待以下内容：\n- 他们可以继续使用 Trainer 的迭代，如 FSDP、DeepSpeed 等，而无需做任何更改。\n- 现在可以在 Trainer 中使用 Accelerate Launcher（建议使用）。\n\n通过 Accelerate Launcher 使用 Trainer 的步骤：\n1. 确保已安装 🤗 Accelerate，无论如何，如果没有它，你无法使用 `Trainer`。如果没有，请执行 `pip install accelerate`。你可能还需要更新 Accelerate 的版本：`pip install accelerate --upgrade`。\n2. 运行 `accelerate config` 并填写问题。以下是一些加速配置的示例：\n\n  a. DDP 多节点多 GPU 配置：\n\n    ```yaml\n    compute_environment: LOCAL_MACHINE\n    distributed_type: MULTI_GPU\n    downcast_bf16: 'no'\n    gpu_ids: all\n    machine_rank: 0 #change rank as per the node\n    main_process_ip: 192.168.20.1\n    main_process_port: 9898\n    main_training_function: main\n    mixed_precision: fp16\n    num_machines: 2\n    num_processes: 8\n    rdzv_backend: static\n    same_network: true\n    tpu_env: []\n    tpu_use_cluster: false\n    tpu_use_sudo: false\n    use_cpu: false\n    ```\n\n  b. FSDP 配置：\n\n    ```yaml\n    compute_environment: LOCAL_MACHINE\n    distributed_type: FSDP\n    downcast_bf16: 'no'\n    fsdp_config:\n      fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n      fsdp_backward_prefetch_policy: BACKWARD_PRE\n      fsdp_forward_prefetch: true\n      fsdp_offload_params: false\n      fsdp_sharding_strategy: 1\n      fsdp_state_dict_type: FULL_STATE_DICT\n      fsdp_sync_module_states: true\n      fsdp_transformer_layer_cls_to_wrap: BertLayer\n      fsdp_use_orig_params: true\n    machine_rank: 0\n    main_training_function: main\n    mixed_precision: bf16\n    num_machines: 1\n    num_processes: 2\n    rdzv_backend: static\n    same_network: true\n    tpu_env: []\n    tpu_use_cluster: false\n    tpu_use_sudo: false\n    use_cpu: false\n    ```\n\n  c. 指向文件的 DeepSpeed 配置：\n\n    ```yaml\n    compute_environment: LOCAL_MACHINE\n    deepspeed_config:\n      deepspeed_config_file: /home/user/configs/ds_zero3_config.json\n      zero3_init_flag: true\n    distributed_type: DEEPSPEED\n    downcast_bf16: 'no'\n    machine_rank: 0\n    main_training_function: main\n    num_machines: 1\n    num_processes: 4\n    rdzv_backend: static\n    same_network: true\n    tpu_env: []\n    tpu_use_cluster: false\n    tpu_use_sudo: false\n    use_cpu: false\n    ```\n\n  d. 使用 accelerate 插件的 DeepSpeed 配置：\n\n    ```yaml\n    compute_environment: LOCAL_MACHINE\n    deepspeed_config:\n      gradient_accumulation_steps: 1\n      gradient_clipping: 0.7\n      offload_optimizer_device: cpu\n      offload_param_device: cpu\n      zero3_init_flag: true\n      zero_stage: 2\n    distributed_type: DEEPSPEED\n    downcast_bf16: 'no'\n    machine_rank: 0\n    main_training_function: main\n    mixed_precision: bf16\n    num_machines: 1\n    num_processes: 4\n    rdzv_backend: static\n    same_network: true\n    tpu_env: []\n    tpu_use_cluster: false\n    tpu_use_sudo: false\n    use_cpu: false\n    ```\n\n3. 使用accelerate配置文件参数或启动器参数以外的参数运行Trainer脚本。以下是一个使用上述FSDP配置从accelerate启动器运行`run_glue.py`的示例。\n\n```bash\ncd transformers\n\naccelerate launch \\\n./examples/pytorch/text-classification/run_glue.py \\\n--model_name_or_path google-bert/bert-base-cased \\\n--task_name $TASK_NAME \\\n--do_train \\\n--do_eval \\\n--max_seq_length 128 \\\n--per_device_train_batch_size 16 \\\n--learning_rate 5e-5 \\\n--num_train_epochs 3 \\\n--output_dir /tmp/$TASK_NAME/ \\\n--overwrite_output_dir\n```\n\n4. 你也可以直接使用`accelerate launch`的cmd参数。上面的示例将映射到：\n\n```bash\ncd transformers\n\naccelerate launch --num_processes=2 \\\n--use_fsdp \\\n--mixed_precision=bf16 \\\n--fsdp_auto_wrap_policy=TRANSFORMER_BASED_WRAP  \\\n--fsdp_transformer_layer_cls_to_wrap=\"BertLayer\" \\\n--fsdp_sharding_strategy=1 \\\n--fsdp_state_dict_type=FULL_STATE_DICT \\\n./examples/pytorch/text-classification/run_glue.py\n--model_name_or_path google-bert/bert-base-cased \\\n--task_name $TASK_NAME \\\n--do_train \\\n--do_eval \\\n--max_seq_length 128 \\\n--per_device_train_batch_size 16 \\\n--learning_rate 5e-5 \\\n--num_train_epochs 3 \\\n--output_dir /tmp/$TASK_NAME/ \\\n--overwrite_output_dir\n```\n\n有关更多信息，请参阅 🤗 Accelerate CLI 指南：[启动您的 🤗 Accelerate 脚本](https://huggingface.co/docs/accelerate/basic_tutorials/launch)。\n\n已移动的部分：\n\n[ <a href=\"./deepspeed#deepspeed-trainer-integration\">DeepSpeed</a><a id=\"deepspeed\"></a> | <a href=\"./deepspeed#deepspeed-installation\">Installation</a><a id=\"installation\"></a> | <a href=\"./deepspeed#deepspeed-multi-gpu\">Deployment with multiple GPUs</a><a id=\"deployment-with-multiple-gpus\"></a> | <a href=\"./deepspeed#deepspeed-one-gpu\">Deployment with one GPU</a><a id=\"deployment-with-one-gpu\"></a> | <a href=\"./deepspeed#deepspeed-notebook\">Deployment in Notebooks</a><a id=\"deployment-in-notebooks\"></a> | <a href=\"./deepspeed#deepspeed-config\">Configuration</a><a id=\"configuration\"></a> | <a href=\"./deepspeed#deepspeed-config-passing\">Passing Configuration</a><a id=\"passing-configuration\"></a> | <a href=\"./deepspeed#deepspeed-config-shared\">Shared Configuration</a><a id=\"shared-configuration\"></a> | <a href=\"./deepspeed#deepspeed-zero\">ZeRO</a><a id=\"zero\"></a> | <a href=\"./deepspeed#deepspeed-zero2-config\">ZeRO-2 Config</a><a id=\"zero-2-config\"></a> | <a href=\"./deepspeed#deepspeed-zero3-config\">ZeRO-3 Config</a><a id=\"zero-3-config\"></a> | <a href=\"./deepspeed#deepspeed-nvme\">NVMe Support</a><a id=\"nvme-support\"></a> | <a href=\"./deepspeed#deepspeed-zero2-zero3-performance\">ZeRO-2 vs ZeRO-3 Performance</a><a id=\"zero-2-vs-zero-3-performance\"></a> | <a href=\"./deepspeed#deepspeed-zero2-example\">ZeRO-2 Example</a><a id=\"zero-2-example\"></a> | <a href=\"./deepspeed#deepspeed-zero3-example\">ZeRO-3 Example</a><a id=\"zero-3-example\"></a> | <a href=\"./deepspeed#deepspeed-optimizer\">Optimizer</a><a id=\"optimizer\"></a> | <a href=\"./deepspeed#deepspeed-scheduler\">Scheduler</a><a id=\"scheduler\"></a> | <a href=\"./deepspeed#deepspeed-fp32\">fp32 Precision</a><a id=\"fp32-precision\"></a> | <a href=\"./deepspeed#deepspeed-amp\">Automatic Mixed Precision</a><a id=\"automatic-mixed-precision\"></a> | <a href=\"./deepspeed#deepspeed-bs\">Batch Size</a><a id=\"batch-size\"></a> | <a href=\"./deepspeed#deepspeed-grad-acc\">Gradient Accumulation</a><a id=\"gradient-accumulation\"></a> | <a href=\"./deepspeed#deepspeed-grad-clip\">Gradient Clipping</a><a id=\"gradient-clipping\"></a> | <a href=\"./deepspeed#deepspeed-weight-extraction\">Getting The Model Weights Out</a><a id=\"getting-the-model-weights-out\"></a>]",
    "1176": "一级标题：Trainer\n二级标题：通过 NEFTune 提升微调性能\n内容：\nNEFTune 是一种提升聊天模型性能的技术，由 Jain 等人在论文“NEFTune: Noisy Embeddings Improve Instruction Finetuning” 中引入。该技术在训练过程中向embedding向量添加噪音。根据论文摘要：\n\n> 使用 Alpaca 对 LLaMA-2-7B 进行标准微调，可以在 AlpacaEval 上达到 29.79%，而使用带有噪音embedding的情况下，性能提高至 64.69%。NEFTune 还在modern instruction数据集上大大优于基线。Evol-Instruct 训练的模型表现提高了 10%，ShareGPT 提高了 8%，OpenPlatypus 提高了 8%。即使像 LLaMA-2-Chat 这样通过 RLHF 进一步细化的强大模型，通过 NEFTune 的额外训练也能受益。\n\n<div style=\"text-align: center\">\n<img src=\"https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/neft-screenshot.png\">\n</div>\n\n要在 `Trainer` 中使用它，只需在创建 `TrainingArguments` 实例时传递 `neftune_noise_alpha`。请注意，为了避免任何意外行为，NEFTune在训练后被禁止，以此恢复原始的embedding层。\n\n```python\nfrom transformers import Trainer, TrainingArguments\n\nargs = TrainingArguments(..., neftune_noise_alpha=0.1)\ntrainer = Trainer(..., args=args)\n\n...\n\ntrainer.train()\n```",
    "1177": "一级标题：BERT\n二级标题：无\n内容：\n[BERT](https://huggingface.co/papers/1810.04805) 是一个在无标签的文本数据上预训练的双向 transformer，用于预测句子中被掩码的（masked） token，以及预测一个句子是否跟随在另一个句子之后。其主要思想是，在预训练过程中，通过随机掩码一些 token，让模型利用左右上下文的信息预测它们，从而获得更全面深入的理解。此外，BERT 具有很强的通用性，其学习到的语言表示可以通过额外的层或头进行微调，从而适配其他下游 NLP 任务。\n\n你可以在 [BERT](https://huggingface.co/collections/google/bert-release-64ff5e7a4be99045d1896dbc) 集合下找到 BERT 的所有原始 checkpoint。\n\n> [!TIP]\n> 点击右侧边栏中的 BERT 模型，以查看将 BERT 应用于不同语言任务的更多示例。\n\n下面的示例演示了如何使用 [`Pipeline`], [`AutoModel`] 和命令行预测 `[MASK]` token。\n\n<hfoptions id=\"usage\">\n<hfoption id=\"Pipeline\">\n\n```py\nimport torch\nfrom transformers import pipeline\n\npipeline = pipeline(\n    task=\"fill-mask\",\n    model=\"google-bert/bert-base-uncased\",\n    torch_dtype=torch.float16,\n    device=0\n)\npipeline(\"Plants create [MASK] through a process known as photosynthesis.\")\n```\n\n</hfoption>\n<hfoption id=\"AutoModel\">\n\n```py\nimport torch\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"google-bert/bert-base-uncased\",\n)\nmodel = AutoModelForMaskedLM.from_pretrained(\n    \"google-bert/bert-base-uncased\",\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    attn_implementation=\"sdpa\"\n)\ninputs = tokenizer(\"Plants create [MASK] through a process known as photosynthesis.\", return_tensors=\"pt\").to(\"cuda\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n    predictions = outputs.logits\n\nmasked_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]\npredicted_token_id = predictions[0, masked_index].argmax(dim=-1)\npredicted_token = tokenizer.decode(predicted_token_id)\n\nprint(f\"The predicted token is: {predicted_token}\")\n```\n\n</hfoption>\n<hfoption id=\"transformers-cli\">\n\n```bash\necho -e \"Plants create [MASK] through a process known as photosynthesis.\" | transformers-cli run --task fill-mask --model google-bert/bert-base-uncased --device 0\n```\n\n</hfoption>\n</hfoptions>",
    "1178": "一级标题：BERT\n二级标题：注意\n内容：\n- 输入内容应在右侧进行填充，因为 BERT 使用绝对位置嵌入。",
    "1179": "一级标题：BERT\n二级标题：BertConfig\n内容：\n[[autodoc]] BertConfig\n    - all",
    "1180": "一级标题：BERT\n二级标题：BertTokenizer\n内容：\n[[autodoc]] BertTokenizer\n    - build_inputs_with_special_tokens\n    - get_special_tokens_mask\n    - create_token_type_ids_from_sequences\n    - save_vocabulary",
    "1181": "一级标题：BERT\n二级标题：BertTokenizerFast\n内容：\n[[autodoc]] BertTokenizerFast",
    "1182": "一级标题：BERT\n二级标题：BertModel\n内容：\n[[autodoc]] BertModel\n    - forward",
    "1183": "一级标题：BERT\n二级标题：BertForPreTraining\n内容：\n[[autodoc]] BertForPreTraining\n    - forward",
    "1184": "一级标题：BERT\n二级标题：BertLMHeadModel\n内容：\n[[autodoc]] BertLMHeadModel\n    - forward",
    "1185": "一级标题：BERT\n二级标题：BertForMaskedLM\n内容：\n[[autodoc]] BertForMaskedLM\n    - forward",
    "1186": "一级标题：BERT\n二级标题：BertForNextSentencePrediction\n内容：\n[[autodoc]] BertForNextSentencePrediction\n    - forward",
    "1187": "一级标题：BERT\n二级标题：BertForSequenceClassification\n内容：\n[[autodoc]] BertForSequenceClassification\n    - forward",
    "1188": "一级标题：BERT\n二级标题：BertForMultipleChoice\n内容：\n[[autodoc]] BertForMultipleChoice\n    - forward",
    "1189": "一级标题：BERT\n二级标题：BertForTokenClassification\n内容：\n[[autodoc]] BertForTokenClassification\n    - forward",
    "1190": "一级标题：BERT\n二级标题：BertForQuestionAnswering\n内容：\n[[autodoc]] BertForQuestionAnswering\n    - forward",
    "1191": "一级标题：BERT\n二级标题：TFBertTokenizer\n内容：\n[[autodoc]] TFBertTokenizer",
    "1192": "一级标题：BERT\n二级标题：TFBertModel\n内容：\n[[autodoc]] TFBertModel\n    - call",
    "1193": "一级标题：BERT\n二级标题：TFBertForPreTraining\n内容：\n[[autodoc]] TFBertForPreTraining\n    - call",
    "1194": "一级标题：BERT\n二级标题：TFBertModelLMHeadModel\n内容：\n[[autodoc]] TFBertLMHeadModel\n    - call",
    "1195": "一级标题：BERT\n二级标题：TFBertForMaskedLM\n内容：\n[[autodoc]] TFBertForMaskedLM\n    - call",
    "1196": "一级标题：BERT\n二级标题：TFBertForNextSentencePrediction\n内容：\n[[autodoc]] TFBertForNextSentencePrediction\n    - call",
    "1197": "一级标题：BERT\n二级标题：TFBertForSequenceClassification\n内容：\n[[autodoc]] TFBertForSequenceClassification\n    - call",
    "1198": "一级标题：BERT\n二级标题：TFBertForMultipleChoice\n内容：\n[[autodoc]] TFBertForMultipleChoice\n    - call",
    "1199": "一级标题：BERT\n二级标题：TFBertForTokenClassification\n内容：\n[[autodoc]] TFBertForTokenClassification\n    - call",
    "1200": "一级标题：BERT\n二级标题：TFBertForQuestionAnswering\n内容：\n[[autodoc]] TFBertForQuestionAnswering\n    - call",
    "1201": "一级标题：BERT\n二级标题：FlaxBertModel\n内容：\n[[autodoc]] FlaxBertModel\n    - __call__",
    "1202": "一级标题：BERT\n二级标题：FlaxBertForPreTraining\n内容：\n[[autodoc]] FlaxBertForPreTraining\n    - __call__",
    "1203": "一级标题：BERT\n二级标题：FlaxBertForCausalLM\n内容：\n[[autodoc]] FlaxBertForCausalLM\n    - __call__",
    "1204": "一级标题：BERT\n二级标题：FlaxBertForMaskedLM\n内容：\n[[autodoc]] FlaxBertForMaskedLM\n    - __call__",
    "1205": "一级标题：BERT\n二级标题：FlaxBertForNextSentencePrediction\n内容：\n[[autodoc]] FlaxBertForNextSentencePrediction\n    - __call__",
    "1206": "一级标题：BERT\n二级标题：FlaxBertForSequenceClassification\n内容：\n[[autodoc]] FlaxBertForSequenceClassification\n    - __call__",
    "1207": "一级标题：BERT\n二级标题：FlaxBertForMultipleChoice\n内容：\n[[autodoc]] FlaxBertForMultipleChoice\n    - __call__",
    "1208": "一级标题：BERT\n二级标题：FlaxBertForTokenClassification\n内容：\n[[autodoc]] FlaxBertForTokenClassification\n    - __call__",
    "1209": "一级标题：BERT\n二级标题：FlaxBertForQuestionAnswering\n内容：\n[[autodoc]] FlaxBertForQuestionAnswering\n    - __call__",
    "1210": "一级标题：BERT\n二级标题：Bert specific outputs\n内容：\n[[autodoc]] models.bert.modeling_bert.BertForPreTrainingOutput\n\n[[autodoc]] models.bert.modeling_tf_bert.TFBertForPreTrainingOutput\n\n[[autodoc]] models.bert.modeling_flax_bert.FlaxBertForPreTrainingOutput",
    "1211": "一级标题：分享模型\n二级标题：无\n内容：\n最后两个教程展示了如何使用PyTorch、Keras和 🤗 Accelerate进行分布式设置来微调模型。下一步是将您的模型与社区分享！在Hugging Face，我们相信公开分享知识和资源，能实现人工智能的普及化，让每个人都能受益。我们鼓励您将您的模型与社区分享，以帮助他人节省时间和精力。\n\n在本教程中，您将学习两种在[Model Hub](https://huggingface.co/models)上共享训练好的或微调的模型的方法：\n\n- 通过编程将文件推送到Hub。\n- 使用Web界面将文件拖放到Hub。\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/XvSGPZFEjDY\" title=\"YouTube video player\"\nframeborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope;\npicture-in-picture\" allowfullscreen></iframe>\n\n<Tip>\n\n要与社区共享模型，您需要在[huggingface.co](https://huggingface.co/join)上拥有一个帐户。您还可以加入现有的组织或创建一个新的组织。\n\n</Tip>",
    "1212": "一级标题：分享模型\n二级标题：仓库功能\n内容：\nModel Hub上的每个仓库都像是一个典型的GitHub仓库。我们的仓库提供版本控制、提交历史记录以及可视化差异的能力。\n\nModel Hub的内置版本控制基于git和[git-lfs](https://git-lfs.github.com/)。换句话说，您可以将一个模型视为一个仓库，从而实现更好的访问控制和可扩展性。版本控制允许使用*修订*方法来固定特定版本的模型，可以使用提交哈希值、标签或分支来标记。\n\n因此，您可以通过`revision`参数加载特定的模型版本：\n\n```py\n>>> model = AutoModel.from_pretrained(\n...     \"julien-c/EsperBERTo-small\", revision=\"4c77982\"  # tag name, or branch name, or commit hash\n... )\n```\n\n文件也可以轻松地在仓库中编辑，您可以查看提交历史记录以及差异：\n![vis_diff](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vis_diff.png)",
    "1213": "一级标题：分享模型\n二级标题：设置\n内容：\n在将模型共享到Hub之前，您需要拥有Hugging Face的凭证。如果您有访问终端的权限，请在安装🤗 Transformers的虚拟环境中运行以下命令。这将在您的Hugging Face缓存文件夹（默认为`~/.cache/`）中存储您的`access token`：\n\n\n```bash\nhf auth login\n```\n\n如果您正在使用像Jupyter或Colaboratory这样的`notebook`，请确保您已安装了[`huggingface_hub`](https://huggingface.co/docs/hub/adding-a-library)库。该库允许您以编程方式与Hub进行交互。\n\n```bash\npip install huggingface_hub\n```\n然后使用`notebook_login`登录到Hub，并按照[这里](https://huggingface.co/settings/token)的链接生成一个token进行登录：\n\n\n```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```",
    "1214": "一级标题：分享模型\n二级标题：转换模型适用于所有框架\n内容：\n为确保您的模型可以被使用不同框架的人使用，我们建议您将PyTorch和TensorFlow `checkpoints`都转换并上传。如果您跳过此步骤，用户仍然可以从其他框架加载您的模型，但速度会变慢，因为🤗 Transformers需要实时转换`checkpoints`。\n\n为另一个框架转换`checkpoints`很容易。确保您已安装PyTorch和TensorFlow（请参阅[此处](installation)的安装说明），然后在其他框架中找到适合您任务的特定模型。\n\n<frameworkcontent>\n<pt>\n\n指定`from_tf=True`将checkpoint从TensorFlow转换为PyTorch。\n\n```py\n>>> pt_model = DistilBertForSequenceClassification.from_pretrained(\"path/to/awesome-name-you-picked\", from_tf=True)\n>>> pt_model.save_pretrained(\"path/to/awesome-name-you-picked\")\n```\n</pt>\n<tf>\n\n指定`from_pt=True`将checkpoint从PyTorch转换为TensorFlow。\n\n```py\n>>> tf_model = TFDistilBertForSequenceClassification.from_pretrained(\"path/to/awesome-name-you-picked\", from_pt=True)\n```\n\n然后，您可以使用新的checkpoint保存您的新TensorFlow模型：\n\n```py\n>>> tf_model.save_pretrained(\"path/to/awesome-name-you-picked\")\n```\n</tf>\n<jax>\n\n如果模型在Flax中可用，您还可以将PyTorch checkpoint转换为Flax：\n\n```py\n>>> flax_model = FlaxDistilBertForSequenceClassification.from_pretrained(\n...     \"path/to/awesome-name-you-picked\", from_pt=True\n... )\n```\n</jax>\n</frameworkcontent>",
    "1215": "一级标题：分享模型\n二级标题：在训练过程中推送模型\n内容：\n<frameworkcontent>\n<pt>\n<Youtube id=\"Z1-XMy-GNLQ\"/>\n\n将模型分享到Hub就像添加一个额外的参数或回调函数一样简单。请记住，在[微调教程](training)中，`TrainingArguments`类是您指定超参数和附加训练选项的地方。其中一项训练选项包括直接将模型推送到Hub的能力。在您的`TrainingArguments`中设置`push_to_hub=True`：\n\n\n```py\n>>> training_args = TrainingArguments(output_dir=\"my-awesome-model\", push_to_hub=True)\n```\n\n像往常一样将您的训练参数传递给[`Trainer`]：\n\n```py\n>>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     train_dataset=small_train_dataset,\n...     eval_dataset=small_eval_dataset,\n...     compute_metrics=compute_metrics,\n... )\n```\n\n在您微调完模型后，在[`Trainer`]上调用[`~transformers.Trainer.push_to_hub`]将训练好的模型推送到Hub。🤗 Transformers甚至会自动将训练超参数、训练结果和框架版本添加到你的模型卡片中！\n\n```py\n>>> trainer.push_to_hub()\n```\n</pt>\n<tf>\n\n使用[`PushToHubCallback`]将模型分享到Hub。在[`PushToHubCallback`]函数中，添加以下内容：\n\n- 一个用于存储模型的输出目录。\n- 一个tokenizer。\n- `hub_model_id`，即您的Hub用户名和模型名称。\n\n\n```py\n>>> from transformers import PushToHubCallback\n\n>>> push_to_hub_callback = PushToHubCallback(\n...     output_dir=\"./your_model_save_path\", tokenizer=tokenizer, hub_model_id=\"your-username/my-awesome-model\"\n... )\n```\n\n将回调函数添加到 [`fit`](https://keras.io/api/models/model_training_apis/)中，然后🤗 Transformers 会将训练好的模型推送到 Hub：\n\n```py\n>>> model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3, callbacks=push_to_hub_callback)\n```\n</tf>\n</frameworkcontent>",
    "1216": "一级标题：分享模型\n二级标题：使用`push_to_hub`功能\n内容：\n您可以直接在您的模型上调用`push_to_hub`来将其上传到Hub。\n\n在`push_to_hub`中指定你的模型名称：\n\n```py\n>>> pt_model.push_to_hub(\"my-awesome-model\")\n```\n\n这会在您的用户名下创建一个名为`my-awesome-model`的仓库。用户现在可以使用`from_pretrained`函数加载您的模型：\n\n```py\n>>> from transformers import AutoModel\n\n>>> model = AutoModel.from_pretrained(\"your_username/my-awesome-model\")\n```\n\n如果您属于一个组织，并希望将您的模型推送到组织名称下，只需将其添加到`repo_id`中：\n\n```py\n>>> pt_model.push_to_hub(\"my-awesome-org/my-awesome-model\")\n```\n\n`push_to_hub`函数还可以用于向模型仓库添加其他文件。例如，向模型仓库中添加一个`tokenizer`：\n\n```py\n>>> tokenizer.push_to_hub(\"my-awesome-model\")\n```\n\n或者，您可能希望将您的微调后的PyTorch模型的TensorFlow版本添加进去：\n\n```py\n>>> tf_model.push_to_hub(\"my-awesome-model\")\n```\n现在，当您导航到您的Hugging Face个人资料时，您应该看到您新创建的模型仓库。点击**文件**选项卡将显示您已上传到仓库的所有文件。\n\n有关如何创建和上传文件到仓库的更多详细信息，请参考Hub文档[这里](https://huggingface.co/docs/hub/how-to-upstream)。",
    "1217": "一级标题：分享模型\n二级标题：使用Web界面上传\n内容：\n喜欢无代码方法的用户可以通过Hugging Face的Web界面上传模型。访问[huggingface.co/new](https://huggingface.co/new)创建一个新的仓库：\n\n![new_model_repo](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/new_model_repo.png)\n\n从这里开始，添加一些关于您的模型的信息：\n\n- 选择仓库的**所有者**。这可以是您本人或者您所属的任何组织。\n- 为您的项目选择一个名称，该名称也将成为仓库的名称。\n- 选择您的模型是公开还是私有。\n- 指定您的模型的许可证使用情况。\n\n现在点击**文件**选项卡，然后点击**添加文件**按钮将一个新文件上传到你的仓库。接着拖放一个文件进行上传，并添加提交信息。\n\n![upload_file](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/upload_file.png)",
    "1218": "一级标题：分享模型\n二级标题：添加模型卡片\n内容：\n为了确保用户了解您的模型的能力、限制、潜在偏差和伦理考虑，请在仓库中添加一个模型卡片。模型卡片在`README.md`文件中定义。你可以通过以下方式添加模型卡片：\n\n* 手动创建并上传一个`README.md`文件。\n* 在你的模型仓库中点击**编辑模型卡片**按钮。\n\n可以参考DistilBert的[模型卡片](https://huggingface.co/distilbert/distilbert-base-uncased)来了解模型卡片应该包含的信息类型。有关您可以在`README.md`文件中控制的更多选项的细节，例如模型的碳足迹或小部件示例，请参考文档[这里](https://huggingface.co/docs/hub/models-cards)。",
    "1219": "一级标题：用于推理的多语言模型\n二级标题：无\n内容：\n[[open-in-colab]]\n\n🤗 Transformers 中有多种多语言模型，它们的推理用法与单语言模型不同。但是，并非*所有*的多语言模型用法都不同。一些模型，例如 [google-bert/bert-base-multilingual-uncased](https://huggingface.co/google-bert/bert-base-multilingual-uncased) 就可以像单语言模型一样使用。本指南将向您展示如何使用不同用途的多语言模型进行推理。",
    "1220": "一级标题：用于推理的多语言模型\n二级标题：XLM\n内容：\nXLM 有十个不同的检查点，其中只有一个是单语言的。剩下的九个检查点可以归为两类：使用语言嵌入的检查点和不使用语言嵌入的检查点。\n\n### 带有语言嵌入的 XLM\n\n以下 XLM 模型使用语言嵌入来指定推理中使用的语言：\n\n- `FacebookAI/xlm-mlm-ende-1024` （掩码语言建模，英语-德语）\n- `FacebookAI/xlm-mlm-enfr-1024` （掩码语言建模，英语-法语）\n- `FacebookAI/xlm-mlm-enro-1024` （掩码语言建模，英语-罗马尼亚语）\n- `FacebookAI/xlm-mlm-xnli15-1024` （掩码语言建模，XNLI 数据集语言）\n- `FacebookAI/xlm-mlm-tlm-xnli15-1024` （掩码语言建模+翻译，XNLI 数据集语言）\n- `FacebookAI/xlm-clm-enfr-1024` （因果语言建模，英语-法语）\n- `FacebookAI/xlm-clm-ende-1024` （因果语言建模，英语-德语）\n\n语言嵌入被表示一个张量，其形状与传递给模型的 `input_ids` 相同。这些张量中的值取决于所使用的语言，并由分词器的 `lang2id` 和 `id2lang`  属性识别。\n\n在此示例中，加载 `FacebookAI/xlm-clm-enfr-1024` 检查点（因果语言建模，英语-法语）：\n\n```py\n>>> import torch\n>>> from transformers import XLMTokenizer, XLMWithLMHeadModel\n\n>>> tokenizer = XLMTokenizer.from_pretrained(\"FacebookAI/xlm-clm-enfr-1024\")\n>>> model = XLMWithLMHeadModel.from_pretrained(\"FacebookAI/xlm-clm-enfr-1024\")\n```\n\n分词器的 `lang2id` 属性显示了该模型的语言及其对应的id：\n\n```py\n>>> print(tokenizer.lang2id)\n{'en': 0, 'fr': 1}\n```\n\n接下来，创建一个示例输入：\n\n```py\n>>> input_ids = torch.tensor([tokenizer.encode(\"Wikipedia was used to\")])  # batch size 为 1\n```\n\n将语言 id 设置为 `\"en\"` 并用其定义语言嵌入。语言嵌入是一个用 `0` 填充的张量，这个张量应该与 `input_ids` 大小相同。\n\n```py\n>>> language_id = tokenizer.lang2id[\"en\"]  # 0\n>>> langs = torch.tensor([language_id] * input_ids.shape[1])  # torch.tensor([0, 0, 0, ..., 0])\n\n>>> # 我们将其 reshape 为 (batch_size, sequence_length) 大小\n>>> langs = langs.view(1, -1)  # 现在的形状是 [1, sequence_length] (我们的 batch size 为 1)\n```\n\n现在，你可以将 `input_ids` 和语言嵌入传递给模型：\n\n```py\n>>> outputs = model(input_ids, langs=langs)\n```\n\n[run_generation.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-generation/run_generation.py) 脚本可以使用 `xlm-clm` 检查点生成带有语言嵌入的文本。\n\n### 不带语言嵌入的 XLM\n\n以下 XLM 模型在推理时不需要语言嵌入：\n\n- `FacebookAI/xlm-mlm-17-1280` （掩码语言建模，支持 17 种语言）\n- `FacebookAI/xlm-mlm-100-1280` （掩码语言建模，支持 100 种语言）\n\n与之前的 XLM 检查点不同，这些模型用于通用句子表示。",
    "1221": "一级标题：用于推理的多语言模型\n二级标题：BERT\n内容：\n以下 BERT 模型可用于多语言任务：\n\n- `google-bert/bert-base-multilingual-uncased` （掩码语言建模 + 下一句预测，支持 102 种语言）\n- `google-bert/bert-base-multilingual-cased` （掩码语言建模 + 下一句预测，支持 104 种语言）\n\n这些模型在推理时不需要语言嵌入。它们应该能够从上下文中识别语言并进行相应的推理。",
    "1222": "一级标题：用于推理的多语言模型\n二级标题：XLM-RoBERTa\n内容：\n以下 XLM-RoBERTa 模型可用于多语言任务：\n\n- `FacebookAI/xlm-roberta-base` （掩码语言建模，支持 100 种语言）\n- `FacebookAI/xlm-roberta-large` （掩码语言建模，支持 100 种语言）\n\nXLM-RoBERTa 使用 100 种语言的 2.5TB 新创建和清理的 CommonCrawl 数据进行了训练。与之前发布的 mBERT 或 XLM 等多语言模型相比，它在分类、序列标记和问答等下游任务上提供了更强大的优势。",
    "1223": "一级标题：用于推理的多语言模型\n二级标题：M2M100\n内容：\n以下 M2M100 模型可用于多语言翻译：\n\n- `facebook/m2m100_418M` （翻译）\n- `facebook/m2m100_1.2B` （翻译）\n\n在此示例中，加载 `facebook/m2m100_418M` 检查点以将中文翻译为英文。你可以在分词器中设置源语言：\n\n```py\n>>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n\n>>> en_text = \"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\"\n>>> chinese_text = \"不要插手巫師的事務, 因為他們是微妙的, 很快就會發怒.\"\n\n>>> tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\", src_lang=\"zh\")\n>>> model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n```\n\n对文本进行分词：\n\n```py\n>>> encoded_zh = tokenizer(chinese_text, return_tensors=\"pt\")\n```\n\nM2M100 强制将目标语言 id 作为第一个生成的标记，以进行到目标语言的翻译。在 `generate` 方法中将 `forced_bos_token_id` 设置为 `en` 以翻译成英语：\n\n```py\n>>> generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\n>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n'Do not interfere with the matters of the witches, because they are delicate and will soon be angry.'\n```",
    "1224": "一级标题：用于推理的多语言模型\n二级标题：MBart\n内容：\n以下 MBart 模型可用于多语言翻译：\n\n- `facebook/mbart-large-50-one-to-many-mmt` （一对多多语言机器翻译，支持 50 种语言）\n- `facebook/mbart-large-50-many-to-many-mmt` （多对多多语言机器翻译，支持 50 种语言）\n- `facebook/mbart-large-50-many-to-one-mmt` （多对一多语言机器翻译，支持 50 种语言）\n- `facebook/mbart-large-50` （多语言翻译，支持 50 种语言）\n- `facebook/mbart-large-cc25`\n\n在此示例中，加载  `facebook/mbart-large-50-many-to-many-mmt` 检查点以将芬兰语翻译为英语。 你可以在分词器中设置源语言：\n\n```py\n>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n>>> en_text = \"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\"\n>>> fi_text = \"Älä sekaannu velhojen asioihin, sillä ne ovat hienovaraisia ja nopeasti vihaisia.\"\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\", src_lang=\"fi_FI\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n```\n\n对文本进行分词：\n\n```py\n>>> encoded_en = tokenizer(en_text, return_tensors=\"pt\")\n```\n\nMBart 强制将目标语言 id 作为第一个生成的标记，以进行到目标语言的翻译。在 `generate` 方法中将 `forced_bos_token_id` 设置为 `en` 以翻译成英语：\n\n```py\n>>> generated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"])\n>>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n\"Don't interfere with the wizard's affairs, because they are subtle, will soon get angry.\"\n```\n\n如果你使用的是 `facebook/mbart-large-50-many-to-one-mmt` 检查点，则无需强制目标语言 id 作为第一个生成的令牌，否则用法是相同的。",
    "1225": "一级标题：使用 🤗 PEFT 加载adapters\n二级标题：无\n内容：\n[[open-in-colab]]\n\n[参数高效微调（PEFT）方法](https://huggingface.co/blog/peft)在微调过程中冻结预训练模型的参数，并在其顶部添加少量可训练参数（adapters）。adapters被训练以学习特定任务的信息。这种方法已被证明非常节省内存，同时具有较低的计算使用量，同时产生与完全微调模型相当的结果。\n\n使用PEFT训练的adapters通常比完整模型小一个数量级，使其方便共享、存储和加载。\n\n<div class=\"flex flex-col justify-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/PEFT-hub-screenshot.png\"/>\n  <figcaption class=\"text-center\">与完整尺寸的模型权重（约为700MB）相比，存储在Hub上的OPTForCausalLM模型的adapter权重仅为~6MB。</figcaption>\n</div>\n\n如果您对学习更多关于🤗 PEFT库感兴趣，请查看[文档](https://huggingface.co/docs/peft/index)。",
    "1226": "一级标题：使用 🤗 PEFT 加载adapters\n二级标题：设置\n内容：\n首先安装 🤗 PEFT：\n\n```bash\npip install peft\n```\n\n如果你想尝试全新的特性，你可能会有兴趣从源代码安装这个库：\n\n```bash\npip install git+https://github.com/huggingface/peft.git\n```",
    "1227": "一级标题：使用 🤗 PEFT 加载adapters\n二级标题：支持的 PEFT 模型\n内容：\nTransformers原生支持一些PEFT方法，这意味着你可以加载本地存储或在Hub上的adapter权重，并使用几行代码轻松运行或训练它们。以下是受支持的方法：\n\n- [Low Rank Adapters](https://huggingface.co/docs/peft/conceptual_guides/lora)\n- [IA3](https://huggingface.co/docs/peft/conceptual_guides/ia3)\n- [AdaLoRA](https://huggingface.co/papers/2303.10512)\n\n如果你想使用其他PEFT方法，例如提示学习或提示微调，或者关于通用的 🤗 PEFT库，请参阅[文档](https://huggingface.co/docs/peft/index)。",
    "1228": "一级标题：使用 🤗 PEFT 加载adapters\n二级标题：加载 PEFT adapter\n内容：\n要从huggingface的Transformers库中加载并使用PEFTadapter模型，请确保Hub仓库或本地目录包含一个`adapter_config.json`文件和adapter权重，如上例所示。然后，您可以使用`AutoModelFor`类加载PEFT adapter模型。例如，要为因果语言建模加载一个PEFT adapter模型：\n\n1. 指定PEFT模型id\n2. 将其传递给[`AutoModelForCausalLM`]类\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\npeft_model_id = \"ybelkada/opt-350m-lora\"\nmodel = AutoModelForCausalLM.from_pretrained(peft_model_id)\n```\n\n<Tip>\n\n你可以使用`AutoModelFor`类或基础模型类（如`OPTForCausalLM`或`LlamaForCausalLM`）来加载一个PEFT adapter。\n\n\n</Tip>\n\n您也可以通过`load_adapter`方法来加载 PEFT adapter。\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"facebook/opt-350m\"\npeft_model_id = \"ybelkada/opt-350m-lora\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\nmodel.load_adapter(peft_model_id)\n```",
    "1229": "一级标题：使用 🤗 PEFT 加载adapters\n二级标题：基于8bit或4bit进行加载\n内容：\n`bitsandbytes`集成支持8bit和4bit精度数据类型，这对于加载大模型非常有用，因为它可以节省内存（请参阅`bitsandbytes`[指南](./quantization#bitsandbytes-integration)以了解更多信息）。要有效地将模型分配到您的硬件，请在[`~PreTrainedModel.from_pretrained`]中添加`load_in_8bit`或`load_in_4bit`参数，并将`device_map=\"auto\"`设置为：\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\npeft_model_id = \"ybelkada/opt-350m-lora\"\nmodel = AutoModelForCausalLM.from_pretrained(peft_model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True))\n```",
    "1230": "一级标题：使用 🤗 PEFT 加载adapters\n二级标题：添加新的adapter\n内容：\n你可以使用[`~peft.PeftModel.add_adapter`]方法为一个已有adapter的模型添加一个新的adapter，只要新adapter的类型与当前adapter相同即可。例如，如果你有一个附加到模型上的LoRA adapter：\n\n```py\nfrom transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer\nfrom peft import PeftConfig\n\nmodel_id = \"facebook/opt-350m\"\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\nlora_config = LoraConfig(\n    target_modules=[\"q_proj\", \"k_proj\"],\n    init_lora_weights=False\n)\n\nmodel.add_adapter(lora_config, adapter_name=\"adapter_1\")\n```\n\n\n添加一个新的adapter：\n\n```py\n# attach new adapter with same config\nmodel.add_adapter(lora_config, adapter_name=\"adapter_2\")\n```\n现在您可以使用[`~peft.PeftModel.set_adapter`]来设置要使用的adapter。\n\n```py\n# use adapter_1\nmodel.set_adapter(\"adapter_1\")\noutput = model.generate(**inputs)\nprint(tokenizer.decode(output_disabled[0], skip_special_tokens=True))\n\n# use adapter_2\nmodel.set_adapter(\"adapter_2\")\noutput_enabled = model.generate(**inputs)\nprint(tokenizer.decode(output_enabled[0], skip_special_tokens=True))\n```",
    "1231": "一级标题：使用 🤗 PEFT 加载adapters\n二级标题：启用和禁用adapters\n内容：\n一旦您将adapter添加到模型中，您可以启用或禁用adapter模块。要启用adapter模块：\n\n\n```py\nfrom transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer\nfrom peft import PeftConfig\n\nmodel_id = \"facebook/opt-350m\"\nadapter_model_id = \"ybelkada/opt-350m-lora\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntext = \"Hello\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\npeft_config = PeftConfig.from_pretrained(adapter_model_id)\n\n# to initiate with random weights\npeft_config.init_lora_weights = False\n\nmodel.add_adapter(peft_config)\nmodel.enable_adapters()\noutput = model.generate(**inputs)\n```\n要禁用adapter模块：\n\n```py\nmodel.disable_adapters()\noutput = model.generate(**inputs)\n```",
    "1232": "一级标题：使用 🤗 PEFT 加载adapters\n二级标题：训练一个 PEFT adapter\n内容：\nPEFT适配器受[`Trainer`]类支持，因此您可以为您的特定用例训练适配器。它只需要添加几行代码即可。例如，要训练一个LoRA adapter：\n\n\n<Tip>\n\n如果你不熟悉如何使用[`Trainer`]微调模型，请查看[微调预训练模型](training)教程。\n\n</Tip>\n\n1. 使用任务类型和超参数定义adapter配置（参见[`~peft.LoraConfig`]以了解超参数的详细信息）。\n\n```py\nfrom peft import LoraConfig\n\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n```\n\n2. 将adapter添加到模型中。\n\n```py\nmodel.add_adapter(peft_config)\n```\n\n3. 现在可以将模型传递给[`Trainer`]了！\n\n```py\ntrainer = Trainer(model=model, ...)\ntrainer.train()\n```\n\n要保存训练好的adapter并重新加载它：\n\n```py\nmodel.save_pretrained(save_dir)\nmodel = AutoModelForCausalLM.from_pretrained(save_dir)\n```\n\n<!--\nTODO: (@younesbelkada @stevhliu)\n-   Link to PEFT docs for further details\n-   Trainer\n-   8-bit / 4-bit examples ?\n-->",
    "1233": "一级标题：训练用的定制硬件\n二级标题：无\n内容：\n您用来运行模型训练和推断的硬件可能会对性能产生重大影响。要深入了解 GPU，务必查看 Tim Dettmer 出色的[博文](https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/)。\n\n让我们来看一些关于 GPU 配置的实用建议。",
    "1234": "一级标题：训练用的定制硬件\n二级标题：GPU\n内容：\n当你训练更大的模型时，基本上有三种选择：\n\n- 更大的 GPU\n- 更多的 GPU\n- 更多的 CPU 和 NVMe（通过[DeepSpeed-Infinity](main_classes/deepspeed#nvme-support)实现）\n\n让我们从只有一块GPU的情况开始。\n\n### 供电和散热\n\n如果您购买了昂贵的高端GPU，请确保为其提供正确的供电和足够的散热。\n\n**供电**：\n\n一些高端消费者级GPU卡具有2个，有时甚至3个PCI-E-8针电源插口。请确保将与插口数量相同的独立12V PCI-E-8针线缆插入卡中。不要使用同一根线缆两端的2个分叉（也称为pigtail cable）。也就是说，如果您的GPU上有2个插口，您需要使用2条PCI-E-8针线缆连接电源和卡，而不是使用一条末端有2个PCI-E-8针连接器的线缆！否则，您无法充分发挥卡的性能。\n\n每个PCI-E-8针电源线缆需要插入电源侧的12V轨上，并且可以提供最多150W的功率。\n\n其他一些卡可能使用PCI-E-12针连接器，这些连接器可以提供最多500-600W的功率。\n\n低端卡可能使用6针连接器，这些连接器可提供最多75W的功率。\n\n此外，您需要选择具有稳定电压的高端电源。一些质量较低的电源可能无法为卡提供所需的稳定电压以发挥其最大性能。\n\n当然，电源还需要有足够的未使用的瓦数来为卡供电。\n\n**散热**：\n\n当GPU过热时，它将开始降频，不会提供完整的性能。如果温度过高，可能会缩短GPU的使用寿命。\n\n当GPU负载很重时，很难确定最佳温度是多少，但任何低于+80度的温度都是好的，越低越好，也许在70-75度之间是一个非常好的范围。降频可能从大约84-90度开始。但是除了降频外，持续的高温可能会缩短GPU的使用寿命。\n\n接下来让我们看一下拥有多个GPU时最重要的方面之一：连接。\n\n### 多GPU连接\n\n如果您使用多个GPU，则卡之间的互连方式可能会对总训练时间产生巨大影响。如果GPU位于同一物理节点上，您可以运行以下代码：\n\n```bash\nnvidia-smi topo -m\n```\n\n它将告诉您GPU如何互连。在具有双GPU并通过NVLink连接的机器上，您最有可能看到类似以下内容：\n\n```\n        GPU0    GPU1    CPU Affinity    NUMA Affinity\nGPU0     X      NV2     0-23            N/A\nGPU1    NV2      X      0-23            N/A\n```\n\n在不同的机器上，如果没有NVLink，我们可能会看到：\n```\n        GPU0    GPU1    CPU Affinity    NUMA Affinity\nGPU0     X      PHB     0-11            N/A\nGPU1    PHB      X      0-11            N/A\n```\n\n这个报告包括了这个输出：\n\n```\n  X    = Self\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n  PIX  = Connection traversing at most a single PCIe bridge\n  NV#  = Connection traversing a bonded set of # NVLinks\n```\n\n因此，第一个报告`NV2`告诉我们GPU通过2个NVLink互连，而第二个报告`PHB`展示了典型的消费者级PCIe+Bridge设置。\n\n检查你的设置中具有哪种连接类型。其中一些会使卡之间的通信更快（例如NVLink），而其他则较慢（例如PHB）。\n\n根据使用的扩展解决方案的类型，连接速度可能会产生重大或较小的影响。如果GPU很少需要同步，就像在DDP中一样，那么较慢的连接的影响将不那么显著。如果GPU经常需要相互发送消息，就像在ZeRO-DP中一样，那么更快的连接对于实现更快的训练变得非常重要。\n\n\n#### NVlink\n\n[NVLink](https://en.wikipedia.org/wiki/NVLink)是由Nvidia开发的一种基于线缆的串行多通道近程通信链接。\n\n每个新一代提供更快的带宽，例如在[Nvidia Ampere GA102 GPU架构](https://www.nvidia.com/content/dam/en-zz/Solutions/geforce/ampere/pdf/NVIDIA-ampere-GA102-GPU-Architecture-Whitepaper-V1.pdf)中有这样的引述：\n\n> Third-Generation NVLink®\n> GA102 GPUs utilize NVIDIA’s third-generation NVLink interface, which includes four x4 links,\n> with each link providing 14.0625 GB/sec bandwidth in each direction between two GPUs. Four\n> links provide 56.25 GB/sec bandwidth in each direction, and 112.5 GB/sec total bandwidth\n> between two GPUs. Two RTX 3090 GPUs can be connected together for SLI using NVLink.\n> (Note that 3-Way and 4-Way SLI configurations are not supported.)\n\n所以，在`nvidia-smi topo -m`输出的`NVX`报告中获取到的更高的`X`值意味着更好的性能。生成的结果将取决于您的GPU架构。\n\n让我们比较在小样本wikitext上训练gpt2语言模型的执行结果。\n\n结果是：\n\n\n| NVlink | Time |\n| -----  | ---: |\n| Y      | 101s |\n| N      | 131s |\n\n\n可以看到，NVLink使训练速度提高了约23%。在第二个基准测试中，我们使用`NCCL_P2P_DISABLE=1`告诉GPU不要使用NVLink。\n\n这里是完整的基准测试代码和输出：\n\n```bash\n# DDP w/ NVLink\n\nrm -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 torchrun \\\n--nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py --model_name_or_path openai-community/gpt2 \\\n--dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --do_train \\\n--output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200\n\n{'train_runtime': 101.9003, 'train_samples_per_second': 1.963, 'epoch': 0.69}\n\n# DDP w/o NVLink\n\nrm -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 NCCL_P2P_DISABLE=1 torchrun \\\n--nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py --model_name_or_path openai-community/gpt2 \\\n--dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 --do_train\n--output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200\n\n{'train_runtime': 131.4367, 'train_samples_per_second': 1.522, 'epoch': 0.69}\n```\n\n硬件: 2x TITAN RTX 24GB each + NVlink with 2 NVLinks (`NV2` in `nvidia-smi topo -m`)\n软件: `pytorch-1.8-to-be` + `cuda-11.0` / `transformers==4.3.0.dev0`",
    "1235": "一级标题：多GPU推理\n二级标题：无\n内容：\n某些模型现已支持内置的**张量并行**（Tensor Parallelism, TP），并通过 PyTorch 实现。张量并行技术将模型切分到多个 GPU 上，从而支持更大的模型尺寸，并对诸如矩阵乘法等计算任务进行并行化。\n\n要启用张量并行，只需在调用 [`~AutoModelForCausalLM.from_pretrained`] 时传递参数 `tp_plan=\"auto\"`：\n\n```python\nimport os\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n# 初始化分布式环境\nrank = int(os.environ[\"RANK\"])\ndevice = torch.device(f\"cuda:{rank}\")\ntorch.cuda.set_device(device)\ntorch.distributed.init_process_group(\"nccl\", device_id=device)\n\n# 获取支持张量并行的模型\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    tp_plan=\"auto\",\n)\n\n# 准备输入tokens\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nprompt = \"Can I help\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n\n# 分布式运行\noutputs = model(inputs)\n```\n\n您可以使用 `torchrun` 命令启动上述脚本，多进程模式会自动将每个进程映射到一张 GPU：\n\n```\ntorchrun --nproc-per-node 4 demo.py\n```\n\n目前，PyTorch 张量并行支持以下模型：\n* [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel)\n\n如果您希望对其他模型添加张量并行支持，可以通过提交 GitHub Issue 或 Pull Request 来提出请求。\n\n### 预期性能提升\n\n对于推理场景（尤其是处理大批量或长序列的输入），张量并行可以显著提升计算速度。\n\n以下是 [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel) 模型在序列长度为 512 且不同批量大小情况下的单次前向推理的预期加速效果：\n\n<div style=\"text-align: center\">\n<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Meta-Llama-3-8B-Instruct%2C%20seqlen%20%3D%20512%2C%20python%2C%20w_%20compile.png\">\n</div>",
    "1236": "一级标题：使用 torch.compile() 优化推理\n二级标题：无\n内容：\n本指南旨在为使用[`torch.compile()`](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)在[🤗 Transformers中的计算机视觉模型](https://huggingface.co/models?pipeline_tag=image-classification&library=transformers&sort=trending)中引入的推理速度提升提供一个基准。",
    "1237": "一级标题：使用 torch.compile() 优化推理\n二级标题：torch.compile 的优势\n内容：\n根据模型和GPU的不同，`torch.compile()`在推理过程中可以提高多达30%的速度。要使用`torch.compile()`，只需安装2.0及以上版本的`torch`即可。\n\n编译模型需要时间，因此如果您只需要编译一次模型而不是每次推理都编译，那么它非常有用。\n要编译您选择的任何计算机视觉模型，请按照以下方式调用`torch.compile()`：\n\n\n```diff\nfrom transformers import AutoModelForImageClassification\n\nmodel = AutoModelForImageClassification.from_pretrained(MODEL_ID).to(\"cuda\")\n+ model = torch.compile(model)\n```\n\n`compile()` 提供了多种编译模式，它们在编译时间和推理开销上有所不同。`max-autotune` 比 `reduce-overhead` 需要更长的时间，但会得到更快的推理速度。默认模式在编译时最快，但在推理时间上与 `reduce-overhead` 相比效率较低。在本指南中，我们使用了默认模式。您可以在[这里](https://pytorch.org/get-started/pytorch-2.0/#user-experience)了解更多信息。\n\n我们在 PyTorch 2.0.1 版本上使用不同的计算机视觉模型、任务、硬件类型和数据批量大小对 `torch.compile` 进行了基准测试。",
    "1238": "一级标题：使用 torch.compile() 优化推理\n二级标题：基准测试代码\n内容：\n以下是每个任务的基准测试代码。我们在推理之前”预热“GPU，并取300次推理的平均值，每次使用相同的图像。\n\n### 使用 ViT 进行图像分类\n\n```python\nimport torch\nfrom PIL import Image\nimport requests\nimport numpy as np\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification\n\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\nmodel = AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\").to(\"cuda\")\nmodel = torch.compile(model)\n\nprocessed_input = processor(image, return_tensors='pt').to(device=\"cuda\")\n\nwith torch.no_grad():\n    _ = model(**processed_input)\n\n```\n\n#### 使用 DETR 进行目标检测\n\n```python\nfrom transformers import AutoImageProcessor, AutoModelForObjectDetection\n\nprocessor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = AutoModelForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\").to(\"cuda\")\nmodel = torch.compile(model)\n\ntexts = [\"a photo of a cat\", \"a photo of a dog\"]\ninputs = processor(text=texts, images=image, return_tensors=\"pt\").to(\"cuda\")\n\nwith torch.no_grad():\n    _ = model(**inputs)\n```\n\n#### 使用 Segformer 进行图像分割\n\n```python\nfrom transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n\nprocessor = SegformerImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\nmodel = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\").to(\"cuda\")\nmodel = torch.compile(model)\nseg_inputs = processor(images=image, return_tensors=\"pt\").to(\"cuda\")\n\nwith torch.no_grad():\n    _ = model(**seg_inputs)\n```\n\n以下是我们进行基准测试的模型列表。\n\n**图像分类**\n- [google/vit-base-patch16-224](https://huggingface.co/google/vit-base-patch16-224)\n- [microsoft/beit-base-patch16-224-pt22k-ft22k](https://huggingface.co/microsoft/beit-base-patch16-224-pt22k-ft22k)\n- [facebook/convnext-large-224](https://huggingface.co/facebook/convnext-large-224)\n- [microsoft/resnet-50](https://huggingface.co/)\n\n**图像分割**\n- [nvidia/segformer-b0-finetuned-ade-512-512](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512)\n- [facebook/mask2former-swin-tiny-coco-panoptic](https://huggingface.co/facebook/mask2former-swin-tiny-coco-panoptic)\n- [facebook/maskformer-swin-base-ade](https://huggingface.co/facebook/maskformer-swin-base-ade)\n- [google/deeplabv3_mobilenet_v2_1.0_513](https://huggingface.co/google/deeplabv3_mobilenet_v2_1.0_513)\n\n**目标检测**\n- [google/owlvit-base-patch32](https://huggingface.co/google/owlvit-base-patch32)\n- [facebook/detr-resnet-101](https://huggingface.co/facebook/detr-resnet-101)\n- [microsoft/conditional-detr-resnet-50](https://huggingface.co/microsoft/conditional-detr-resnet-50)\n\n 下面是使用和不使用`torch.compile()`的推理持续时间可视化，以及每个模型在不同硬件和数据批量大小下的改进百分比。\n\n\n<div class=\"flex\">\n  <div>\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/a100_batch_comp.png\" />\n  </div>\n  <div>\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/v100_batch_comp.png\" />\n  </div>\n   <div>\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/t4_batch_comp.png\" />\n  </div>\n</div>\n\n<div class=\"flex\">\n  <div>\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/A100_1_duration.png\" />\n  </div>\n  <div>\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/A100_1_percentage.png\" />\n  </div>\n</div>\n\n\n![Duration Comparison on V100 with Batch Size of 1](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/v100_1_duration.png)\n\n![Percentage Improvement on T4 with Batch Size of 4](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/T4_4_percentage.png)\n\n下面可以找到每个模型使用和不使用`compile()`的推理时间（毫秒）。请注意，OwlViT在大批量大小下会导致内存溢出。\n\n### A100 (batch size: 1)\n\n| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |\n|:---:|:---:|:---:|\n| Image Classification/ViT | 9.325 | 7.584 |\n| Image Segmentation/Segformer | 11.759 | 10.500 |\n| Object Detection/OwlViT | 24.978 | 18.420 |\n| Image Classification/BeiT | 11.282 | 8.448 |\n| Object Detection/DETR | 34.619 | 19.040 |\n| Image Classification/ConvNeXT | 10.410 | 10.208 |\n| Image Classification/ResNet | 6.531 | 4.124 |\n| Image Segmentation/Mask2former | 60.188 | 49.117 |\n| Image Segmentation/Maskformer | 75.764 | 59.487 |\n| Image Segmentation/MobileNet | 8.583 | 3.974 |\n| Object Detection/Resnet-101 | 36.276 | 18.197 |\n| Object Detection/Conditional-DETR | 31.219 | 17.993 |\n\n\n### A100 (batch size: 4)\n\n| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |\n|:---:|:---:|:---:|\n| Image Classification/ViT | 14.832 | 14.499 |\n| Image Segmentation/Segformer | 18.838 | 16.476 |\n| Image Classification/BeiT | 13.205 | 13.048 |\n| Object Detection/DETR | 48.657 | 32.418|\n| Image Classification/ConvNeXT | 22.940 | 21.631 |\n| Image Classification/ResNet | 6.657 | 4.268 |\n| Image Segmentation/Mask2former | 74.277 | 61.781 |\n| Image Segmentation/Maskformer | 180.700 | 159.116 |\n| Image Segmentation/MobileNet | 14.174 | 8.515 |\n| Object Detection/Resnet-101 | 68.101 | 44.998 |\n| Object Detection/Conditional-DETR | 56.470 | 35.552 |\n\n### A100 (batch size: 16)\n\n| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |\n|:---:|:---:|:---:|\n| Image Classification/ViT | 40.944 | 40.010 |\n| Image Segmentation/Segformer | 37.005 | 31.144 |\n| Image Classification/BeiT | 41.854 | 41.048 |\n| Object Detection/DETR | 164.382 | 161.902 |\n| Image Classification/ConvNeXT | 82.258 | 75.561 |\n| Image Classification/ResNet | 7.018 | 5.024 |\n| Image Segmentation/Mask2former | 178.945 | 154.814 |\n| Image Segmentation/Maskformer | 638.570 | 579.826 |\n| Image Segmentation/MobileNet | 51.693 | 30.310 |\n| Object Detection/Resnet-101 | 232.887 | 155.021 |\n| Object Detection/Conditional-DETR | 180.491 | 124.032 |\n\n### V100 (batch size: 1)\n\n| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |\n|:---:|:---:|:---:|\n| Image Classification/ViT | 10.495 | 6.00 |\n| Image Segmentation/Segformer | 13.321 | 5.862 |\n| Object Detection/OwlViT | 25.769 | 22.395 |\n| Image Classification/BeiT | 11.347 | 7.234 |\n| Object Detection/DETR | 33.951 | 19.388 |\n| Image Classification/ConvNeXT | 11.623 | 10.412 |\n| Image Classification/ResNet | 6.484 | 3.820 |\n| Image Segmentation/Mask2former | 64.640 | 49.873 |\n| Image Segmentation/Maskformer | 95.532 | 72.207 |\n| Image Segmentation/MobileNet | 9.217 | 4.753 |\n| Object Detection/Resnet-101 | 52.818 | 28.367 |\n| Object Detection/Conditional-DETR | 39.512 | 20.816 |\n\n### V100 (batch size: 4)\n\n| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |\n|:---:|:---:|:---:|\n| Image Classification/ViT | 15.181 | 14.501 |\n| Image Segmentation/Segformer | 16.787 | 16.188 |\n| Image Classification/BeiT | 15.171 | 14.753 |\n| Object Detection/DETR | 88.529 | 64.195 |\n| Image Classification/ConvNeXT | 29.574 | 27.085 |\n| Image Classification/ResNet | 6.109 | 4.731 |\n| Image Segmentation/Mask2former | 90.402 | 76.926 |\n| Image Segmentation/Maskformer | 234.261 | 205.456 |\n| Image Segmentation/MobileNet | 24.623 | 14.816 |\n| Object Detection/Resnet-101 | 134.672 | 101.304 |\n| Object Detection/Conditional-DETR | 97.464 | 69.739 |\n\n### V100 (batch size: 16)\n\n| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |\n|:---:|:---:|:---:|\n| Image Classification/ViT | 52.209 | 51.633 |\n| Image Segmentation/Segformer | 61.013 | 55.499 |\n| Image Classification/BeiT | 53.938 | 53.581  |\n| Object Detection/DETR | OOM | OOM |\n| Image Classification/ConvNeXT | 109.682 | 100.771 |\n| Image Classification/ResNet | 14.857 | 12.089 |\n| Image Segmentation/Mask2former | 249.605 | 222.801 |\n| Image Segmentation/Maskformer | 831.142 | 743.645 |\n| Image Segmentation/MobileNet | 93.129 | 55.365 |\n| Object Detection/Resnet-101 | 482.425 | 361.843 |\n| Object Detection/Conditional-DETR | 344.661 | 255.298 |\n\n### T4 (batch size: 1)\n\n| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |\n|:---:|:---:|:---:|\n| Image Classification/ViT | 16.520 | 15.786 |\n| Image Segmentation/Segformer | 16.116 | 14.205 |\n| Object Detection/OwlViT | 53.634 | 51.105 |\n| Image Classification/BeiT | 16.464 | 15.710 |\n| Object Detection/DETR | 73.100 | 53.99 |\n| Image Classification/ConvNeXT | 32.932 | 30.845 |\n| Image Classification/ResNet | 6.031 | 4.321 |\n| Image Segmentation/Mask2former | 79.192 | 66.815 |\n| Image Segmentation/Maskformer | 200.026 | 188.268 |\n| Image Segmentation/MobileNet | 18.908 | 11.997 |\n| Object Detection/Resnet-101 | 106.622 | 82.566 |\n| Object Detection/Conditional-DETR | 77.594 | 56.984 |\n\n### T4 (batch size: 4)\n\n| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |\n|:---:|:---:|:---:|\n| Image Classification/ViT | 43.653 | 43.626 |\n| Image Segmentation/Segformer | 45.327 | 42.445 |\n| Image Classification/BeiT | 52.007 | 51.354 |\n| Object Detection/DETR | 277.850 | 268.003 |\n| Image Classification/ConvNeXT | 119.259 | 105.580 |\n| Image Classification/ResNet | 13.039 | 11.388 |\n| Image Segmentation/Mask2former | 201.540 | 184.670 |\n| Image Segmentation/Maskformer | 764.052 | 711.280 |\n| Image Segmentation/MobileNet | 74.289 | 48.677 |\n| Object Detection/Resnet-101 | 421.859 | 357.614 |\n| Object Detection/Conditional-DETR | 289.002 | 226.945 |\n\n### T4 (batch size: 16)\n\n| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |\n|:---:|:---:|:---:|\n| Image Classification/ViT | 163.914 | 160.907 |\n| Image Segmentation/Segformer | 192.412 | 163.620 |\n| Image Classification/BeiT | 188.978 | 187.976 |\n| Object Detection/DETR | OOM | OOM |\n| Image Classification/ConvNeXT | 422.886 | 388.078 |\n| Image Classification/ResNet | 44.114 | 37.604 |\n| Image Segmentation/Mask2former | 756.337 | 695.291 |\n| Image Segmentation/Maskformer | 2842.940 | 2656.88 |\n| Image Segmentation/MobileNet | 299.003 | 201.942 |\n| Object Detection/Resnet-101 |  1619.505 | 1262.758 |\n| Object Detection/Conditional-DETR | 1137.513 | 897.390|",
    "1239": "一级标题：使用 torch.compile() 优化推理\n二级标题：PyTorch Nightly\n内容：\n我们还在 PyTorch Nightly 版本（2.1.0dev）上进行了基准测试，可以在[这里](https://download.pytorch.org/whl/nightly/cu118)找到 Nightly 版本的安装包，并观察到了未编译和编译模型的延迟性能改善。\n\n### A100\n\n| **Task/Model** | **Batch Size** | **torch 2.0 - no compile** | **torch 2.0 -<br> compile** |\n|:---:|:---:|:---:|:---:|\n| Image Classification/BeiT | Unbatched | 12.462 | 6.954 |\n| Image Classification/BeiT | 4 | 14.109 | 12.851 |\n| Image Classification/BeiT | 16 | 42.179 | 42.147 |\n| Object Detection/DETR | Unbatched | 30.484 | 15.221 |\n| Object Detection/DETR | 4 | 46.816 | 30.942 |\n| Object Detection/DETR | 16 | 163.749 | 163.706  |\n\n### T4\n\n| **Task/Model** | **Batch Size** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |\n|:---:|:---:|:---:|:---:|\n| Image Classification/BeiT | Unbatched | 14.408 | 14.052 |\n| Image Classification/BeiT | 4 | 47.381 | 46.604 |\n| Image Classification/BeiT | 16 | 42.179 | 42.147  |\n| Object Detection/DETR | Unbatched | 68.382 | 53.481 |\n| Object Detection/DETR | 4 | 269.615 | 204.785 |\n| Object Detection/DETR | 16 | OOM | OOM   |\n\n### V100\n\n| **Task/Model** | **Batch Size** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |\n|:---:|:---:|:---:|:---:|\n| Image Classification/BeiT | Unbatched | 13.477 | 7.926 |\n| Image Classification/BeiT | 4 | 15.103 | 14.378 |\n| Image Classification/BeiT | 16 | 52.517 | 51.691  |\n| Object Detection/DETR | Unbatched | 28.706 | 19.077 |\n| Object Detection/DETR | 4 | 88.402 | 62.949|\n| Object Detection/DETR | 16 | OOM | OOM  |",
    "1240": "一级标题：使用 torch.compile() 优化推理\n二级标题：降低开销\n内容：\n我们在 PyTorch Nightly 版本中为 A100 和 T4 进行了 `reduce-overhead` 编译模式的性能基准测试。\n\n### A100\n\n| **Task/Model** | **Batch Size** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |\n|:---:|:---:|:---:|:---:|\n| Image Classification/ConvNeXT | Unbatched | 11.758 | 7.335 |\n| Image Classification/ConvNeXT | 4 | 23.171 | 21.490 |\n| Image Classification/ResNet | Unbatched | 7.435 | 3.801 |\n| Image Classification/ResNet | 4 | 7.261 | 2.187 |\n| Object Detection/Conditional-DETR | Unbatched | 32.823 | 11.627  |\n| Object Detection/Conditional-DETR | 4 | 50.622 | 33.831  |\n| Image Segmentation/MobileNet | Unbatched | 9.869 | 4.244 |\n| Image Segmentation/MobileNet | 4 | 14.385 | 7.946 |\n\n\n### T4\n\n| **Task/Model** | **Batch Size** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |\n|:---:|:---:|:---:|:---:|\n| Image Classification/ConvNeXT | Unbatched | 32.137 | 31.84 |\n| Image Classification/ConvNeXT | 4 | 120.944 | 110.209 |\n| Image Classification/ResNet | Unbatched | 9.761 | 7.698 |\n| Image Classification/ResNet | 4 | 15.215 | 13.871 |\n| Object Detection/Conditional-DETR | Unbatched | 72.150 | 57.660  |\n| Object Detection/Conditional-DETR | 4 | 301.494 | 247.543  |\n| Image Segmentation/MobileNet | Unbatched | 22.266 | 19.339  |\n| Image Segmentation/MobileNet | 4 | 78.311 | 50.983 |",
    "1241": "一级标题：在CPU上进行高效训练\n二级标题：无\n内容：\n本指南将重点介绍如何在CPU上高效训练大型模型。",
    "1242": "一级标题：在CPU上进行高效训练\n二级标题：使用IPEX进行混合精度训练\n内容：\n混合精度训练在模型中可以同时使用单精度（fp32）和半精度（bf16/fp16）的数据类型来加速训练或推理过程，并且仍然能保留大部分单精度的准确性。现代的CPU，例如第三代、第四代和第五代Intel® Xeon® Scalable处理器，原生支持bf16，而第六代Intel® Xeon® Scalable处理器原生支持bf16和fp16。您在训练时启用bf16或fp16的混合精度训练可以直接提高处理性能。\n\n为了进一步最大化训练性能，您可以使用Intel® PyTorch扩展（IPEX）。IPEX是一个基于PyTorch构建的库，增加了额外的CPU指令集架构（ISA）级别的支持，比如Intel®高级向量扩展512（Intel® AVX512-VNNI）和Intel®高级矩阵扩展（Intel® AMX）。这为Intel CPU提供额外的性能提升。然而，仅支持AVX2的CPU（例如AMD或较旧的Intel CPU）在使用IPEX时并不保证能提高性能。\n\n从PyTorch 1.10版本起，CPU后端已经启用了自动混合精度（AMP）。IPEX还支持bf16/fp16的AMP和bf16/fp16算子优化，并且部分功能已经上游到PyTorch主分支。通过IPEX AMP，您可以获得更好的性能和用户体验。\n\n点击[这里](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/features/amp.html)查看**自动混合精度**的更多详细信息。\n\n\n### IPEX 安装:\n\nIPEX 的发布与 PyTorch 一致，您可以通过 pip 安装：\n\n| PyTorch Version   | IPEX version   |\n| :---------------: | :----------:   |\n| 2.5.0             |  2.5.0+cpu     |\n| 2.4.0             |  2.4.0+cpu     |\n| 2.3.0             |  2.3.0+cpu     |\n| 2.2.0             |  2.2.0+cpu     |\n\n请运行 `pip list | grep torch` 以获取您的 `pytorch_version`，然后根据该版本安装相应的 `IPEX version_name`。\n```bash\npip install intel_extension_for_pytorch==<version_name> -f https://developer.intel.com/ipex-whl-stable-cpu\n```\n\n如果需要的话，您可以在 [ipex-whl-stable-cpu](https://developer.intel.com/ipex-whl-stable-cpu) 查看最新版本。\n\n查看更多 [安装IPEX](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/installation.html) 的方法。\n\n\n### 在 Trainer 中使用 IPEX\n在 Trainer 中使用 IPEX 时，您应在训练命令参数中添加 `use_ipex`、`bf16` 或 `fp16` 以及 `no_cuda` 来启用自动混合精度。\n\n以 [Transformers 问答任务](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering)为例：\n\n- 在 CPU 上使用 BF16 自动混合精度训练 IPEX 的示例如下：\n<pre> python examples/pytorch/question-answering/run_qa.py \\\n--model_name_or_path google-bert/bert-base-uncased \\\n--dataset_name squad \\\n--do_train \\\n--do_eval \\\n--per_device_train_batch_size 12 \\\n--learning_rate 3e-5 \\\n--num_train_epochs 2 \\\n--max_seq_length 384 \\\n--doc_stride 128 \\\n--output_dir /tmp/debug_squad/ \\\n<b>--use_ipex</b> \\\n<b>--bf16</b> \\\n<b>--use_cpu</b></pre>\n\n如果您想在脚本中启用 `use_ipex` 和 `bf16`，请像下面这样将这些参数添加到 `TrainingArguments` 中：\n```diff\ntraining_args = TrainingArguments(\n    output_dir=args.output_path,\n+   bf16=True,\n+   use_ipex=True,\n+   use_cpu=True,\n    **kwargs\n)\n```\n\n### 实践示例\n\n博客: [使用 Intel Sapphire Rapids 加速 PyTorch Transformers](https://huggingface.co/blog/intel-sapphire-rapids)",
    "1243": "一级标题：在 Apple Silicon 芯片上进行 PyTorch 训练\n二级标题：无\n内容：\n之前，在 Mac 上训练模型仅限于使用 CPU 训练。不过随着PyTorch v1.12的发布，您可以通过在 Apple Silicon 芯片的 GPU 上训练模型来显著提高性能和训练速度。这是通过将 Apple 的 Metal 性能着色器 (Metal Performance Shaders, MPS) 作为后端集成到PyTorch中实现的。[MPS后端](https://pytorch.org/docs/stable/notes/mps.html) 将 PyTorch 操作视为自定义的 Metal 着色器来实现，并将对应模块部署到`mps`设备上。\n\n<Tip warning={true}>\n\n某些 PyTorch 操作目前还未在 MPS 上实现，可能会抛出错误提示。可以通过设置环境变量`PYTORCH_ENABLE_MPS_FALLBACK=1`来使用CPU内核以避免这种情况发生（您仍然会看到一个`UserWarning`）。\n\n<br>\n\n如果您遇到任何其他错误，请在[PyTorch库](https://github.com/pytorch/pytorch/issues)中创建一个 issue，因为[`Trainer`]类中只集成了 MPS 后端.\n\n</Tip>\n\n配置好`mps`设备后，您可以：\n\n* 在本地训练更大的网络或更大的批量大小\n* 降低数据获取延迟，因为 GPU 的统一内存架构允许直接访问整个内存存储\n* 降低成本，因为您不需要再在云端 GPU 上训练或增加额外的本地 GPU\n\n在确保已安装PyTorch后就可以开始使用了。 MPS 加速支持macOS 12.3及以上版本。\n\n```bash\npip install torch torchvision torchaudio\n```\n\n[`TrainingArguments`]类默认使用`mps`设备(如果可用)因此无需显式设置设备。例如，您可以直接运行[run_glue.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py)脚本，在无需进行任何修改的情况下自动启用 MPS 后端。\n\n```diff\nexport TASK_NAME=mrpc\n\npython examples/pytorch/text-classification/run_glue.py \\\n  --model_name_or_path google-bert/bert-base-cased \\\n  --task_name $TASK_NAME \\\n- --use_mps_device \\\n  --do_train \\\n  --do_eval \\\n  --max_seq_length 128 \\\n  --per_device_train_batch_size 32 \\\n  --learning_rate 2e-5 \\\n  --num_train_epochs 3 \\\n  --output_dir /tmp/$TASK_NAME/ \\\n  --overwrite_output_dir\n```\n\n用于[分布式设置](https://pytorch.org/docs/stable/distributed.html#backends)的后端(如`gloo`和`nccl`)不支持`mps`设备，这也意味着使用 MPS 后端时只能在单个 GPU 上进行训练。\n\n您可以在[Introducing Accelerated PyTorch Training on Mac](https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/)博客文章中了解有关 MPS 后端的更多信息。",
    "1244": "一级标题：性能与可扩展性\n二级标题：无\n内容：\n训练大型transformer模型并将其部署到生产环境会面临各种挑战。\n在训练过程中，模型可能需要比可用的GPU内存更多的资源，或者表现出较慢的训练速度。在部署阶段，模型可能在生产环境中难以处理所需的吞吐量。\n\n本文档旨在帮助您克服这些挑战，并找到适合您使用场景的最佳设置。教程分为训练和推理部分，因为每个部分都有不同的挑战和解决方案。在每个部分中，您将找到针对不同硬件配置的单独指南，例如单GPU与多GPU用于训练或CPU与GPU用于推理。\n\n将此文档作为您的起点，进一步导航到与您的情况匹配的方法。",
    "1245": "一级标题：性能与可扩展性\n二级标题：训练\n内容：\n高效训练大型transformer模型需要使用加速器硬件，如GPU或TPU。最常见的情况是您只有一个GPU。您应用于单个GPU上提高训练效率的方法可以扩展到其他设置，如多个GPU。然而，也有一些特定于多GPU或CPU训练的技术。我们在单独的部分中介绍它们。\n\n* [在单个GPU上进行高效训练的方法和工具](perf_train_gpu_one)：从这里开始学习常见的方法，可以帮助优化GPU内存利用率、加快训练速度或两者兼备。\n* [多GPU训练部分](perf_train_gpu_many)：探索此部分以了解适用于多GPU设置的进一步优化方法，例如数据并行、张量并行和流水线并行。\n* [CPU训练部分](perf_train_cpu)：了解在CPU上的混合精度训练。\n* [在多个CPU上进行高效训练](perf_train_cpu_many)：了解分布式CPU训练。\n* [使用TensorFlow在TPU上进行训练](perf_train_tpu_tf)：如果您对TPU还不熟悉，请参考此部分，了解有关在TPU上进行训练和使用XLA的建议性介绍。\n* [自定义硬件进行训练](perf_hardware)：在构建自己的深度学习机器时查找技巧和窍门。\n* [使用Trainer API进行超参数搜索](hpo_train)",
    "1246": "一级标题：性能与可扩展性\n二级标题：推理\n内容：\n在生产环境中对大型模型进行高效推理可能与训练它们一样具有挑战性。在接下来的部分中，我们将详细介绍如何在CPU和单/多GPU设置上进行推理的步骤。\n\n* [在单个CPU上进行推理](perf_infer_cpu)\n* [在单个GPU上进行推理](perf_infer_gpu_one)\n* [多GPU推理](perf_infer_gpu_one)\n* [TensorFlow模型的XLA集成](tf_xla)",
    "1247": "一级标题：性能与可扩展性\n二级标题：训练和推理\n内容：\n在这里，您将找到适用于训练模型或使用它进行推理的技巧、窍门和技巧。\n\n* [实例化大型模型](big_models)\n* [解决性能问题](debugging)",
    "1248": "一级标题：性能与可扩展性\n二级标题：贡献\n内容：\n这份文档还远远没有完成，还有很多需要添加的内容，所以如果你有补充或更正的内容，请毫不犹豫地提交一个PR（Pull Request），或者如果你不确定，可以创建一个Issue，我们可以在那里讨论细节。\n\n在做出贡献时，如果A比B更好，请尽量包含可重复的基准测试和(或)该信息来源的链接（除非它直接来自您）。",
    "1249": "一级标题：Transformers 的设计理念\n二级标题：无\n内容：\n🤗 Transformers 是一个专为以下用户群体构建的库：\n\n- 寻求使用、研究或扩展大规模 Transformers 模型的机器学习研究人员和教育者。\n- 希望微调这些模型或在生产环境中使用它们（或两者兼而有之）的实际操作者。\n- 只想下载预训练模型并将其用于解决给定机器学习任务的工程师。\n\nTransformers 设计时有两个主要目标：\n\n1. 尽可能简单快速地使用：\n\n   - 我们尽可能地限制用户能接触的抽象层，实际上几乎没有抽象。用户只需学习三个标准类即可使用每个模型：[configuration](main_classes/configuration)、[models](main_classes/model) 和一个预处理类（用于 NLP 的 [tokenizer](main_classes/tokenizer)，用于视觉的 [image processor](main_classes/image_processor)，用于音频的 [feature extractor](main_classes/feature_extractor)，以及用于多模态输入的 [processor](main_classes/processors)）。\n   - 所有这些类都可以通过一个通用的 `from_pretrained()` 方法从预训练实例中简单统一地初始化，该方法会从提供在 [Hugging Face Hub](https://huggingface.co/models) 上的预训练检查点（如果需要的话）下载、缓存和加载相关类实例及相关数据（配置的超参数、分词器的词汇表和模型的权重）。\n   - 在这三个基本类之上，该库提供了两种 API：[`pipeline`] 用于快速在给定任务上使用模型进行推断，以及 [`Trainer`] 用于快速训练或微调 PyTorch 模型（所有 TensorFlow 模型与 `Keras.fit` 兼容）。\n   - 因此，Transformers 不是神经网络的模块化工具箱。如果要基于 Transformers 扩展或搭建新项目，请使用常规的 Python、PyTorch、TensorFlow、Keras 模块，并从 Transformers 的基类继承以重用模型加载和保存等功能。如果想了解更多有关我们的模型代码的设计理念，请查看我们的[重复自己](https://huggingface.co/blog/transformers-design-philosophy)博文。\n\n2. 提供与原始模型性能尽可能接近的最新模型：\n\n   - 我们为每种架构提供至少一个示例，复现了该架构官方作者提供的结果。\n   - 代码通常尽可能接近原始代码库，这意味着某些 PyTorch 代码可能不够*pytorchic*，因为它是转换后的 TensorFlow 代码，反之亦然。\n\n其他几个目标：\n\n- 尽可能一致地公开模型的内部：\n\n   - 我们使用单一 API 提供对完整隐藏状态和注意力权重的访问。\n   - 预处理类和基本模型 API 标准化，便于在不同模型之间轻松切换。\n\n- 结合主观选择的有前途的工具进行模型微调和调查：\n\n   - 简单一致的方法来向词汇表和嵌入中添加新标记以进行微调。\n   - 简单的方法来屏蔽和修剪 Transformer 头部。\n\n- 轻松在 PyTorch、TensorFlow 2.0 和 Flax 之间切换，允许使用一个框架进行训练并使用另一个进行推断。",
    "1250": "一级标题：Transformers 的设计理念\n二级标题：主要概念\n内容：\n该库围绕每个模型的三类类构建：\n\n- **模型类** 可以是 PyTorch 模型（[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)）、Keras 模型（[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)）或 JAX/Flax 模型（[flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)），这些模型可以使用库中提供的预训练权重。\n- **配置类** 存储构建模型所需的超参数（如层数和隐藏大小）。通常情况下，如果您使用不进行任何修改的预训练模型，则创建模型将自动处理配置的实例化（配置是模型的一部分）。\n- **预处理类** 将原始数据转换为模型可接受的格式。一个 [tokenizer](main_classes/tokenizer) 存储每个模型的词汇表，并提供编码和解码字符串为要馈送到模型的令牌嵌入索引列表的方法。[Image processors](main_classes/image_processor) 预处理视觉输入，[feature extractors](main_classes/feature_extractor) 预处理音频输入，而 [processor](main_classes/processors) 则处理多模态输入。\n\n所有这些类都可以从预训练实例中实例化、本地保存，并通过以下三种方法与 Hub 共享：\n\n- `from_pretrained()` 允许您从库自身提供的预训练版本（支持的模型可在 [Model Hub](https://huggingface.co/models) 上找到）或用户本地（或服务器上）存储的版本实例化模型、配置和预处理类。\n- `save_pretrained()` 允许您本地保存模型、配置和预处理类，以便可以使用 `from_pretrained()` 重新加载。\n- `push_to_hub()` 允许您将模型、配置和预处理类共享到 Hub，以便所有人都可以轻松访问。",
    "1251": "一级标题：推理pipeline\n二级标题：无\n内容：\n[`pipeline`] 让使用[Hub](https://huggingface.co/models)上的任何模型进行任何语言、计算机视觉、语音以及多模态任务的推理变得非常简单。即使您对特定的模态没有经验，或者不熟悉模型的源码，您仍然可以使用[`pipeline`]进行推理！本教程将教您：\n\n- 如何使用[`pipeline`] 进行推理。\n- 如何使用特定的`tokenizer`(分词器)或模型。\n- 如何使用[`pipeline`] 进行音频、视觉和多模态任务的推理。\n\n<Tip>\n\n请查看[`pipeline`]文档以获取已支持的任务和可用参数的完整列表。\n\n</Tip>",
    "1252": "一级标题：推理pipeline\n二级标题：Pipeline使用\n内容：\n虽然每个任务都有一个关联的[`pipeline`]，但使用通用的抽象的[`pipeline`]更加简单，其中包含所有特定任务的`pipelines`。[`pipeline`]会自动加载一个默认模型和一个能够进行任务推理的预处理类。让我们以使用[`pipeline`]进行自动语音识别（ASR）或语音转文本为例。\n\n1. 首先，创建一个[`pipeline`]并指定推理任务：\n\n```py\n>>> from transformers import pipeline\n\n>>> transcriber = pipeline(task=\"automatic-speech-recognition\")\n```\n\n2. 将您的输入传递给[`pipeline`]。对于语音识别，这通常是一个音频输入文件：\n\n\n```py\n>>> transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n{'text': 'I HAVE A DREAM BUT ONE DAY THIS NATION WILL RISE UP LIVE UP THE TRUE MEANING OF ITS TREES'}\n```\n\n您没有得到您期望的结果？可以在Hub上查看一些[最受欢迎的自动语音识别模型](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=trending)\n，看看是否可以获得更好的转录。\n\n让我们尝试来自 OpenAI 的[Whisper large-v2](https://huggingface.co/openai/whisper-large) 模型。Whisperb比Wav2Vec2晚2年发布，使用接近10倍的数据进行了训练。因此，它在大多数下游基准测试上击败了Wav2Vec2。\n它还具有预测标点和大小写的附加优势，而Wav2Vec2则无法实现这些功能。\n\n让我们在这里尝试一下，看看它的表现如何：\n\n\n```py\n>>> transcriber = pipeline(model=\"openai/whisper-large-v2\")\n>>> transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}\n```\n\n现在这个结果看起来更准确了！要进行深入的Wav2Vec2与Whisper比较，请参阅[音频变换器课程](https://huggingface.co/learn/audio-course/chapter5/asr_models)。\n我们鼓励您在 Hub 上查看不同语言的模型，以及专业领域的模型等。您可以在Hub上直接查看并比较模型的结果，以确定是否适合或处理边缘情况是否比其他模型更好。如果您没有找到适用于您的用例的模型，您始终可以[训练](training)自己的模型！\n\n如果您有多个输入，您可以将输入作为列表传递：\n\n\n```py\ntranscriber(\n    [\n        \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\",\n        \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac\",\n    ]\n)\n```\n\n`Pipelines`非常适合用于测试，因为从一个模型切换到另一个模型非常琐碎；但是，还有一些方法可以将它们优化后用于大型工作负载而不仅仅是测试。请查看以下指南，深入探讨如何迭代整个数据集或在Web服务器中使用`Pipelines`：\n* [在数据集上使用流水线](#using-pipelines-on-a-dataset)\n* [在Web服务器中使用流水线](./pipeline_webserver)",
    "1253": "一级标题：推理pipeline\n二级标题：参数\n内容：\n[`pipeline`] 支持许多参数；有些是适用于特定任务的，而有些适用于所有`pipeline`。通常情况下，您可以在任何地方指定对应参数：\n\n\n```py\ntranscriber = pipeline(model=\"openai/whisper-large-v2\", my_parameter=1)\n\nout = transcriber(...)  # This will use `my_parameter=1`.\nout = transcriber(..., my_parameter=2)  # This will override and use `my_parameter=2`.\nout = transcriber(...)  # This will go back to using `my_parameter=1`.\n```\n\n让我们查看其中的三个重要参数：\n\n\n### 设备\n\n如果您使用 `device=n`，`pipeline`会自动将模型放在指定的设备上。无论您使用PyTorch还是Tensorflow，这都可以工作。\n\n\n```py\ntranscriber = pipeline(model=\"openai/whisper-large-v2\", device=0)\n```\n\n如果模型对于单个GPU来说过于庞大，并且您正在使用PyTorch，您可以设置 `device_map=\"auto\"` 以自动确定如何加载和存储模型权重。使用 `device_map` 参数需要安装🤗 [Accelerate](https://huggingface.co/docs/accelerate) 软件包：\n\n\n```bash\npip install --upgrade accelerate\n```\n\n以下代码会自动在各个设备上加载和存储模型权重：\n\n\n```py\ntranscriber = pipeline(model=\"openai/whisper-large-v2\", device_map=\"auto\")\n```\n\n请注意，如果传递了 `device_map=\"auto\"`，在实例化您的 `pipeline` 时不需要添加 `device=device` 参数，否则可能会遇到一些意外的状况！\n\n### 批量大小\n\n默认情况下，`pipelines`不会进行批量推理，原因在[这里](https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching)详细解释。因为批处理不一定更快，实际上在某些情况下可能会更慢。\n\n但如果在您的用例中起作用，您可以使用：\n\n\n```py\ntranscriber = pipeline(model=\"openai/whisper-large-v2\", device=0, batch_size=2)\naudio_filenames = [f\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/{i}.flac\" for i in range(1, 5)]\ntexts = transcriber(audio_filenames)\n```\n\n以上代码会在提供的4个音频文件上运行`pipeline`，它会将它们以2个一组的批次传递给模型（模型在GPU上，此时批处理更有可能有所帮助），而您无需编写额外的代码。输出应始终与没有批处理时收到的结果相一致。它只是一种帮助您更快地使用`pipeline`的方式。\n\n`pipeline`也可以减轻一些批处理的复杂性，因为对于某些`pipeline`，需要将单个项目（如长音频文件）分成多个部分以供模型处理。`pipeline`为您执行这种[*chunk batching*](./main_classes/pipelines#pipeline-chunk-batching)。\n\n### 任务特定参数\n\n所有任务都提供了特定于任务的参数，这些参数提供额外的灵活性和选择，以帮助您完成工作。\n例如，[`transformers.AutomaticSpeechRecognitionPipeline.__call__`] 方法具有一个 `return_timestamps` 参数，对于字幕视频似乎很有帮助：\n\n```py\n>>> transcriber = pipeline(model=\"openai/whisper-large-v2\", return_timestamps=True)\n>>> transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.', 'chunks': [{'timestamp': (0.0, 11.88), 'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its'}, {'timestamp': (11.88, 12.38), 'text': ' creed.'}]}\n```\n\n正如您所看到的，模型推断出了文本，还输出了各个句子发音的**时间**。\n\n每个任务都有许多可用的参数，因此请查看每个任务的API参考，以了解您可以进行哪些调整！例如，[`~transformers.AutomaticSpeechRecognitionPipeline`] 具有 `chunk_length_s` 参数，对于处理非常长的音频文件（例如，为整部电影或长达一小时的视频配字幕）非常有帮助，这通常是模型无法单独处理的：\n\n```python\n>>> transcriber = pipeline(model=\"openai/whisper-large-v2\", chunk_length_s=30, return_timestamps=True)\n>>> transcriber(\"https://huggingface.co/datasets/sanchit-gandhi/librispeech_long/resolve/main/audio.wav\")\n{'text': \" Chapter 16. I might have told you of the beginning of this liaison in a few lines, but I wanted you to see every step by which we came.  I, too, agree to whatever Marguerite wished, Marguerite to be unable to live apart from me. It was the day after the evening...\n```\n\n如果您找不到一个真正有帮助的参数，欢迎[提出请求](https://github.com/huggingface/transformers/issues/new?assignees=&labels=feature&template=feature-request.yml)！",
    "1254": "一级标题：推理pipeline\n二级标题：在数据集上使用pipelines\n内容：\n`pipelines`也可以对大型数据集进行推理。我们建议使用迭代器来完成这一任务，这是最简单的方法：\n\n\n```py\ndef data():\n    for i in range(1000):\n        yield f\"My example {i}\"\n\n\npipe = pipeline(model=\"openai-community/gpt2\", device=0)\ngenerated_characters = 0\nfor out in pipe(data()):\n    generated_characters += len(out[0][\"generated_text\"])\n```\n\n迭代器 `data()` 会产生每个结果，`pipelines`会自动识别输入为可迭代对象，并在GPU上处理数据的同时开始获取数据（在底层使用[DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)）。这一点非常重要，因为您不必为整个数据集分配内存，可以尽可能快地将数据传送到GPU。\n\n由于批处理可以加速处理，因此在这里尝试调整 `batch_size` 参数可能会很有用。\n\n迭代数据集的最简单方法就是从🤗 [Datasets](https://github.com/huggingface/datasets/) 中加载数据集：\n\n\n```py\n# KeyDataset is a util that will just output the item we're interested in.\nfrom transformers.pipelines.pt_utils import KeyDataset\nfrom datasets import load_dataset\n\npipe = pipeline(model=\"hf-internal-testing/tiny-random-wav2vec2\", device=0)\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation[:10]\")\n\nfor out in pipe(KeyDataset(dataset, \"audio\")):\n    print(out)\n```",
    "1255": "一级标题：推理pipeline\n二级标题：在Web服务器上使用pipelines\n内容：\n<Tip>\n创建推理引擎是一个复杂的主题，值得有自己的页面。\n</Tip>\n\n[链接](./pipeline_webserver)",
    "1256": "一级标题：推理pipeline\n二级标题：视觉流水线\n内容：\n对于视觉任务，使用[`pipeline`] 几乎是相同的。\n\n指定您的任务并将图像传递给分类器。图像可以是链接、本地路径或base64编码的图像。例如，下面显示的是哪种品种的猫？\n\n![pipeline-cat-chonk](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg)\n```py\n>>> from transformers import pipeline\n\n>>> vision_classifier = pipeline(model=\"google/vit-base-patch16-224\")\n>>> preds = vision_classifier(\n...     images=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n... )\n>>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n>>> preds\n[{'score': 0.4335, 'label': 'lynx, catamount'}, {'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}, {'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}, {'score': 0.0239, 'label': 'Egyptian cat'}, {'score': 0.0229, 'label': 'tiger cat'}]\n```",
    "1257": "一级标题：推理pipeline\n二级标题：文本流水线\n内容：\n对于NLP任务，使用[`pipeline`] 几乎是相同的。\n\n\n```py\n>>> from transformers import pipeline\n\n>>> # This model is a `zero-shot-classification` model.\n>>> # It will classify text, except you are free to choose any label you might imagine\n>>> classifier = pipeline(model=\"facebook/bart-large-mnli\")\n>>> classifier(\n...     \"I have a problem with my iphone that needs to be resolved asap!!\",\n...     candidate_labels=[\"urgent\", \"not urgent\", \"phone\", \"tablet\", \"computer\"],\n... )\n{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!', 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'], 'scores': [0.504, 0.479, 0.013, 0.003, 0.002]}\n```",
    "1258": "一级标题：推理pipeline\n二级标题：多模态流水线\n内容：\n[`pipeline`] 支持多个模态。例如，视觉问题回答（VQA）任务结合了文本和图像。请随意使用您喜欢的任何图像链接和您想要问关于该图像的问题。图像可以是URL或图像的本地路径。\n\n例如，如果您使用这个[invoice image](https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png)：\n\n\n```py\n>>> from transformers import pipeline\n\n>>> vqa = pipeline(model=\"impira/layoutlm-document-qa\")\n>>> output = vqa(\n...     image=\"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png\",\n...     question=\"What is the invoice number?\",\n... )\n>>> output[0][\"score\"] = round(output[0][\"score\"], 3)\n>>> output\n[{'score': 0.425, 'answer': 'us-001', 'start': 16, 'end': 16}]\n```\n\n<Tip>\n\n要运行上面的示例，除了🤗 Transformers之外，您需要安装[`pytesseract`](https://pypi.org/project/pytesseract/)。\n\n\n```bash\nsudo apt install -y tesseract-ocr\npip install pytesseract\n```\n\n</Tip>",
    "1259": "一级标题：推理pipeline\n二级标题：在大模型上使用🤗 `accelerate`和`pipeline`：\n内容：\n您可以轻松地使用🤗 `accelerate`在大模型上运行 `pipeline`！首先确保您已经使用 `pip install accelerate` 安装了 `accelerate`。\n\n首先使用 `device_map=\"auto\"` 加载您的模型！我们将在示例中使用 `facebook/opt-1.3b`。\n\n\n```py\n# pip install accelerate\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(model=\"facebook/opt-1.3b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\noutput = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n```\n\n如果安装 `bitsandbytes` 并添加参数 `load_in_8bit=True`，您还可以传递8位加载的模型。\n\n\n```py\n# pip install accelerate bitsandbytes\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(model=\"facebook/opt-1.3b\", device_map=\"auto\", model_kwargs={\"load_in_8bit\": True})\noutput = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n```\n\n请注意，您可以将`checkpoint `替换为任何支持大模型加载的Hugging Face模型，比如BLOOM！",
    "1260": "一级标题：预处理\n二级标题：无\n内容：\n[[open-in-colab]]\n\n在您可以在数据集上训练模型之前，数据需要被预处理为期望的模型输入格式。无论您的数据是文本、图像还是音频，它们都需要被转换并组合成批量的张量。🤗 Transformers 提供了一组预处理类来帮助准备数据以供模型使用。在本教程中，您将了解以下内容：\n\n* 对于文本，使用[分词器](./main_classes/tokenizer)(`Tokenizer`)将文本转换为一系列标记(`tokens`)，并创建`tokens`的数字表示，将它们组合成张量。\n* 对于语音和音频，使用[特征提取器](./main_classes/feature_extractor)(`Feature extractor`)从音频波形中提取顺序特征并将其转换为张量。\n* 图像输入使用[图像处理器](./main_classes/image)(`ImageProcessor`)将图像转换为张量。\n* 多模态输入，使用[处理器](./main_classes/processors)(`Processor`)结合了`Tokenizer`和`ImageProcessor`或`Processor`。\n\n<Tip>\n\n`AutoProcessor` **始终**有效的自动选择适用于您使用的模型的正确`class`，无论您使用的是`Tokenizer`、`ImageProcessor`、`Feature extractor`还是`Processor`。\n\n</Tip>\n\n在开始之前，请安装🤗 Datasets，以便您可以加载一些数据集来进行实验：\n\n\n```bash\npip install datasets\n```",
    "1261": "一级标题：预处理\n二级标题：自然语言处理\n内容：\n<Youtube id=\"Yffk5aydLzg\"/>\n\n处理文本数据的主要工具是[Tokenizer](main_classes/tokenizer)。`Tokenizer`根据一组规则将文本拆分为`tokens`。然后将这些`tokens`转换为数字，然后转换为张量，成为模型的输入。模型所需的任何附加输入都由`Tokenizer`添加。\n\n<Tip>\n\n如果您计划使用预训练模型，重要的是使用与之关联的预训练`Tokenizer`。这确保文本的拆分方式与预训练语料库相同，并在预训练期间使用相同的标记-索引的对应关系（通常称为*词汇表*-`vocab`）。\n\n</Tip>\n\n开始使用[`AutoTokenizer.from_pretrained`]方法加载一个预训练`tokenizer`。这将下载模型预训练的`vocab`：\n\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n```\n\n然后将您的文本传递给`tokenizer`：\n\n\n```py\n>>> encoded_input = tokenizer(\"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n>>> print(encoded_input)\n{'input_ids': [101, 2079, 2025, 19960, 10362, 1999, 1996, 3821, 1997, 16657, 1010, 2005, 2027, 2024, 11259, 1998, 4248, 2000, 4963, 1012, 102],\n 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n```\n\n`tokenizer`返回一个包含三个重要对象的字典：\n\n* [input_ids](glossary#input-ids) 是与句子中每个`token`对应的索引。\n* [attention_mask](glossary#attention-mask) 指示是否应该关注一个`token`。\n* [token_type_ids](glossary#token-type-ids) 在存在多个序列时标识一个`token`属于哪个序列。\n\n通过解码 `input_ids` 来返回您的输入：\n\n\n```py\n>>> tokenizer.decode(encoded_input[\"input_ids\"])\n'[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]'\n```\n\n如您所见，`tokenizer`向句子中添加了两个特殊`token` - `CLS` 和 `SEP`（分类器和分隔符）。并非所有模型都需要特殊`token`，但如果需要，`tokenizer`会自动为您添加。\n\n如果有多个句子需要预处理，将它们作为列表传递给`tokenizer`：\n\n\n```py\n>>> batch_sentences = [\n...     \"But what about second breakfast?\",\n...     \"Don't think he knows about second breakfast, Pip.\",\n...     \"What about elevensies?\",\n... ]\n>>> encoded_inputs = tokenizer(batch_sentences)\n>>> print(encoded_inputs)\n{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102],\n               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n               [101, 1327, 1164, 5450, 23434, 136, 102]],\n 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0],\n                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                    [0, 0, 0, 0, 0, 0, 0]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1],\n                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n                    [1, 1, 1, 1, 1, 1, 1]]}\n```\n\n### 填充\n\n句子的长度并不总是相同，这可能会成为一个问题，因为模型输入的张量需要具有统一的形状。填充是一种策略，通过在较短的句子中添加一个特殊的`padding token`，以确保张量是矩形的。\n\n将 `padding` 参数设置为 `True`，以使批次中较短的序列填充到与最长序列相匹配的长度：\n\n```py\n>>> batch_sentences = [\n...     \"But what about second breakfast?\",\n...     \"Don't think he knows about second breakfast, Pip.\",\n...     \"What about elevensies?\",\n... ]\n>>> encoded_input = tokenizer(batch_sentences, padding=True)\n>>> print(encoded_input)\n{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],\n               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],\n 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}\n```\n\n第一句和第三句因为较短，通过`0`进行填充，。\n\n### 截断\n\n另一方面，有时候一个序列可能对模型来说太长了。在这种情况下，您需要将序列截断为更短的长度。\n\n将 `truncation` 参数设置为 `True`，以将序列截断为模型接受的最大长度：\n\n\n```py\n>>> batch_sentences = [\n...     \"But what about second breakfast?\",\n...     \"Don't think he knows about second breakfast, Pip.\",\n...     \"What about elevensies?\",\n... ]\n>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True)\n>>> print(encoded_input)\n{'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],\n               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],\n 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}\n```\n\n<Tip>\n\n查看[填充和截断](./pad_truncation)概念指南，了解更多有关填充和截断参数的信息。\n\n</Tip>\n\n### 构建张量\n\n最后，`tokenizer`可以返回实际输入到模型的张量。\n\n将 `return_tensors` 参数设置为 `pt`（对于PyTorch）或 `tf`（对于TensorFlow）：\n\n<frameworkcontent>\n<pt>\n\n\n```py\n>>> batch_sentences = [\n...     \"But what about second breakfast?\",\n...     \"Don't think he knows about second breakfast, Pip.\",\n...     \"What about elevensies?\",\n... ]\n>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n>>> print(encoded_input)\n{'input_ids': tensor([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],\n                      [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n                      [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]]),\n 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n                           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n                           [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n```\n</pt>\n<tf>\n\n```py\n>>> batch_sentences = [\n...     \"But what about second breakfast?\",\n...     \"Don't think he knows about second breakfast, Pip.\",\n...     \"What about elevensies?\",\n... ]\n>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"tf\")\n>>> print(encoded_input)\n{'input_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\narray([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],\n       [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n       [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],\n      dtype=int32)>,\n 'token_type_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>,\n 'attention_mask': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\narray([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}\n```\n</tf>\n</frameworkcontent>",
    "1262": "一级标题：预处理\n二级标题：音频\n内容：\n对于音频任务，您需要[feature extractor](main_classes/feature_extractor)来准备您的数据集以供模型使用。`feature extractor`旨在从原始音频数据中提取特征，并将它们转换为张量。\n\n加载[MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)数据集（有关如何加载数据集的更多详细信息，请参阅🤗 [Datasets教程](https://huggingface.co/docs/datasets/load_hub)）以了解如何在音频数据集中使用`feature extractor`：\n\n\n```py\n>>> from datasets import load_dataset, Audio\n\n>>> dataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\n```\n\n访问 `audio` 列的第一个元素以查看输入。调用 `audio` 列会自动加载和重新采样音频文件：\n\n```py\n>>> dataset[0][\"audio\"]\n{'array': array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,\n         0.        ,  0.        ], dtype=float32),\n 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',\n 'sampling_rate': 8000}\n```\n\n这会返回三个对象：\n\n* `array` 是加载的语音信号 - 并在必要时重新采为`1D array`。\n* `path` 指向音频文件的位置。\n* `sampling_rate` 是每秒测量的语音信号数据点数量。\n\n对于本教程，您将使用[Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base)模型。查看模型卡片，您将了解到Wav2Vec2是在16kHz采样的语音音频数据上预训练的。重要的是，您的音频数据的采样率要与用于预训练模型的数据集的采样率匹配。如果您的数据的采样率不同，那么您需要对数据进行重新采样。\n\n1. 使用🤗 Datasets的[`~datasets.Dataset.cast_column`]方法将采样率提升到16kHz：\n\n```py\n>>> dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n```\n\n2. 再次调用 `audio` 列以重新采样音频文件：\n\n\n```py\n>>> dataset[0][\"audio\"]\n{'array': array([ 2.3443763e-05,  2.1729663e-04,  2.2145823e-04, ...,\n         3.8356509e-05, -7.3497440e-06, -2.1754686e-05], dtype=float32),\n 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav',\n 'sampling_rate': 16000}\n```\n\n接下来，加载一个`feature extractor`以对输入进行标准化和填充。当填充文本数据时，会为较短的序列添加 `0`。相同的理念适用于音频数据。`feature extractor`添加 `0` - 被解释为静音 - 到`array` 。\n\n使用 [`AutoFeatureExtractor.from_pretrained`] 加载`feature extractor`：\n\n\n```py\n>>> from transformers import AutoFeatureExtractor\n\n>>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n```\n\n将音频 `array` 传递给`feature extractor`。我们还建议在`feature extractor`中添加 `sampling_rate` 参数，以更好地调试可能发生的静音错误：\n\n\n```py\n>>> audio_input = [dataset[0][\"audio\"][\"array\"]]\n>>> feature_extractor(audio_input, sampling_rate=16000)\n{'input_values': [array([ 3.8106556e-04,  2.7506407e-03,  2.8015103e-03, ...,\n        5.6335266e-04,  4.6588284e-06, -1.7142107e-04], dtype=float32)]}\n```\n\n就像`tokenizer`一样，您可以应用填充或截断来处理批次中的可变序列。请查看这两个音频样本的序列长度：\n\n\n```py\n>>> dataset[0][\"audio\"][\"array\"].shape\n(173398,)\n\n>>> dataset[1][\"audio\"][\"array\"].shape\n(106496,)\n```\n\n创建一个函数来预处理数据集，以使音频样本具有相同的长度。通过指定最大样本长度，`feature extractor`将填充或截断序列以使其匹配：\n\n\n```py\n>>> def preprocess_function(examples):\n...     audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n...     inputs = feature_extractor(\n...         audio_arrays,\n...         sampling_rate=16000,\n...         padding=True,\n...         max_length=100000,\n...         truncation=True,\n...     )\n...     return inputs\n```\n\n将`preprocess_function`应用于数据集中的前几个示例：\n\n\n```py\n>>> processed_dataset = preprocess_function(dataset[:5])\n```\n\n现在样本长度是相同的，并且与指定的最大长度匹配。您现在可以将经过处理的数据集传递给模型了！\n\n\n```py\n>>> processed_dataset[\"input_values\"][0].shape\n(100000,)\n\n>>> processed_dataset[\"input_values\"][1].shape\n(100000,)\n```",
    "1263": "一级标题：预处理\n二级标题：计算机视觉\n内容：\n对于计算机视觉任务，您需要一个[ image processor](main_classes/image_processor)来准备数据集以供模型使用。图像预处理包括多个步骤将图像转换为模型期望输入的格式。这些步骤包括但不限于调整大小、标准化、颜色通道校正以及将图像转换为张量。\n\n<Tip>\n\n图像预处理通常遵循某种形式的图像增强。图像预处理和图像增强都会改变图像数据，但它们有不同的目的：\n\n* 图像增强可以帮助防止过拟合并增加模型的鲁棒性。您可以在数据增强方面充分发挥创造性 - 调整亮度和颜色、裁剪、旋转、调整大小、缩放等。但要注意不要改变图像的含义。\n* 图像预处理确保图像与模型预期的输入格式匹配。在微调计算机视觉模型时，必须对图像进行与模型训练时相同的预处理。\n\n您可以使用任何您喜欢的图像增强库。对于图像预处理，请使用与模型相关联的`ImageProcessor`。\n\n</Tip>\n\n加载[food101](https://huggingface.co/datasets/food101)数据集（有关如何加载数据集的更多详细信息，请参阅🤗 [Datasets教程](https://huggingface.co/docs/datasets/load_hub)）以了解如何在计算机视觉数据集中使用图像处理器：\n\n<Tip>\n\n因为数据集相当大，请使用🤗 Datasets的`split`参数加载训练集中的少量样本！\n\n</Tip>\n\n\n```py\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"food101\", split=\"train[:100]\")\n```\n\n接下来，使用🤗 Datasets的[`Image`](https://huggingface.co/docs/datasets/package_reference/main_classes?highlight=image#datasets.Image)功能查看图像：\n\n\n```py\n>>> dataset[0][\"image\"]\n```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vision-preprocess-tutorial.png\"/>\n</div>\n\n使用 [`AutoImageProcessor.from_pretrained`] 加载`image processor`：\n\n```py\n>>> from transformers import AutoImageProcessor\n\n>>> image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n```\n\n首先，让我们进行图像增强。您可以使用任何您喜欢的库，但在本教程中，我们将使用torchvision的[`transforms`](https://pytorch.org/vision/stable/transforms.html)模块。如果您有兴趣使用其他数据增强库，请参阅[Albumentations](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb)或[Kornia notebooks](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb)中的示例。\n\n1. 在这里，我们使用[`Compose`](https://pytorch.org/vision/master/generated/torchvision.transforms.Compose.html)将[`RandomResizedCrop`](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html)和 [`ColorJitter`](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html)变换连接在一起。请注意，对于调整大小，我们可以从`image_processor`中获取图像尺寸要求。对于一些模型，精确的高度和宽度需要被定义，对于其他模型只需定义`shortest_edge`。\n\n\n```py\n>>> from torchvision.transforms import RandomResizedCrop, ColorJitter, Compose\n\n>>> size = (\n...     image_processor.size[\"shortest_edge\"]\n...     if \"shortest_edge\" in image_processor.size\n...     else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n... )\n\n>>> _transforms = Compose([RandomResizedCrop(size), ColorJitter(brightness=0.5, hue=0.5)])\n```\n\n2. 模型接受 [`pixel_values`](model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel.forward.pixel_values) 作为输入。`ImageProcessor` 可以进行图像的标准化，并生成适当的张量。创建一个函数，将图像增强和图像预处理步骤组合起来处理批量图像，并生成 `pixel_values`：\n\n\n```py\n>>> def transforms(examples):\n...     images = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n...     examples[\"pixel_values\"] = image_processor(images, do_resize=False, return_tensors=\"pt\")[\"pixel_values\"]\n...     return examples\n```\n\n<Tip>\n\n在上面的示例中，我们设置`do_resize=False`，因为我们已经在图像增强转换中调整了图像的大小，并利用了适当的`image_processor`的`size`属性。如果您在图像增强期间不调整图像的大小，请将此参数排除在外。默认情况下`ImageProcessor`将处理调整大小。\n\n如果希望将图像标准化步骤为图像增强的一部分，请使用`image_processor.image_mean`和`image_processor.image_std`。\n\n</Tip>\n\n3. 然后使用🤗 Datasets的[`set_transform`](https://huggingface.co/docs/datasets/process#format-transform)在运行时应用这些变换：\n\n\n```py\n>>> dataset.set_transform(transforms)\n```\n\n4. 现在，当您访问图像时，您将注意到`image processor`已添加了 `pixel_values`。您现在可以将经过处理的数据集传递给模型了！\n\n\n```py\n>>> dataset[0].keys()\n```\n\n这是在应用变换后的图像样子。图像已被随机裁剪，并其颜色属性发生了变化。\n\n\n```py\n>>> import numpy as np\n>>> import matplotlib.pyplot as plt\n\n>>> img = dataset[0][\"pixel_values\"]\n>>> plt.imshow(img.permute(1, 2, 0))\n```\n\n<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/preprocessed_image.png\"/>\n</div>\n\n<Tip>\n\n对于诸如目标检测、语义分割、实例分割和全景分割等任务，`ImageProcessor`提供了训练后处理方法。这些方法将模型的原始输出转换为有意义的预测，如边界框或分割地图。\n\n</Tip>\n\n### 填充\n\n在某些情况下，例如，在微调[DETR](./model_doc/detr)时，模型在训练时应用了尺度增强。这可能导致批处理中的图像大小不同。您可以使用[`DetrImageProcessor.pad`]来指定自定义的`collate_fn`将图像批处理在一起。\n\n```py\n>>> def collate_fn(batch):\n...     pixel_values = [item[\"pixel_values\"] for item in batch]\n...     encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n...     labels = [item[\"labels\"] for item in batch]\n...     batch = {}\n...     batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n...     batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n...     batch[\"labels\"] = labels\n...     return batch\n```",
    "1264": "一级标题：预处理\n二级标题：多模态\n内容：\n对于涉及多模态输入的任务，您需要[processor](main_classes/processors)来为模型准备数据集。`processor`将两个处理对象-例如`tokenizer`和`feature extractor`-组合在一起。\n\n加载[LJ Speech](https://huggingface.co/datasets/lj_speech)数据集（有关如何加载数据集的更多详细信息，请参阅🤗 [Datasets 教程](https://huggingface.co/docs/datasets/load_hub)）以了解如何使用`processor`进行自动语音识别（ASR）：\n\n\n```py\n>>> from datasets import load_dataset\n\n>>> lj_speech = load_dataset(\"lj_speech\", split=\"train\")\n```\n\n对于ASR（自动语音识别），主要关注`audio`和`text`，因此可以删除其他列：\n\n\n```py\n>>> lj_speech = lj_speech.map(remove_columns=[\"file\", \"id\", \"normalized_text\"])\n```\n\n现在查看`audio`和`text`列：\n\n```py\n>>> lj_speech[0][\"audio\"]\n{'array': array([-7.3242188e-04, -7.6293945e-04, -6.4086914e-04, ...,\n         7.3242188e-04,  2.1362305e-04,  6.1035156e-05], dtype=float32),\n 'path': '/root/.cache/huggingface/datasets/downloads/extracted/917ece08c95cf0c4115e45294e3cd0dee724a1165b7fc11798369308a465bd26/LJSpeech-1.1/wavs/LJ001-0001.wav',\n 'sampling_rate': 22050}\n\n>>> lj_speech[0][\"text\"]\n'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition'\n```\n\n请记住，您应始终[重新采样](preprocessing#audio)音频数据集的采样率，以匹配用于预训练模型数据集的采样率！\n\n\n```py\n>>> lj_speech = lj_speech.cast_column(\"audio\", Audio(sampling_rate=16_000))\n```\n\n使用[`AutoProcessor.from_pretrained`]加载一个`processor`：\n\n\n```py\n>>> from transformers import AutoProcessor\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n```\n\n1. 创建一个函数，用于将包含在 `array` 中的音频数据处理为 `input_values`，并将 `text` 标记为 `labels`。这些将是输入模型的数据：\n\n```py\n>>> def prepare_dataset(example):\n...     audio = example[\"audio\"]\n\n...     example.update(processor(audio=audio[\"array\"], text=example[\"text\"], sampling_rate=16000))\n\n...     return example\n```\n\n2. 将 `prepare_dataset` 函数应用于一个示例：\n\n```py\n>>> prepare_dataset(lj_speech[0])\n```\n\n`processor`现在已经添加了 `input_values` 和 `labels`，并且采样率也正确降低为为16kHz。现在可以将处理后的数据集传递给模型！",
    "1265": "一级标题：快速上手\n二级标题：无\n内容：\n[[open-in-colab]]\n\n快来使用 🤗 Transformers 吧！无论你是开发人员还是日常用户，这篇快速上手教程都将帮助你入门并且向你展示如何使用 [`pipeline`] 进行推理，使用 [AutoClass](./model_doc/auto) 加载一个预训练模型和预处理器，以及使用 PyTorch 或 TensorFlow 快速训练一个模型。如果你是一个初学者，我们建议你接下来查看我们的教程或者[课程](https://huggingface.co/course/chapter1/1)，来更深入地了解在这里介绍到的概念。\n\n在开始之前，确保你已经安装了所有必要的库：\n\n```bash\n!pip install transformers datasets evaluate accelerate\n```\n\n你还需要安装喜欢的机器学习框架：\n\n<frameworkcontent>\n<pt>\n\n```bash\npip install torch\n```\n</pt>\n<tf>\n\n```bash\npip install tensorflow\n```\n</tf>\n</frameworkcontent>",
    "1266": "一级标题：快速上手\n二级标题：Pipeline\n内容：\n<Youtube id=\"tiZFewofSLM\"/>\n\n使用 [`pipeline`] 是利用预训练模型进行推理的最简单的方式。你能够将 [`pipeline`] 开箱即用地用于跨不同模态的多种任务。来看看它支持的任务列表：\n\n| **任务**                     | **描述**                            | **模态**        | **Pipeline**                       |\n|------------------------------|-----------------------------------|-----------------|-----------------------------------------------|\n| 文本分类                      | 为给定的文本序列分配一个标签                    | NLP             | pipeline(task=\"sentiment-analysis\")           |\n| 文本生成                      | 根据给定的提示生成文本                       | NLP             | pipeline(task=\"text-generation\")              |\n| 命名实体识别                  | 为序列里的每个 token 分配一个标签（人, 组织, 地址等等） | NLP             | pipeline(task=\"ner\")                          |\n| 问答系统                      | 通过给定的上下文和问题, 在文本中提取答案             | NLP             | pipeline(task=\"question-answering\")           |\n| 掩盖填充                      | 预测出正确的在序列中被掩盖的token               | NLP             | pipeline(task=\"fill-mask\")                    |\n| 文本摘要                      | 为文本序列或文档生成总结                      | NLP             | pipeline(task=\"summarization\")                |\n| 文本翻译                      | 将文本从一种语言翻译为另一种语言                  | NLP             | pipeline(task=\"translation\")                  |\n| 图像分类                      | 为图像分配一个标签                         | Computer vision | pipeline(task=\"image-classification\")         |\n| 图像分割                      | 为图像中每个独立的像素分配标签（支持语义、全景和实例分割）     | Computer vision | pipeline(task=\"image-segmentation\")           |\n| 目标检测                      | 预测图像中目标对象的边界框和类别                  | Computer vision | pipeline(task=\"object-detection\")             |\n| 音频分类                      | 给音频文件分配一个标签                       | Audio           | pipeline(task=\"audio-classification\")         |\n| 自动语音识别                   | 将音频文件中的语音提取为文本                    | Audio           | pipeline(task=\"automatic-speech-recognition\") |\n| 视觉问答                      | 给定一个图像和一个问题，正确地回答有关图像的问题          | Multimodal      | pipeline(task=\"vqa\")                          |\n\n创建一个 [`pipeline`] 实例并且指定你想要将它用于的任务，就可以开始了。你可以将 [`pipeline`] 用于任何一个上面提到的任务，如果想知道支持的任务的完整列表，可以查阅 [pipeline API 参考](./main_classes/pipelines)。不过, 在这篇教程中，你将把 [`pipeline`] 用在一个情感分析示例上：\n\n```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(\"sentiment-analysis\")\n```\n\n[`pipeline`] 会下载并缓存一个用于情感分析的默认的[预训练模型](https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english)和分词器。现在你可以在目标文本上使用 `classifier` 了：\n\n```py\n>>> classifier(\"We are very happy to show you the 🤗 Transformers library.\")\n[{'label': 'POSITIVE', 'score': 0.9998}]\n```\n\n如果你有不止一个输入，可以把所有输入放入一个列表然后传给[`pipeline`]，它将会返回一个字典列表：\n\n```py\n>>> results = classifier([\"We are very happy to show you the 🤗 Transformers library.\", \"We hope you don't hate it.\"])\n>>> for result in results:\n...     print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\nlabel: POSITIVE, with score: 0.9998\nlabel: NEGATIVE, with score: 0.5309\n```\n\n[`pipeline`] 也可以为任何你喜欢的任务遍历整个数据集。在下面这个示例中，让我们选择自动语音识别作为我们的任务：\n\n```py\n>>> import torch\n>>> from transformers import pipeline\n\n>>> speech_recognizer = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\")\n```\n\n加载一个你想遍历的音频数据集（查阅 🤗 Datasets [快速开始](https://huggingface.co/docs/datasets/quickstart#audio) 获得更多信息）。比如，加载 [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) 数据集：\n\n```py\n>>> from datasets import load_dataset, Audio\n\n>>> dataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")  # doctest: +IGNORE_RESULT\n```\n\n你需要确保数据集中的音频的采样率与 [`facebook/wav2vec2-base-960h`](https://huggingface.co/facebook/wav2vec2-base-960h) 训练用到的音频的采样率一致：\n\n```py\n>>> dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))\n```\n\n当调用 `\"audio\"` 列时, 音频文件将会自动加载并重采样。\n从前四个样本中提取原始波形数组，将它作为列表传给 pipeline：\n\n```py\n>>> result = speech_recognizer(dataset[:4][\"audio\"])\n>>> print([d[\"text\"] for d in result])\n['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', \"FODING HOW I'D SET UP A JOIN TO HET WITH MY WIFE AND WHERE THE AP MIGHT BE\", \"I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE AP SO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AND I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS\", 'HOW DO I THURN A JOIN A COUNT']\n```\n\n对于输入非常庞大的大型数据集（比如语音或视觉），你会想到使用一个生成器，而不是一个将所有输入都加载进内存的列表。查阅 [pipeline API 参考](./main_classes/pipelines) 来获取更多信息。\n\n### 在 pipeline 中使用另一个模型和分词器\n\n[`pipeline`] 可以容纳 [Hub](https://huggingface.co/models) 中的任何模型，这让 [`pipeline`] 更容易适用于其他用例。比如，你想要一个能够处理法语文本的模型，就可以使用 Hub 上的标记来筛选出合适的模型。靠前的筛选结果会返回一个为情感分析微调的多语言的 [BERT 模型](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment)，你可以将它用于法语文本：\n\n```py\n>>> model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n```\n\n<frameworkcontent>\n<pt>\n使用 [`AutoModelForSequenceClassification`] 和 [`AutoTokenizer`] 来加载预训练模型和它关联的分词器（更多信息可以参考下一节的 `AutoClass`）：\n\n```py\n>>> from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n>>> model = AutoModelForSequenceClassification.from_pretrained(model_name)\n>>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n</pt>\n<tf>\n使用 [`TFAutoModelForSequenceClassification`] 和 [`AutoTokenizer`] 来加载预训练模型和它关联的分词器（更多信息可以参考下一节的 `TFAutoClass`）：\n\n```py\n>>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n\n>>> model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n>>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n</tf>\n</frameworkcontent>\n\n在 [`pipeline`] 中指定模型和分词器，现在你就可以在法语文本上使用 `classifier` 了：\n\n```py\n>>> classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n>>> classifier(\"Nous sommes très heureux de vous présenter la bibliothèque 🤗 Transformers.\")\n[{'label': '5 stars', 'score': 0.7273}]\n```\n\n如果你没有找到适合你的模型，就需要在你的数据上微调一个预训练模型了。查看 [微调教程](./training) 来学习怎样进行微调。最后，微调完模型后，考虑一下在 Hub 上与社区 [分享](./model_sharing) 这个模型，把机器学习普及到每一个人! 🤗",
    "1267": "一级标题：快速上手\n二级标题：AutoClass\n内容：\n<Youtube id=\"AhChOFRegn4\"/>\n\n在幕后，是由 [`AutoModelForSequenceClassification`] 和 [`AutoTokenizer`] 一起支持你在上面用到的 [`pipeline`]。[AutoClass](./model_doc/auto) 是一个能够通过预训练模型的名称或路径自动查找其架构的快捷方式。你只需要为你的任务选择合适的 `AutoClass` 和它关联的预处理类。\n\n让我们回过头来看上一节的示例，看看怎样使用 `AutoClass` 来重现使用 [`pipeline`] 的结果。\n\n### AutoTokenizer\n\n分词器负责预处理文本，将文本转换为用于输入模型的数字数组。有多个用来管理分词过程的规则，包括如何拆分单词和在什么样的级别上拆分单词（在 [分词器总结](./tokenizer_summary) 学习更多关于分词的信息）。要记住最重要的是你需要实例化的分词器要与模型的名称相同, 来确保和模型训练时使用相同的分词规则。\n\n使用 [`AutoTokenizer`] 加载一个分词器:\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n>>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n将文本传入分词器：\n\n```py\n>>> encoding = tokenizer(\"We are very happy to show you the 🤗 Transformers library.\")\n>>> print(encoding)\n{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102],\n 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n```\n\n分词器返回了含有如下内容的字典:\n\n* [input_ids](./glossary#input-ids)：用数字表示的 token。\n* [attention_mask](.glossary#attention-mask)：应该关注哪些 token 的指示。\n\n分词器也可以接受列表作为输入，并填充和截断文本，返回具有统一长度的批次：\n\n<frameworkcontent>\n<pt>\n\n```py\n>>> pt_batch = tokenizer(\n...     [\"We are very happy to show you the 🤗 Transformers library.\", \"We hope you don't hate it.\"],\n...     padding=True,\n...     truncation=True,\n...     max_length=512,\n...     return_tensors=\"pt\",\n... )\n```\n</pt>\n<tf>\n\n```py\n>>> tf_batch = tokenizer(\n...     [\"We are very happy to show you the 🤗 Transformers library.\", \"We hope you don't hate it.\"],\n...     padding=True,\n...     truncation=True,\n...     max_length=512,\n...     return_tensors=\"tf\",\n... )\n```\n</tf>\n</frameworkcontent>\n\n<Tip>\n\n查阅[预处理](./preprocessing)教程来获得有关分词的更详细的信息，以及如何使用 [`AutoFeatureExtractor`] 和 [`AutoProcessor`] 来处理图像，音频，还有多模式输入。\n\n</Tip>\n\n### AutoModel\n\n<frameworkcontent>\n<pt>\n🤗 Transformers 提供了一种简单统一的方式来加载预训练的实例. 这表示你可以像加载 [`AutoTokenizer`] 一样加载 [`AutoModel`]。唯一不同的地方是为你的任务选择正确的[`AutoModel`]。对于文本（或序列）分类，你应该加载[`AutoModelForSequenceClassification`]：\n\n```py\n>>> from transformers import AutoModelForSequenceClassification\n\n>>> model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n>>> pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n```\n\n<Tip>\n\n通过 [任务摘要](./task_summary) 查找 [`AutoModel`] 支持的任务.\n\n</Tip>\n\n现在可以把预处理好的输入批次直接送进模型。你只需要通过 `**` 来解包字典:\n\n```py\n>>> pt_outputs = pt_model(**pt_batch)\n```\n\n模型在 `logits` 属性输出最终的激活结果. 在 `logits` 上应用 softmax 函数来查询概率:\n\n```py\n>>> from torch import nn\n\n>>> pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)\n>>> print(pt_predictions)\ntensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)\n```\n</pt>\n<tf>\n🤗 Transformers 提供了一种简单统一的方式来加载预训练的实例。这表示你可以像加载 [`AutoTokenizer`] 一样加载 [`TFAutoModel`]。唯一不同的地方是为你的任务选择正确的 [`TFAutoModel`]，对于文本（或序列）分类，你应该加载 [`TFAutoModelForSequenceClassification`]：\n\n```py\n>>> from transformers import TFAutoModelForSequenceClassification\n\n>>> model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n```\n\n<Tip>\n\n通过 [任务摘要](./task_summary) 查找 [`AutoModel`] 支持的任务.\n\n</Tip>\n\n现在通过直接将字典的键传给张量，将预处理的输入批次传给模型。\n\n```py\n>>> tf_outputs = tf_model(tf_batch)\n```\n\n模型在 `logits` 属性输出最终的激活结果。在 `logits` 上应用softmax函数来查询概率：\n\n```py\n>>> import tensorflow as tf\n\n>>> tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\n>>> tf_predictions  # doctest: +IGNORE_RESULT\n```\n</tf>\n</frameworkcontent>\n\n<Tip>\n\n所有 🤗 Transformers 模型（PyTorch 或 TensorFlow）在最终的激活函数（比如 softmax）*之前* 输出张量，\n因为最终的激活函数常常与 loss 融合。模型的输出是特殊的数据类，所以它们的属性可以在 IDE 中被自动补全。模型的输出就像一个元组或字典（你可以通过整数、切片或字符串来索引它），在这种情况下，为 None 的属性会被忽略。\n\n</Tip>\n\n### 保存模型\n\n<frameworkcontent>\n<pt>\n当你的模型微调完成，你就可以使用 [`PreTrainedModel.save_pretrained`] 把它和它的分词器保存下来：\n\n```py\n>>> pt_save_directory = \"./pt_save_pretrained\"\n>>> tokenizer.save_pretrained(pt_save_directory)  # doctest: +IGNORE_RESULT\n>>> pt_model.save_pretrained(pt_save_directory)\n```\n\n当你准备再次使用这个模型时，就可以使用 [`PreTrainedModel.from_pretrained`] 加载它了：\n\n```py\n>>> pt_model = AutoModelForSequenceClassification.from_pretrained(\"./pt_save_pretrained\")\n```\n</pt>\n<tf>\n当你的模型微调完成，你就可以使用 [`TFPreTrainedModel.save_pretrained`] 把它和它的分词器保存下来：\n\n```py\n>>> tf_save_directory = \"./tf_save_pretrained\"\n>>> tokenizer.save_pretrained(tf_save_directory)  # doctest: +IGNORE_RESULT\n>>> tf_model.save_pretrained(tf_save_directory)\n```\n\n当你准备再次使用这个模型时，就可以使用 [`TFPreTrainedModel.from_pretrained`] 加载它了：\n\n```py\n>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(\"./tf_save_pretrained\")\n```\n</tf>\n</frameworkcontent>\n\n🤗 Transformers 有一个特别酷的功能，它能够保存一个模型，并且将它加载为 PyTorch 或 TensorFlow 模型。`from_pt` 或 `from_tf` 参数可以将模型从一个框架转换为另一个框架：\n\n<frameworkcontent>\n<pt>\n\n```py\n>>> from transformers import AutoModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n>>> pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n```\n</pt>\n<tf>\n\n```py\n>>> from transformers import TFAutoModel\n\n>>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n```\n</tf>\n</frameworkcontent>",
    "1268": "一级标题：快速上手\n二级标题：自定义模型构建\n内容：\n你可以修改模型的配置类来改变模型的构建方式。配置指明了模型的属性，比如隐藏层或者注意力头的数量。当你从自定义的配置类初始化模型时，你就开始自定义模型构建了。模型属性是随机初始化的，你需要先训练模型，然后才能得到有意义的结果。\n\n通过导入 [`AutoConfig`] 来开始，之后加载你想修改的预训练模型。在 [`AutoConfig.from_pretrained`] 中，你能够指定想要修改的属性，比如注意力头的数量：\n\n```py\n>>> from transformers import AutoConfig\n\n>>> my_config = AutoConfig.from_pretrained(\"distilbert/distilbert-base-uncased\", n_heads=12)\n```\n\n<frameworkcontent>\n<pt>\n使用 [`AutoModel.from_config`] 根据你的自定义配置创建一个模型：\n\n```py\n>>> from transformers import AutoModel\n\n>>> my_model = AutoModel.from_config(my_config)\n```\n</pt>\n<tf>\n使用 [`TFAutoModel.from_config`] 根据你的自定义配置创建一个模型：\n\n```py\n>>> from transformers import TFAutoModel\n\n>>> my_model = TFAutoModel.from_config(my_config)\n```\n</tf>\n</frameworkcontent>\n\n查阅 [创建一个自定义结构](./create_a_model) 指南获取更多关于构建自定义配置的信息。",
    "1269": "一级标题：快速上手\n二级标题：Trainer - PyTorch 优化训练循环\n内容：\n所有的模型都是标准的 [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)，所以你可以在任何典型的训练模型中使用它们。当你编写自己的训练循环时，🤗 Transformers 为 PyTorch 提供了一个 [`Trainer`] 类，它包含了基础的训练循环并且为诸如分布式训练，混合精度等特性增加了额外的功能。\n\n取决于你的任务, 你通常可以传递以下的参数给 [`Trainer`]：\n\n1. [`PreTrainedModel`] 或者 [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)：\n\n   ```py\n   >>> from transformers import AutoModelForSequenceClassification\n\n   >>> model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n   ```\n\n2. [`TrainingArguments`] 含有你可以修改的模型超参数，比如学习率，批次大小和训练时的迭代次数。如果你没有指定训练参数，那么它会使用默认值：\n\n   ```py\n   >>> from transformers import TrainingArguments\n\n   >>> training_args = TrainingArguments(\n   ...     output_dir=\"path/to/save/folder/\",\n   ...     learning_rate=2e-5,\n   ...     per_device_train_batch_size=8,\n   ...     per_device_eval_batch_size=8,\n   ...     num_train_epochs=2,\n   ... )\n   ```\n\n3. 一个预处理类，比如分词器，特征提取器或者处理器：\n\n   ```py\n   >>> from transformers import AutoTokenizer\n\n   >>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n   ```\n\n4. 加载一个数据集：\n\n   ```py\n   >>> from datasets import load_dataset\n\n   >>> dataset = load_dataset(\"rotten_tomatoes\")  # doctest: +IGNORE_RESULT\n   ```\n\n5. 创建一个给数据集分词的函数，并且使用 [`~datasets.Dataset.map`] 应用到整个数据集：\n\n   ```py\n   >>> def tokenize_dataset(dataset):\n   ...     return tokenizer(dataset[\"text\"])\n\n   >>> dataset = dataset.map(tokenize_dataset, batched=True)\n   ```\n\n6. 用来从数据集中创建批次的 [`DataCollatorWithPadding`]：\n\n   ```py\n   >>> from transformers import DataCollatorWithPadding\n\n   >>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n   ```\n\n现在把所有的类传给 [`Trainer`]：\n\n```py\n>>> from transformers import Trainer\n\n>>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     train_dataset=dataset[\"train\"],\n...     eval_dataset=dataset[\"test\"],\n...     processing_class=tokenizer,\n...     data_collator=data_collator,\n... )  # doctest: +SKIP\n```\n\n一切准备就绪后，调用 [`~Trainer.train`] 进行训练：\n\n```py\n>>> trainer.train()  # doctest: +SKIP\n```\n\n<Tip>\n\n对于像翻译或摘要这些使用序列到序列模型的任务，用 [`Seq2SeqTrainer`] 和 [`Seq2SeqTrainingArguments`] 来替代。\n\n</Tip>\n\n你可以通过子类化 [`Trainer`] 中的方法来自定义训练循环。这样你就可以自定义像损失函数，优化器和调度器这样的特性。查阅 [`Trainer`] 参考手册了解哪些方法能够被子类化。\n\n另一个自定义训练循环的方式是通过[回调](./main_classes/callback)。你可以使用回调来与其他库集成，查看训练循环来报告进度或提前结束训练。回调不会修改训练循环。如果想自定义损失函数等，就需要子类化 [`Trainer`] 了。",
    "1270": "一级标题：快速上手\n二级标题：使用 Tensorflow 训练\n内容：\n所有模型都是标准的 [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)，所以你可以通过 [Keras](https://keras.io/) API 实现在 Tensorflow 中训练。🤗 Transformers 提供了 [`~TFPreTrainedModel.prepare_tf_dataset`] 方法来轻松地将数据集加载为 `tf.data.Dataset`，这样你就可以使用 Keras 的 [`compile`](https://keras.io/api/models/model_training_apis/#compile-method) 和 [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) 方法马上开始训练。\n\n1. 使用 [`TFPreTrainedModel`] 或者 [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) 来开始：\n\n   ```py\n   >>> from transformers import TFAutoModelForSequenceClassification\n\n   >>> model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n   ```\n\n2. 一个预处理类，比如分词器，特征提取器或者处理器：\n\n   ```py\n   >>> from transformers import AutoTokenizer\n\n   >>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n   ```\n\n3. 创建一个给数据集分词的函数\n\n   ```py\n   >>> def tokenize_dataset(dataset):\n   ...     return tokenizer(dataset[\"text\"])  # doctest: +SKIP\n   ```\n\n4. 使用 [`~datasets.Dataset.map`] 将分词器应用到整个数据集，之后将数据集和分词器传给 [`~TFPreTrainedModel.prepare_tf_dataset`]。如果你需要的话，也可以在这里改变批次大小和是否打乱数据集：\n\n   ```py\n   >>> dataset = dataset.map(tokenize_dataset)  # doctest: +SKIP\n   >>> tf_dataset = model.prepare_tf_dataset(\n   ...     dataset, batch_size=16, shuffle=True, tokenizer=tokenizer\n   ... )  # doctest: +SKIP\n   ```\n\n5. 一切准备就绪后，调用 `compile` 和 `fit` 开始训练：\n\n   ```py\n   >>> from tensorflow.keras.optimizers import Adam\n\n   >>> model.compile(optimizer=Adam(3e-5))\n   >>> model.fit(dataset)  # doctest: +SKIP\n   ```",
    "1271": "一级标题：快速上手\n二级标题：接下来做什么?\n内容：\n现在你已经完成了 🤗 Transformers 的快速上手教程，来看看我们的指南并且学习如何做一些更具体的事情，比如写一个自定义模型，为某个任务微调一个模型以及如何使用脚本来训练模型。如果你有兴趣了解更多 🤗 Transformers 的核心章节，那就喝杯咖啡然后来看看我们的概念指南吧！",
    "1272": "一级标题：使用脚本进行训练\n二级标题：无\n内容：\n除了 🤗 Transformers [notebooks](./notebooks)，还有示例脚本演示了如何使用[PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch)、[TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow)或[JAX/Flax](https://github.com/huggingface/transformers/tree/main/examples/flax)训练模型以解决特定任务。\n\n您还可以在这些示例中找到我们在[研究项目](https://github.com/huggingface/transformers-research-projects/)和[遗留示例](https://github.com/huggingface/transformers/tree/main/examples/legacy)中使用过的脚本，这些脚本主要是由社区贡献的。这些脚本已不再被积极维护，需要使用特定版本的🤗 Transformers， 可能与库的最新版本不兼容。\n\n示例脚本可能无法在初始配置下直接解决每个问题，您可能需要根据要解决的问题调整脚本。为了帮助您，大多数脚本都完全暴露了数据预处理的方式，允许您根据需要对其进行编辑。\n\n如果您想在示例脚本中实现任何功能，请在[论坛](https://discuss.huggingface.co/)或[issue](https://github.com/huggingface/transformers/issues)上讨论，然后再提交Pull Request。虽然我们欢迎修复错误，但不太可能合并添加更多功能的Pull Request，因为这会降低可读性。\n\n本指南将向您展示如何在[PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization)和[TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization)中运行示例摘要训练脚本。除非另有说明，否则所有示例都可以在两个框架中工作。",
    "1273": "一级标题：使用脚本进行训练\n二级标题：设置\n内容：\n要成功运行示例脚本的最新版本，您必须在新虚拟环境中**从源代码安装 🤗 Transformers**：\n\n```bash\ngit clone https://github.com/huggingface/transformers\ncd transformers\npip install .\n```\n\n对于旧版本的示例脚本，请点击下面的切换按钮：\n\n<details>\n  <summary>老版本🤗 Transformers示例 </summary>\n\t<ul>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v4.5.1/examples\">v4.5.1</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v4.4.2/examples\">v4.4.2</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v4.3.3/examples\">v4.3.3</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v4.2.2/examples\">v4.2.2</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v4.1.1/examples\">v4.1.1</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v4.0.1/examples\">v4.0.1</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v3.5.1/examples\">v3.5.1</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v3.4.0/examples\">v3.4.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v3.3.1/examples\">v3.3.1</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v3.2.0/examples\">v3.2.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v3.1.0/examples\">v3.1.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v3.0.2/examples\">v3.0.2</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v2.11.0/examples\">v2.11.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v2.10.0/examples\">v2.10.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v2.9.1/examples\">v2.9.1</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v2.8.0/examples\">v2.8.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v2.7.0/examples\">v2.7.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v2.6.0/examples\">v2.6.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v2.5.1/examples\">v2.5.1</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v2.4.0/examples\">v2.4.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v2.3.0/examples\">v2.3.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v2.2.0/examples\">v2.2.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v2.1.0/examples\">v2.1.1</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v2.0.0/examples\">v2.0.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v1.2.0/examples\">v1.2.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v1.1.0/examples\">v1.1.0</a></li>\n\t\t<li><a href=\"https://github.com/huggingface/transformers/tree/v1.0.0/examples\">v1.0.0</a></li>\n\t</ul>\n</details>\n\n然后切换您clone的 🤗 Transformers 仓到特定的版本，例如v3.5.1：\n\n```bash\ngit checkout tags/v3.5.1\n```\n\n在安装了正确的库版本后，进入您选择的版本的`example`文件夹并安装例子要求的环境：\n\n```bash\npip install -r requirements.txt\n```",
    "1274": "一级标题：使用脚本进行训练\n二级标题：运行脚本\n内容：\n<frameworkcontent>\n<pt>\n\n示例脚本从🤗 [Datasets](https://huggingface.co/docs/datasets/)库下载并预处理数据集。然后，脚本通过[Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)使用支持摘要任务的架构对数据集进行微调。以下示例展示了如何在[CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail)数据集上微调[T5-small](https://huggingface.co/google-t5/t5-small)。由于T5模型的训练方式，它需要一个额外的`source_prefix`参数。这个提示让T5知道这是一个摘要任务。\n\n```bash\npython examples/pytorch/summarization/run_summarization.py \\\n    --model_name_or_path google-t5/t5-small \\\n    --do_train \\\n    --do_eval \\\n    --dataset_name cnn_dailymail \\\n    --dataset_config \"3.0.0\" \\\n    --source_prefix \"summarize: \" \\\n    --output_dir /tmp/tst-summarization \\\n    --per_device_train_batch_size=4 \\\n    --per_device_eval_batch_size=4 \\\n    --overwrite_output_dir \\\n    --predict_with_generate\n```\n</pt>\n<tf>\n\n示例脚本从  🤗 [Datasets](https://huggingface.co/docs/datasets/) 库下载并预处理数据集。然后，脚本使用 Keras 在支持摘要的架构上微调数据集。以下示例展示了如何在 [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail) 数据集上微调 [T5-small](https://huggingface.co/google-t5/t5-small)。T5 模型由于训练方式需要额外的 `source_prefix` 参数。这个提示让 T5 知道这是一个摘要任务。\n\n```bash\npython examples/tensorflow/summarization/run_summarization.py  \\\n    --model_name_or_path google-t5/t5-small \\\n    --dataset_name cnn_dailymail \\\n    --dataset_config \"3.0.0\" \\\n    --output_dir /tmp/tst-summarization  \\\n    --per_device_train_batch_size 8 \\\n    --per_device_eval_batch_size 16 \\\n    --num_train_epochs 3 \\\n    --do_train \\\n    --do_eval\n```\n</tf>\n</frameworkcontent>",
    "1275": "一级标题：使用脚本进行训练\n二级标题：分布式训练和混合精度\n内容：\n[Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) 支持分布式训练和混合精度，这意味着你也可以在脚本中使用它。要启用这两个功能，可以做如下设置：\n\n- 添加 `fp16` 参数以启用混合精度。\n- 使用 `nproc_per_node` 参数设置使用的GPU数量。\n\n\n```bash\ntorchrun \\\n    --nproc_per_node 8 pytorch/summarization/run_summarization.py \\\n    --fp16 \\\n    --model_name_or_path google-t5/t5-small \\\n    --do_train \\\n    --do_eval \\\n    --dataset_name cnn_dailymail \\\n    --dataset_config \"3.0.0\" \\\n    --source_prefix \"summarize: \" \\\n    --output_dir /tmp/tst-summarization \\\n    --per_device_train_batch_size=4 \\\n    --per_device_eval_batch_size=4 \\\n    --overwrite_output_dir \\\n    --predict_with_generate\n```\n\nTensorFlow脚本使用[`MirroredStrategy`](https://www.tensorflow.org/guide/distributed_training#mirroredstrategy)进行分布式训练，您无需在训练脚本中添加任何其他参数。如果可用，TensorFlow脚本将默认使用多个GPU。",
    "1276": "一级标题：使用脚本进行训练\n二级标题：在TPU上运行脚本\n内容：\n<frameworkcontent>\n<pt>\n\n张量处理单元（TPUs）是专门设计用于加速性能的。PyTorch使用[XLA](https://www.tensorflow.org/xla)深度学习编译器支持TPU（更多细节请参见[这里](https://github.com/pytorch/xla/blob/master/README.md)）。要使用TPU，请启动`xla_spawn.py`脚本并使用`num_cores`参数设置要使用的TPU核心数量。\n\n```bash\npython xla_spawn.py --num_cores 8 \\\n    summarization/run_summarization.py \\\n    --model_name_or_path google-t5/t5-small \\\n    --do_train \\\n    --do_eval \\\n    --dataset_name cnn_dailymail \\\n    --dataset_config \"3.0.0\" \\\n    --source_prefix \"summarize: \" \\\n    --output_dir /tmp/tst-summarization \\\n    --per_device_train_batch_size=4 \\\n    --per_device_eval_batch_size=4 \\\n    --overwrite_output_dir \\\n    --predict_with_generate\n```\n</pt>\n<tf>\n\n张量处理单元（TPUs）是专门设计用于加速性能的。TensorFlow脚本使用[`TPUStrategy`](https://www.tensorflow.org/guide/distributed_training#tpustrategy)在TPU上进行训练。要使用TPU，请将TPU资源的名称传递给`tpu`参数。\n\n```bash\npython run_summarization.py  \\\n    --tpu name_of_tpu_resource \\\n    --model_name_or_path google-t5/t5-small \\\n    --dataset_name cnn_dailymail \\\n    --dataset_config \"3.0.0\" \\\n    --output_dir /tmp/tst-summarization  \\\n    --per_device_train_batch_size 8 \\\n    --per_device_eval_batch_size 16 \\\n    --num_train_epochs 3 \\\n    --do_train \\\n    --do_eval\n```\n</tf>\n</frameworkcontent>",
    "1277": "一级标题：使用脚本进行训练\n二级标题：基于🤗 Accelerate运行脚本\n内容：\n🤗 [Accelerate](https://huggingface.co/docs/accelerate) 是一个仅支持 PyTorch 的库，它提供了一种统一的方法来在不同类型的设置（仅 CPU、多个 GPU、多个TPU）上训练模型，同时保持对 PyTorch 训练循环的完全可见性。如果你还没有安装 🤗 Accelerate，请确保你已经安装了它：\n\n> 注意：由于 Accelerate 正在快速发展，因此必须安装 git 版本的 accelerate 来运行脚本。\n\n```bash\npip install git+https://github.com/huggingface/accelerate\n```\n\n你需要使用`run_summarization_no_trainer.py`脚本，而不是`run_summarization.py`脚本。🤗 Accelerate支持的脚本需要在文件夹中有一个`task_no_trainer.py`文件。首先运行以下命令以创建并保存配置文件：\n\n```bash\naccelerate config\n```\n检测您的设置以确保配置正确：\n\n```bash\naccelerate test\n```\n\n现在您可以开始训练模型了：\n\n```bash\naccelerate launch run_summarization_no_trainer.py \\\n    --model_name_or_path google-t5/t5-small \\\n    --dataset_name cnn_dailymail \\\n    --dataset_config \"3.0.0\" \\\n    --source_prefix \"summarize: \" \\\n    --output_dir ~/tmp/tst-summarization\n```",
    "1278": "一级标题：使用脚本进行训练\n二级标题：使用自定义数据集\n内容：\n摘要脚本支持自定义数据集，只要它们是CSV或JSON Line文件。当你使用自己的数据集时，需要指定一些额外的参数：\n- `train_file` 和 `validation_file` 分别指定您的训练和验证文件的路径。\n- `text_column` 是输入要进行摘要的文本。\n- `summary_column` 是目标输出的文本。\n\n使用自定义数据集的摘要脚本看起来是这样的：\n\n\n```bash\npython examples/pytorch/summarization/run_summarization.py \\\n    --model_name_or_path google-t5/t5-small \\\n    --do_train \\\n    --do_eval \\\n    --train_file path_to_csv_or_jsonlines_file \\\n    --validation_file path_to_csv_or_jsonlines_file \\\n    --text_column text_column_name \\\n    --summary_column summary_column_name \\\n    --source_prefix \"summarize: \" \\\n    --output_dir /tmp/tst-summarization \\\n    --overwrite_output_dir \\\n    --per_device_train_batch_size=4 \\\n    --per_device_eval_batch_size=4 \\\n    --predict_with_generate\n```",
    "1279": "一级标题：使用脚本进行训练\n二级标题：测试脚本\n内容：\n通常，在提交整个数据集之前，最好先在较少的数据集示例上运行脚本，以确保一切按预期工作,因为完整数据集的处理可能需要花费几个小时的时间。使用以下参数将数据集截断为最大样本数：\n\n- `max_train_samples`\n- `max_eval_samples`\n- `max_predict_samples`\n\n\n```bash\npython examples/pytorch/summarization/run_summarization.py \\\n    --model_name_or_path google-t5/t5-small \\\n    --max_train_samples 50 \\\n    --max_eval_samples 50 \\\n    --max_predict_samples 50 \\\n    --do_train \\\n    --do_eval \\\n    --dataset_name cnn_dailymail \\\n    --dataset_config \"3.0.0\" \\\n    --source_prefix \"summarize: \" \\\n    --output_dir /tmp/tst-summarization \\\n    --per_device_train_batch_size=4 \\\n    --per_device_eval_batch_size=4 \\\n    --overwrite_output_dir \\\n    --predict_with_generate\n```\n\n并非所有示例脚本都支持`max_predict_samples`参数。如果您不确定您的脚本是否支持此参数，请添加`-h`参数进行检查：\n\n```bash\nexamples/pytorch/summarization/run_summarization.py -h\n```",
    "1280": "一级标题：使用脚本进行训练\n二级标题：从checkpoint恢复训练\n内容：\n另一个有用的选项是从之前的checkpoint恢复训练。这将确保在训练中断时，您可以从之前停止的地方继续进行，而无需重新开始。有两种方法可以从checkpoint恢复训练。\n\n第一种方法使用`output_dir previous_output_dir`参数从存储在`output_dir`中的最新的checkpoint恢复训练。在这种情况下，您应该删除`overwrite_output_dir`：\n\n```bash\npython examples/pytorch/summarization/run_summarization.py\n    --model_name_or_path google-t5/t5-small \\\n    --do_train \\\n    --do_eval \\\n    --dataset_name cnn_dailymail \\\n    --dataset_config \"3.0.0\" \\\n    --source_prefix \"summarize: \" \\\n    --output_dir /tmp/tst-summarization \\\n    --per_device_train_batch_size=4 \\\n    --per_device_eval_batch_size=4 \\\n    --output_dir previous_output_dir \\\n    --predict_with_generate\n```\n\n第二种方法使用`resume_from_checkpoint path_to_specific_checkpoint`参数从特定的checkpoint文件夹恢复训练。\n\n\n```bash\npython examples/pytorch/summarization/run_summarization.py\n    --model_name_or_path google-t5/t5-small \\\n    --do_train \\\n    --do_eval \\\n    --dataset_name cnn_dailymail \\\n    --dataset_config \"3.0.0\" \\\n    --source_prefix \"summarize: \" \\\n    --output_dir /tmp/tst-summarization \\\n    --per_device_train_batch_size=4 \\\n    --per_device_eval_batch_size=4 \\\n    --overwrite_output_dir \\\n    --resume_from_checkpoint path_to_specific_checkpoint \\\n    --predict_with_generate\n```",
    "1281": "一级标题：使用脚本进行训练\n二级标题：分享模型\n内容：\n所有脚本都可以将您的最终模型上传到[Model Hub](https://huggingface.co/models)。在开始之前，请确保您已登录Hugging Face：\n\n```bash\nhf auth login\n```\n\n然后，在脚本中添加`push_to_hub`参数。这个参数会创建一个带有您Hugging Face用户名和`output_dir`中指定的文件夹名称的仓库。\n\n为了给您的仓库指定一个特定的名称，使用`push_to_hub_model_id`参数来添加它。该仓库将自动列出在您的命名空间下。\n\n以下示例展示了如何上传具有特定仓库名称的模型：\n\n\n```bash\npython examples/pytorch/summarization/run_summarization.py\n    --model_name_or_path google-t5/t5-small \\\n    --do_train \\\n    --do_eval \\\n    --dataset_name cnn_dailymail \\\n    --dataset_config \"3.0.0\" \\\n    --source_prefix \"summarize: \" \\\n    --push_to_hub \\\n    --push_to_hub_model_id finetuned-t5-cnn_dailymail \\\n    --output_dir /tmp/tst-summarization \\\n    --per_device_train_batch_size=4 \\\n    --per_device_eval_batch_size=4 \\\n    --overwrite_output_dir \\\n    --predict_with_generate\n```",
    "1282": "一级标题：导出为 ONNX\n二级标题：无\n内容：\n在生产环境中部署 🤗 Transformers 模型通常需要或者能够受益于，将模型导出为可在专门的运行时和硬件上加载和执行的序列化格式。\n\n🤗 Optimum 是 Transformers 的扩展，可以通过其 `exporters` 模块将模型从 PyTorch 或 TensorFlow 导出为 ONNX 及 TFLite 等序列化格式。🤗 Optimum 还提供了一套性能优化工具，可以在目标硬件上以最高效率训练和运行模型。\n\n本指南演示了如何使用 🤗 Optimum 将 🤗 Transformers 模型导出为 ONNX。有关将模型导出为 TFLite 的指南，请参考 [导出为 TFLite 页面](tflite)。",
    "1283": "一级标题：导出为 ONNX\n二级标题：导出为 ONNX\n内容：\n[ONNX (Open Neural Network eXchange 开放神经网络交换)](http://onnx.ai) 是一个开放的标准，它定义了一组通用的运算符和一种通用的文件格式，用于表示包括 PyTorch 和 TensorFlow 在内的各种框架中的深度学习模型。当一个模型被导出为 ONNX时，这些运算符被用于构建计算图（通常被称为*中间表示*），该图表示数据在神经网络中的流动。\n\n通过公开具有标准化运算符和数据类型的图，ONNX使得模型能够轻松在不同深度学习框架间切换。例如，在 PyTorch 中训练的模型可以被导出为 ONNX，然后再导入到 TensorFlow（反之亦然）。\n\n导出为 ONNX 后，模型可以：\n- 通过 [图优化（graph optimization）](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization) 和 [量化（quantization）](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization) 等技术进行推理优化。\n- 通过 [`ORTModelForXXX` 类](https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort) 使用 ONNX Runtime 运行，它同样遵循你熟悉的 Transformers 中的 `AutoModel` API。\n- 使用 [优化推理流水线（pipeline）](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/pipelines) 运行，其 API 与 🤗 Transformers 中的 [`pipeline`] 函数相同。\n\n🤗 Optimum 通过利用配置对象提供对 ONNX 导出的支持。多种模型架构已经有现成的配置对象，并且配置对象也被设计得易于扩展以适用于其他架构。\n\n现有的配置列表请参考 [🤗 Optimum 文档](https://huggingface.co/docs/optimum/exporters/onnx/overview)。\n\n有两种方式可以将 🤗 Transformers 模型导出为 ONNX，这里我们展示这两种方法：\n\n- 使用 🤗 Optimum 的 CLI（命令行）导出。\n- 使用 🤗 Optimum 的 `optimum.onnxruntime` 模块导出。\n\n### 使用 CLI 将 🤗 Transformers 模型导出为 ONNX\n\n要将 🤗 Transformers 模型导出为 ONNX，首先需要安装额外的依赖项：\n\n```bash\npip install optimum[exporters]\n```\n\n请参阅 [🤗 Optimum 文档](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli) 以查看所有可用参数，或者在命令行中查看帮助：\n\n```bash\noptimum-cli export onnx --help\n```\n\n运行以下命令，以从 🤗 Hub 导出模型的检查点（checkpoint），以 `distilbert/distilbert-base-uncased-distilled-squad` 为例：\n\n```bash\noptimum-cli export onnx --model distilbert/distilbert-base-uncased-distilled-squad distilbert_base_uncased_squad_onnx/\n```\n\n你应该能在日志中看到导出进度以及生成的 `model.onnx` 文件的保存位置，如下所示：\n\n```bash\nValidating ONNX model distilbert_base_uncased_squad_onnx/model.onnx...\n\t-[✓] ONNX model output names match reference model (start_logits, end_logits)\n\t- Validating ONNX Model output \"start_logits\":\n\t\t-[✓] (2, 16) matches (2, 16)\n\t\t-[✓] all values close (atol: 0.0001)\n\t- Validating ONNX Model output \"end_logits\":\n\t\t-[✓] (2, 16) matches (2, 16)\n\t\t-[✓] all values close (atol: 0.0001)\nThe ONNX export succeeded and the exported model was saved at: distilbert_base_uncased_squad_onnx\n```\n\n上面的示例说明了从 🤗 Hub 导出检查点的过程。导出本地模型时，首先需要确保将模型的权重和分词器文件保存在同一目录（`local_path`）中。在使用 CLI 时，将 `local_path` 传递给 `model` 参数，而不是 🤗 Hub 上的检查点名称，并提供 `--task` 参数。你可以在 [🤗 Optimum 文档](https://huggingface.co/docs/optimum/exporters/task_manager)中查看支持的任务列表。如果未提供 `task` 参数，将默认导出不带特定任务头的模型架构。\n\n```bash\noptimum-cli export onnx --model local_path --task question-answering distilbert_base_uncased_squad_onnx/\n```\n\n生成的 `model.onnx` 文件可以在支持 ONNX 标准的 [许多加速引擎（accelerators）](https://onnx.ai/supported-tools.html#deployModel) 之一上运行。例如，我们可以使用 [ONNX Runtime](https://onnxruntime.ai/) 加载和运行模型，如下所示：\n\n```python\n>>> from transformers import AutoTokenizer\n>>> from optimum.onnxruntime import ORTModelForQuestionAnswering\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert_base_uncased_squad_onnx\")\n>>> model = ORTModelForQuestionAnswering.from_pretrained(\"distilbert_base_uncased_squad_onnx\")\n>>> inputs = tokenizer(\"What am I using?\", \"Using DistilBERT with ONNX Runtime!\", return_tensors=\"pt\")\n>>> outputs = model(**inputs)\n```\n\n从 Hub 导出 TensorFlow 检查点的过程也一样。例如，以下是从 [Keras 组织](https://huggingface.co/keras-io) 导出纯 TensorFlow 检查点的命令：\n\n```bash\noptimum-cli export onnx --model keras-io/transformers-qa distilbert_base_cased_squad_onnx/\n```\n\n### 使用 `optimum.onnxruntime` 将 🤗 Transformers 模型导出为 ONNX\n\n除了 CLI 之外，你还可以使用代码将 🤗 Transformers 模型导出为 ONNX，如下所示：\n\n```python\n>>> from optimum.onnxruntime import ORTModelForSequenceClassification\n>>> from transformers import AutoTokenizer\n\n>>> model_checkpoint = \"distilbert_base_uncased_squad\"\n>>> save_directory = \"onnx/\"\n\n>>> # 从 transformers 加载模型并将其导出为 ONNX\n>>> ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)\n>>> tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\n>>> # 保存 onnx 模型以及分词器\n>>> ort_model.save_pretrained(save_directory)\n>>> tokenizer.save_pretrained(save_directory)\n```\n\n### 导出尚未支持的架构的模型\n\n如果你想要为当前无法导出的模型添加支持，请先检查 [`optimum.exporters.onnx`](https://huggingface.co/docs/optimum/exporters/onnx/overview) 是否支持该模型，如果不支持，你可以 [直接为 🤗 Optimum 贡献代码](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/contribute)。\n\n### 使用 `transformers.onnx` 导出模型\n\n<Tip warning={true}>\n\n`transformers.onnx` 不再进行维护，请如上所述，使用 🤗 Optimum 导出模型。这部分内容将在未来版本中删除。\n\n</Tip>\n\n要使用 `transformers.onnx` 将 🤗 Transformers 模型导出为 ONNX，请安装额外的依赖项：\n\n```bash\npip install transformers[onnx]\n```\n\n将 `transformers.onnx` 包作为 Python 模块使用，以使用现成的配置导出检查点：\n\n```bash\npython -m transformers.onnx --model=distilbert/distilbert-base-uncased onnx/\n```\n\n以上代码将导出由 `--model` 参数定义的检查点的 ONNX 图。传入任何 🤗 Hub 上或者存储与本地的检查点。生成的 `model.onnx` 文件可以在支持 ONNX 标准的众多加速引擎上运行。例如，使用 ONNX Runtime 加载并运行模型，如下所示：\n\n```python\n>>> from transformers import AutoTokenizer\n>>> from onnxruntime import InferenceSession\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n>>> session = InferenceSession(\"onnx/model.onnx\")\n>>> # ONNX Runtime expects NumPy arrays as input\n>>> inputs = tokenizer(\"Using DistilBERT with ONNX Runtime!\", return_tensors=\"np\")\n>>> outputs = session.run(output_names=[\"last_hidden_state\"], input_feed=dict(inputs))\n```\n\n可以通过查看每个模型的 ONNX 配置来获取所需的输出名（例如 `[\"last_hidden_state\"]`）。例如，对于 DistilBERT，可以用以下代码获取输出名称：\n\n```python\n>>> from transformers.models.distilbert import DistilBertConfig, DistilBertOnnxConfig\n\n>>> config = DistilBertConfig()\n>>> onnx_config = DistilBertOnnxConfig(config)\n>>> print(list(onnx_config.outputs.keys()))\n[\"last_hidden_state\"]\n```\n\n从 Hub 导出 TensorFlow 检查点的过程也一样。导出纯 TensorFlow 检查点的示例代码如下：\n\n```bash\npython -m transformers.onnx --model=keras-io/transformers-qa onnx/\n```\n\n要导出本地存储的模型，请将模型的权重和分词器文件保存在同一目录中（例如 `local-pt-checkpoint`），然后通过将 `transformers.onnx` 包的 `--model` 参数指向该目录，将其导出为 ONNX：\n\n```bash\npython -m transformers.onnx --model=local-pt-checkpoint onnx/\n```",
    "1284": "一级标题：🤗 Transformers 能做什么\n二级标题：无\n内容：\n🤗 Transformers是一个用于自然语言处理（NLP）、计算机视觉和音频和语音处理任务的预训练模型库。该库不仅包含Transformer模型，还包括用于计算机视觉任务的现代卷积网络等非Transformer模型。如果您看看今天最受欢迎的一些消费产品，比如智能手机、应用程序和电视，很可能背后都有某种深度学习技术的支持。想要从您智能手机拍摄的照片中删除背景对象吗？这里是一个全景分割任务的例子（如果您还不了解这是什么意思，我们将在以下部分进行描述！）。\n\n本页面提供了使用🤗 Transformers库仅用三行代码解决不同的语音和音频、计算机视觉和NLP任务的概述！",
    "1285": "一级标题：🤗 Transformers 能做什么\n二级标题：音频\n内容：\n音频和语音处理任务与其他模态略有不同，主要是因为音频作为输入是一个连续的信号。与文本不同，原始音频波形不能像句子可以被划分为单词那样被整齐地分割成离散的块。为了解决这个问题，通常在固定的时间间隔内对原始音频信号进行采样。如果在每个时间间隔内采样更多样本，采样率就会更高，音频更接近原始音频源。\n\n以前的方法是预处理音频以从中提取有用的特征。现在更常见的做法是直接将原始音频波形输入到特征编码器中，以提取音频表示。这样可以简化预处理步骤，并允许模型学习最重要的特征。\n\n### 音频分类\n\n音频分类是一项将音频数据从预定义的类别集合中进行标记的任务。这是一个广泛的类别，具有许多具体的应用，其中一些包括：\n\n* 声学场景分类：使用场景标签（\"办公室\"、\"海滩\"、\"体育场\"）对音频进行标记。\n* 声学事件检测：使用声音事件标签（\"汽车喇叭声\"、\"鲸鱼叫声\"、\"玻璃破碎声\"）对音频进行标记。\n* 标记：对包含多种声音的音频进行标记（鸟鸣、会议中的说话人识别）。\n* 音乐分类：使用流派标签（\"金属\"、\"嘻哈\"、\"乡村\"）对音乐进行标记。\n\n```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(task=\"audio-classification\", model=\"superb/hubert-base-superb-er\")\n>>> preds = classifier(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n>>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n>>> preds\n[{'score': 0.4532, 'label': 'hap'},\n {'score': 0.3622, 'label': 'sad'},\n {'score': 0.0943, 'label': 'neu'},\n {'score': 0.0903, 'label': 'ang'}]\n```\n\n### 自动语音识别\n\n自动语音识别（ASR）将语音转录为文本。这是最常见的音频任务之一，部分原因是因为语音是人类交流的自然形式。如今，ASR系统嵌入在智能技术产品中，如扬声器、电话和汽车。我们可以要求虚拟助手播放音乐、设置提醒和告诉我们天气。\n\n但是，Transformer架构帮助解决的一个关键挑战是低资源语言。通过在大量语音数据上进行预训练，仅在一个低资源语言的一小时标记语音数据上进行微调，仍然可以产生与以前在100倍更多标记数据上训练的ASR系统相比高质量的结果。\n\n```py\n>>> from transformers import pipeline\n\n>>> transcriber = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-small\")\n>>> transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}\n```",
    "1286": "一级标题：🤗 Transformers 能做什么\n二级标题：计算机视觉\n内容：\n计算机视觉任务中最早成功之一是使用卷积神经网络（[CNN](glossary#convolution)）识别邮政编码数字图像。图像由像素组成，每个像素都有一个数值。这使得将图像表示为像素值矩阵变得容易。每个像素值组合描述了图像的颜色。\n\n计算机视觉任务可以通过以下两种通用方式解决：\n\n1. 使用卷积来学习图像的层次特征，从低级特征到高级抽象特征。\n2. 将图像分成块，并使用Transformer逐步学习每个图像块如何相互关联以形成图像。与CNN偏好的自底向上方法不同，这种方法有点像从一个模糊的图像开始，然后逐渐将其聚焦清晰。\n\n### 图像分类\n\n图像分类将整个图像从预定义的类别集合中进行标记。像大多数分类任务一样，图像分类有许多实际用例，其中一些包括：\n\n* 医疗保健：标记医学图像以检测疾病或监测患者健康状况\n* 环境：标记卫星图像以监测森林砍伐、提供野外管理信息或检测野火\n* 农业：标记农作物图像以监测植物健康或用于土地使用监测的卫星图像\n* 生态学：标记动物或植物物种的图像以监测野生动物种群或跟踪濒危物种\n\n```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(task=\"image-classification\")\n>>> preds = classifier(\n...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n... )\n>>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n>>> print(*preds, sep=\"\\n\")\n{'score': 0.4335, 'label': 'lynx, catamount'}\n{'score': 0.0348, 'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor'}\n{'score': 0.0324, 'label': 'snow leopard, ounce, Panthera uncia'}\n{'score': 0.0239, 'label': 'Egyptian cat'}\n{'score': 0.0229, 'label': 'tiger cat'}\n```\n\n### 目标检测\n\n与图像分类不同，目标检测在图像中识别多个对象以及这些对象在图像中的位置（由边界框定义）。目标检测的一些示例应用包括：\n\n* 自动驾驶车辆：检测日常交通对象，如其他车辆、行人和红绿灯\n* 遥感：灾害监测、城市规划和天气预报\n* 缺陷检测：检测建筑物中的裂缝或结构损坏，以及制造业产品缺陷\n\n\n```py\n>>> from transformers import pipeline\n\n>>> detector = pipeline(task=\"object-detection\")\n>>> preds = detector(\n...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n... )\n>>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"], \"box\": pred[\"box\"]} for pred in preds]\n>>> preds\n[{'score': 0.9865,\n  'label': 'cat',\n  'box': {'xmin': 178, 'ymin': 154, 'xmax': 882, 'ymax': 598}}]\n```\n\n### 图像分割\n\n图像分割是一项像素级任务，将图像中的每个像素分配给一个类别。它与使用边界框标记和预测图像中的对象的目标检测不同，因为分割更加精细。分割可以在像素级别检测对象。有几种类型的图像分割：\n\n* 实例分割：除了标记对象的类别外，还标记每个对象的不同实例（“dog-1”，“dog-2”）\n* 全景分割：语义分割和实例分割的组合； 它使用语义类为每个像素标记并标记每个对象的不同实例\n\n分割任务对于自动驾驶车辆很有帮助，可以创建周围世界的像素级地图，以便它们可以在行人和其他车辆周围安全导航。它还适用于医学成像，其中任务的更精细粒度可以帮助识别异常细胞或器官特征。图像分割也可以用于电子商务，通过您的相机在现实世界中覆盖物体来虚拟试穿衣服或创建增强现实体验。\n\n```py\n>>> from transformers import pipeline\n\n>>> segmenter = pipeline(task=\"image-segmentation\")\n>>> preds = segmenter(\n...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n... )\n>>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n>>> print(*preds, sep=\"\\n\")\n{'score': 0.9879, 'label': 'LABEL_184'}\n{'score': 0.9973, 'label': 'snow'}\n{'score': 0.9972, 'label': 'cat'}\n```\n\n### 深度估计\n\n深度估计预测图像中每个像素到相机的距离。这个计算机视觉任务对于场景理解和重建尤为重要。例如，在自动驾驶汽车中，车辆需要了解行人、交通标志和其他车辆等物体的距离，以避免障碍物和碰撞。深度信息还有助于从2D图像构建3D表示，并可用于创建生物结构或建筑物的高质量3D表示。\n\n有两种方法可以进行深度估计：\n\n* stereo（立体）：通过比较同一图像的两个略微不同角度的图像来估计深度\n* monocular（单目）：从单个图像中估计深度\n\n\n```py\n>>> from transformers import pipeline\n\n>>> depth_estimator = pipeline(task=\"depth-estimation\")\n>>> preds = depth_estimator(\n...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n... )\n```",
    "1287": "一级标题：🤗 Transformers 能做什么\n二级标题：自然语言处理\n内容：\nNLP任务是最常见的类型之一，因为文本是我们进行交流的自然方式。为了让文本变成模型识别的格式，需要对其进行分词。这意味着将一段文本分成单独的单词或子词（`tokens`），然后将这些`tokens`转换为数字。因此，可以将一段文本表示为一系列数字，一旦有了一系列的数字，就可以将其输入到模型中以解决各种NLP任务！\n\n### 文本分类\n\n像任何模态的分类任务一样，文本分类将一段文本（可以是句子级别、段落或文档）从预定义的类别集合中进行标记。文本分类有许多实际应用，其中一些包括：\n\n* 情感分析：根据某些极性（如`积极`或`消极`）对文本进行标记，可以支持政治、金融和营销等领域的决策制定\n* 内容分类：根据某些主题对文本进行标记，有助于组织和过滤新闻和社交媒体提要中的信息（`天气`、`体育`、`金融`等）\n\n\n```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(task=\"sentiment-analysis\")\n>>> preds = classifier(\"Hugging Face is the best thing since sliced bread!\")\n>>> preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n>>> preds\n[{'score': 0.9991, 'label': 'POSITIVE'}]\n```\n\n### Token分类\n\n在任何NLP任务中，文本都经过预处理，将文本序列分成单个单词或子词。这些被称为[tokens](/glossary#token)。Token分类将每个`token`分配一个来自预定义类别集的标签。\n\n两种常见的Token分类是：\n\n* 命名实体识别（NER）：根据实体类别（如组织、人员、位置或日期）对`token`进行标记。NER在生物医学设置中特别受欢迎，可以标记基因、蛋白质和药物名称。\n* 词性标注（POS）：根据其词性（如名词、动词或形容词）对标记进行标记。POS对于帮助翻译系统了解两个相同的单词如何在语法上不同很有用（作为名词的银行与作为动词的银行）。\n\n```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(task=\"ner\")\n>>> preds = classifier(\"Hugging Face is a French company based in New York City.\")\n>>> preds = [\n...     {\n...         \"entity\": pred[\"entity\"],\n...         \"score\": round(pred[\"score\"], 4),\n...         \"index\": pred[\"index\"],\n...         \"word\": pred[\"word\"],\n...         \"start\": pred[\"start\"],\n...         \"end\": pred[\"end\"],\n...     }\n...     for pred in preds\n... ]\n>>> print(*preds, sep=\"\\n\")\n{'entity': 'I-ORG', 'score': 0.9968, 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}\n{'entity': 'I-ORG', 'score': 0.9293, 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}\n{'entity': 'I-ORG', 'score': 0.9763, 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}\n{'entity': 'I-MISC', 'score': 0.9983, 'index': 6, 'word': 'French', 'start': 18, 'end': 24}\n{'entity': 'I-LOC', 'score': 0.999, 'index': 10, 'word': 'New', 'start': 42, 'end': 45}\n{'entity': 'I-LOC', 'score': 0.9987, 'index': 11, 'word': 'York', 'start': 46, 'end': 50}\n{'entity': 'I-LOC', 'score': 0.9992, 'index': 12, 'word': 'City', 'start': 51, 'end': 55}\n```\n\n### 问答\n\n问答是另一个`token-level`的任务，返回一个问题的答案，有时带有上下文（开放领域），有时不带上下文（封闭领域）。每当我们向虚拟助手提出问题时，例如询问一家餐厅是否营业，就会发生这种情况。它还可以提供客户或技术支持，并帮助搜索引擎检索您要求的相关信息。\n\n有两种常见的问答类型：\n\n* 提取式：给定一个问题和一些上下文，答案是从模型必须提取的上下文中的一段文本跨度。\n* 抽象式：给定一个问题和一些上下文，答案从上下文中生成；这种方法由[`Text2TextGenerationPipeline`]处理，而不是下面显示的[`QuestionAnsweringPipeline`]。\n\n\n```py\n>>> from transformers import pipeline\n\n>>> question_answerer = pipeline(task=\"question-answering\")\n>>> preds = question_answerer(\n...     question=\"What is the name of the repository?\",\n...     context=\"The name of the repository is huggingface/transformers\",\n... )\n>>> print(\n...     f\"score: {round(preds['score'], 4)}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}\"\n... )\nscore: 0.9327, start: 30, end: 54, answer: huggingface/transformers\n```\n\n### 摘要\n\n摘要从较长的文本中创建一个较短的版本，同时尽可能保留原始文档的大部分含义。摘要是一个序列到序列的任务；它输出比输入更短的文本序列。有许多长篇文档可以进行摘要，以帮助读者快速了解主要要点。法案、法律和财务文件、专利和科学论文等文档可以摘要，以节省读者的时间并作为阅读辅助工具。\n\n像问答一样，摘要有两种类型：\n\n* 提取式：从原始文本中识别和提取最重要的句子\n* 抽象式：从原始文本生成目标摘要（可能包括不在输入文档中的新单词）；[`SummarizationPipeline`]使用抽象方法。\n\n\n```py\n>>> from transformers import pipeline\n\n>>> summarizer = pipeline(task=\"summarization\")\n>>> summarizer(\n...     \"In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\"\n... )\n[{'summary_text': ' The Transformer is the first sequence transduction model based entirely on attention . It replaces the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention . For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}]\n```\n\n### 翻译\n\n翻译将一种语言的文本序列转换为另一种语言。它对于帮助来自不同背景的人们相互交流、帮助翻译内容以吸引更广泛的受众，甚至成为学习工具以帮助人们学习一门新语言都非常重要。除了摘要之外，翻译也是一个序列到序列的任务，意味着模型接收输入序列并返回目标输出序列。\n\n在早期，翻译模型大多是单语的，但最近，越来越多的人对可以在多种语言之间进行翻译的多语言模型感兴趣。\n\n```py\n>>> from transformers import pipeline\n\n>>> text = \"translate English to French: Hugging Face is a community-based open-source platform for machine learning.\"\n>>> translator = pipeline(task=\"translation\", model=\"google-t5/t5-small\")\n>>> translator(text)\n[{'translation_text': \"Hugging Face est une tribune communautaire de l'apprentissage des machines.\"}]\n```\n\n### 语言模型\n\n语言模型是一种预测文本序列中单词的任务。它已成为一种非常流行的NLP任务，因为预训练的语言模型可以微调用于许多其他下游任务。最近，人们对大型语言模型（LLMs）表现出了极大的兴趣，这些模型展示了`zero learning`或`few-shot learning`的能力。这意味着模型可以解决它未被明确训练过的任务！语言模型可用于生成流畅和令人信服的文本，但需要小心，因为文本可能并不总是准确的。\n\n有两种类型的话语模型：\n\n* causal：模型的目标是预测序列中的下一个`token`，而未来的`tokens`被遮盖。\n\n    ```py\n    >>> from transformers import pipeline\n\n    >>> prompt = \"Hugging Face is a community-based open-source platform for machine learning.\"\n    >>> generator = pipeline(task=\"text-generation\")\n    >>> generator(prompt)  # doctest: +SKIP\n    ```\n\n* masked：模型的目标是预测序列中被遮蔽的`token`，同时具有对序列中所有`tokens`的完全访问权限。\n\n    ```py\n    >>> text = \"Hugging Face is a community-based open-source <mask> for machine learning.\"\n    >>> fill_mask = pipeline(task=\"fill-mask\")\n    >>> preds = fill_mask(text, top_k=1)\n    >>> preds = [\n    ...     {\n    ...         \"score\": round(pred[\"score\"], 4),\n    ...         \"token\": pred[\"token\"],\n    ...         \"token_str\": pred[\"token_str\"],\n    ...         \"sequence\": pred[\"sequence\"],\n    ...     }\n    ...     for pred in preds\n    ... ]\n    >>> preds\n    [{'score': 0.2236,\n      'token': 1761,\n      'token_str': ' platform',\n      'sequence': 'Hugging Face is a community-based open-source platform for machine learning.'}]\n    ```",
    "1288": "一级标题：🤗 Transformers 能做什么\n二级标题：多模态\n内容：\n多模态任务要求模型处理多种数据模态（文本、图像、音频、视频）以解决特定问题。图像描述是一个多模态任务的例子，其中模型将图像作为输入并输出描述图像或图像某些属性的文本序列。\n\n虽然多模态模型处理不同的数据类型或模态，但内部预处理步骤帮助模型将所有数据类型转换为`embeddings`（向量或数字列表，包含有关数据的有意义信息）。对于像图像描述这样的任务，模型学习图像嵌入和文本嵌入之间的关系。\n\n### 文档问答\n\n文档问答是从文档中回答自然语言问题的任务。与`token-level`问答任务不同，文档问答将包含问题的文档的图像作为输入，并返回答案。文档问答可用于解析结构化文档并从中提取关键信息。在下面的例子中，可以从收据中提取总金额和找零金额。\n\n```py\n>>> from transformers import pipeline\n>>> from PIL import Image\n>>> import requests\n\n>>> url = \"https://huggingface.co/datasets/hf-internal-testing/example-documents/resolve/main/jpeg_images/2.jpg\"\n>>> image = Image.open(requests.get(url, stream=True).raw)\n\n>>> doc_question_answerer = pipeline(\"document-question-answering\", model=\"magorshunov/layoutlm-invoices\")\n>>> preds = doc_question_answerer(\n...     question=\"What is the total amount?\",\n...     image=image,\n... )\n>>> preds\n[{'score': 0.8531, 'answer': '17,000', 'start': 4, 'end': 4}]\n```\n\n希望这个页面为您提供了一些有关每种模态中所有类型任务的背景信息以及每个任务的实际重要性。在[下一节](tasks_explained)中，您将了解Transformers如何解决这些任务。",
    "1289": "一级标题：自动语音识别\n二级标题：无\n内容：\n[[open-in-colab]]\n\n<Youtube id=\"TksaY_FDgnk\"/>\n\n自动语音识别（ASR）将语音信号转换为文本，将一系列音频输入映射到文本输出。\nSiri 和 Alexa 这类虚拟助手使用 ASR 模型来帮助用户日常生活，还有许多其他面向用户的有用应用，如会议实时字幕和会议纪要。\n\n本指南将向您展示如何：\n\n1. 在 [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) 数据集上对\n   [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) 进行微调，以将音频转录为文本。\n2. 使用微调后的模型进行推断。\n\n<Tip>\n\n如果您想查看所有与本任务兼容的架构和检查点，最好查看[任务页](https://huggingface.co/tasks/automatic-speech-recognition)。\n\n</Tip>\n\n在开始之前，请确保您已安装所有必要的库：\n\n```bash\npip install transformers datasets evaluate jiwer\n```\n\n我们鼓励您登录自己的 Hugging Face 账户，这样您就可以上传并与社区分享您的模型。\n出现提示时，输入您的令牌登录：\n\n```py\n>>> from huggingface_hub import notebook_login\n\n>>> notebook_login()\n```",
    "1290": "一级标题：自动语音识别\n二级标题：加载 MInDS-14 数据集\n内容：\n首先从🤗 Datasets 库中加载 [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14)\n数据集的一个较小子集。这将让您有机会先进行实验，确保一切正常，然后再花更多时间在完整数据集上进行训练。\n\n```py\n>>> from datasets import load_dataset, Audio\n\n>>> minds = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train[:100]\")\n```\n\n使用 [`~Dataset.train_test_split`] 方法将数据集的 `train` 拆分为训练集和测试集：\n\n```py\n>>> minds = minds.train_test_split(test_size=0.2)\n```\n\n然后看看数据集：\n\n```py\n>>> minds\nDatasetDict({\n    train: Dataset({\n        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],\n        num_rows: 16\n    })\n    test: Dataset({\n        features: ['path', 'audio', 'transcription', 'english_transcription', 'intent_class', 'lang_id'],\n        num_rows: 4\n    })\n})\n```\n\n虽然数据集包含 `lang_id `和 `english_transcription` 等许多有用的信息，但在本指南中，\n您将专注于 `audio` 和 `transcription`。使用 [`~datasets.Dataset.remove_columns`] 方法删除其他列：\n\n```py\n>>> minds = minds.remove_columns([\"english_transcription\", \"intent_class\", \"lang_id\"])\n```\n\n再看看示例：\n\n```py\n>>> minds[\"train\"][0]\n{'audio': {'array': array([-0.00024414,  0.        ,  0.        , ...,  0.00024414,\n          0.00024414,  0.00024414], dtype=float32),\n  'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',\n  'sampling_rate': 8000},\n 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',\n 'transcription': \"hi I'm trying to use the banking app on my phone and currently my checking and savings account balance is not refreshing\"}\n```\n\n有 2 个字段：\n\n- `audio`：由语音信号形成的一维 `array`，用于加载和重新采样音频文件。\n- `transcription`：目标文本。",
    "1291": "一级标题：自动语音识别\n二级标题：预处理\n内容：\n下一步是加载一个 Wav2Vec2 处理器来处理音频信号：\n\n```py\n>>> from transformers import AutoProcessor\n\n>>> processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base\")\n```\n\nMInDS-14 数据集的采样率为 8000kHz（您可以在其[数据集卡片](https://huggingface.co/datasets/PolyAI/minds14)中找到此信息），\n这意味着您需要将数据集重新采样为 16000kHz 以使用预训练的 Wav2Vec2 模型：\n\n```py\n>>> minds = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n>>> minds[\"train\"][0]\n{'audio': {'array': array([-2.38064706e-04, -1.58618059e-04, -5.43987835e-06, ...,\n          2.78103951e-04,  2.38446111e-04,  1.18740834e-04], dtype=float32),\n  'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',\n  'sampling_rate': 16000},\n 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav',\n 'transcription': \"hi I'm trying to use the banking app on my phone and currently my checking and savings account balance is not refreshing\"}\n```\n\n如您在上面的 `transcription` 中所看到的，文本包含大小写字符的混合。\nWav2Vec2 分词器仅训练了大写字符，因此您需要确保文本与分词器的词汇表匹配：\n\n```py\n>>> def uppercase(example):\n...     return {\"transcription\": example[\"transcription\"].upper()}\n\n\n>>> minds = minds.map(uppercase)\n```\n\n现在创建一个预处理函数，该函数应该：\n\n1. 调用 `audio` 列以加载和重新采样音频文件。\n2. 从音频文件中提取 `input_values` 并使用处理器对 `transcription` 列执行 tokenizer 操作。\n\n```py\n>>> def prepare_dataset(batch):\n...     audio = batch[\"audio\"]\n...     batch = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], text=batch[\"transcription\"])\n...     batch[\"input_length\"] = len(batch[\"input_values\"][0])\n...     return batch\n```\n\n要在整个数据集上应用预处理函数，可以使用🤗 Datasets 的 [`~datasets.Dataset.map`] 函数。\n您可以通过增加 `num_proc` 参数来加速 `map` 的处理进程数量。\n使用 [`~datasets.Dataset.remove_columns`] 方法删除不需要的列：\n\n```py\n>>> encoded_minds = minds.map(prepare_dataset, remove_columns=minds.column_names[\"train\"], num_proc=4)\n```\n\n🤗 Transformers 没有用于 ASR 的数据整理器，因此您需要调整 [`DataCollatorWithPadding`] 来创建一个示例批次。\n它还会动态地将您的文本和标签填充到其批次中最长元素的长度（而不是整个数据集），以使它们具有统一的长度。\n虽然可以通过在 `tokenizer` 函数中设置 `padding=True` 来填充文本，但动态填充更有效。\n\n与其他数据整理器不同，这个特定的数据整理器需要对 `input_values` 和 `labels `应用不同的填充方法：\n\n```py\n>>> import torch\n\n>>> from dataclasses import dataclass, field\n>>> from typing import Any, Dict, List, Optional, Union\n\n\n>>> @dataclass\n... class DataCollatorCTCWithPadding:\n...     processor: AutoProcessor\n...     padding: Union[bool, str] = \"longest\"\n\n...     def __call__(self, features: list[dict[str, Union[list[int], torch.Tensor]]]) -> dict[str, torch.Tensor]:\n...         # split inputs and labels since they have to be of different lengths and need\n...         # different padding methods\n...         input_features = [{\"input_values\": feature[\"input_values\"][0]} for feature in features]\n...         label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n\n...         batch = self.processor.pad(input_features, padding=self.padding, return_tensors=\"pt\")\n\n...         labels_batch = self.processor.pad(labels=label_features, padding=self.padding, return_tensors=\"pt\")\n\n...         # replace padding with -100 to ignore loss correctly\n...         labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n\n...         batch[\"labels\"] = labels\n\n...         return batch\n```\n\n现在实例化您的 `DataCollatorForCTCWithPadding`：\n\n```py\n>>> data_collator = DataCollatorCTCWithPadding(processor=processor, padding=\"longest\")\n```",
    "1292": "一级标题：自动语音识别\n二级标题：评估\n内容：\n在训练过程中包含一个指标通常有助于评估模型的性能。\n您可以通过🤗 [Evaluate](https://huggingface.co/docs/evaluate/index) 库快速加载一个评估方法。\n对于这个任务，加载 [word error rate](https://huggingface.co/spaces/evaluate-metric/wer)（WER）指标\n（请参阅🤗 Evaluate [快速上手](https://huggingface.co/docs/evaluate/a_quick_tour)以了解如何加载和计算指标）：\n\n```py\n>>> import evaluate\n\n>>> wer = evaluate.load(\"wer\")\n```\n\n然后创建一个函数，将您的预测和标签传递给 [`~evaluate.EvaluationModule.compute`] 来计算 WER：\n\n```py\n>>> import numpy as np\n\n\n>>> def compute_metrics(pred):\n...     pred_logits = pred.predictions\n...     pred_ids = np.argmax(pred_logits, axis=-1)\n\n...     pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n\n...     pred_str = processor.batch_decode(pred_ids)\n...     label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n\n...     wer = wer.compute(predictions=pred_str, references=label_str)\n\n...     return {\"wer\": wer}\n```\n\n您的 `compute_metrics` 函数现在已经准备就绪，当您设置好训练时将返回给此函数。",
    "1293": "一级标题：自动语音识别\n二级标题：训练\n内容：\n<frameworkcontent>\n<pt>\n<Tip>\n\n如果您不熟悉使用[`Trainer`]微调模型，请查看这里的基本教程[here](../training#train-with-pytorch-trainer)！\n\n</Tip>\n\n现在您已经准备好开始训练您的模型了！使用 [`AutoModelForCTC`] 加载 Wav2Vec2。\n使用 `ctc_loss_reduction` 参数指定要应用的减少方式。通常最好使用平均值而不是默认的求和：\n\n```py\n>>> from transformers import AutoModelForCTC, TrainingArguments, Trainer\n\n>>> model = AutoModelForCTC.from_pretrained(\n...     \"facebook/wav2vec2-base\",\n...     ctc_loss_reduction=\"mean\",\n...     pad_token_id=processor.tokenizer.pad_token_id,\n)\n```\n\n此时，只剩下 3 个步骤：\n\n1. 在 [`TrainingArguments`] 中定义您的训练参数。唯一必需的参数是 `output_dir`，用于指定保存模型的位置。\n   您可以通过设置 `push_to_hub=True` 将此模型推送到 Hub（您需要登录到 Hugging Face 才能上传您的模型）。\n   在每个 epoch 结束时，[`Trainer`] 将评估 WER 并保存训练检查点。\n2. 将训练参数与模型、数据集、分词器、数据整理器和 `compute_metrics` 函数一起传递给 [`Trainer`]。\n3. 调用 [`~Trainer.train`] 来微调您的模型。\n\n```py\n>>> training_args = TrainingArguments(\n...     output_dir=\"my_awesome_asr_mind_model\",\n...     per_device_train_batch_size=8,\n...     gradient_accumulation_steps=2,\n...     learning_rate=1e-5,\n...     warmup_steps=500,\n...     max_steps=2000,\n...     gradient_checkpointing=True,\n...     fp16=True,\n...     group_by_length=True,\n...     eval_strategy=\"steps\",\n...     per_device_eval_batch_size=8,\n...     save_steps=1000,\n...     eval_steps=1000,\n...     logging_steps=25,\n...     load_best_model_at_end=True,\n...     metric_for_best_model=\"wer\",\n...     greater_is_better=False,\n...     push_to_hub=True,\n... )\n\n>>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     train_dataset=encoded_minds[\"train\"],\n...     eval_dataset=encoded_minds[\"test\"],\n...     processing_class=processor,\n...     data_collator=data_collator,\n...     compute_metrics=compute_metrics,\n... )\n\n>>> trainer.train()\n```\n\n训练完成后，使用 [`~transformers.Trainer.push_to_hub`] 方法将您的模型分享到 Hub，方便大家使用您的模型：\n\n```py\n>>> trainer.push_to_hub()\n```\n</pt>\n</frameworkcontent>\n\n<Tip>\n\n要深入了解如何微调模型进行自动语音识别，\n请查看这篇博客[文章](https://huggingface.co/blog/fine-tune-wav2vec2-english)以了解英语 ASR，\n还可以参阅[这篇文章](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2)以了解多语言 ASR。\n\n</Tip>",
    "1294": "一级标题：自动语音识别\n二级标题：推断\n内容：\n很好，现在您已经微调了一个模型，您可以用它进行推断了！\n\n加载您想要运行推断的音频文件。请记住，如果需要，将音频文件的采样率重新采样为与模型匹配的采样率！\n\n```py\n>>> from datasets import load_dataset, Audio\n\n>>> dataset = load_dataset(\"PolyAI/minds14\", \"en-US\", split=\"train\")\n>>> dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n>>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n>>> audio_file = dataset[0][\"audio\"][\"path\"]\n```\n\n尝试使用微调后的模型进行推断的最简单方法是使用 [`pipeline`]。\n使用您的模型实例化一个用于自动语音识别的 `pipeline`，并将您的音频文件传递给它：\n\n```py\n>>> from transformers import pipeline\n\n>>> transcriber = pipeline(\"automatic-speech-recognition\", model=\"stevhliu/my_awesome_asr_minds_model\")\n>>> transcriber(audio_file)\n{'text': 'I WOUD LIKE O SET UP JOINT ACOUNT WTH Y PARTNER'}\n```\n\n<Tip>\n\n转录结果还不错，但可以更好！尝试用更多示例微调您的模型，以获得更好的结果！\n\n</Tip>\n\n如果您愿意，您也可以手动复制 `pipeline` 的结果：\n\n<frameworkcontent>\n<pt>\n\n加载一个处理器来预处理音频文件和转录，并将 `input` 返回为 PyTorch 张量：\n\n```py\n>>> from transformers import AutoProcessor\n\n>>> processor = AutoProcessor.from_pretrained(\"stevhliu/my_awesome_asr_mind_model\")\n>>> inputs = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n```\n\n将您的输入传递给模型并返回 logits：\n\n```py\n>>> from transformers import AutoModelForCTC\n\n>>> model = AutoModelForCTC.from_pretrained(\"stevhliu/my_awesome_asr_mind_model\")\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n```\n\n获取具有最高概率的预测 `input_ids`，并使用处理器将预测的 `input_ids` 解码回文本：\n\n```py\n>>> import torch\n\n>>> predicted_ids = torch.argmax(logits, dim=-1)\n>>> transcription = processor.batch_decode(predicted_ids)\n>>> transcription\n['I WOUL LIKE O SET UP JOINT ACOUNT WTH Y PARTNER']\n```\n</pt>\n</frameworkcontent>",
    "1295": "一级标题：用于 TensorFlow 模型的 XLA 集成\n二级标题：无\n内容：\n[[open-in-colab]]\n\n加速线性代数，也称为XLA，是一个用于加速TensorFlow模型运行时间的编译器。从[官方文档](https://www.tensorflow.org/xla)中可以看到：\n\nXLA（加速线性代数）是一种针对线性代数的特定领域编译器，可以在可能不需要更改源代码的情况下加速TensorFlow模型。\n\n在TensorFlow中使用XLA非常简单——它包含在`tensorflow`库中，并且可以使用任何图创建函数中的`jit_compile`参数来触发，例如[`tf.function`](https://www.tensorflow.org/guide/intro_to_graphs)。在使用Keras方法如`fit()`和`predict()`时，只需将`jit_compile`参数传递给`model.compile()`即可启用XLA。然而，XLA不仅限于这些方法 - 它还可以用于加速任何任意的`tf.function`。\n\n在🤗 Transformers中，几个TensorFlow方法已经被重写为与XLA兼容，包括[GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)、[T5](https://huggingface.co/docs/transformers/model_doc/t5)和[OPT](https://huggingface.co/docs/transformers/model_doc/opt)等文本生成模型，以及[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)等语音处理模型。\n\n虽然确切的加速倍数很大程度上取决于模型，但对于🤗 Transformers中的TensorFlow文本生成模型，我们注意到速度提高了约100倍。本文档将解释如何在这些模型上使用XLA获得最大的性能。如果您有兴趣了解更多关于基准测试和我们在XLA集成背后的设计哲学的信息，我们还将提供额外的资源链接。",
    "1296": "一级标题：用于 TensorFlow 模型的 XLA 集成\n二级标题：使用 XLA 运行 TensorFlow 函数\n内容：\n让我们考虑以下TensorFlow 中的模型：\n\n```py\nimport tensorflow as tf\n\nmodel = tf.keras.Sequential(\n    [tf.keras.layers.Dense(10, input_shape=(10,), activation=\"relu\"), tf.keras.layers.Dense(5, activation=\"softmax\")]\n)\n```\n\n上述模型接受维度为 `(10,)` 的输入。我们可以像下面这样使用模型进行前向传播：\n\n```py\n# Generate random inputs for the model.\nbatch_size = 16\ninput_vector_dim = 10\nrandom_inputs = tf.random.normal((batch_size, input_vector_dim))\n\n# Run a forward pass.\n_ = model(random_inputs)\n```\n\n为了使用 XLA 编译的函数运行前向传播，我们需要执行以下操作：\n\n```py\nxla_fn = tf.function(model, jit_compile=True)\n_ = xla_fn(random_inputs)\n```\n\n`model`的默认`call()`函数用于编译XLA图。但如果你想将其他模型函数编译成XLA，也是可以的，如下所示：\n\n```py\nmy_xla_fn = tf.function(model.my_xla_fn, jit_compile=True)\n```",
    "1297": "一级标题：用于 TensorFlow 模型的 XLA 集成\n二级标题：在🤗 Transformers库中使用XLA运行TensorFlow文本生成模型\n内容：\n要在🤗 Transformers中启用XLA加速生成，您需要安装最新版本的`transformers`。您可以通过运行以下命令来安装它：\n\n```bash\npip install transformers --upgrade\n```\n\n然后您可以运行以下代码：\n\n```py\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModelForCausalLM\n\n# Will error if the minimal version of Transformers is not installed.\nfrom transformers.utils import check_min_version\n\ncheck_min_version(\"4.21.0\")\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", padding_side=\"left\", pad_token=\"</s>\")\nmodel = TFAutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\ninput_string = [\"TensorFlow is\"]\n\n# One line to create an XLA generation function\nxla_generate = tf.function(model.generate, jit_compile=True)\n\ntokenized_input = tokenizer(input_string, return_tensors=\"tf\")\ngenerated_tokens = xla_generate(**tokenized_input, num_beams=2)\n\ndecoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\nprint(f\"Generated -- {decoded_text}\")\n# Generated -- TensorFlow is an open-source, open-source, distributed-source application # framework for the\n```\n\n正如您所注意到的，在`generate()`上启用XLA只需要一行代码。其余部分代码保持不变。然而，上面的代码片段中有一些与XLA相关的注意事项。您需要了解这些注意事项，以充分利用XLA可能带来的性能提升。我们将在下面的部分讨论这些内容。",
    "1298": "一级标题：用于 TensorFlow 模型的 XLA 集成\n二级标题：需要关注的注意事项\n内容：\n当您首次执行启用XLA的函数（如上面的`xla_generate()`）时，它将在内部尝试推断计算图，这是一个耗时的过程。这个过程被称为[“tracing”](https://www.tensorflow.org/guide/intro_to_graphs#when_is_a_function_tracing)。\n\n您可能会注意到生成时间并不快。连续调用`xla_generate()`（或任何其他启用了XLA的函数）不需要再次推断计算图，只要函数的输入与最初构建计算图时的形状相匹配。对于具有固定输入形状的模态（例如图像），这不是问题，但如果您正在处理具有可变输入形状的模态（例如文本），则必须注意。\n\n为了确保`xla_generate()`始终使用相同的输入形状，您可以在调用`tokenizer`时指定`padding`参数。\n\n```py\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", padding_side=\"left\", pad_token=\"</s>\")\nmodel = TFAutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\ninput_string = [\"TensorFlow is\"]\n\nxla_generate = tf.function(model.generate, jit_compile=True)\n\n# Here, we call the tokenizer with padding options.\ntokenized_input = tokenizer(input_string, pad_to_multiple_of=8, padding=True, return_tensors=\"tf\")\n\ngenerated_tokens = xla_generate(**tokenized_input, num_beams=2)\ndecoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\nprint(f\"Generated -- {decoded_text}\")\n```\n\n通过这种方式，您可以确保`xla_generate()`的输入始终具有它跟踪的形状，从而加速生成时间。您可以使用以下代码来验证这一点：\n\n```py\nimport time\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", padding_side=\"left\", pad_token=\"</s>\")\nmodel = TFAutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n\nxla_generate = tf.function(model.generate, jit_compile=True)\n\nfor input_string in [\"TensorFlow is\", \"TensorFlow is a\", \"TFLite is a\"]:\n    tokenized_input = tokenizer(input_string, pad_to_multiple_of=8, padding=True, return_tensors=\"tf\")\n    start = time.time_ns()\n    generated_tokens = xla_generate(**tokenized_input, num_beams=2)\n    end = time.time_ns()\n    print(f\"Execution time -- {(end - start) / 1e6:.1f} ms\\n\")\n```\n\n在Tesla T4 GPU上，您可以期望如下的输出：\n\n```bash\nExecution time -- 30819.6 ms\n\nExecution time -- 79.0 ms\n\nExecution time -- 78.9 ms\n```\n\n第一次调用`xla_generate()`会因为`tracing`而耗时，但后续的调用会快得多。请注意，任何时候对生成选项的更改都会触发重新`tracing`，从而导致生成时间减慢。\n\n在本文档中，我们没有涵盖🤗 Transformers提供的所有文本生成选项。我们鼓励您阅读文档以了解高级用例。",
    "1299": "一级标题：用于 TensorFlow 模型的 XLA 集成\n二级标题：附加资源\n内容：\n以下是一些附加资源，如果您想深入了解在🤗 Transformers和其他库下使用XLA：\n\n* [这个Colab Notebook](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/91_tf_xla_generate.ipynb) 提供了一个互动演示，让您可以尝试使用XLA兼容的编码器-解码器（例如[T5](https://huggingface.co/docs/transformers/model_doc/t5)）和仅解码器（例如[GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)）文本生成模型。\n\n* [这篇博客文章](https://huggingface.co/blog/tf-xla-generate) 提供了XLA兼容模型的比较基准概述，以及关于在TensorFlow中使用XLA的友好介绍。\n\n* [这篇博客文章](https://blog.tensorflow.org/2022/11/how-hugging-face-improved-text-generation-performance-with-xla.html) 讨论了我们在🤗 Transformers中为TensorFlow模型添加XLA支持的设计理念。\n\n* 推荐用于更多学习XLA和TensorFlow图的资源：\n    * [XLA：面向机器学习的优化编译器](https://www.tensorflow.org/xla)\n    * [图和tf.function简介](https://www.tensorflow.org/guide/intro_to_graphs)\n    * [使用tf.function获得更好的性能](https://www.tensorflow.org/guide/function)",
    "1300": "一级标题：导出为 TFLite\n二级标题：无\n内容：\n[TensorFlow Lite](https://www.tensorflow.org/lite/guide) 是一个轻量级框架，用于资源受限的设备上，如手机、嵌入式系统和物联网（IoT）设备，部署机器学习模型。TFLite 旨在在计算能力、内存和功耗有限的设备上优化和高效运行模型。模型以一种特殊的高效可移植格式表示，其文件扩展名为 `.tflite`。\n\n🤗 Optimum 通过 `exporters.tflite` 模块提供将 🤗 Transformers 模型导出至 TFLite 格式的功能。请参考 [🤗 Optimum 文档](https://huggingface.co/docs/optimum/exporters/tflite/overview) 以获取支持的模型架构列表。\n\n要将模型导出为 TFLite 格式，请安装所需的依赖项：\n\n```bash\npip install optimum[exporters-tf]\n```\n\n请参阅 [🤗 Optimum 文档](https://huggingface.co/docs/optimum/main/en/exporters/tflite/usage_guides/export_a_model) 以查看所有可用参数，或者在命令行中查看帮助：\n\n```bash\noptimum-cli export tflite --help\n```\n\n运行以下命令，以从 🤗 Hub 导出模型的检查点（checkpoint），以 `google-bert/bert-base-uncased` 为例：\n\n```bash\noptimum-cli export tflite --model google-bert/bert-base-uncased --sequence_length 128 bert_tflite/\n```\n\n你应该能在日志中看到导出进度以及生成的 `model.tflite` 文件的保存位置，如下所示：\n\n```bash\nValidating TFLite model...\n\t-[✓] TFLite model output names match reference model (logits)\n\t- Validating TFLite Model output \"logits\":\n\t\t-[✓] (1, 128, 30522) matches (1, 128, 30522)\n\t\t-[x] values not close enough, max diff: 5.817413330078125e-05 (atol: 1e-05)\nThe TensorFlow Lite export succeeded with the warning: The maximum absolute difference between the output of the reference model and the TFLite exported model is not within the set tolerance 1e-05:\n- logits: max diff = 5.817413330078125e-05.\n The exported model was saved at: bert_tflite\n```\n\n上面的示例说明了从 🤗 Hub 导出检查点的过程。导出本地模型时，首先需要确保将模型的权重和分词器文件保存在同一目录（`local_path`）中。在使用 CLI（命令行）时，将 `local_path` 传递给 `model` 参数，而不是 🤗 Hub 上的检查点名称。",
    "1301": "一级标题：Transformers与Tiktonken的互操作性\n二级标题：无\n内容：\n在🤗 transformers中，当使用`from_pretrained`方法从Hub加载模型时，如果模型包含tiktoken格式的`tokenizer.model`文件，框架可以无缝支持tiktoken模型文件，并自动将其转换为我们的[快速词符化器](https://huggingface.co/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast)。\n\n### 已知包含`tiktoken.model`文件发布的模型：\n    - gpt2\n    - llama3",
    "1302": "一级标题：Transformers与Tiktonken的互操作性\n二级标题：使用示例\n内容：\n为了在transformers中正确加载`tiktoken`文件，请确保`tiktoken.model`文件是tiktoken格式的，并且会在加载`from_pretrained`时自动加载。以下展示如何从同一个文件中加载词符化器(tokenizer)和模型：\n\n```py\nfrom transformers import AutoTokenizer\n\nmodel_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_id, subfolder=\"original\")\n```",
    "1303": "一级标题：Transformers与Tiktonken的互操作性\n二级标题：创建tiktoken词符化器(tokenizer)\n内容：\n`tokenizer.model`文件中不包含任何额外的词符(token)或模式字符串(pattern strings)的信息。如果这些信息很重要，需要将词符化器(tokenizer)转换为适用于[`PreTrainedTokenizerFast`]类的`tokenizer.json`格式。\n\n使用[tiktoken.get_encoding](https://github.com/openai/tiktoken/blob/63527649963def8c759b0f91f2eb69a40934e468/tiktoken/registry.py#L63)生成`tokenizer.model`文件，再使用[`convert_tiktoken_to_fast`]函数将其转换为`tokenizer.json`文件。\n\n```py\n\nfrom transformers.integrations.tiktoken import convert_tiktoken_to_fast\nfrom tiktoken import get_encoding\n\n# You can load your custom encoding or the one provided by OpenAI\nencoding = get_encoding(\"gpt2\")\nconvert_tiktoken_to_fast(encoding, \"config/save/dir\")\n```\n\n生成的`tokenizer.json`文件将被保存到指定的目录，并且可以通过[`PreTrainedTokenizerFast`]类来加载。\n\n```py\ntokenizer = PreTrainedTokenizerFast.from_pretrained(\"config/save/dir\")\n```",
    "1304": "一级标题：分词器的摘要\n二级标题：无\n内容：\n[[open-in-colab]]\n\n在这个页面，我们来仔细研究分词的知识。\n<Youtube id=\"VFp38yj8h3A\"/>\n\n正如我们在[the preprocessing tutorial](preprocessing)所看到的那样，对文本进行分词就是将一段文本分割成很多单词或者子单词，\n这些单词或者子单词然后会通过一个查询表格被转换到id，将单词或者子单词转换到id是很直截了当的，也就是一个简单的映射，\n所以这么来看，我们主要关注将一段文本分割成很多单词或者很多子单词（像：对一段文本进行分词），更加准确的来说，我们将关注\n在🤗 Transformers内用到的三种主要类型的分词器：[Byte-Pair Encoding (BPE)](#byte-pair-encoding), [WordPiece](#wordpiece),\nand [SentencePiece](#sentencepiece)，并且给出了示例，哪个模型用到了哪种类型的分词器。\n\n注意到在每个模型的主页，你可以查看文档上相关的分词器，就可以知道预训练模型使用了哪种类型的分词器。\n举个例子，如果我们查看[`BertTokenizer`]，我们就能看到模型使用了[WordPiece](#wordpiece)。",
    "1305": "一级标题：分词器的摘要\n二级标题：介绍\n内容：\n将一段文本分词到小块是一个比它看起来更加困难的任务，并且有很多方式来实现分词，举个例子，让我们看看这个句子\n`\"Don't you love 🤗 Transformers? We sure do.\"`\n\n<Youtube id=\"nhJxYji1aho\"/>\n\n对这段文本分词的一个简单方式，就是使用空格来分词，得到的结果是：\n\n```\n[\"Don't\", \"you\", \"love\", \"🤗\", \"Transformers?\", \"We\", \"sure\", \"do.\"]\n```\n\n上面的分词是一个明智的开始，但是如果我们查看token `\"Transformers?\"` 和 `\"do.\"`，我们可以观察到标点符号附在单词`\"Transformer\"`\n和 `\"do\"`的后面，这并不是最理想的情况。我们应该将标点符号考虑进来，这样一个模型就没必要学习一个单词和每个可能跟在后面的\n标点符号的不同的组合，这么组合的话，模型需要学习的组合的数量会急剧上升。将标点符号也考虑进来，对范例文本进行分词的结果就是：\n\n```\n[\"Don\", \"'\", \"t\", \"you\", \"love\", \"🤗\", \"Transformers\", \"?\", \"We\", \"sure\", \"do\", \".\"]\n```\n\n分词的结果更好了，然而，这么做也是不好的，分词怎么处理单词`\"Don't\"`，`\"Don't\"`的含义是`\"do not\"`，所以这么分词`[\"Do\", \"n't\"]`\n会更好。现在开始事情就开始变得复杂起来了，部分的原因是每个模型都有它自己的分词类型。依赖于我们应用在文本分词上的规则，\n相同的文本会产生不同的分词输出。用在训练数据上的分词规则，被用来对输入做分词操作，一个预训练模型才会正确的执行。\n\n[spaCy](https://spacy.io/) and [Moses](http://www.statmt.org/moses/?n=Development.GetStarted) 是两个受欢迎的基于规则的\n分词器。将这两个分词器应用在示例文本上，*spaCy* 和 *Moses*会输出类似下面的结果：\n\n```\n[\"Do\", \"n't\", \"you\", \"love\", \"🤗\", \"Transformers\", \"?\", \"We\", \"sure\", \"do\", \".\"]\n```\n\n可见上面的分词使用到了空格和标点符号的分词方式，以及基于规则的分词方式。空格和标点符号分词以及基于规则的分词都是单词分词的例子。\n不那么严格的来说，单词分词的定义就是将句子分割到很多单词。然而将文本分割到更小的块是符合直觉的，当处理大型文本语料库时，上面的\n分词方法会导致很多问题。在这种情况下，空格和标点符号分词通常会产生一个非常大的词典（使用到的所有不重复的单词和tokens的集合）。\n像：[Transformer XL](model_doc/transformerxl)使用空格和标点符号分词，结果会产生一个大小是267,735的词典！\n\n这么大的一个词典容量，迫使模型有着一个巨大的embedding矩阵，以及巨大的输入和输出层，这会增加内存使用量，也会提高时间复杂度。通常\n情况下，transformers模型几乎没有词典容量大于50,000的，特别是只在一种语言上预训练的模型。\n\n所以如果简单的空格和标点符号分词让人不满意，为什么不简单的对字符分词？\n\n<Youtube id=\"ssLq_EK2jLE\"/>\n\n尽管字符分词是非常简单的，并且能极大的减少内存使用，降低时间复杂度，但是这样做会让模型很难学到有意义的输入表达。像：\n比起学到单词`\"today\"`的一个有意义的上下文独立的表达，学到字母`\"t\"`的一个有意义的上下文独立的表达是相当困难的。因此，\n字符分词经常会伴随着性能的下降。所以为了获得最好的结果，transformers模型在单词级别分词和字符级别分词之间使用了一个折中的方案\n被称作**子词**分词。",
    "1306": "一级标题：分词器的摘要\n二级标题：子词分词\n内容：\n<Youtube id=\"zHvTiHr506c\"/>\n\n子词分词算法依赖这样的原则：频繁使用的单词不应该被分割成更小的子词，但是很少使用的单词应该被分解到有意义的子词。举个例子：\n`\"annoyingly\"`能被看作一个很少使用的单词，能被分解成`\"annoying\"`和`\"ly\"`。`\"annoying\"`和`\"ly\"`作为独立地子词，出现\n的次数都很频繁，而且与此同时单词`\"annoyingly\"`的含义可以通过组合`\"annoying\"`和`\"ly\"`的含义来获得。在粘合和胶水语言上，\n像Turkish语言，这么做是相当有用的，在这样的语言里，通过线性组合子词，大多数情况下你能形成任意长的复杂的单词。\n\n子词分词允许模型有一个合理的词典大小，而且能学到有意义的上下文独立地表达。除此以外，子词分词可以让模型处理以前从来没见过的单词，\n方式是通过分解这些单词到已知的子词，举个例子：[`~transformers.BertTokenizer`]对句子`\"I have a new GPU!\"`分词的结果如下：\n\n```py\n>>> from transformers import BertTokenizer\n\n>>> tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n>>> tokenizer.tokenize(\"I have a new GPU!\")\n[\"i\", \"have\", \"a\", \"new\", \"gp\", \"##u\", \"!\"]\n```\n\n因为我们正在考虑不区分大小写的模型，句子首先被转换成小写字母形式。我们可以见到单词`[\"i\", \"have\", \"a\", \"new\"]`在分词器\n的词典内，但是这个单词`\"gpu\"`不在词典内。所以，分词器将`\"gpu\"`分割成已知的子词`[\"gp\" and \"##u\"]`。`\"##\"`意味着剩下的\ntoken应该附着在前面那个token的后面，不带空格的附着（分词的解码或者反向）。\n\n另外一个例子，[`~transformers.XLNetTokenizer`]对前面的文本例子分词结果如下：\n\n```py\n>>> from transformers import XLNetTokenizer\n\n>>> tokenizer = XLNetTokenizer.from_pretrained(\"xlnet/xlnet-base-cased\")\n>>> tokenizer.tokenize(\"Don't you love 🤗 Transformers? We sure do.\")\n[\"▁Don\", \"'\", \"t\", \"▁you\", \"▁love\", \"▁\", \"🤗\", \"▁\", \"Transform\", \"ers\", \"?\", \"▁We\", \"▁sure\", \"▁do\", \".\"]\n```\n\n当我们查看[SentencePiece](#sentencepiece)时会回过头来解释这些`\"▁\"`符号的含义。正如你能见到的，很少使用的单词\n`\"Transformers\"`能被分割到更加频繁使用的子词`\"Transform\"`和`\"ers\"`。\n\n现在让我们来看看不同的子词分割算法是怎么工作的，注意到所有的这些分词算法依赖于某些训练的方式，这些训练通常在语料库上完成，\n相应的模型也是在这个语料库上训练的。\n\n<a id='byte-pair-encoding'></a>\n\n### Byte-Pair Encoding (BPE)\n\nByte-Pair Encoding (BPE)来自于[Neural Machine Translation of Rare Words with Subword Units (Sennrich et\nal., 2015)](https://huggingface.co/papers/1508.07909)。BPE依赖于一个预分词器，这个预分词器会将训练数据分割成单词。预分词可以是简单的\n空格分词，像：：[GPT-2](model_doc/gpt2)，[RoBERTa](model_doc/roberta)。更加先进的预分词方式包括了基于规则的分词，像： [XLM](model_doc/xlm)，[FlauBERT](model_doc/flaubert)，FlauBERT在大多数语言使用了Moses，或者[GPT](model_doc/gpt)，GPT\n使用了Spacy和ftfy，统计了训练语料库中每个单词的频次。\n\n在预分词以后，生成了单词的集合，也确定了训练数据中每个单词出现的频次。下一步，BPE产生了一个基础词典，包含了集合中所有的符号，\nBPE学习融合的规则-组合基础词典中的两个符号来形成一个新的符号。BPE会一直学习直到词典的大小满足了期望的词典大小的要求。注意到\n期望的词典大小是一个超参数，在训练这个分词器以前就需要人为指定。\n\n举个例子，让我们假设在预分词以后，下面的单词集合以及他们的频次都已经确定好了：\n\n```\n(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n```\n\n所以，基础的词典是`[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"]`。将所有单词分割成基础词典内的符号，就可以获得：\n\n```\n(\"h\" \"u\" \"g\", 10), (\"p\" \"u\" \"g\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"u\" \"g\" \"s\", 5)\n```\nBPE接着会统计每个可能的符号对的频次，然后挑出出现最频繁的的符号对，在上面的例子中，`\"h\"`跟了`\"u\"`出现了10 + 5 = 15次\n（10次是出现了10次`\"hug\"`，5次是出现了5次`\"hugs\"`）。然而，最频繁的符号对是`\"u\"`后面跟了个`\"g\"`，总共出现了10 + 5 + 5\n= 20次。因此，分词器学到的第一个融合规则是组合所有的`\"u\"`后面跟了个`\"g\"`符号。下一步，`\"ug\"`被加入到了词典内。单词的集合\n就变成了：\n\n```\n(\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"ug\" \"s\", 5)\n```\n\nBPE接着会统计出下一个最普遍的出现频次最大的符号对。也就是`\"u\"`后面跟了个`\"n\"`，出现了16次。`\"u\"`，`\"n\"`被融合成了`\"un\"`。\n也被加入到了词典中，再下一个出现频次最大的符号对是`\"h\"`后面跟了个`\"ug\"`，出现了15次。又一次这个符号对被融合成了`\"hug\"`，\n也被加入到了词典中。\n\n在当前这步，词典是`[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"]`，我们的单词集合则是：\n\n```\n(\"hug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"hug\" \"s\", 5)\n```\n\n假设，the Byte-Pair Encoding在这个时候停止训练，学到的融合规则并应用到其他新的单词上（只要这些新单词不包括不在基础词典内的符号\n就行）。举个例子，单词`\"bug\"`会被分词到`[\"b\", \"ug\"]`，但是`\"mug\"`会被分词到`[\"<unk>\", \"ug\"]`，因为符号`\"m\"`不在基础词典内。\n通常来看的话，单个字母像`\"m\"`不会被`\"<unk>\"`符号替换掉，因为训练数据通常包括了每个字母，每个字母至少出现了一次，但是在特殊的符号\n中也可能发生像emojis。\n\n就像之前提到的那样，词典的大小，举个例子，基础词典的大小 + 融合的数量，是一个需要配置的超参数。举个例子：[GPT](model_doc/gpt)\n的词典大小是40,478，因为GPT有着478个基础词典内的字符，在40,000次融合以后选择了停止训练。\n\n#### Byte-level BPE\n\n一个包含了所有可能的基础字符的基础字典可能会非常大，如果考虑将所有的unicode字符作为基础字符。为了拥有一个更好的基础词典，[GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)使用了字节\n作为基础词典，这是一个非常聪明的技巧，迫使基础词典是256大小，而且确保了所有基础字符包含在这个词典内。使用了其他的规则\n来处理标点符号，这个GPT2的分词器能对每个文本进行分词，不需要使用到<unk>符号。[GPT-2](model_doc/gpt)有一个大小是50,257\n的词典，对应到256字节的基础tokens，一个特殊的文本结束token，这些符号经过了50,000次融合学习。\n\n<a id='wordpiece'></a>\n\n### WordPiece\n\nWordPiece是子词分词算法，被用在[BERT](model_doc/bert)，[DistilBERT](model_doc/distilbert)，和[Electra](model_doc/electra)。\n这个算法发布在[Japanese and Korean\nVoice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf)\n和BPE非常相似。WordPiece首先初始化一个词典，这个词典包含了出现在训练数据中的每个字符，然后递进的学习一个给定数量的融合规则。和BPE相比较，\nWordPiece不会选择出现频次最大的符号对，而是选择了加入到字典以后能最大化训练数据似然值的符号对。\n\n所以这到底意味着什么？参考前面的例子，最大化训练数据的似然值，等价于找到一个符号对，它们的概率除以这个符号对中第一个符号的概率，\n接着除以第二个符号的概率，在所有的符号对中商最大。像：如果`\"ug\"`的概率除以`\"u\"`除以`\"g\"`的概率的商，比其他任何符号对更大，\n这个时候才能融合`\"u\"`和`\"g\"`。直觉上，WordPiece，和BPE有点点不同，WordPiece是评估融合两个符号会失去的量，来确保这么做是值得的。\n\n<a id='unigram'></a>\n\n### Unigram\n\nUnigram是一个子词分词器算法，介绍见[Subword Regularization: Improving Neural Network Translation\nModels with Multiple Subword Candidates (Kudo, 2018)](https://huggingface.co/papers/1804.10959)。和BPE或者WordPiece相比较\n，Unigram使用大量的符号来初始化它的基础字典，然后逐渐的精简每个符号来获得一个更小的词典。举例来看基础词典能够对应所有的预分词\n的单词以及最常见的子字符串。Unigram没有直接用在任何transformers的任何模型中，但是和[SentencePiece](#sentencepiece)一起联合使用。\n\n在每个训练的步骤，Unigram算法在当前词典的训练数据上定义了一个损失函数（经常定义为log似然函数的），还定义了一个unigram语言模型。\n然后，对词典内的每个符号，算法会计算如果这个符号从词典内移除，总的损失会升高多少。Unigram然后会移除百分之p的符号，这些符号的loss\n升高是最低的（p通常是10%或者20%），像：这些在训练数据上对总的损失影响最小的符号。重复这个过程，直到词典已经达到了期望的大小。\n为了任何单词都能被分词，Unigram算法总是保留基础的字符。\n\n因为Unigram不是基于融合规则（和BPE以及WordPiece相比较），在训练以后算法有几种方式来分词，如果一个训练好的Unigram分词器\n的词典是这个：\n\n```\n[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"],\n```\n`\"hugs\"`可以被分词成`[\"hug\", \"s\"]`, `[\"h\", \"ug\", \"s\"]`或者`[\"h\", \"u\", \"g\", \"s\"]`。所以选择哪一个呢？Unigram在保存\n词典的时候还会保存训练语料库内每个token的概率，所以在训练以后可以计算每个可能的分词结果的概率。实际上算法简单的选择概率\n最大的那个分词结果，但是也会提供概率来根据分词结果的概率来采样一个可能的分词结果。\n\n分词器在损失函数上训练，这些损失函数定义了这些概率。假设训练数据包含了这些单词 $x_{1}$, $\\dots$, $x_{N}$，一个单词$x_{i}$\n的所有可能的分词结果的集合定义为$S(x_{i})$，然后总的损失就可以定义为：\n\n$$\\mathcal{L} = -\\sum_{i=1}^{N} \\log \\left ( \\sum_{x \\in S(x_{i})} p(x) \\right )$$\n\n<a id='sentencepiece'></a>\n\n### SentencePiece\n目前为止描述的所有分词算法都有相同的问题：它们都假设输入的文本使用空格来分开单词。然而，不是所有的语言都使用空格来分开单词。\n一个可能的解决方案是使用某种语言特定的预分词器。像：[XLM](model_doc/xlm)使用了一个特定的中文、日语和Thai的预分词器。\n为了更加广泛的解决这个问题，[SentencePiece: A simple and language independent subword tokenizer and\ndetokenizer for Neural Text Processing (Kudo et al., 2018)](https://huggingface.co/papers/1808.06226)\n将输入文本看作一个原始的输入流，因此使用的符合集合中也包括了空格。SentencePiece然后会使用BPE或者unigram算法来产生合适的\n词典。\n\n举例来说，[`XLNetTokenizer`]使用了SentencePiece，这也是为什么上面的例子中`\"▁\"`符号包含在词典内。SentencePiece解码是非常容易的，因为所有的tokens能被concatenate起来，然后将`\"▁\"`替换成空格。\n\n库内所有使用了SentencePiece的transformers模型，会和unigram组合起来使用，像：使用了SentencePiece的模型是[ALBERT](model_doc/albert),\n[XLNet](model_doc/xlnet)，[Marian](model_doc/marian)，和[T5](model_doc/t5)。",
    "1307": "一级标题：导出为 TorchScript\n二级标题：无\n内容：\n<Tip>\n\n这是开始使用 TorchScript 进行实验的起点，我们仍在探索其在变量输入大小模型中的能力。\n这是我们关注的焦点，我们将在即将发布的版本中深入分析，提供更多的代码示例、更灵活的实现以及比较\nPython 代码与编译 TorchScript 的性能基准。\n\n</Tip>\n\n根据 [TorchScript 文档](https://pytorch.org/docs/stable/jit.html)：\n\n> TorchScript 是从 PyTorch 代码创建可序列化和可优化的模型的一种方式。\n\n有两个 PyTorch 模块：[JIT 和 TRACE](https://pytorch.org/docs/stable/jit.html)。\n这两个模块允许开发人员将其模型导出到其他程序中重用，比如面向效率的 C++ 程序。\n\n我们提供了一个接口，允许您将 🤗 Transformers 模型导出为 TorchScript，\n以便在与基于 PyTorch 的 Python 程序不同的环境中重用。\n本文解释如何使用 TorchScript 导出并使用我们的模型。\n\n导出模型需要两个步骤：\n\n- 使用 `torchscript` 参数实例化模型\n- 使用虚拟输入进行前向传递\n\n这些必要条件意味着开发人员应该注意以下详细信息。",
    "1308": "一级标题：导出为 TorchScript\n二级标题：TorchScript 参数和绑定权重\n内容：\n`torchscript` 参数是必需的，因为大多数 🤗 Transformers 语言模型的 `Embedding` 层和\n`Decoding` 层之间有绑定权重。TorchScript 不允许导出具有绑定权重的模型，因此必须事先解绑和克隆权重。\n\n使用 `torchscript` 参数实例化的模型将其 `Embedding` 层和 `Decoding` 层分开，\n这意味着它们不应该在后续进行训练。训练将导致这两层不同步，产生意外结果。\n\n对于没有语言模型头部的模型，情况不同，因为这些模型没有绑定权重。\n这些模型可以安全地导出而无需 `torchscript` 参数。",
    "1309": "一级标题：导出为 TorchScript\n二级标题：虚拟输入和标准长度\n内容：\n虚拟输入用于模型的前向传递。当输入的值传播到各层时，PyTorch 会跟踪在每个张量上执行的不同操作。\n然后使用记录的操作来创建模型的 *trace* 。\n\n跟踪是相对于输入的维度创建的。因此，它受到虚拟输入的维度限制，对于任何其他序列长度或批量大小都不起作用。\n当尝试使用不同大小时，会引发以下错误：\n\n```text\n`The expanded size of the tensor (3) must match the existing size (7) at non-singleton dimension 2`\n```\n\n我们建议使用至少与推断期间将馈送到模型的最大输入一样大的虚拟输入大小进行跟踪。\n填充可以帮助填补缺失的值。然而，由于模型是使用更大的输入大小进行跟踪的，矩阵的维度也会很大，导致更多的计算。\n\n在每个输入上执行的操作总数要仔细考虑，并在导出不同序列长度模型时密切关注性能。",
    "1310": "一级标题：导出为 TorchScript\n二级标题：在 Python 中使用 TorchScript\n内容：\n本节演示了如何保存和加载模型以及如何使用 trace 进行推断。\n\n### 保存模型\n\n要使用 TorchScript 导出 `BertModel`，请从 `BertConfig` 类实例化 `BertModel`，\n然后将其保存到名为 `traced_bert.pt` 的磁盘文件中：\n\n```python\nfrom transformers import BertModel, BertTokenizer, BertConfig\nimport torch\n\nenc = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n\n# 对输入文本分词\ntext = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\ntokenized_text = enc.tokenize(text)\n\n# 屏蔽一个输入 token\nmasked_index = 8\ntokenized_text[masked_index] = \"[MASK]\"\nindexed_tokens = enc.convert_tokens_to_ids(tokenized_text)\nsegments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n\n# 创建虚拟输入\ntokens_tensor = torch.tensor([indexed_tokens])\nsegments_tensors = torch.tensor([segments_ids])\ndummy_input = [tokens_tensor, segments_tensors]\n\n# 使用 torchscript 参数初始化模型\n# 即使此模型没有 LM Head，也将参数设置为 True。\nconfig = BertConfig(\n    vocab_size_or_config_json_file=32000,\n    hidden_size=768,\n    num_hidden_layers=12,\n    num_attention_heads=12,\n    intermediate_size=3072,\n    torchscript=True,\n)\n\n# 实例化模型\nmodel = BertModel(config)\n\n# 模型需要处于评估模式\nmodel.eval()\n\n# 如果您使用 *from_pretrained* 实例化模型，还可以轻松设置 TorchScript 参数\nmodel = BertModel.from_pretrained(\"google-bert/bert-base-uncased\", torchscript=True)\n\n# 创建 trace\ntraced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])\ntorch.jit.save(traced_model, \"traced_bert.pt\")\n```\n\n### 加载模型\n\n现在，您可以从磁盘加载先前保存的 `BertModel`、`traced_bert.pt`，并在先前初始化的 `dummy_input` 上使用：\n\n```python\nloaded_model = torch.jit.load(\"traced_bert.pt\")\nloaded_model.eval()\n\nall_encoder_layers, pooled_output = loaded_model(*dummy_input)\n```\n\n### 使用 trace 模型进行推断\n\n通过使用其 `__call__` dunder 方法使用 trace 模型进行推断：\n\n```python\ntraced_model(tokens_tensor, segments_tensors)\n```",
    "1311": "一级标题：导出为 TorchScript\n二级标题：使用 Neuron SDK 将 Hugging Face TorchScript 模型部署到 AWS\n内容：\nAWS 引入了用于云端低成本、高性能机器学习推理的\n[Amazon EC2 Inf1](https://aws.amazon.com/ec2/instance-types/inf1/) 实例系列。\nInf1 实例由 AWS Inferentia 芯片提供支持，这是一款专为深度学习推理工作负载而构建的定制硬件加速器。\n[AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/#) 是\nInferentia 的 SDK，支持对 transformers 模型进行跟踪和优化，以便在 Inf1 上部署。Neuron SDK 提供：\n\n1. 简单易用的 API，只需更改一行代码即可为云端推理跟踪和优化 TorchScript 模型。\n2. 针对[改进的性能成本](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/benchmark/)的即插即用性能优化。\n3. 支持使用 [PyTorch](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html)\n   或 [TensorFlow](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/tensorflow/huggingface_bert/huggingface_bert.html)\n   构建的 Hugging Face transformers 模型。\n\n### 影响\n\n基于 [BERT（来自 Transformers 的双向编码器表示）](https://huggingface.co/docs/transformers/main/model_doc/bert)架构的\ntransformers 模型，或其变体，如 [distilBERT](https://huggingface.co/docs/transformers/main/model_doc/distilbert)\n和 [roBERTa](https://huggingface.co/docs/transformers/main/model_doc/roberta) 在 Inf1 上运行最佳，\n可用于生成抽取式问答、序列分类和标记分类等任务。然而，文本生成任务仍可以适应在 Inf1 上运行，\n如这篇 [AWS Neuron MarianMT 教程](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/transformers-marianmt.html)所述。\n有关可以直接在 Inferentia 上转换的模型的更多信息，请参阅 Neuron 文档的[模型架构适配](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/models/models-inferentia.html#models-inferentia)章节。\n\n### 依赖关系\n\n使用 AWS Neuron 将模型转换为模型需要一个\n[Neuron SDK 环境](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/index.html#installation-guide)，\n它已经预先配置在 [AWS 深度学习 AMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-launching.html)上。\n\n### 将模型转换为 AWS Neuron\n\n使用与 [Python 中使用 TorchScript](torchscript#using-torchscript-in-python) 相同的代码来跟踪\n`BertModel` 以将模型转换为 AWS NEURON。导入 `torch.neuron` 框架扩展以通过 Python API 访问 Neuron SDK 的组件：\n\n```python\nfrom transformers import BertModel, BertTokenizer, BertConfig\nimport torch\nimport torch.neuron\n```\n\n您只需要修改下面这一行：\n\n```diff\n- torch.jit.trace(model, [tokens_tensor, segments_tensors])\n+ torch.neuron.trace(model, [token_tensor, segments_tensors])\n```\n\n这样就能使 Neuron SDK 跟踪模型并对其进行优化，以在 Inf1 实例上运行。\n\n要了解有关 AWS Neuron SDK 功能、工具、示例教程和最新更新的更多信息，\n请参阅 [AWS NeuronSDK 文档](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html)。",
    "1312": "一级标题：微调预训练模型\n二级标题：无\n内容：\n[[open-in-colab]]\n\n使用预训练模型有许多显著的好处。它降低了计算成本，减少了碳排放，同时允许您使用最先进的模型，而无需从头开始训练一个。🤗 Transformers 提供了涉及各种任务的成千上万的预训练模型。当您使用预训练模型时，您需要在与任务相关的数据集上训练该模型。这种操作被称为微调，是一种非常强大的训练技术。在本教程中，您将使用您选择的深度学习框架来微调一个预训练模型：\n\n* 使用 🤗 Transformers 的 [`Trainer`] 来微调预训练模型。\n* 在 TensorFlow 中使用 Keras 来微调预训练模型。\n* 在原生 PyTorch 中微调预训练模型。\n\n<a id='data-processing'></a>",
    "1313": "一级标题：微调预训练模型\n二级标题：准备数据集\n内容：\n<Youtube id=\"_BZearw7f0w\"/>\n\n在您进行预训练模型微调之前，需要下载一个数据集并为训练做好准备。之前的教程向您展示了如何处理训练数据，现在您有机会将这些技能付诸实践！\n\n首先，加载[Yelp评论](https://huggingface.co/datasets/yelp_review_full)数据集：\n\n```py\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset(\"yelp_review_full\")\n>>> dataset[\"train\"][100]\n{'label': 0,\n 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\\\"serving off their orders\\\\\" when they didn\\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\'ve eaten at various McDonalds restaurants for over 30 years. I\\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}\n```\n\n正如您现在所知，您需要一个`tokenizer`来处理文本，包括填充和截断操作以处理可变的序列长度。如果要一次性处理您的数据集，可以使用 🤗 Datasets 的 [`map`](https://huggingface.co/docs/datasets/process#map) 方法，将预处理函数应用于整个数据集：\n\n```py\n>>> from transformers import AutoTokenizer\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n\n\n>>> def tokenize_function(examples):\n...     return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n\n>>> tokenized_datasets = dataset.map(tokenize_function, batched=True)\n```\n如果愿意的话，您可以从完整数据集提取一个较小子集来进行微调，以减少训练所需的时间：\n\n```py\n>>> small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n>>> small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n```\n\n<a id='trainer'></a>",
    "1314": "一级标题：微调预训练模型\n二级标题：训练\n内容：\n此时，您应该根据您训练所用的框架来选择对应的教程章节。您可以使用右侧的链接跳转到您想要的章节 - 如果您想隐藏某个框架对应的所有教程内容，只需使用右上角的按钮！\n\n\n<frameworkcontent>\n<pt>\n<Youtube id=\"nvBXf7s7vTI\"/>",
    "1315": "一级标题：微调预训练模型\n二级标题：使用 PyTorch Trainer 进行训练\n内容：\n🤗 Transformers 提供了一个专为训练 🤗 Transformers 模型而优化的 [`Trainer`] 类，使您无需手动编写自己的训练循环步骤而更轻松地开始训练模型。[`Trainer`] API 支持各种训练选项和功能，如日志记录、梯度累积和混合精度。\n\n首先加载您的模型并指定期望的标签数量。根据 Yelp Review [数据集卡片](https://huggingface.co/datasets/yelp_review_full#data-fields)，您知道有五个标签：\n\n\n```py\n>>> from transformers import AutoModelForSequenceClassification\n\n>>> model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\n```\n\n<Tip>\n\n您将会看到一个警告，提到一些预训练权重未被使用，以及一些权重被随机初始化。不用担心，这是完全正常的！BERT 模型的预训练`head`被丢弃，并替换为一个随机初始化的分类`head`。您将在您的序列分类任务上微调这个新模型`head`，将预训练模型的知识转移给它。\n\n</Tip>\n\n### 训练超参数\n\n接下来，创建一个 [`TrainingArguments`] 类，其中包含您可以调整的所有超参数以及用于激活不同训练选项的标志。对于本教程，您可以从默认的训练[超参数](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)开始，但随时可以尝试不同的设置以找到最佳设置。\n\n指定保存训练检查点的位置：\n\n```py\n>>> from transformers import TrainingArguments\n\n>>> training_args = TrainingArguments(output_dir=\"test_trainer\")\n```\n\n### 评估\n\n[`Trainer`] 在训练过程中不会自动评估模型性能。您需要向 [`Trainer`] 传递一个函数来计算和展示指标。[🤗 Evaluate](https://huggingface.co/docs/evaluate/index) 库提供了一个简单的 [`accuracy`](https://huggingface.co/spaces/evaluate-metric/accuracy) 函数，您可以使用 [`evaluate.load`] 函数加载它（有关更多信息，请参阅此[快速入门](https://huggingface.co/docs/evaluate/a_quick_tour)）：\n\n```py\n>>> import numpy as np\n>>> import evaluate\n\n>>> metric = evaluate.load(\"accuracy\")\n```\n在 `metric` 上调用 [`~evaluate.compute`] 来计算您的预测的准确性。在将预测传递给 `compute` 之前，您需要将预测转换为`logits`（请记住，所有 🤗 Transformers 模型都返回对`logits`）：\n\n```py\n>>> def compute_metrics(eval_pred):\n...     logits, labels = eval_pred\n...     predictions = np.argmax(logits, axis=-1)\n...     return metric.compute(predictions=predictions, references=labels)\n```\n\n如果您希望在微调过程中监视评估指标，请在您的训练参数中指定 `eval_strategy` 参数，以在每个`epoch`结束时展示评估指标：\n\n```py\n>>> from transformers import TrainingArguments, Trainer\n\n>>> training_args = TrainingArguments(output_dir=\"test_trainer\", eval_strategy=\"epoch\")\n```\n\n### 训练器\n\n创建一个包含您的模型、训练参数、训练和测试数据集以及评估函数的 [`Trainer`] 对象：\n\n\n```py\n>>> trainer = Trainer(\n...     model=model,\n...     args=training_args,\n...     train_dataset=small_train_dataset,\n...     eval_dataset=small_eval_dataset,\n...     compute_metrics=compute_metrics,\n... )\n```\n然后调用[`~transformers.Trainer.train`]以微调模型：\n\n```py\n>>> trainer.train()\n```\n</pt>\n<tf>\n<a id='keras'></a>\n\n<Youtube id=\"rnTGBy2ax1c\"/>",
    "1316": "一级标题：微调预训练模型\n二级标题：使用keras训练TensorFlow模型\n内容：\n您也可以使用 Keras API 在 TensorFlow 中训练 🤗 Transformers 模型！\n\n### 加载用于 Keras 的数据\n\n当您希望使用 Keras API 训练 🤗 Transformers 模型时，您需要将您的数据集转换为 Keras 可理解的格式。如果您的数据集很小，您可以将整个数据集转换为NumPy数组并传递给 Keras。在进行更复杂的操作之前，让我们先尝试这种方法。\n\n首先，加载一个数据集。我们将使用 [GLUE benchmark](https://huggingface.co/datasets/glue) 中的 CoLA 数据集，因为它是一个简单的二元文本分类任务。现在只使用训练数据集。\n\n\n```py\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"glue\", \"cola\")\ndataset = dataset[\"train\"]  # Just take the training split for now\n```\n接下来，加载一个`tokenizer`并将数据标记为 NumPy 数组。请注意，标签已经是由 0 和 1 组成的`list`，因此我们可以直接将其转换为 NumPy 数组而无需进行分词处理！\n\n```py\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\ntokenized_data = tokenizer(dataset[\"sentence\"], return_tensors=\"np\", padding=True)\n# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\ntokenized_data = dict(tokenized_data)\n\nlabels = np.array(dataset[\"label\"])  # Label is already an array of 0 and 1\n```\n最后，加载、[`compile`](https://keras.io/api/models/model_training_apis/#compile-method) 和 [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) 模型。请注意，Transformers 模型都有一个默认的与任务相关的损失函数，因此除非您希望自定义，否则无需指定一个损失函数：\n\n```py\nfrom transformers import TFAutoModelForSequenceClassification\nfrom tensorflow.keras.optimizers import Adam\n\n# Load and compile our model\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\")\n# Lower learning rates are often better for fine-tuning transformers\nmodel.compile(optimizer=Adam(3e-5))  # No loss argument!\n\nmodel.fit(tokenized_data, labels)\n```\n\n<Tip>\n\n当您使用 `compile()` 编译模型时，无需传递损失参数！如果不指定损失参数，Hugging Face 模型会自动选择适合其任务和模型架构的损失函数。如果需要，您始终可以自己指定损失函数以覆盖默认配置。\n\n</Tip>\n\n这种方法对于较小的数据集效果很好，但对于较大的数据集，您可能会发现它开始变得有问题。为什么呢？因为分词后的数组和标签必须完全加载到内存中，而且由于 NumPy 无法处理“不规则”数组，因此每个分词后的样本长度都必须被填充到数据集中最长样本的长度。这将使您的数组变得更大，而所有这些`padding tokens`也会减慢训练速度！\n\n\n### 将数据加载为 tf.data.Dataset\n\n如果您想避免训练速度减慢，可以将数据加载为 `tf.data.Dataset`。虽然您可以自己编写自己的 `tf.data` 流水线，但我们有两种方便的方法来实现这一点：\n\n- [`~TFPreTrainedModel.prepare_tf_dataset`]：这是我们在大多数情况下推荐的方法。因为它是模型上的一个方法，它可以检查模型以自动确定哪些列可用作模型输入，并丢弃其他列以创建一个更简单、性能更好的数据集。\n- [`~datasets.Dataset.to_tf_dataset`]：这个方法更低级，但当您希望完全控制数据集的创建方式时非常有用，可以通过指定要包括的确切 `columns` 和 `label_cols` 来实现。\n\n在使用 [`~TFPreTrainedModel.prepare_tf_dataset`] 之前，您需要将`tokenizer`的输出添加到数据集作为列，如下面的代码示例所示：\n\n```py\ndef tokenize_dataset(data):\n    # Keys of the returned dictionary will be added to the dataset as columns\n    return tokenizer(data[\"text\"])\n\n\ndataset = dataset.map(tokenize_dataset)\n```\n请记住，默认情况下，Hugging Face 数据集存储在硬盘上，因此这不会增加您的内存使用！一旦列已经添加，您可以从数据集中流式的传输批次数据，并为每个批次添加`padding tokens`，这与为整个数据集添加`padding tokens`相比，大大减少了`padding tokens`的数量。\n\n```py\n>>> tf_dataset = model.prepare_tf_dataset(dataset[\"train\"], batch_size=16, shuffle=True, tokenizer=tokenizer)\n```\n请注意，在上面的代码示例中，您需要将`tokenizer`传递给`prepare_tf_dataset`，以便它可以在加载批次时正确填充它们。如果数据集中的所有样本都具有相同的长度而且不需要填充，您可以跳过此参数。如果需要执行比填充样本更复杂的操作（例如，用于掩码语言模型的`tokens` 替换），则可以使用 `collate_fn` 参数，而不是传递一个函数来将样本列表转换为批次并应用任何所需的预处理。请查看我们的[示例](https://github.com/huggingface/transformers/tree/main/examples)或[笔记](https://huggingface.co/docs/transformers/notebooks)以了解此方法的实际操作。\n\n一旦创建了 `tf.data.Dataset`，您可以像以前一样编译和训练模型：\n\n```py\nmodel.compile(optimizer=Adam(3e-5))  # No loss argument!\n\nmodel.fit(tf_dataset)\n```\n\n</tf>\n</frameworkcontent>\n\n<a id='pytorch_native'></a>",
    "1317": "一级标题：微调预训练模型\n二级标题：在原生 PyTorch 中训练\n内容：\n<frameworkcontent>\n<pt>\n<Youtube id=\"Dh9CL8fyG80\"/>\n\n[`Trainer`] 负责训练循环，允许您在一行代码中微调模型。对于喜欢编写自己训练循环的用户，您也可以在原生 PyTorch 中微调 🤗 Transformers 模型。\n\n现在，您可能需要重新启动您的`notebook`，或执行以下代码以释放一些内存：\n\n```py\ndel model\ndel trainer\ntorch.cuda.empty_cache()\n```\n\n接下来，手动处理 `tokenized_dataset` 以准备进行训练。\n\n1. 移除 text 列，因为模型不接受原始文本作为输入：\n\n    ```py\n    >>> tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n    ```\n\n2. 将 label 列重命名为 labels，因为模型期望参数的名称为 labels：\n\n    ```py\n    >>> tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n    ```\n\n3. 设置数据集的格式以返回 PyTorch 张量而不是`lists`：\n\n    ```py\n    >>> tokenized_datasets.set_format(\"torch\")\n    ```\n\n接着，创建一个先前展示的数据集的较小子集，以加速微调过程\n\n```py\n>>> small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n>>> small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n```\n\n### DataLoader\n\n您的训练和测试数据集创建一个`DataLoader`类，以便可以迭代处理数据批次\n\n```py\n>>> from torch.utils.data import DataLoader\n\n>>> train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n>>> eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)\n```\n\n加载您的模型，并指定期望的标签数量：\n\n```py\n>>> from transformers import AutoModelForSequenceClassification\n\n>>> model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\n```\n\n### Optimizer and learning rate scheduler\n\n创建一个`optimizer`和`learning rate scheduler`以进行模型微调。让我们使用 PyTorch 中的 [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) 优化器：\n\n```py\n>>> from torch.optim import AdamW\n\n>>> optimizer = AdamW(model.parameters(), lr=5e-5)\n```\n\n创建来自 [`Trainer`] 的默认`learning rate scheduler`：\n\n\n```py\n>>> from transformers import get_scheduler\n\n>>> num_epochs = 3\n>>> num_training_steps = num_epochs * len(train_dataloader)\n>>> lr_scheduler = get_scheduler(\n...     name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n... )\n```\n\n最后，指定 `device` 以使用 GPU（如果有的话）。否则，使用 CPU 进行训练可能需要几个小时，而不是几分钟。\n\n\n```py\n>>> import torch\n\n>>> device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n>>> model.to(device)\n```\n\n<Tip>\n\n如果没有 GPU，可以通过notebook平台如 [Colaboratory](https://colab.research.google.com/) 或 [SageMaker StudioLab](https://studiolab.sagemaker.aws/) 来免费获得云端GPU使用。\n\n</Tip>\n\n现在您已经准备好训练了！🥳\n\n### 训练循环\n\n为了跟踪训练进度，使用 [tqdm](https://tqdm.github.io/) 库来添加一个进度条，显示训练步数的进展：\n\n```py\n>>> from tqdm.auto import tqdm\n\n>>> progress_bar = tqdm(range(num_training_steps))\n\n>>> model.train()\n>>> for epoch in range(num_epochs):\n...     for batch in train_dataloader:\n...         batch = {k: v.to(device) for k, v in batch.items()}\n...         outputs = model(**batch)\n...         loss = outputs.loss\n...         loss.backward()\n\n...         optimizer.step()\n...         lr_scheduler.step()\n...         optimizer.zero_grad()\n...         progress_bar.update(1)\n```\n\n### 评估\n\n就像您在 [`Trainer`] 中添加了一个评估函数一样，当您编写自己的训练循环时，您需要做同样的事情。但与在每个`epoch`结束时计算和展示指标不同，这一次您将使用 [`~evaluate.add_batch`] 累积所有批次，并在最后计算指标。\n\n```py\n>>> import evaluate\n\n>>> metric = evaluate.load(\"accuracy\")\n>>> model.eval()\n>>> for batch in eval_dataloader:\n...     batch = {k: v.to(device) for k, v in batch.items()}\n...     with torch.no_grad():\n...         outputs = model(**batch)\n\n...     logits = outputs.logits\n...     predictions = torch.argmax(logits, dim=-1)\n...     metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n\n>>> metric.compute()\n```\n</pt>\n</frameworkcontent>\n\n<a id='additional-resources'></a>",
    "1318": "一级标题：微调预训练模型\n二级标题：附加资源\n内容：\n更多微调例子可参考如下链接：\n\n- [🤗 Transformers 示例](https://github.com/huggingface/transformers/tree/main/examples) 包含用于在 PyTorch 和 TensorFlow 中训练常见自然语言处理任务的脚本。\n\n- [🤗 Transformers 笔记](notebooks) 包含针对特定任务在 PyTorch 和 TensorFlow 中微调模型的各种`notebook`。",
    "1319": "一级标题：torch.accelerator\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.accelerator\n```\n\n```{eval-rst}\n.. currentmodule:: torch.accelerator\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    device_count\n    is_available\n    current_accelerator\n    set_device_index\n    set_device_idx\n    current_device_index\n    current_device_idx\n    set_stream\n    current_stream\n    synchronize\n    device_index\n```\n\n```{eval-rst}\n.. automodule:: torch.accelerator.memory\n```\n```{eval-rst}\n.. currentmodule:: torch.accelerator.memory\n```",
    "1320": "一级标题：torch.accelerator\n二级标题：Memory management\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n     empty_cache\n     max_memory_allocated\n     max_memory_reserved\n     memory_allocated\n     memory_reserved\n     memory_stats\n     reset_accumulated_memory_stats\n     reset_peak_memory_stats\n```",
    "1321": "一级标题：Automatic Mixed Precision package - torch.amp\n二级标题：无\n内容：\n% Both modules below are missing doc entry. Adding them here for now.\n\n% This does not add anything to the rendered page\n\n```{eval-rst}\n.. py:module:: torch.cpu.amp\n```\n\n```{eval-rst}\n.. py:module:: torch.cuda.amp\n```\n\n```{eval-rst}\n.. automodule:: torch.amp\n```\n\n```{eval-rst}\n.. currentmodule:: torch.amp\n```\n\n{class}`torch.amp` provides convenience methods for mixed precision,\nwhere some operations use the `torch.float32` (`float`) datatype and other operations\nuse lower precision floating point datatype (`lower_precision_fp`): `torch.float16` (`half`) or `torch.bfloat16`. Some ops, like linear layers and convolutions,\nare much faster in `lower_precision_fp`. Other ops, like reductions, often require the dynamic\nrange of `float32`. Mixed precision tries to match each op to its appropriate datatype.\n\nOrdinarily, \"automatic mixed precision training\" with datatype of `torch.float16` uses {class}`torch.autocast` and\n{class}`torch.amp.GradScaler` together, as shown in the {ref}`Automatic Mixed Precision examples<amp-examples>`\nand [Automatic Mixed Precision recipe](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html).\nHowever, {class}`torch.autocast` and {class}`torch.GradScaler` are modular, and may be used separately if desired.\nAs shown in the CPU example section of {class}`torch.autocast`, \"automatic mixed precision training/inference\" on CPU with\ndatatype of `torch.bfloat16` only uses {class}`torch.autocast`.\n\n:::{warning}\n`torch.cuda.amp.autocast(args...)` and `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast(\"cuda\", args...)` or `torch.amp.autocast(\"cpu\", args...)` instead.\n`torch.cuda.amp.GradScaler(args...)` and `torch.cpu.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler(\"cuda\", args...)` or `torch.amp.GradScaler(\"cpu\", args...)` instead.\n:::\n\n{class}`torch.autocast` and {class}`torch.cpu.amp.autocast` are new in version `1.10`.\n\n```{contents}\n:local: true\n```\n\n(autocasting)=",
    "1322": "一级标题：Automatic Mixed Precision package - torch.amp\n二级标题：Autocasting\n内容：\n```{eval-rst}\n.. currentmodule:: torch.amp.autocast_mode\n```\n\n```{eval-rst}\n.. autofunction::  is_autocast_available\n```\n\n```{eval-rst}\n.. currentmodule:: torch\n```\n\n```{eval-rst}\n.. autoclass:: autocast\n    :members:\n```\n\n```{eval-rst}\n.. currentmodule:: torch.amp\n```\n\n```{eval-rst}\n.. autofunction::  custom_fwd\n```\n\n```{eval-rst}\n.. autofunction::  custom_bwd\n```\n\n```{eval-rst}\n.. currentmodule:: torch.cuda.amp\n```\n\n```{eval-rst}\n.. autoclass:: autocast\n    :members:\n```\n\n```{eval-rst}\n.. autofunction::  custom_fwd\n```\n\n```{eval-rst}\n.. autofunction::  custom_bwd\n```\n\n```{eval-rst}\n.. currentmodule:: torch.cpu.amp\n```\n\n```{eval-rst}\n.. autoclass:: autocast\n    :members:\n```\n\n(gradient-scaling)=",
    "1323": "一级标题：Automatic Mixed Precision package - torch.amp\n二级标题：Gradient Scaling\n内容：\nIf the forward pass for a particular op has `float16` inputs, the backward pass for\nthat op will produce `float16` gradients.\nGradient values with small magnitudes may not be representable in `float16`.\nThese values will flush to zero (\"underflow\"), so the update for the corresponding parameters will be lost.\n\nTo prevent underflow, \"gradient scaling\" multiplies the network's loss(es) by a scale factor and\ninvokes a backward pass on the scaled loss(es). Gradients flowing backward through the network are\nthen scaled by the same factor. In other words, gradient values have a larger magnitude,\nso they don't flush to zero.\n\nEach parameter's gradient (`.grad` attribute) should be unscaled before the optimizer\nupdates the parameters, so the scale factor does not interfere with the learning rate.\n\n:::{note}\nAMP/fp16 may not work for every model! For example, most bf16-pretrained models cannot operate in\nthe fp16 numerical range of max 65504 and will cause gradients to overflow instead of underflow. In\nthis case, the scale factor may decrease under 1 as an attempt to bring gradients to a number\nrepresentable in the fp16 dynamic range. While one may expect the scale to always be above 1, our\nGradScaler does NOT make this guarantee to maintain performance. If you encounter NaNs in your loss\nor gradients when running with AMP/fp16, verify your model is compatible.\n:::\n\n```{eval-rst}\n.. currentmodule:: torch.cuda.amp\n```\n\n```{eval-rst}\n.. autoclass:: GradScaler\n    :members:\n```\n\n```{eval-rst}\n.. currentmodule:: torch.cpu.amp\n```\n\n```{eval-rst}\n.. autoclass:: GradScaler\n    :members:\n```\n\n(autocast-op-reference)=",
    "1324": "一级标题：Automatic Mixed Precision package - torch.amp\n二级标题：Autocast Op Reference\n内容：\n(autocast-eligibility)=\n\n### Op Eligibility\n\nOps that run in `float64` or non-floating-point dtypes are not eligible, and will\nrun in these types whether or not autocast is enabled.\n\nOnly out-of-place ops and Tensor methods are eligible.\nIn-place variants and calls that explicitly supply an `out=...` Tensor\nare allowed in autocast-enabled regions, but won't go through autocasting.\nFor example, in an autocast-enabled region `a.addmm(b, c)` can autocast,\nbut `a.addmm_(b, c)` and `a.addmm(b, c, out=d)` cannot.\nFor best performance and stability, prefer out-of-place ops in autocast-enabled\nregions.\n\nOps called with an explicit `dtype=...` argument are not eligible,\nand will produce output that respects the `dtype` argument.\n\n(autocast-cuda-op-reference)=\n\n### CUDA Op-Specific Behavior\n\nThe following lists describe the behavior of eligible ops in autocast-enabled regions.\nThese ops always go through autocasting whether they are invoked as part of a {class}`torch.nn.Module`,\nas a function, or as a {class}`torch.Tensor` method. If functions are exposed in multiple namespaces,\nthey go through autocasting regardless of the namespace.\n\nOps not listed below do not go through autocasting. They run in the type\ndefined by their inputs. However, autocasting may still change the type\nin which unlisted ops run if they're downstream from autocasted ops.\n\nIf an op is unlisted, we assume it's numerically stable in `float16`.\nIf you believe an unlisted op is numerically unstable in `float16`,\nplease file an issue.\n\n#### CUDA Ops that can autocast to `float16`\n\n`__matmul__`,\n`addbmm`,\n`addmm`,\n`addmv`,\n`addr`,\n`baddbmm`,\n`bmm`,\n`chain_matmul`,\n`multi_dot`,\n`conv1d`,\n`conv2d`,\n`conv3d`,\n`conv_transpose1d`,\n`conv_transpose2d`,\n`conv_transpose3d`,\n`GRUCell`,\n`linear`,\n`LSTMCell`,\n`matmul`,\n`mm`,\n`mv`,\n`prelu`,\n`RNNCell`\n\n#### CUDA Ops that can autocast to `float32`\n\n`__pow__`,\n`__rdiv__`,\n`__rpow__`,\n`__rtruediv__`,\n`acos`,\n`asin`,\n`binary_cross_entropy_with_logits`,\n`cosh`,\n`cosine_embedding_loss`,\n`cdist`,\n`cosine_similarity`,\n`cross_entropy`,\n`cumprod`,\n`cumsum`,\n`dist`,\n`erfinv`,\n`exp`,\n`expm1`,\n`group_norm`,\n`hinge_embedding_loss`,\n`kl_div`,\n`l1_loss`,\n`layer_norm`,\n`log`,\n`log_softmax`,\n`log10`,\n`log1p`,\n`log2`,\n`margin_ranking_loss`,\n`mse_loss`,\n`multilabel_margin_loss`,\n`multi_margin_loss`,\n`nll_loss`,\n`norm`,\n`normalize`,\n`pdist`,\n`poisson_nll_loss`,\n`pow`,\n`prod`,\n`reciprocal`,\n`rsqrt`,\n`sinh`,\n`smooth_l1_loss`,\n`soft_margin_loss`,\n`softmax`,\n`softmin`,\n`softplus`,\n`sum`,\n`renorm`,\n`tan`,\n`triplet_margin_loss`\n\n#### CUDA Ops that promote to the widest input type\n\nThese ops don't require a particular dtype for stability, but take multiple inputs\nand require that the inputs' dtypes match. If all of the inputs are\n`float16`, the op runs in `float16`. If any of the inputs is `float32`,\nautocast casts all inputs to `float32` and runs the op in `float32`.\n\n`addcdiv`,\n`addcmul`,\n`atan2`,\n`bilinear`,\n`cross`,\n`dot`,\n`grid_sample`,\n`index_put`,\n`scatter_add`,\n`tensordot`\n\nSome ops not listed here (e.g., binary ops like `add`) natively promote\ninputs without autocasting's intervention. If inputs are a mixture of `float16`\nand `float32`, these ops run in `float32` and produce `float32` output,\nregardless of whether autocast is enabled.\n\n#### Prefer `binary_cross_entropy_with_logits` over `binary_cross_entropy`\n\nThe backward passes of {func}`torch.nn.functional.binary_cross_entropy` (and {mod}`torch.nn.BCELoss`, which wraps it)\ncan produce gradients that aren't representable in `float16`. In autocast-enabled regions, the forward input\nmay be `float16`, which means the backward gradient must be representable in `float16` (autocasting `float16`\nforward inputs to `float32` doesn't help, because that cast must be reversed in backward).\nTherefore, `binary_cross_entropy` and `BCELoss` raise an error in autocast-enabled regions.\n\nMany models use a sigmoid layer right before the binary cross entropy layer.\nIn this case, combine the two layers using {func}`torch.nn.functional.binary_cross_entropy_with_logits`\nor {mod}`torch.nn.BCEWithLogitsLoss`. `binary_cross_entropy_with_logits` and `BCEWithLogits`\nare safe to autocast.\n\n(autocast-xpu-op-reference)=\n\n### XPU Op-Specific Behavior (Experimental)\n\nThe following lists describe the behavior of eligible ops in autocast-enabled regions.\nThese ops always go through autocasting whether they are invoked as part of a {class}`torch.nn.Module`,\nas a function, or as a {class}`torch.Tensor` method. If functions are exposed in multiple namespaces,\nthey go through autocasting regardless of the namespace.\n\nOps not listed below do not go through autocasting. They run in the type\ndefined by their inputs. However, autocasting may still change the type\nin which unlisted ops run if they're downstream from autocasted ops.\n\nIf an op is unlisted, we assume it's numerically stable in `float16`.\nIf you believe an unlisted op is numerically unstable in `float16`,\nplease file an issue.\n\n#### XPU Ops that can autocast to `float16`\n\n`addbmm`,\n`addmm`,\n`addmv`,\n`addr`,\n`baddbmm`,\n`bmm`,\n`chain_matmul`,\n`multi_dot`,\n`conv1d`,\n`conv2d`,\n`conv3d`,\n`conv_transpose1d`,\n`conv_transpose2d`,\n`conv_transpose3d`,\n`GRUCell`,\n`linear`,\n`LSTMCell`,\n`matmul`,\n`mm`,\n`mv`,\n`RNNCell`\n\n#### XPU Ops that can autocast to `float32`\n\n`__pow__`,\n`__rdiv__`,\n`__rpow__`,\n`__rtruediv__`,\n`binary_cross_entropy_with_logits`,\n`cosine_embedding_loss`,\n`cosine_similarity`,\n`cumsum`,\n`dist`,\n`exp`,\n`group_norm`,\n`hinge_embedding_loss`,\n`kl_div`,\n`l1_loss`,\n`layer_norm`,\n`log`,\n`log_softmax`,\n`margin_ranking_loss`,\n`nll_loss`,\n`normalize`,\n`poisson_nll_loss`,\n`pow`,\n`reciprocal`,\n`rsqrt`,\n`soft_margin_loss`,\n`softmax`,\n`softmin`,\n`sum`,\n`triplet_margin_loss`\n\n#### XPU Ops that promote to the widest input type\n\nThese ops don't require a particular dtype for stability, but take multiple inputs\nand require that the inputs' dtypes match. If all of the inputs are\n`float16`, the op runs in `float16`. If any of the inputs is `float32`,\nautocast casts all inputs to `float32` and runs the op in `float32`.\n\n`bilinear`,\n`cross`,\n`grid_sample`,\n`index_put`,\n`scatter_add`,\n`tensordot`\n\nSome ops not listed here (e.g., binary ops like `add`) natively promote\ninputs without autocasting's intervention. If inputs are a mixture of `float16`\nand `float32`, these ops run in `float32` and produce `float32` output,\nregardless of whether autocast is enabled.\n\n(autocast-cpu-op-reference)=\n\n### CPU Op-Specific Behavior\n\nThe following lists describe the behavior of eligible ops in autocast-enabled regions.\nThese ops always go through autocasting whether they are invoked as part of a {class}`torch.nn.Module`,\nas a function, or as a {class}`torch.Tensor` method. If functions are exposed in multiple namespaces,\nthey go through autocasting regardless of the namespace.\n\nOps not listed below do not go through autocasting. They run in the type\ndefined by their inputs. However, autocasting may still change the type\nin which unlisted ops run if they're downstream from autocasted ops.\n\nIf an op is unlisted, we assume it's numerically stable in `bfloat16`.\nIf you believe an unlisted op is numerically unstable in `bfloat16`,\nplease file an issue. `float16` shares the lists of `bfloat16`.\n\n#### CPU Ops that can autocast to `bfloat16`\n\n`conv1d`,\n`conv2d`,\n`conv3d`,\n`bmm`,\n`mm`,\n`linalg_vecdot`,\n`baddbmm`,\n`addmm`,\n`addbmm`,\n`linear`,\n`matmul`,\n`_convolution`,\n`conv_tbc`,\n`mkldnn_rnn_layer`,\n`conv_transpose1d`,\n`conv_transpose2d`,\n`conv_transpose3d`,\n`prelu`,\n`scaled_dot_product_attention`,\n`_native_multi_head_attention`\n\n#### CPU Ops that can autocast to `float32`\n\n`avg_pool3d`,\n`binary_cross_entropy`,\n`grid_sampler`,\n`grid_sampler_2d`,\n`_grid_sampler_2d_cpu_fallback`,\n`grid_sampler_3d`,\n`polar`,\n`prod`,\n`quantile`,\n`nanquantile`,\n`stft`,\n`cdist`,\n`trace`,\n`view_as_complex`,\n`cholesky`,\n`cholesky_inverse`,\n`cholesky_solve`,\n`inverse`,\n`lu_solve`,\n`orgqr`,\n`inverse`,\n`ormqr`,\n`pinverse`,\n`max_pool3d`,\n`max_unpool2d`,\n`max_unpool3d`,\n`adaptive_avg_pool3d`,\n`reflection_pad1d`,\n`reflection_pad2d`,\n`replication_pad1d`,\n`replication_pad2d`,\n`replication_pad3d`,\n`mse_loss`,\n`cosine_embedding_loss`,\n`nll_loss`,\n`nll_loss2d`,\n`hinge_embedding_loss`,\n`poisson_nll_loss`,\n`cross_entropy_loss`,\n`l1_loss`,\n`huber_loss`,\n`margin_ranking_loss`,\n`soft_margin_loss`,\n`triplet_margin_loss`,\n`multi_margin_loss`,\n`ctc_loss`,\n`kl_div`,\n`multilabel_margin_loss`,\n`binary_cross_entropy_with_logits`,\n`fft_fft`,\n`fft_ifft`,\n`fft_fft2`,\n`fft_ifft2`,\n`fft_fftn`,\n`fft_ifftn`,\n`fft_rfft`,\n`fft_irfft`,\n`fft_rfft2`,\n`fft_irfft2`,\n`fft_rfftn`,\n`fft_irfftn`,\n`fft_hfft`,\n`fft_ihfft`,\n`linalg_cond`,\n`linalg_matrix_rank`,\n`linalg_solve`,\n`linalg_cholesky`,\n`linalg_svdvals`,\n`linalg_eigvals`,\n`linalg_eigvalsh`,\n`linalg_inv`,\n`linalg_householder_product`,\n`linalg_tensorinv`,\n`linalg_tensorsolve`,\n`fake_quantize_per_tensor_affine`,\n`geqrf`,\n`_lu_with_info`,\n`qr`,\n`svd`,\n`triangular_solve`,\n`fractional_max_pool2d`,\n`fractional_max_pool3d`,\n`adaptive_max_pool3d`,\n`multilabel_margin_loss_forward`,\n`linalg_qr`,\n`linalg_cholesky_ex`,\n`linalg_svd`,\n`linalg_eig`,\n`linalg_eigh`,\n`linalg_lstsq`,\n`linalg_inv_ex`\n\n#### CPU Ops that promote to the widest input type\n\nThese ops don't require a particular dtype for stability, but take multiple inputs\nand require that the inputs' dtypes match. If all of the inputs are\n`bfloat16`, the op runs in `bfloat16`. If any of the inputs is `float32`,\nautocast casts all inputs to `float32` and runs the op in `float32`.\n\n`cat`,\n`stack`,\n`index_copy`\n\nSome ops not listed here (e.g., binary ops like `add`) natively promote\ninputs without autocasting's intervention. If inputs are a mixture of `bfloat16`\nand `float32`, these ops run in `float32` and produce `float32` output,\nregardless of whether autocast is enabled.\n\n% This module needs to be documented. Adding here in the meantime\n\n% for tracking purposes\n\n```{eval-rst}\n.. py:module:: torch.amp.autocast_mode\n```\n\n```{eval-rst}\n.. py:module:: torch.cpu.amp.autocast_mode\n```\n\n```{eval-rst}\n.. py:module:: torch.cuda.amp.autocast_mode\n```\n\n```{eval-rst}\n.. py:module:: torch.cuda.amp.common\n```\n\n```{eval-rst}\n.. py:module:: torch.amp.grad_scaler\n```\n\n```{eval-rst}\n.. py:module:: torch.cpu.amp.grad_scaler\n```\n\n```{eval-rst}\n.. py:module:: torch.cuda.amp.grad_scaler\n```",
    "1325": "一级标题：Automatic differentiation package - torch.autograd\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.autograd\n```\n\n```{eval-rst}\n.. currentmodule:: torch.autograd\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    backward\n    grad\n```\n\n(forward-mode-ad)=",
    "1326": "一级标题：Automatic differentiation package - torch.autograd\n二级标题：Forward-mode Automatic Differentiation\n内容：\n:::{warning}\nThis API is in beta. Even though the function signatures are very unlikely to change, improved\noperator coverage is planned before we consider this stable.\n:::\n\nPlease see the [forward-mode AD tutorial](https://pytorch.org/tutorials/intermediate/forward_ad_usage.html)\nfor detailed steps on how to use this API.\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    forward_ad.dual_level\n    forward_ad.make_dual\n    forward_ad.unpack_dual\n    forward_ad.enter_dual_level\n    forward_ad.exit_dual_level\n    forward_ad.UnpackedDualTensor\n```\n\n(functional-api)=",
    "1327": "一级标题：Automatic differentiation package - torch.autograd\n二级标题：Functional higher level API\n内容：\n:::{warning}\nThis API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable.\n:::\n\nThis section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc.\n\nThis API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don't have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function `f` that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as `f(input, constant, flag=flag)`\nyou can use it as `functional.jacobian(lambda x: f(x, constant, flag=flag), input)`.\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    functional.jacobian\n    functional.hessian\n    functional.vjp\n    functional.jvp\n    functional.vhp\n    functional.hvp\n```\n\n(locally-disable-grad)=",
    "1328": "一级标题：Automatic differentiation package - torch.autograd\n二级标题：Locally disabling gradient computation\n内容：\nSee {ref}`locally-disable-grad-doc` for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two. Also see {ref}`torch-rst-local-disable-grad`\nfor a list of functions that can be used to locally disable gradients.\n\n(default-grad-layouts)=",
    "1329": "一级标题：Automatic differentiation package - torch.autograd\n二级标题：Default gradient layouts\n内容：\nWhen a non-sparse `param` receives a non-sparse gradient during\n{func}`torch.autograd.backward` or {func}`torch.Tensor.backward`\n`param.grad` is accumulated as follows.\n\nIf `param.grad` is initially `None`:\n\n1. If `param`'s memory is non-overlapping and dense, `.grad` is\n   created with strides matching `param` (thus matching `param`'s\n   layout).\n2. Otherwise, `.grad` is created with rowmajor-contiguous strides.\n\nIf `param` already has a non-sparse `.grad` attribute:\n\n3. If `create_graph=False`, `backward()` accumulates into `.grad`\n   in-place, which preserves its strides.\n4. If `create_graph=True`, `backward()` replaces `.grad` with a\n   new tensor `.grad + new grad`, which attempts (but does not guarantee)\n   matching the preexisting `.grad`'s strides.\n\nThe default behavior (letting `.grad`s be `None` before the first\n`backward()`, such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to `model.zero_grad()` or `optimizer.zero_grad()` will not affect `.grad`\nlayouts.\n\nIn fact, resetting all `.grad`s to `None` before each\naccumulation phase, e.g.:\n\n```\nfor iterations...\n    ...\n    for param in model.parameters():\n        param.grad = None\n    loss.backward()\n```\n\nsuch that they're recreated according to 1 or 2 every time,\nis a valid alternative to `model.zero_grad()` or `optimizer.zero_grad()`\nthat may improve performance for some networks.\n\n### Manual gradient layouts\n\nIf you need manual control over `.grad`'s strides,\nassign `param.grad =` a zeroed tensor with desired strides\nbefore the first `backward()`, and never reset it to `None`.\n3 guarantees your layout is preserved as long as `create_graph=False`.\n4 indicates your layout is *likely* preserved even if `create_graph=True`.",
    "1330": "一级标题：Automatic differentiation package - torch.autograd\n二级标题：In-place operations on Tensors\n内容：\nSupporting in-place operations in autograd is a hard matter, and we discourage\ntheir use in most cases. Autograd's aggressive buffer freeing and reuse makes\nit very efficient and there are very few occasions when in-place operations\nactually lower memory usage by any significant amount. Unless you're operating\nunder heavy memory pressure, you might never need to use them.\n\n### In-place correctness checks\n\nAll {class}`Tensor` s keep track of in-place operations applied to them, and\nif the implementation detects that a tensor was saved for backward in one of\nthe functions, but it was modified in-place afterwards, an error will be raised\nonce backward pass is started. This ensures that if you're using in-place\nfunctions and not seeing any errors, you can be sure that the computed\ngradients are correct.",
    "1331": "一级标题：Automatic differentiation package - torch.autograd\n二级标题：Variable (deprecated)\n内容：\n:::{warning}\nThe Variable API has been deprecated: Variables are no longer necessary to\nuse autograd with tensors. Autograd automatically supports Tensors with\n`requires_grad` set to `True`. Below please find a quick guide on what\nhas changed:\n\n- `Variable(tensor)` and `Variable(tensor, requires_grad)` still work as expected,\n  but they return Tensors instead of Variables.\n- `var.data` is the same thing as `tensor.data`.\n- Methods such as `var.backward(), var.detach(), var.register_hook()` now work on tensors\n  with the same method names.\n\nIn addition, one can now create tensors with `requires_grad=True` using factory\nmethods such as {func}`torch.randn`, {func}`torch.zeros`, {func}`torch.ones`, and others\nlike the following:\n\n`autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)`\n:::",
    "1332": "一级标题：Automatic differentiation package - torch.autograd\n二级标题：Tensor autograd functions\n内容：\n```{eval-rst}\n.. autosummary::\n    :nosignatures:\n\n   torch.Tensor.grad\n   torch.Tensor.requires_grad\n   torch.Tensor.is_leaf\n   torch.Tensor.backward\n   torch.Tensor.detach\n   torch.Tensor.detach_\n   torch.Tensor.register_hook\n   torch.Tensor.register_post_accumulate_grad_hook\n   torch.Tensor.retain_grad\n```",
    "1333": "一级标题：Automatic differentiation package - torch.autograd\n二级标题：{hidden}`Function`\n内容：\n```{eval-rst}\n.. autoclass:: Function\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    Function.forward\n    Function.backward\n    Function.jvp\n    Function.vmap\n```\n\n(context-method-mixins)=",
    "1334": "一级标题：Automatic differentiation package - torch.autograd\n二级标题：Context method mixins\n内容：\nWhen creating a new {class}`Function`, the following methods are available to `ctx`.\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    function.FunctionCtx.mark_dirty\n    function.FunctionCtx.mark_non_differentiable\n    function.FunctionCtx.save_for_backward\n    function.FunctionCtx.set_materialize_grads\n```",
    "1335": "一级标题：Automatic differentiation package - torch.autograd\n二级标题：Custom Function utilities\n内容：\nDecorator for backward method.\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    function.once_differentiable\n```\n\nBase custom {class}`Function` used to build PyTorch utilities\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    function.BackwardCFunction\n    function.InplaceFunction\n    function.NestedIOFunction\n\n```\n\n(grad-check)=",
    "1336": "一级标题：Automatic differentiation package - torch.autograd\n二级标题：Numerical gradient checking\n内容：\n```{eval-rst}\n.. automodule:: torch.autograd.gradcheck\n```\n\n```{eval-rst}\n.. currentmodule:: torch.autograd.gradcheck\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    gradcheck\n    gradgradcheck\n    GradcheckError\n```\n\n% Just to reset the base path for the rest of this file\n\n```{eval-rst}\n.. currentmodule:: torch.autograd\n```",
    "1337": "一级标题：Automatic differentiation package - torch.autograd\n二级标题：Profiler\n内容：\nAutograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are three modes\nimplemented at the moment - CPU-only using {class}`~torch.autograd.profiler.profile`.\nnvprof based (registers both CPU and GPU activity) using\n{class}`~torch.autograd.profiler.emit_nvtx`.\nand vtune profiler based using\n{class}`~torch.autograd.profiler.emit_itt`.\n\n```{eval-rst}\n.. autoclass:: torch.autograd.profiler.profile\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    profiler.profile.export_chrome_trace\n    profiler.profile.key_averages\n    profiler.profile.self_cpu_time_total\n    profiler.profile.total_average\n    profiler.parse_nvprof_trace\n    profiler.EnforceUnique\n    profiler.KinetoStepTracker\n    profiler.record_function\n    profiler_util.Interval\n    profiler_util.Kernel\n    profiler_util.MemRecordsAcc\n    profiler_util.StringTable\n```\n\n```{eval-rst}\n.. autoclass:: torch.autograd.profiler.emit_nvtx\n```\n\n```{eval-rst}\n.. autoclass:: torch.autograd.profiler.emit_itt\n\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    profiler.load_nvprof\n```",
    "1338": "一级标题：Automatic differentiation package - torch.autograd\n二级标题：Debugging and anomaly detection\n内容：\n```{eval-rst}\n.. autoclass:: detect_anomaly\n```\n\n```{eval-rst}\n.. autoclass:: set_detect_anomaly\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    grad_mode.set_multithreading_enabled\n\n\n```",
    "1339": "一级标题：Automatic differentiation package - torch.autograd\n二级标题：Autograd graph\n内容：\nAutograd exposes methods that allow one to inspect the graph and interpose behavior during\nthe backward pass.\n\nThe `grad_fn` attribute of a {class}`torch.Tensor` holds a {class}`torch.autograd.graph.Node`\nif the tensor is the output of a operation that was recorded by autograd (i.e., grad_mode is\nenabled and at least one of the inputs required gradients), or `None` otherwise.\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    graph.Node.name\n    graph.Node.metadata\n    graph.Node.next_functions\n    graph.Node.register_hook\n    graph.Node.register_prehook\n    graph.increment_version\n```\n\nSome operations need intermediary results to be saved during the forward pass\nin order to execute the backward pass.\nThese intermediary results are saved as attributes on the `grad_fn` and can be accessed.\nFor example:\n\n```\n>>> a = torch.tensor([0., 0., 0.], requires_grad=True)\n>>> b = a.exp()\n>>> print(isinstance(b.grad_fn, torch.autograd.graph.Node))\nTrue\n>>> print(dir(b.grad_fn))\n['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_raw_saved_result', '_register_hook_dict', '_saved_result', 'metadata', 'name', 'next_functions', 'register_hook', 'register_prehook', 'requires_grad']\n>>> print(torch.allclose(b.grad_fn._saved_result, b))\nTrue\n```\n\nYou can also define how these saved tensors should be packed / unpacked using hooks.\nA common application is to trade compute for memory by saving those intermediary results\nto disk or to CPU instead of leaving them on the GPU. This is especially useful if you\nnotice your model fits on GPU during evaluation, but not training.\nAlso see {ref}`saved-tensors-hooks-doc`.\n\n```{eval-rst}\n.. autoclass:: torch.autograd.graph.saved_tensors_hooks\n```\n\n```{eval-rst}\n.. autoclass:: torch.autograd.graph.save_on_cpu\n```\n\n```{eval-rst}\n.. autoclass:: torch.autograd.graph.disable_saved_tensors_hooks\n```\n\n```{eval-rst}\n.. autoclass:: torch.autograd.graph.register_multi_grad_hook\n```\n\n```{eval-rst}\n.. autoclass:: torch.autograd.graph.allow_mutation_on_saved_tensors\n```\n\n```{eval-rst}\n.. autoclass:: torch.autograd.graph.GradientEdge\n```\n\n```{eval-rst}\n.. autofunction:: torch.autograd.graph.get_gradient_edge\n\n\n```\n\n% This module needs to be documented. Adding here in the meantime\n\n% for tracking purposes\n\n```{eval-rst}\n.. py:module:: torch.autograd.anomaly_mode\n```\n\n```{eval-rst}\n.. py:module:: torch.autograd.forward_ad\n```\n\n```{eval-rst}\n.. py:module:: torch.autograd.function\n```\n\n```{eval-rst}\n.. py:module:: torch.autograd.functional\n```\n\n```{eval-rst}\n.. py:module:: torch.autograd.grad_mode\n```\n\n```{eval-rst}\n.. py:module:: torch.autograd.graph\n```\n\n```{eval-rst}\n.. py:module:: torch.autograd.profiler\n```\n\n```{eval-rst}\n.. py:module:: torch.autograd.profiler_legacy\n```\n\n```{eval-rst}\n.. py:module:: torch.autograd.profiler_util\n```\n\n```{eval-rst}\n.. py:module:: torch.autograd.variable\n```",
    "1340": "一级标题：torch.backends\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.backends\n```\n\n`torch.backends` controls the behavior of various backends that PyTorch supports.\n\nThese backends include:\n\n- `torch.backends.cpu`\n- `torch.backends.cuda`\n- `torch.backends.cudnn`\n- `torch.backends.cusparselt`\n- `torch.backends.mha`\n- `torch.backends.mps`\n- `torch.backends.mkl`\n- `torch.backends.mkldnn`\n- `torch.backends.nnpack`\n- `torch.backends.openmp`\n- `torch.backends.opt_einsum`\n- `torch.backends.xeon`",
    "1341": "一级标题：torch.backends\n二级标题：torch.backends.cpu\n内容：\n```{eval-rst}\n.. automodule:: torch.backends.cpu\n```\n\n```{eval-rst}\n.. autofunction::  torch.backends.cpu.get_cpu_capability\n```",
    "1342": "一级标题：torch.backends\n二级标题：torch.backends.cuda\n内容：\n```{eval-rst}\n.. automodule:: torch.backends.cuda\n```\n\n```{eval-rst}\n.. autofunction::  torch.backends.cuda.is_built\n```\n\n```{eval-rst}\n.. currentmodule:: torch.backends.cuda.matmul\n```\n\n```{eval-rst}\n.. attribute::  allow_tf32\n\n    A :class:`bool` that controls whether TensorFloat-32 tensor cores may be used in matrix\n    multiplications on Ampere or newer GPUs. allow_tf32 is going to be deprecated. See :ref:`tf32_on_ampere`.\n```\n\n```{eval-rst}\n.. attribute::  allow_fp16_reduced_precision_reduction\n\n    A :class:`bool` that controls whether reduced precision reductions (e.g., with fp16 accumulation type) are allowed with fp16 GEMMs.\n```\n\n```{eval-rst}\n.. attribute::  allow_bf16_reduced_precision_reduction\n\n    A :class:`bool` that controls whether reduced precision reductions are allowed with bf16 GEMMs.\n```\n\n```{eval-rst}\n.. currentmodule:: torch.backends.cuda\n```\n\n```{eval-rst}\n.. attribute::  cufft_plan_cache\n\n    ``cufft_plan_cache`` contains the cuFFT plan caches for each CUDA device.\n    Query a specific device `i`'s cache via `torch.backends.cuda.cufft_plan_cache[i]`.\n\n    .. currentmodule:: torch.backends.cuda.cufft_plan_cache\n    .. attribute::  size\n\n        A readonly :class:`int` that shows the number of plans currently in a cuFFT plan cache.\n\n    .. attribute::  max_size\n\n        A :class:`int` that controls the capacity of a cuFFT plan cache.\n\n    .. method::  clear()\n\n        Clears a cuFFT plan cache.\n```\n\n```{eval-rst}\n.. autofunction:: torch.backends.cuda.preferred_blas_library\n```\n\n```{eval-rst}\n.. autofunction:: torch.backends.cuda.preferred_rocm_fa_library\n```\n\n```{eval-rst}\n.. autofunction:: torch.backends.cuda.preferred_linalg_library\n```\n\n```{eval-rst}\n.. autoclass:: torch.backends.cuda.SDPAParams\n```\n\n```{eval-rst}\n.. autofunction:: torch.backends.cuda.flash_sdp_enabled\n```\n\n```{eval-rst}\n.. autofunction:: torch.backends.cuda.enable_mem_efficient_sdp\n```\n\n```{eval-rst}\n.. autofunction:: torch.backends.cuda.mem_efficient_sdp_enabled\n```\n\n```{eval-rst}\n.. autofunction:: torch.backends.cuda.enable_flash_sdp\n```\n\n```{eval-rst}\n.. autofunction:: torch.backends.cuda.math_sdp_enabled\n```\n\n```{eval-rst}\n.. autofunction:: torch.backends.cuda.enable_math_sdp\n```\n\n```{eval-rst}\n.. autofunction:: torch.backends.cuda.fp16_bf16_reduction_math_sdp_allowed\n```\n\n```{eval-rst}\n.. autofunction:: torch.backends.cuda.allow_fp16_bf16_reduction_math_sdp\n```\n\n```{eval-rst}\n.. autofunction:: torch.backends.cuda.cudnn_sdp_enabled\n```\n\n```{eval-rst}\n.. autofunction:: torch.backends.cuda.enable_cudnn_sdp\n```\n\n```{eval-rst}\n.. autofunction:: torch.backends.cuda.is_flash_attention_available\n```\n\n```{eval-rst}\n.. autofunction:: torch.backends.cuda.can_use_flash_attention\n```\n\n```{eval-rst}\n.. autofunction:: torch.backends.cuda.can_use_efficient_attention\n```\n\n```{eval-rst}\n.. autofunction:: torch.backends.cuda.can_use_cudnn_attention\n```\n\n```{eval-rst}\n.. autofunction:: torch.backends.cuda.sdp_kernel\n```",
    "1343": "一级标题：torch.backends\n二级标题：torch.backends.cudnn\n内容：\n```{eval-rst}\n.. automodule:: torch.backends.cudnn\n```\n\n```{eval-rst}\n.. autofunction:: torch.backends.cudnn.version\n```\n\n```{eval-rst}\n.. autofunction:: torch.backends.cudnn.is_available\n```\n\n```{eval-rst}\n.. attribute::  enabled\n\n    A :class:`bool` that controls whether cuDNN is enabled.\n```\n\n```{eval-rst}\n.. attribute::  allow_tf32\n\n    A :class:`bool` that controls where TensorFloat-32 tensor cores may be used in cuDNN\n    convolutions on Ampere or newer GPUs. allow_tf32 is going to be deprecated. See :ref:`tf32_on_ampere`.\n```\n\n```{eval-rst}\n.. attribute::  deterministic\n\n    A :class:`bool` that, if True, causes cuDNN to only use deterministic convolution algorithms.\n    See also :func:`torch.are_deterministic_algorithms_enabled` and\n    :func:`torch.use_deterministic_algorithms`.\n```\n\n```{eval-rst}\n.. attribute::  benchmark\n\n    A :class:`bool` that, if True, causes cuDNN to benchmark multiple convolution algorithms\n    and select the fastest.\n```\n\n```{eval-rst}\n.. attribute::  benchmark_limit\n\n    A :class:`int` that specifies the maximum number of cuDNN convolution algorithms to try when\n    `torch.backends.cudnn.benchmark` is True. Set `benchmark_limit` to zero to try every\n    available algorithm. Note that this setting only affects convolutions dispatched via the\n    cuDNN v8 API.\n```\n\n```{eval-rst}\n.. py:module:: torch.backends.cudnn.rnn\n```",
    "1344": "一级标题：torch.backends\n二级标题：torch.backends.cusparselt\n内容：\n```{eval-rst}\n.. automodule:: torch.backends.cusparselt\n```\n\n```{eval-rst}\n.. autofunction:: torch.backends.cusparselt.version\n```\n\n```{eval-rst}\n.. autofunction:: torch.backends.cusparselt.is_available\n```",
    "1345": "一级标题：torch.backends\n二级标题：torch.backends.mha\n内容：\n```{eval-rst}\n.. automodule:: torch.backends.mha\n```\n\n```{eval-rst}\n.. autofunction::  torch.backends.mha.get_fastpath_enabled\n```\n\n```{eval-rst}\n.. autofunction::  torch.backends.mha.set_fastpath_enabled\n\n```",
    "1346": "一级标题：torch.backends\n二级标题：torch.backends.miopen\n内容：\n```{eval-rst}\n.. automodule:: torch.backends.miopen\n```\n\n```{eval-rst}\n.. attribute::  immediate\n\n    A :class:`bool` that, if True, causes MIOpen to use Immediate Mode\n    (https://rocm.docs.amd.com/projects/MIOpen/en/latest/how-to/find-and-immediate.html).\n```",
    "1347": "一级标题：torch.backends\n二级标题：torch.backends.mps\n内容：\n```{eval-rst}\n.. automodule:: torch.backends.mps\n```\n\n```{eval-rst}\n.. autofunction::  torch.backends.mps.is_available\n```\n\n```{eval-rst}\n.. autofunction::  torch.backends.mps.is_built\n\n```",
    "1348": "一级标题：torch.backends\n二级标题：torch.backends.mkl\n内容：\n```{eval-rst}\n.. automodule:: torch.backends.mkl\n```\n\n```{eval-rst}\n.. autofunction::  torch.backends.mkl.is_available\n```\n\n```{eval-rst}\n.. autoclass::  torch.backends.mkl.verbose\n\n```",
    "1349": "一级标题：torch.backends\n二级标题：torch.backends.mkldnn\n内容：\n```{eval-rst}\n.. automodule:: torch.backends.mkldnn\n```\n\n```{eval-rst}\n.. autofunction::  torch.backends.mkldnn.is_available\n```\n\n```{eval-rst}\n.. autoclass::  torch.backends.mkldnn.verbose\n```",
    "1350": "一级标题：torch.backends\n二级标题：torch.backends.nnpack\n内容：\n```{eval-rst}\n.. automodule:: torch.backends.nnpack\n```\n\n```{eval-rst}\n.. autofunction::  torch.backends.nnpack.is_available\n```\n\n```{eval-rst}\n.. autofunction::  torch.backends.nnpack.flags\n```\n\n```{eval-rst}\n.. autofunction::  torch.backends.nnpack.set_flags\n```",
    "1351": "一级标题：torch.backends\n二级标题：torch.backends.openmp\n内容：\n```{eval-rst}\n.. automodule:: torch.backends.openmp\n```\n\n```{eval-rst}\n.. autofunction::  torch.backends.openmp.is_available\n```\n\n% Docs for other backends need to be added here.\n% Automodules are just here to ensure checks run but they don't actually\n% add anything to the rendered page for now.\n\n```{eval-rst}\n.. py:module:: torch.backends.quantized\n```\n\n```{eval-rst}\n.. py:module:: torch.backends.xnnpack\n```\n\n```{eval-rst}\n.. py:module:: torch.backends.kleidiai\n\n```",
    "1352": "一级标题：torch.backends\n二级标题：torch.backends.opt_einsum\n内容：\n```{eval-rst}\n.. automodule:: torch.backends.opt_einsum\n```\n\n```{eval-rst}\n.. autofunction:: torch.backends.opt_einsum.is_available\n```\n\n```{eval-rst}\n.. autofunction:: torch.backends.opt_einsum.get_opt_einsum\n```\n\n```{eval-rst}\n.. attribute::  enabled\n\n    A :class:`bool` that controls whether opt_einsum is enabled (``True`` by default). If so,\n    torch.einsum will use opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html)\n    if available to calculate an optimal path of contraction for faster performance.\n\n    If opt_einsum is not available, torch.einsum will fall back to the default contraction path\n    of left to right.\n```\n\n```{eval-rst}\n.. attribute::  strategy\n\n    A :class:`str` that specifies which strategies to try when ``torch.backends.opt_einsum.enabled``\n    is ``True``. By default, torch.einsum will try the \"auto\" strategy, but the \"greedy\" and \"optimal\"\n    strategies are also supported. Note that the \"optimal\" strategy is factorial on the number of\n    inputs as it tries all possible paths. See more details in opt_einsum's docs\n    (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html).\n\n```",
    "1353": "一级标题：torch.backends\n二级标题：torch.backends.xeon\n内容：\n```{eval-rst}\n.. automodule:: torch.backends.xeon\n```\n\n```{eval-rst}\n.. py:module:: torch.backends.xeon.run_cpu\n```",
    "1354": "一级标题：Benchmark Utils - torch.utils.benchmark\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.utils.benchmark\n```\n\n```{eval-rst}\n.. currentmodule:: torch.utils.benchmark\n```\n\n```{eval-rst}\n.. autoclass:: Timer\n    :members:\n```\n\n```{eval-rst}\n.. autoclass:: Measurement\n    :members:\n```\n\n```{eval-rst}\n.. autoclass:: CallgrindStats\n    :members:\n```\n\n```{eval-rst}\n.. autoclass:: FunctionCounts\n    :members:\n```\n\n```{eval-rst}\n.. autoclass:: Compare\n    :members:\n```\n\n% These are missing documentation. Adding them here until a better place\n% is made in this file.\n\n```{eval-rst}\n.. py:module:: torch.utils.benchmark.examples\n```\n\n```{eval-rst}\n.. py:module:: torch.utils.benchmark.op_fuzzers\n```\n\n```{eval-rst}\n.. py:module:: torch.utils.benchmark.utils\n```\n\n```{eval-rst}\n.. py:module:: torch.utils.benchmark.utils.valgrind_wrapper\n```",
    "1355": "一级标题：torch.utils.checkpoint\n二级标题：无\n内容：\n```{note}\nCheckpointing is implemented by rerunning a forward-pass segment for\neach checkpointed segment during backward propagation.  This can cause persistent\nstates like the RNG state to be more advanced than they would without\ncheckpointing.  By default, checkpointing includes logic to juggle\nthe RNG state such that checkpointed passes making use of RNG\n(through dropout for example) have deterministic output as\ncompared to non-checkpointed passes.  The logic to stash and restore\nRNG states can incur a moderate performance hit depending on the runtime\nof checkpointed operations.  If deterministic output compared to\nnon-checkpointed passes is not required, supply `preserve_rng_state=False`\nto `checkpoint` or `checkpoint_sequential` to omit stashing and\nrestoring the RNG state during each checkpoint.\n\nThe stashing logic saves and restores the RNG state for CPU and another\ndevice type (infer the device type from Tensor arguments excluding CPU\ntensors by `_infer_device_type`) to the `run_fn`. If there are multiple\ndevice, device state will only be saved for devices of a single device type,\nand the remaining devices will be ignored. Consequently, if any checkpointed\nfunctions involve randomness, this may result in incorrect gradients. (Note\nthat if CUDA devices are among the devices detected, it will be prioritized;\notherwise, the first device encountered will be selected.) If there are no\nCPU-tensors, the default device type state (default value is `cuda`, and it\ncould be set to other device by `DefaultDeviceType`) will be saved and restored.\nHowever, the logic has no way to anticipate if the user will move\nTensors to a new device within the `run_fn` itself.  Therefore, if you move\nTensors to a new device (\"new\" meaning not belonging to the set of\n[current device + devices of Tensor arguments]) within `run_fn`, deterministic\noutput compared to non-checkpointed passes is never guaranteed.\n```\n\n```{eval-rst}\n.. currentmodule:: torch.utils.checkpoint\n.. autofunction:: checkpoint\n.. autofunction:: checkpoint_sequential\n.. autofunction:: set_checkpoint_debug_enabled\n.. autoclass:: CheckpointPolicy\n.. autoclass:: SelectiveCheckpointContext\n.. autofunction:: create_selective_checkpoint_contexts\n```",
    "1356": "一级标题：Common Graph Breaks\n二级标题：无\n内容：\nBelow are some common graph breaks and some workarounds.",
    "1357": "一级标题：Common Graph Breaks\n二级标题：Incorrect Code\n内容：\nYour code might contain errors (meaning it doesn't execute even without `torch.compile`). In the example below, there's a typo in the `torch.sin` call due to an extra argument. **Always disable `torch.compile` to check if the code runs correctly.**\n\n\n```{code-cell}\n@torch.compile\ndef fn(x):\n    y = torch.sin(x, x)\n    return y\n\ntry:\n    fn(torch.ones(3, 3))\nexcept Exception as e:\n    pass\n```\n\nDynamo makes a best-effort attempt to hint if a graph break is caused by your code.\nBut it can still sometimes be difficult to tell from the logs if the graph break is caused by an error in your code,\nis a more complicated graph break, or is a `torch.compile` bug. In order to differentiate, we recommend trying to run your code without `torch.compile` to see if you still get the error reported by the graph break.",
    "1358": "一级标题：Common Graph Breaks\n二级标题：Data-dependent operations\n内容：\n`torch.compile` graph breaks on data-dependent operations such as data-dependent control flow (if-statements, loops with tensors) and direct tensor data accesses (`.item`, `.data_ptr`).\n\n```{code-cell}\n@torch.compile\ndef fn(x):\n    y = x.sum()\n    if y > 0:\n        return x + y.item()\n    return x - y.item()\n\nprint(fn(torch.ones(3, 3)))\n```\n\nThe general workaround for these graph breaks is to avoid doing data-dependent operations. Some specific workarounds are:\n\n- If your control flow doesn't actually depend on data values, consider modifying your code to perform control flow on constants.\n\n\n```{code-cell}\n# old\nx = torch.randn(3, 3)\n@torch.compile\ndef fn(y):\n    if x.sum() > 0:\n        return y + x\n    else:\n        return y - x\n\nprint(fn(torch.ones(3, 3)))\n```\n\n```{code-cell}\n# new\nx = torch.randn(3, 3)\ncond = (x.sum() > 0).item()\n@torch.compile\ndef fn(y):\n    if cond:\n        return y + x\n    else:\n        return y - x\n\nprint(fn(torch.ones(3, 3)))\n```\n\n- Use higher-order ops like {ref}`cond` in place of data-dependent control flow\n\n\n```{code-cell}\n# old\n@torch.compile\ndef fn(x):\n    if x.sum() > 0:\n        return x + 1\n    return x - 1\n\nprint(fn(torch.ones(3, 3)))\n```\n\n```{code-cell}\n# new\n@torch.compile\ndef fn(x):\n    return torch.cond(\n        x.sum() > 0,\n        lambda x: x + 1,\n        lambda x: x - 1,\n        (x,),\n    )\n\nprint(fn(torch.ones(3, 3)))\n```\n\n- If you have a `.item()` call, try `torch._dynamo.config.capture_scalar_outputs = True`\nor `TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1`.\n- Wrap problematic parts of the function in a custom operator",
    "1359": "一级标题：Common Graph Breaks\n二级标题：Printing and logging\n内容：\nPrinting/logging/issuing warnings will result in a graph break.\nYou can try working around this by using `torch._dynamo.config.reorderable_logging_functions`.\nThis config is used to reorder logging functions so that they are called at the end of the\ntraced function, thus avoiding a graph break.\nHowever, the logged contents may differ if, for example, a mutation occurs.\n\n\n```{code-cell}\ntorch._dynamo.config.reorderable_logging_functions.add(print)\n\n@torch.compile\ndef fn(x):\n    x += 1\n    print(\"log!\")\n    return torch.sin(x)\n\nprint(fn(torch.ones(3, 3)))\n```",
    "1360": "一级标题：Disabling and Suppressing Errors\n二级标题：无\n内容：\nFor some model architectures, there are portions of the model which are particularly difficult to compile -\neither there are many graph breaks, or there are crashes.\nYou may want to explicitly disable these portions of the model which are problematic so that you can apply\n`torch.compile` to the parts that work. You can do this by using the `@torch.compiler.disable` decorator.\nWhen `torch.compile` attempts to call a disabled function, it breaks the graph and skips tracing the disabled function,\nresuming tracing after the call. By default, all recursive calls made from a disabled function are also disabled.\nUse the `recursive=False` option to allow compilation for recursive calls.\n\n```{code-cell}\ndef inner1(x):\n    torch._dynamo.graph_break()  # not traced\n    return x + 1  # not traced\n\n@torch.compiler.disable\ndef outer1(x):\n    x = x + 2  # not traced\n    torch._dynamo.graph_break()  # not traced\n    return inner1(x)\n\n@torch.compile\ndef f(x):\n    x = outer1(x)\n    return x + 4  # traced\n\nprint(f(torch.ones(3)))\n```\n\n```{code-cell}\ndef inner2(x):\n    torch._dynamo.graph_break()  # traced\n    return x + 1  # traced\n\n@torch.compiler.disable(recursive=False)\ndef outer2(x):\n    x = x + 2  # not traced\n    torch._dynamo.graph_break()  # not traced\n    return inner2(x)\n\n@torch.compile\ndef g(x):\n    x = outer2(x)\n    return x + 4  # traced\n\nprint(g(torch.ones(3)))\n```\n\nFor example, one can use `torch.compiler.disable` to disable `torch.compile` on sparse architecture in\nrecommendation models, as the sparse arch is difficult to compile.\nPreprocessing and logging functions are other examples of functions that typically cause\na lot of graph breaks and do not get value from being compiled.\n\nIf you are experiencing compiler crashes and you want to continue regardless,\nyou can set `torch._dynamo.config.suppress_errors = True`.\nWhen the compiler crashes, we will just skip tracing the function and try again later.\n**This is not best practice** - it is better to eventually manually add `disable` annotations as necessary.",
    "1361": "一级标题：Custom Operators\n二级标题：无\n内容：\n**Summary:**\n- Use custom operators to have `torch.compile` treat a function as opaque. `torch.compile` will never trace into the function and Inductor (the backend) will run the function as-is.\n\nYou may wish to use a custom operator in any of the following situations:\n- Your code calls some C/C++/CUDA code. Dynamo is a Python bytecode interpreter and generally does not know how to handle calls to C/C++/CUDA functions that are bound to Python.\n- Dynamo and non-strict tracing have trouble tracing through a function and you want it to be ignored by `torch.compile`.\n\nPlease see [the Python custom ops tutorial](https://pytorch.org/tutorials/advanced/python_custom_ops.html#python-custom-ops-tutorial)for more details on how to wrap a Python function into a `torch.compile`-understood custom operator.\n\nFor more advanced use cases, you may wish to use our C++ Custom Operator API; please see [here](https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html) for more information.",
    "1362": "一级标题：Dynamo Core Concepts\n二级标题：无\n内容：\n**Summary:**\n\n- Dynamo, `torch.compile`'s frontend, performs **tracing** to capture the semantics of a Python function\n  (and its nested function calls) into a linear sequence of operations (the \"(FX) graph\"),\n  residual bytecode, and \"guards\" (a list of conditions under which the graph and bytecode are valid).\n- Unsupported Python features lead to **graph breaks**, where Dynamo compiles a partial graph acquired from tracing,\n  then runs the unsupported code, then resumes tracing.\n- Graph breaks may lead to slowness in torch.compile and prevent backend optimization opportunities.\n  If you're not seeing the performance you expect, then check for graph breaks.",
    "1363": "一级标题：Dynamo Core Concepts\n二级标题：Dynamo Tracing\n内容：\n`torch.compile`'s frontend (Dynamo) is a custom Python bytecode interpreter designed to allow graph compilation\nin PyTorch programs while retaining the full flexibility of Python. Given a function to be compiled, Dynamo\ninterprets Python bytecode to extract sequences of PyTorch operations into 1 or more FX graphs that may be further optimized by a backend.\n\n![Summary diagram of Dynamo](_static/dynamo_summary_diagram.png)\n\nFor example, for the function `f` in the above diagram, Dynamo produces:\n- a single **FX graph** that takes in the original input plus some additional inputs required by the function.\n- **Python bytecode** that can be used as a drop-in replacement for `f`. In our example, the bytecode retrieves\n  the additional inputs and passes it to the graph and also contains unoptimizable Python side effects (the list append)\n- **guards** that specify the conditions under which the graph and bytecode are valid. Unless otherwise specified,\n  the graph produced by Dynamo specializes on the shapes of input Tensors.\n\n(programming_model.dynamo_core_concepts.graph_breaks)=",
    "1364": "一级标题：Dynamo Core Concepts\n二级标题：Graph Breaks\n内容：\nDynamo traces your code and attempts to capture your PyTorch code into a single computation graph of PyTorch\noperators (FX graph). However, this is not always possible. When encountering code that can't be traced, a \"**graph break**\" occurs.\nIn the default `torch.compile` settings, a graph break involves compiling the FX graph that has been determined so far,\nrunning the unsupported code in regular Python, then resuming tracing after the unsupported code with a new FX graph.\n\nGraph breaks are a feature that allows Dynamo to run over arbitrary Python code and carve out functional subgraphs that can each be individually optimized.\n\nHowever, it is possible for graph breaks to lead to unexpected slowness in `torch.compile`.\nIf you're not getting the speedups you expect, we recommend checking for graph breaks and removing them.\n\nGraph breaks may occur on things like:\n\n- Data-dependent if-statements\n- Many Python built-in functions\n- C functions\n\n```{code-cell}\n:tags: [remove-cell]\ntorch._logging.set_logs(graph_breaks=True)\n```\n\nBelow is an example of a graph break due to calling an unsupported operation `torch.save`:\n\n```{code-cell}\n@torch.compile\ndef f(x):\n   y = x ** 2  / 2\n   torch.save(y, \"foo.pt\")  # torch.save is an unsupported operation\n   z = y ** 3 / 6\n   return z\n\nx = torch.randn(3)\nprint(f(x))\n```\n\n```{code-cell}\n:tags: [remove-cell]\nimport os\nos.remove(\"foo.pt\")\n```\n\nThe semantics of `torch.compile(f)(x)` are roughly this:\n\n```python\ndef compiled_f_semantics(x):\n   y = torch.compile(g, fullgraph=True)(x)\n   torch.save(y, \"foo.pt\")\n   z = torch.compile(h, fullgraph=True)(x)\n   return z\n\ndef g(x):\n    return x ** 2  / 2\n\ndef h(x):\n    return y ** 3 / 6\n```",
    "1365": "一级标题：Dynamo Core Concepts\n二级标题：Guards\n内容：\n`torch.compile` makes some assumptions about runtime values as we trace through code. During tracing, we generate \"guards\",\nwhich are runtime checks for these assumptions. Guards are run in future calls to the compiled function to determine if we\ncan reuse previously compiled code. Examples of runtime checks are constant values, types, and object IDs.\n\nBelow is an example of generated guards. The `TENSOR_MATCH` guard checks for the input's type, device, dtype, shape, etc.\n\n```{code-cell}\n:tags: [remove-cell]\ntorch._logging.set_logs(guards=True)\n```\n\n```{code-cell}\n@torch.compile\ndef fn(x):\n    return x + 1\n\nprint(fn(torch.ones(3, 3)))\n```",
    "1366": "一级标题：Dynamo Core Concepts\n二级标题：Recompilations\n内容：\nIf the guards fail for every instance of previously compiled code, then `torch.compile` must \"recompile\" the function,\nrequiring the original code to be traced again. In the example below, recompilation is necessary because the guard checking the tensor argument's shape failed.\n\n```{code-cell}\n:tags: [remove-cell]\ntorch._logging.set_logs(recompiles=True)\n```\n\n```{code-cell}\n@torch.compile\ndef fn(x):\n    return x + 1\n\nprint(fn(torch.ones(3, 3)))\nprint(fn(torch.ones(4, 4)))\n```",
    "1367": "一级标题：Dynamo Core Concepts\n二级标题：Dynamic Shapes\n内容：\n`torch.compile` initially assumes tensor shapes are static/constant and guards based on these assumptions. By using \"dynamic shapes,\"\nwe can get `torch.compile` to produce compiled code that can accept tensor inputs with different shapes - we avoid recompiling every time shapes differ.\nBy default, automatic dynamic shapes are enabled in `torch.compile(dynamic=None)` - if compilation fails due to shape mismatch,\nrecompilation is attempted with dynamic shapes. Dynamic shapes can also be fully enabled (`dynamic=True`) or disabled (`dynamic=False`).\n\nBelow, we enable dynamic shapes and note that we no longer need to recompile.\n\n```{code-cell}\n:tags: [remove-cell]\nimport logging\ntorch._logging.set_logs(dynamic=logging.DEBUG, recompiles=True)\n```\n\n```{code-cell}\n@torch.compile(dynamic=True)\ndef fn(x):\n    return x + 1\n\nprint(fn(torch.ones(3, 3)))\nprint(fn(torch.ones(4, 4)))\n```\n\nFor more information on dynamic shapes, see [The dynamic shapes manual](https://docs.google.com/document/d/1GgvOe7C8_NVOMLOCwDaYV1mXXyHMXY7ExoewHqooxrs/edit?tab=t.0#heading=h.fh8zzonyw8ng).",
    "1368": "一级标题：Use `torch._dynamo.nonstrict_trace`\n二级标题：无\n内容：\n**Summary:**\n- Use `nonstrict_trace` to trace a function with non-strict tracing inside of a `torch.compile`'d region.\n  You may wish to do this because the Dynamo graph breaks on something inside of the function\n  and you are sure that the function is non-strict traceable.\n\nConsider the following scenario:\n\n```{code-cell}\ndef get_magic_num():\n    # This explicit graph break call is meant to emulate any kind of Dynamo\n    # graph break, e.g., the function is implemented in C, or uses some python\n    # language feature Dynamo doesn't yet support.\n    torch._dynamo.graph_break()\n    return torch.tensor([42])\n@torch.compile(fullgraph=True)\ndef func(x):\n    n = get_magic_num()\n    return x + n\ntry:\n    func(torch.rand(10))\nexcept Exception as e:\n    print(e)\n```\n\nIf we run the code above, we'll get an error from Dynamo, because it sees a graph break while the user specified `fullgraph=True`.\n\nIn these situations, if a user still wants to keep `fullgraph=True`, they typically have several options:\n\n1. The graph break is due to a language feature Dynamo doesn't yet support.\n   In this case, the user either rewrites their code, or files an issue on GitHub.\n2. The graph break is due to a call to a function implemented in C.\n   In this case, the user can try to use a custom op.\n   The user could also try providing a polyfill (a reference implementation in Python)\n   so that Dynamo can trace through it.\n3. Worst case scenario -- an internal compiler error. In this case, the user likely has to file an issue on GitHub.\n\nIn addition to all these options, PyTorch does provide an alternative `torch._dynamo.nonstrict_trace`, if the function call that induced the graph break satisfies certain requirements:\n\n- The requirements of [general non-strict tracing](programming_model.non_strict_tracing_model).\n- The inputs and outputs must contain either basic types (e.g., `int`, `float`, `list`, `dict`, `torch.Tensor`),\n  or user-defined types that are registered to `torch.utils._pytree`.\n- The function must be defined outside the `torch.compile`'d region.\n- Any non-input values read by the function will be treated as a constant\n  (e.g., a global tensor), and will not be guarded on.\n\nWhen tracing through a call to a `torch._dynamo.nonstrict_trace`'d function, `torch.compile` switches to [non-strict tracing](programming_model.non_strict_tracing_model),\nand the FX graph will eventually contain all the relevant tensor operations which happened inside that function.\n\nFor the example above, we can use `torch._dynamo.nonstrict_trace to eliminate` the graph break:\n\n```{code-cell}\n@torch._dynamo.nonstrict_trace\ndef get_magic_num():\n    # This explicit graph break call is meant to emulate any kind of Dynamo\n    # graph break, e.g., the function is implemented in C, or uses some python\n    # language feature Dynamo doesn't yet support.\n    torch._dynamo.graph_break()\n    return torch.tensor([42])\n@torch.compile(fullgraph=True)\ndef func(x):\n    n = get_magic_num()\n    return x + n\nprint(func(torch.rand(10)))\n# No graph break and no error.\n```\n\nNote that one can use it inside a `torch.compile`'d region as well:\n\n```{code-cell}\ndef get_magic_num():\n    # This explicit graph break call is meant to emulate any kind of Dynamo\n    # graph break, e.g., the function is implemented in C, or uses some python\n    # language feature Dynamo doesn't yet support.\n    torch._dynamo.graph_break()\n    return torch.tensor([42])\n@torch.compile(fullgraph=True)\ndef func(x):\n    n = torch._dynamo.nonstrict_trace(get_magic_num)()\n    return x + n\nprint(func(torch.rand(10)))\n# No graph break and no error.\n```",
    "1369": "一级标题：Working with `fullgraph=False`\n二级标题：无\n内容：\nWhile `fullgraph=False` is the default `torch.compile` setting, the semantics of resuming compilation upon encountering a graph break are more complicated.\nYou can find details on the `fullgraph=False` semantics in the subsections.\n\nThe strategy for using `torch.compile(fullgraph=False)` is as follows:\n\n1. [Determine the ideal location to place `torch.compile`](programming_model.where_to_apply_compile). Normally, it is the highest-level function that doesn’t result in excessive graph breaks.\n   Functions that do a lot of preprocessing or I/O operations are examples of functions that result in many graph breaks and do not significantly benefit from `torch.compile`.\n   a. You can isolate issues by first compiling individual functions/modules before compiling entire models.\n2. [Apply `torch.compiler.disable` to functions in the compiled region that result in a lot of graph breaks\n   and do not benefit from compilation](programming_model.compiler_disable). In this case, one graph break is better than potentially tens or hundreds.\n3. [Use `TORCH_LOGS=\"graph_breaks\"` or tlparse to investigate remaining graph breaks.](programming_model.observability)\n   Work around these graph breaks using the same approaches as working around graph breaks under\n   the `fullgraph=True` programming model. Not all graph breaks need to be removed - some may\n   impact performance more than others. The general rule is to focus on graph breaks that are happening during model computation.\n   a. We recommend using `torch.compile(backend='eager')` when debugging graph breaks, for faster debugging iteration times\n\n\n```{toctree}\nprogramming_model.where_to_apply_compile\nprogramming_model.compiler_disable\nprogramming_model.nested_graph_breaks\nprogramming_model.skipped_functions\n```",
    "1370": "一级标题：Use `fullgraph=True` to Identify and Eliminate Graph Breaks\n二级标题：无\n内容：\nUsing `torch.compile(fullgraph=False)` (the default) is a good way to get started with `torch.compile`: it supports all Python programs out-of-the-box via the ability to graph break and gives good performance on common cases.\n\nHowever, if you're trying to get more performance out of your model, you should explicitly think about what regions of code should be compiled:\n- We recommend using `torch.compile(fullgraph=True)` to find and eliminate graph breaks in your code.\n- If you're a library developer (or testing if your code \"works\" with `torch.compile`), we recommend testing using `torch.compile(fullgraph=True)`.\n\n`torch.compile(fullgraph=True)` offers stronger guarantees over `fullgraph=False`:\nwe will always capture a single FX graph to be compiled (or error if we cannot due to a graph break).\n**In particular, you are forced to resolve every graph break that is encountered.**\n\nThere are a number of strategies for resolving a graph break.",
    "1371": "一级标题：Use `fullgraph=True` to Identify and Eliminate Graph Breaks\n二级标题：Strategy 1:  Rewrite the unsupported code to use features supported by Dynamo\n内容：\nMany graph break error messages will give some suggestions on how to rewrite code to avoid the graph break.\nIf the graph break is still difficult to resolve, then please move on to the next strategy\nor submit an issue to the [PyTorch GitHub repo](https://github.com/pytorch/pytorch/issues).\n\nMore graph break examples and how to resolve them can be found in [Common Graph Breaks](programming_model.common_graph_breaks).\n\nExample: Dynamo does not support calling `next` on a `list_iterator` object that was an input to the function being compiled.\n\n```{code-cell}\n@torch.compile(fullgraph=True)\ndef f(xs):\n    a = next(xs)\n    b = next(xs)\n    return a + b\n\nxs = [torch.tensor(1.), torch.tensor(2.)]\ntry:\n    out = f(iter(xs))\nexcept Exception as e:\n    print(e)\n```\n\nInstead, rewrite the compiled function to accept a list.\n\n```{code-cell}\n@torch.compile(fullgraph=True)\ndef f_rewritten(xs):\n    it = iter(xs)\n    a = next(it)\n    b = next(it)\n    return a + b\n\nf_rewritten(xs)\n```",
    "1372": "一级标题：Use `fullgraph=True` to Identify and Eliminate Graph Breaks\n二级标题：Strategy 2: Pure functions can always be compiled via an escape hatch.\n内容：\n**Summary**: The space of all Python functions is vast and thus it is impractical for Dynamo to be able to trace\nthrough every Python function without graph breaks. For Python functions considered to be \"pure\"\nthat Dynamo cannot trace through without graph breaks, we provide some escape hatches to attempt\nto trace through these functions anyway:\n\n1. Use `custom_op` or `triton_op` on pure triton kernels.\n2. Use `nonstrict_trace` for pure functions that only use PyTorch Tensor ops.\n3. Use `custom_op` for all other pure functions.\n\nA \"pure function\" is a function with the following properties:\n\n- Determinism. Given the same inputs, the pure function will always return the same output\n- No external side effects. A pure function does not have any externally-visible side effects,\n  such as modifying external state or performing I/O operations.\n  Side effects that remain internal to the function are allowed (e.g. mutating intermediate tensors).\n  One notable exception is that mutating `torch.*` ops on function input Tensors are generally allowed.\n- Explicit input/output. All the input data must be passed through the function parameters and all of the outputs are returned from the function.\n\nSee [Pure Functions](programming_model.non_strict_tracing_model.pure_functions) for examples.\n\nDynamo is theoretically able to handle a wide variety of impure functions, but may be lacking coverage for specific\nPython language features. However, pure functions can always be compiled via an escape hatch.\n\nIf you have a graph break it may be possible to refactor the code around it into a pure function and use an escape hatch that bypasses Dynamo tracing:\n\n1. Use `torch._dynamo.nonstrict_trace` if you want the Tensor operations in the function to show up in the Dynamo output graph (and therefore be optimizable). `nonstrict_trace` tells Dynamo to use **non-strict tracing**.\n2. Use custom operators if you want the function to be opaque w.r.t. to `torch.compile` (both the frontend Dynamo and the backend).\n\nNote that there is nothing preventing these escape hatches from being applied to impure functions,\nbut **we do not provide any soundness guarantees**.\n\nExample: If Dynamo doesn't support some Python feature or API that is non-strict traceable (e.g. it uses PyTorch operations), [use `torch._dynamo.nonstrict_trace` to capture it instead](programming_model.dynamo_nonstrict_trace).\n\n```{code-cell}\n# this is a function that Dynamo doesn't support (due to the graph_break() call).\ndef g(x):\n    y = x.sin()\n    torch._dynamo.graph_break()\n    z = y.sin()\n    return z\n\n@torch.compile(fullgraph=True)\ndef f(x):\n    w = x.sin()\n    return g(w)\n\nx = torch.randn(3)\ntry:\n    f(x)  # Graph Break: there was a call to torch._dynamo.graph_break()\nexcept Exception as e:\n    print(e)\n\n@torch.compile(fullgraph=True)\ndef f_rewritten(x):\n    w = x.sin()\n    return torch._dynamo.nonstrict_trace(g)(w)\nf_rewritten(x)  # works\n```\n\nExample: use [custom operators](programming_model.custom_ops) to create opaque functions w.r.t. to `torch.compile`\n\n```{code-cell}\nfrom torch.utils.cpp_extension import load_inline\n\n# C++ source code for the square operation\ncpp_source = \"\"\"\ntorch::Tensor square_cpu(torch::Tensor input) {\n    // Check that input is a CPU tensor\n    TORCH_CHECK(input.device().is_cpu(), \"Input must be a CPU tensor\");\n\n    // Create output tensor with same shape and dtype as input\n    torch::Tensor output = torch::empty_like(input);\n\n    // Get data pointers\n    float* input_data = input.data_ptr<float>();\n    float* output_data = output.data_ptr<float>();\n\n    // Get total number of elements\n    int64_t numel = input.numel();\n\n    // For loop to compute square of each element\n    for (int64_t i = 0; i < numel; i++) {\n        output_data[i] = input_data[i] * input_data[i];\n    }\n\n    return output;\n}\n\"\"\"\n\n# Load the extension inline\nsquare_module = load_inline(\n    name=\"square_cpu_kernel\",\n    cpp_sources=cpp_source,\n    functions=[\"square_cpu\"],\n    verbose=True\n)\n\ndef square(x):\n    return square_module.square_cpu(x)\n\n@torch.compile(fullgraph=True)\ndef f(x):\n    return square(x)\n\ntry:\n    f(torch.randn(3, 3))  # graph break\nexcept Exception as e:\n    print(e)\n```\n\n```{code-cell}\n# Use torch.library.custom_op to define a new custom operator.\n# Custom operators are opaque with respect to torch.compile:\n# that is, torch.compile does not peek into them.\n\n@torch.library.custom_op(\"mylib::square\", mutates_args=())\ndef square(x: torch.Tensor) -> torch.Tensor:\n    return square_module.square_cpu(x)\n\n# Use register_fake to add a ``FakeTensor`` kernel for the operator\n@square.register_fake\ndef _(x):\n    return x.new_empty(x.size())\n\nprint(f(torch.randn(3, 3)))  # no graph break\n```\n\nFor more information on `triton_op` for custom triton kernels, see the\n[user-defined triton kernel tutorial](https://docs.pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html).",
    "1373": "一级标题：Use `fullgraph=True` to Identify and Eliminate Graph Breaks\n二级标题：Strategy 3: Don't compile the code\n内容：\nNot all code is amenable to being compiled. `torch.compile` is a compiler for Tensor computation;\nit will not be able to optimize things like disk IO. Try to refactor the code such that the unsupported\ncode is not called in the compiled region.\n\n```{code-cell}\n@torch.compile(fullgraph=True)\ndef f(x):\n   y = x ** 2  / 2\n   torch.save(y, \"foo.pt\")\n   z = y ** 3 / 6\n   return z\n\nx = torch.randn(3)\ntry:\n    f(x)  # Graph Break: torch.save not supported\nexcept Exception as e:\n    print(e)\n```\n\n```{code-cell}\ndef f_rewritten(x):\n   y = g(x)\n   torch.save(y, \"foo.pt\")\n   z = h(y)\n   return z\n\n@torch.compile(fullgraph=True)\ndef g(x):\n   y = x ** 2  / 2\n   return y\n\n@torch.compile(fullgraph=True)\ndef h(y):\n   z = y ** 3 / 6\n   return z\n\nf_rewritten(x)\n```\n\n```{code-cell}\n:tags: [remove-cell]\nimport os\nos.remove(\"foo.pt\")\n```",
    "1374": "一级标题：Working with Graph Breaks\n二级标题：无\n内容：\nAs you might remember from (Dynamo Core Concepts)[programming_model.dynamo_core_concepts] that Dynamo performs a graph break when\nit encounters code that can't be traced. In the default `torch.compile` settings, Dynamo compiles the FX graph\nthat has been determined up to that point, executes the unsupported code in regular Python, and then resumes tracing.\n\nGraph breaks enable Dynamo to trace through arbitrary Python code and carve out functional\nsubgraphs that can each be individually optimized.\n\nHowever, graph breaks may cause unexpected slowness in `torch.compile`.\nIf you're not seeing the expected speedups, we recommend checking for graph breaks and removing them.\n\nThe following sections outline strategies for addressing graph breaks.\n\n```{toctree}\nprogramming_model.fullgraph_true\nprogramming_model.common_graph_breaks\nprogramming_model.dynamo_nonstrict_trace\nprogramming_model.custom_ops\nprogramming_model.fullgraph_false\n```",
    "1375": "一级标题：torch.compile Programming Model\n二级标题：无\n内容：\nThe `torch.compile` programming model:\n1. Clarifies some internal behaviors of `torch.compile` so that one can better predict compiler behavior on user code and\n2. Provides ways for one to take more fine-grained control over `torch.compile`.\n\nBy understanding the `torch.compile` programming model, one can systematically unblock themselves when encountering issues with `torch.compile`.\n\n```{toctree}\nprogramming_model.dynamo_core_concepts\nprogramming_model.graph_breaks_index\nprogramming_model.non_strict_tracing_model\nprogramming_model.recompilation\nprogramming_model.observability\nprogramming_model.reporting_issues\n```",
    "1376": "一级标题：Nested Graph Breaks\n二级标题：无\n内容：\nSummary:\n- Graph breaks in nested functions can result in hard-to-understand compiler behavior, which we document below\n- A nested graph break results in {math}`\\mathcal O(N)` duplicate graph break behavior\n\nRecall that when `torch.compile` is applied to a function, any nested function calls are also traced.\nA **nested graph break** refers to any graph break that happens in a nested function call.\n\n```python\ndef inner(x):\n    ...\n    torch._dynamo.graph_break()  # nested graph break\n    ...\n\n@torch.compile\ndef outer(x):\n    ...\n    y = inner(x)\n    ...\n```\n\nThe resumption semantics around nested graph breaks can be confusing, so we describe the behavior here.\n\nRecall that in `fullgraph=False`, [graph breaks are handled](programming_model.dynamo_core_concepts.graph_breaks) by compiling the FX graph that has been determined so far,\nrunning the unsupported code in regular Python, then resuming tracing after the unsupported code with a new FX graph.\nResuming a function is actually a fairly complicated technical feat, so resuming tracing is only supported on top-level functions.\n\nWe can therefore resume tracing after a nested graph break with this restriction in the following way:\n\nFirst, consider the below example where `torch.compile` traces from `f` and traces all the way until the\ngraph break in `inner1` is encountered.\n\n```python\ndef inner1(x):\n    x = x + 1\n    torch._dynamo.graph_break()  # stop tracing due to graph break\n    return x + 2\n\ndef inner2(x):\n    x = x + 4\n    x = inner1(x)\n    x = x + 8\n\n@torch.compile\ndef f(x):\n    # start tracing from here\n    x = x + 16\n    x = inner2(x)\n    x = x + 32\n\nf(torch.randn(3))\n```\n\nSince we can only resume from top-level functions, we graph break on the `inner2` call in `f`.\n```python\n# The semantics of torch.compile(f)(x) is roughly this:\ndef compiled_f_semantics(x):\n    y = x + 16\n    z = inner2(y)\n    return torch.compile(resume_f_semantics)(z)\n\ndef resume_f_semantics(x):\n    return x + 32\n\ncompiled_f_semantics(torch.randn(3))\n```\n\n`inner2` is then automatically compiled as a top-level function.\nWe trace all the way until the graph break in `inner1` is encountered again.\n\n```python\ndef inner1(x):\n    x = x + 1\n    torch._dynamo.graph_break()  # stop tracing due to graph break\n    return x + 2\n\n# this torch.compile is automatically applied\n@torch.compile\ndef inner2(x):\n    # start tracing from here\n    x = x + 4\n    x = inner1(x)\n    x = x + 8\n\ndef compiled_f_semantics(x):\n    y = x + 16\n    z = inner2(y)\n    return torch.compile(resume_f_semantics)(z)\n\ndef resume_f_semantics(x):\n    return x + 32\n\ncompiled_f_semantics(torch.randn(3))\n```\n\nThen we graph break on the `inner1` call in `inner2`.\n```python\ndef compiled_inner2_semantics(x):\n    y = x + 4\n    z = inner1(y)\n    return torch.compile(resume_inner2_semantics)(z)\n\ndef resume_inner2_semantics(x):\n    return x + 8\n```\n\n`inner1` is then automatically compiled as a top-level function.\nThe graph break is from `inner1`, so we handle the graph break normally.\n```python\n# this torch.compile is automatically applied\n@torch.compile\ndef inner1(x):\n    # start tracing from here\n    x = x + 1\n    torch._dynamo.graph_break()  # stop tracing due to graph break\n    return x + 2\n\ndef compiled_f_semantics(x):\n    y = x + 16\n    z = compiled_inner2_semantics(y)\n    return torch.compile(resume_f_semantics)(z)\n\ndef resume_f_semantics(x):\n    return x + 32\n\ndef compiled_inner2_semantics(x):\n    y = x + 4\n    z = inner1(y)\n    return torch.compile(resume_inner2_semantics)(z)\n\ndef resume_inner2_semantics(x):\n    return x + 8\n\ncompiled_f_semantics(torch.randn(3))\n```\n\n`inner1` is handled normally:\n\n```python\ndef compiled_inner1_semantics(x):\n    y = x + 1\n    torch._dynamo.graph_break()\n    return torch.compile(resume_inner1_semantics)(y)\n\ndef resume_inner1_semantics(x):\n    return x + 2\n```\n\nSo the initial code is semantically equivalent to\n```python\ndef compiled_f_semantics(x):\n    y = x + 16\n    z = compiled_inner2_semantics(y)\n    return torch.compile(resume_f_semantics)(z)\n\ndef resume_f_semantics(x):\n    return x + 32\n\ndef compiled_inner2_semantics(x):\n    y = x + 4\n    z = compiled_inner1_semantics(y)\n    return torch.compile(resume_inner2_semantics)(z)\n\ndef resume_inner2_semantics(x):\n    return x + 8\n\ndef compiled_inner1_semantics(x):\n    y = x + 1\n    torch._dynamo.graph_break()\n    return torch.compile(resume_inner1_semantics)(y)\n\ndef resume_inner1_semantics(x):\n    return x + 2\n\ncompiled_f_semantics(torch.randn(3))\n```\n\nNote in particular that we traced 3 top-level functions, and that we traced the same graph break 3 times.\n**This explains why you may encounter duplicate graph breaks when using `torch.compile`.**\n\nIn summary, nested graph breaks are handled by:\n- Tracing from the top-level function all the way to the nested graph break\n- Graph breaking on the top-level function at the call to the second-level function\n- Compiling the PyTorch ops tracked so far and running the compiled graph\n- Calling the second-level function, which gets automatically compiled as a top-level function\n- Resuming tracing after the second-level function call\n\nNote that the runtime of handling this graph break is {math}`\\mathcal O(NK)`, where {math}`N` is the nesting depth,\nand {math}`K` is the number of instructions from the top-level function to the graph break.\nWe end up tracing {math}`\\mathcal O(N^2)` frames, and we trace the same graph break {math}`\\mathcal O(N)` times.",
    "1377": "一级标题：Non-strict Tracing Programming Model\n二级标题：无\n内容：\n**Summary:**\n- **Non-strict tracing** is a way to trace Python code that is less strict than Dynamo, but may result in silent incorrectness.\n- Non-strict tracing runs a Python function and uses Python and PyTorch’s operator overloading capabilities to record what Tensor operations occurred during execution into a trace.\n- A function is **non-strict traceable** if it complies with some constraints, namely, that the function is **pure** and does not directly manipulate Tensor.data_ptr().\n- Non-strict tracing may **specialize** on certain variables and treat them as **constants**, baking the values of the variables into the trace.\n\n`torch.compile` internals (`make_fx`, AOTDispatcher) use **non-strict tracing**. [`torch._dynamo.nonstrict_trace`](programming_model.dynamo_nonstrict_trace) can also be used in `torch.compile`d code to mark sections of code to be traced with non-strict tracing.\nNon-strict tracing runs a Python function and uses Python and PyTorch’s operator overloading capabilities to record what Tensor operations occurred during execution into a trace.\n\n**`make_fx`** is the main entrypoint for non-strict tracing. For the following function, only the top branch is taken during execution of the inputs, so it captures a graph with only that branch.\n\n```{code-cell}\nfrom torch.fx.experimental.proxy_tensor import make_fx\ndef f(x):\n    if x.shape[0] > 2:\n        return x ** 2 / 6\n    else:\n        return x * 3\nx = torch.randn(3)\ngm = make_fx(f, tracing_mode=\"fake\")(x)\ngm.print_readable()\n```\n\nNon-strict tracing differs from Dynamo (strict) tracing in that **it is unsafe**, that is, given a function, it captures a graph of Tensor operations that may have different semantics than the original function.\nGiven a Python function, Dynamo Tracing captures a graph of Tensor operations and residual bytecode that when combined give the same semantics as the Python function.\n\n(programming_model.non_strict_tracing_model.pure_functions)=",
    "1378": "一级标题：Non-strict Tracing Programming Model\n二级标题：Pure Functions\n内容：\nNon-strict tracing is sound only on **pure functions**, and thus only pure functions should be non-strict traced.\n\nA pure function is a function with the following properties:\n\n- **Determinism.** Given the same inputs, the pure function will always return the same output.\n- **No side effects.** A pure function does not have any side effects such as modifying external state or performing I/O operations.\n- **Explicit input/output.** All the input data must be passed through the function parameters and all of the outputs are returned from the function.\n\nHere are some examples of impure functions for which the captured graph behaves differently from the original function.\n\n### Example 1: No explicit input (e.g. accesses global tensor)\n```{code-cell}\nvar = torch.tensor(1)\ndef function_with_global_access(y):\n    return y + var\nx = torch.tensor([0, 1, 2])\n# _allow_non_fake_inputs=True is needed to capture the global variable\n# for demonstration purposes.\ngm = make_fx(\n    function_with_global_access, tracing_mode=\"fake\", _allow_non_fake_inputs=True\n)(x)\n# Non-strict Tracing captures the value of the global (1.)\nprint(\"1. call function\", function_with_global_access(x))\nprint(\"1. call graph\", gm(x))\n# However, after changing the global, the captured graph\n# produces a different result from the original function\nvar = torch.tensor(2)\nprint(\"2. call function\", function_with_global_access(x))\nprint(\"2. call graph\", gm(x))\n# To capture a graph that can have a varying `var` tensor,\n# it must be an explicit input:\ndef function_fixed(y, var):\n    return y + var\nvar = torch.tensor(3)\ngm = make_fx(function_fixed, tracing_mode=\"fake\")(x, var)\nprint(\"3. call function\", function_fixed(x, var))\nprint(\"3. call graph\", gm(x, var))\nvar = torch.tensor(4)\nprint(\"4. call function\", function_fixed(x, var))\nprint(\"4. call graph\", gm(x, var))\n```\n\nSee [Specialization and Constants](specialization-and-constants) for an explanation of why.\n\n### Example 2: Side effect (printing)\n\n```{code-cell}\ndef function_with_side_effect(y):\n    print(y)\nx = torch.tensor([0, 1, 2])\n_ = function_with_side_effect(x)\n```\n\nRunning `f` in Python prints a Tensor as a side effect.\n\n```{code-cell}\ngm = make_fx(function_with_side_effect, tracing_mode=\"fake\")(x)\n```\n\nDuring non-strict tracing, this print occurs during the graph capture.\n\n```{code-cell}\n_ = gm(x)\n```\n\nThe graph does not store a call to the `print` statement, so executing the graph doesn’t print anything.\n\n### Example 3: Side effect (input list mutation)\n\n```{code-cell}\nlst = []\ndef function_with_input_list_mutation(lst):\n    val = lst.pop()\n    return val\nx = torch.tensor([0, 1, 2])\ny = torch.tensor([0, 1, 2])\n# Each time the function is executed, the list shrinks in size\nlst = [x, y]\nfunction_with_input_list_mutation(lst)\nprint(\"len(lst) after one call\", len(lst))\nfunction_with_input_list_mutation(lst)\nprint(\"len(lst) after two calls\", len(lst))\n# With Non-strict Tracing, the length of the list shrinks during\n# the graph capture but not in invocations of the graph.\nlst = [x, y]\ngm = make_fx(function_with_input_list_mutation, tracing_mode=\"fake\")(lst)\nprint(\"len(lst) after graph capture\", len(lst))\ngm(lst)\nprint(\"len(lst) after one call to graph\", len(lst))\ngm(lst)\nprint(\"len(lst) after two calls to graph\", len(lst))\n```\n\n### No direct data_ptr manipulation\nDirectly manipulating `Tensor.data_ptr` is not non-strict traceable. The intuition behind this is that PyTorch is unable to tell *how* you manipulated the `data_ptr`.\n\n```{code-cell}\nimport ctypes\n# Create a tensor with a single element\ntensor = torch.tensor([42], dtype=torch.int32)  # Using int32 for simplicity\ndef function_with_data_ptr(tensor):\n    # Get the data pointer\n    ptr = tensor.data_ptr()\n    # Cast the pointer to a ctypes pointer\n    ctypes_ptr = ctypes.cast(ptr, ctypes.POINTER(ctypes.c_int32))\n    # Increment the value at the pointer\n    ctypes_ptr.contents.value += 1\n    return tensor\ntry:\n    make_fx(function_with_data_ptr, tracing_mode=\"fake\")(tensor)\nexcept Exception as e:\n    print(e)\n```\n\n(specialization-and-constants)=",
    "1379": "一级标题：Non-strict Tracing Programming Model\n二级标题：Specialization and Constants\n内容：\nNon-strict tracing captures a graph that may be specialized on some values. What this means is the captured graph is only valid for these values. We say the graph treats those values as **constant**.\n\nAll non-Tensor variables are treated as constant during Non-strict Tracing:\n\n```{code-cell}\ndef f(x, y):\n    return x + y\nx = torch.tensor([0, 1, 2])\ny = 3.14\ngm = make_fx(f, tracing_mode=\"fake\")(x, y)\ngm.print_readable()\n```\n\n3.14 is a constant in the graph.\n\nNon-strict tracing will also specialize on properties of the input Tensors.\n\n```{code-cell}\ndef f(x):\n    if x.shape[0] > 2:\n        return x ** 2 / 6\n    else:\n        return x * 3\nx = torch.randn(3)\ngm = make_fx(f, tracing_mode=\"fake\")(x)\ngm.print_readable()\n```\n\nAnd it will also specialize on any variables not directly passed into the function:\n\n```{code-cell}\nvar = torch.tensor(1)\ndef f(x):\n    return x + y\nx = torch.randn(3)\ngm = make_fx(f, tracing_mode=\"fake\")(x)\ngm.print_readable()\n```",
    "1380": "一级标题：tlparse / TORCH_TRACE\n二级标题：无\n内容：\ntlparse / `TORCH_TRACE` are a pair of tools that produce compilation reports that look [like this](https://web.mit.edu/~ezyang/Public/bhack-20240609-tlparse/index.html).\n\nTraces are fairly straightforward to collect. To collect a trace, run your model like so:\n\n```bash\nTORCH_TRACE=\"/tmp/tracedir\" python foo.py\npip install tlparse\ntlparse /tmp/tracedir\n```\n\nThis approach works even if you are running a distributed job, providing a trace for each rank.\nIt will open your browser with HTML similar to what’s generated above.\nIf you are making a bug report for a complicated problem that you don’t have a standalone reproduction for,\nyou can still greatly assist PyTorch developers by attaching the trace log generated in `/tmp/tracedir`.\n\n```{warning}\nThe trace log contains all of your model code.\nDo not share the trace log if the model you are working on is sensitive. The trace log does NOT contain weights.\n```\n\n```{raw} html\n    <style>\n        .red {background-color:#ff0000;}\n        .green {background-color:#00ff00;}\n        .dark-green {background-color:#027f02;}\n    </style>\n```\n\n```{eval-rst}\n.. role:: red\n.. role:: green\n.. role:: dark-green\n```\n\nThe output of `tlparse` is primarily aimed for PyTorch developers,\nand the log format is easy to upload and share on GitHub.\nHowever,  as a non-PyTorch developer, you can still extract useful information from it.\nWe recommend starting with the inline help text in the report, which explains its contents.\nHere are some insights you can gain from a `tlparse`:\n\n- What model code was compiled by looking at the stack trie?\n  This is especially useful if you're not familiar with the codebase being compiled!\n- How many graph breaks / distinct compilation regions are there?\n  (Each distinct compile is its own color coded block like {dark-green}`[0/0]`).\n  Frames that are potentially graph-broken are light green {green}`[2/4]`.\n  If there are a lot of frames, that is suspicious, and suggests that you had some catastrophic graph breaks,\n  or maybe your code isn't a good match for `torch.compile`.\n- How many times did I recompile a particular frame? Something that recompiled a lot will look like:\n  {dark-green}`[10/0]` {dark-green}`[10/1]` {dark-green}`[10/2]`\n  \\- if something is being recompiled a lot, that is very suspicious and worth looking into, even if it isn't the root cause of your problem.\n- Was there a compilation error? Frames that errored will look like {red}`[0/1]`.\n- What intermediate compiler products did I generate for a given frame?\n  For example, you can look at the high-level generated FX graph or the generated Triton code.\n- Is there relevant information for a particular frame? You can find these in `compilation_metrics`.",
    "1381": "一级标题：tlparse / TORCH_TRACE\n二级标题：TORCH_LOGS\n内容：\nYou can use the `TORCH_LOGS` environment variable to selectively enable parts of the `torch.compile` stack to log.\n`TORCH_LOGS` is in fact the source of logs for `tlparse`. The format of the `TORCH_LOGS` environment variable looks like this:\n\n```bash\nTORCH_LOGS=\"<option1>,<option2>,...\" python foo.py\n```\n\nYou can also programmatically set logging options using `torch._logging.set_logs`:\n\n```python\nimport logging\ntorch._logging.set_logs(graph_breaks=True, dynamic=logging.DEBUG)\n```\n\nThe most useful options are:\n\n- `graph_breaks`: logs locations of graph breaks in user code and the reason for the graph break\n- `guards`: logs guards that are generated\n- `recompiles`: logs which function recompiled and the guards that failed, leading to the recompilation\n- `dynamic`: logs related to dynamic shapes\n- `output_code`: logs the code generated by Inductor\n\nSome more helpful `TORCH_LOGS` options include:\n\n```{eval-rst}\n.. list-table::\n    :widths: 25 50\n    :header-rows: 1\n\n    * - Option\n      - Description\n    * - +all\n      - Output debug logs from all ``torch.compile`` components\n    * - +dynamo\n      - Output debug logs from TorchDynamo\n    * - +aot\n      - Output debug logs from AOTAutograd\n    * - +inductor\n      - Output debug logs from TorchInductor\n    * - dynamic\n      - Output logs from dynamic shapes\n    * - graph_code\n      - Output the Python code for the FX graph that Dynamo generated\n    * - graph_sizes\n      - Output the tensor sizes of the FX graph that Dynamo generated\n    * - trace_bytecode\n      - Output the bytecode instructions that Dynamo is tracing through and the symbolic interpreter stack Dynamo is keeping track of\n    * - trace_source\n      - Output the line of code in the original source that Dynamo is currently tracing through\n    * - bytecode\n      - Output Dynamo-generated bytecode\n    * - guards\n      - Output generated guards\n    * - recompiles\n      - Output recompilation reasons (only the first guard check that fails)\n    * - recompiles_verbose\n      - Output all guard checks that fail when a recompilation occurs\n    * - aot_graphs\n      - Output graph generated by AOTAutograd\n    * - aot_joint_graphs\n      - Output the joint forward-backward graph generated by AOTAutograd\n    * - output_code\n      - Output code generated by Inductor\n    * - kernel_code\n      - Output code generated by Inductor on a per-kernel basis\n    * - schedule\n      - Output Inductor scheduling logs\n    * - perf_hints\n      - Output Inductor perf hint logs\n    * - fusion\n      - Output Inductor fusion logs\n```\n\nFor the full list of options, see [torch.\\_logging](https://pytorch.org/docs/stable/logging.html)\nand [torch.\\_logging.set_logs](https://pytorch.org/docs/stable/generated/torch._logging.set_logs.html#torch._logging.set_logs).",
    "1382": "一级标题：tlparse / TORCH_TRACE\n二级标题：tlparse vs. TORCH_LOGS\n内容：\nGenerally, we suggest first using `tlparse` when encountering issues.\n`tlparse` is ideal for debugging large models and gaining a high-level overview of how your model was compiled.\nOn the other hand, `TORCH_LOGS` is preferred for small examples and fine-grained debugging detail,\nwhen we already have an idea of which `torch.compile` component is causing the problem.",
    "1383": "一级标题：Dealing with Recompilations\n二级标题：无\n内容：\nRecompilations are necessary for `torch.compile` soundness, but can result in significantly increased compile time.\nThus, minimizing recompilations while preserving soundness is essential for reducing compile time.\n\nYou can view recompilations and their reasons using tlparse or `TORCH_LOGS=recompiles`.",
    "1384": "一级标题：Dealing with Recompilations\n二级标题：Is Dynamic Shapes Enabled?\n内容：\nIn the below example, we recompile due to mismatched shapes:\n\n```{code-cell}\n@torch.compile\ndef fn(x):\n    return x + 1\nfn(torch.ones(3))\nfn(torch.ones(4))\n```\n\nMake sure that the dynamic option of `torch.compile` is not set to `False`.\nThe default option, `dynamic=None`, will only attempt dynamic shapes after the first compilation.\nYou can set `dynamic=True` to upfront compile as dynamic as possible:\n\n```{code-cell}\n@torch.compile(dynamic=True)\ndef gn(x):\n    return x + 1\ngn(torch.ones(3))\ngn(torch.ones(4))\n```\n\nFor more information on dynamic shapes, including dealing with errors/recompilations due to\ndynamic shapes, see [the dynamic shapes manual](https://docs.google.com/document/d/1GgvOe7C8_NVOMLOCwDaYV1mXXyHMXY7ExoewHqooxrs/edit?tab=t.0#heading=h.fh8zzonyw8ng).",
    "1385": "一级标题：Dealing with Recompilations\n二级标题：Wrapping Constants with Tensors\n内容：\nBy default, `int` / `float` variables are treated as constants and are guarded on their exact value.\nIn the below example, we have a recompilation for each function call.\n\n```{code-cell}\n@torch.compile\ndef fn(x, c):\n    return x + c\nfor i in range(5):\n    fn(torch.ones(i), 0.5 + i)\n```\n\nIn particular, for LR schedulers, initializing with a constant can lead to recompilations:\n\n```{code-cell}\nmod = torch.nn.Linear(3, 3)\nopt = torch.optim.Adam(mod.parameters(), lr=0.01)\nsched = torch.optim.lr_scheduler.ExponentialLR(opt, 0.9)\n@torch.compile\ndef gn(inp):\n    opt.zero_grad(True)\n    out = mod(inp).sum()\n    out.backward()\n    opt.step()\n    sched.step()\nfor i in range(5):\n    gn(torch.ones(3, 3))\n```\n\nIn both examples, we can wrap `float` variables in tensors in order to prevent recompilations.\n\n```{code-cell}\n:tags: [remove-cell]\ntorch._dynamo.reset()\n```\n\n```{code-cell}\n# first example\nfor i in range(5):\n    fn(torch.ones(i), torch.tensor(0.5 + i))\n# second example\nopt = torch.optim.Adam(mod.parameters(), lr=torch.tensor(0.01))\nsched = torch.optim.lr_scheduler.ExponentialLR(opt, torch.tensor(0.9))\nfor i in range(5):\n    gn(torch.ones(3, 3))\n```\n\n(programming_model.recompilation.changing_cache_size_limit)=",
    "1386": "一级标题：Dealing with Recompilations\n二级标题：Changing the Cache Size Limit\n内容：\nThere is a limit to how many times a function can be recompiled,\ndetermined by `torch._dynamo.config.cache_size_limit` and `torch._dynamo.config.accumulated_cache_size_limit`\n(The exact difference between these 2 values is detailed in [`torch/_dynamo/cache_size.py`](https://github.com/pytorch/pytorch/blob/4ce6e6ec8890a3f6ee604c9efb3ff153825ce575/torch/_dynamo/cache_size.py#L14)).\nIf the Dynamo cache limit is hit, then all future compilation attempts **will result in the function being skipped (run eagerly)**.\nDynamo will still attempt to use previously compiled bytecode for future function calls, if the guards pass.\nNote that in the case of a recompilation limit hit, **all nested function calls WILL be skipped**\n(Dynamo will try to use previously compiled bytecode for the nested functions).\nDynamo will also issue a warning containing the affected function and which limit was hit.\nIn the example below, each function call results in a recompile attempt.\nWhen we hit the cache size limit (by default, 8), we stop attempting to recompile.\n(Note that we set `dynamic=False` for demonstration purposes to force recompilation every time).\n\n```{code-cell}\n@torch.compile(dynamic=False)\ndef fn(x):\n    return x + 1\nfor i in range(1, 10):\n    # recompile every time due to dynamic=False\n    fn(torch.ones(i))\n```\n\nIf you know that the number of recompilations has a reasonable constant upper bound, you can raise the cache size limit.\nIf the cost of recompilation outweighs the benefit of compilation, then you can consider lowering the cache size limit.\n\n```{code-cell}\ntorch._dynamo.config.cache_size_limit = 16\n@torch.compile(dynamic=False)\ndef gn(x):\n    return x + 1\nfor i in range(1, 10):\n    gn(torch.ones(i))\n```",
    "1387": "一级标题：Dealing with Recompilations\n二级标题：Graph Breaking to Reduce Recompilation Costs\n内容：\nIf a large graph is recompiling and causing high compile time, you can intentionally introduce\na graph break in order to reduce recompilation costs, at the expense of introducing a performance hit.\n\n```{code-cell}\ndef very_large_function(x):\n    return x + 1\n\n@torch.compile(dynamic=False)\ndef fn(x, c):\n    y = very_large_function(x)  # recompiled every time\n    return y + c\n\nfor i in range(1, 5):\n    fn(torch.ones(3), i)\n\n@torch.compile(dynamic=False)\ndef gn(x, c):\n    y = very_large_function(x)  # compiled only once\n    torch._dynamo.graph_break()\n    return y + c  # recompiled every time\n\nfor i in range(1, 5):\n    gn(torch.ones(3), i)\n```",
    "1388": "一级标题：Reporting Issues\n二级标题：无\n内容：\nIf the provided workarounds were not enough to get `torch.compile` working,\nthen you should consider reporting the issue to PyTorch.\nBut there are a few things that you can do to make our lives significantly easier.",
    "1389": "一级标题：Reporting Issues\n二级标题：Ablation\n内容：\nCheck which component of the `torch.compile` stack is the one causing the issue using the `backend=` option for `torch.compile`.\nIn particular, try:\n\n- `torch.compile(fn, backend=\"eager\")`, which only runs TorchDynamo, the graph capture component of `torch.compile`.\n- `torch.compile(fn, backend=\"aot_eager\")`, which runs TorchDynamo and AOTAutograd, which additionally generates the backward graph during compilation.\n- `torch.compile(fn, backend=\"aot_eager_decomp_partition\")`, which runs TorchDynamo and AOTAutograd with operator decompositions/partitions.\n- `torch.compile(fn, backend=\"inductor\")`, which runs TorchDynamo, AOTAutograd, and TorchInductor, the backend ML compiler that generates compiled kernels.\n\nIf you only fail with the Inductor backend, you can additionally test various Inductor modes:\n\n- `torch.compile(fn, backend=\"inductor\", mode=\"default\")`\n- `torch.compile(fn, backend=\"inductor\", mode=\"reduce-overhead\")`\n- `torch.compile(fn, backend=\"inductor\", mode=\"max-autotune\")`\n\nYou can also check if dynamic shapes is causing issues with any backend:\n\n- `torch.compile(fn, dynamic=True)` (always use dynamic shapes)\n- `torch.compile(fn, dynamic=False)` (never use dynamic shapes)\n- `torch.compile(fn, dynamic=None)` (automatic dynamic shapes)",
    "1390": "一级标题：Reporting Issues\n二级标题：Bisecting\n内容：\nDid you try on the latest nightly? Did something work in the past but now no longer works?\nCan you bisect to determine the first nightly where your issue occurs?\nBisecting is especially helpful for performance, accuracy, or compile time regressions,\nwhere it is not immediately obvious where the problem originates from.",
    "1391": "一级标题：Reporting Issues\n二级标题：Creating a reproducer\n内容：\nCreating reproducers is a lot of work, and it is perfectly fine if you do not have the time to do it.\nHowever, if you are a motivated user unfamiliar with the internals of `torch.compile`,\ncreating a standalone reproducer can have a huge impact on our ability to fix the bug.\nWithout a reproducer, your bug report must contain enough information for us to identify the root cause of the problem and write a reproducer from scratch.\n\nHere's a list of useful reproducers, ranked from most to least preferred:\n\n1. **Self-contained, small reproducer:** A script with no external dependencies, under 100 lines of code, that reproduces the problem when run.\n2. **Self-contained, large reproducer:** Even if it's large, being self-contained is a huge advantage!\n3. **Non-self-contained reproducer with manageable dependencies:**\n   For example, if you can reproduce the problem by running a script after `pip install transformers`,\n   that's manageable. We can likely run it and investigate.\n4. **Non-self-contained reproducer requiring substantial setup:** This might involve downloading datasets,\n   multiple environment setup steps, or specific system library versions requiring a Docker image.\n   The more complex the setup, the harder it is for us to recreate the environment.\n\n:::{note}\nDocker simplifies setup but complicates changes to the environment, so it's not a perfect solution, though we'll use it if necessary.\n:::\n\nIf possible, try to make your reproducer single-process, as those are easier to debug than a multi-process reproducer.\n\nAdditionally, below is a non-exhaustive list of aspects to check in your\nissue that you can attempt to replicate in your reproducer:\n\n- **Autograd**. Did you have tensor inputs with `requires_grad=True`? Did you call `backward()` on the output?\n- **Dynamic shapes**. Did you set `dynamic=True`? Or did you run the test code multiple times with varying shapes?\n- **Custom operators**. Is there a custom operator involved in the real workflow?\n  Can you replicate some of its important characteristics using the Python custom operator API?\n- **Configuration**. Did you set all the same configuration?\n  This includes `torch._dynamo.config` and `torch._inductor.config` settings,\n  as well as arguments to `torch.compile` like `backend` / `mode`.\n- **Context managers**. Did you replicate any active context managers?\n  This could be `torch.no_grad`, automatic mixed precision, `TorchFunctionMode` / `TorchDispatchMode`,\n  activation checkpointing, compiled autograd etc.\n- **Tensor subclasses**. Is there a tensor subclass involved?",
    "1392": "一级标题：Skipped Functions\n二级标题：无\n内容：\n**Summary:**\n- Sometimes, `torch.compile` completely gives up compiling a function and runs it eagerly instead,\n  resulting in potentially lost optimization opportunities.\n- There are ways to work around skipped functions in order to re-enable tracing around the problematic code.\n\nSometimes, `torch.compile` with `fullgraph=False` is unable to resume tracing when encountering a graph break\nor other compiler error. In many of these cases, `torch.compile` will skip compiling the function entirely and run it eagerly.\n\nNote that the skip is only applied to the current function and NOT any nested function calls.\n`torch.compile` will still attempt to compile nested calls.\n\n<!-- TODO: fix logging for skipped functions. -->\n\n```{code-cell}\ndef inner1(x):\n    return x + 1\ndef inner2(x):\n    return x + 2\n@torch.compile\ndef fn(x):\n    x = inner1(x)\n    torch._dynamo.skip_frame()\n    x = inner2(x)\nfn(torch.randn(3))\n```\n\nIn the above example, `torch.compile` will trace `fn` (including `inner1`) up until the `skip_frame`.\nThen `fn` is skipped and run eagerly - `inner1` and `inner2` are compiled when they are called.\n\nSkipping functions may result in lost optimization opportunities,\nso it is important to check if code you want compiled is being skipped, and if so, to work around the skip.",
    "1393": "一级标题：Skipped Functions\n二级标题：Graph Break in a Loop\n内容：\n`torch.compile` cannot resume tracing if a graph break occurs in a loop:\n\n```{code-cell}\n@torch.compile\ndef fn(x):\n    for i in range(5):\n        x = x + 1\n        if i == 3:\n            torch._dynamo.graph_break()\n    return x\nfn(torch.randn(3))\n```\n\nIn this example, we can avoid skipping by unrolling the loop:\n\n```{code-cell}\n@torch.compile\ndef fn(x):\n    def inner(i):\n        nonlocal x\n        x = x + 1\n        if i == 3:\n            torch._dynamo.graph_break()\n    inner(0)\n    inner(1)\n    inner(2)\n    inner(3)\n    inner(4)\n    return x\nfn(torch.randn(3))\n```\n\nIn general, resolving the graph break causing the skip will also resolve the skip.",
    "1394": "一级标题：Skipped Functions\n二级标题：Graph Break in a Context Manager\n内容：\nAnother common example of an unresumable graph break is a graph break in most context managers:\n\n```{code-cell}\nclass CustomCtxManager:\n    def __enter__(self):\n        pass\n    def __exit__(self, exc_type, exc_value, traceback):\n        pass\n@torch.compile\ndef fn(x):\n    with CustomCtxManager():\n        x = x + 1\n        torch._dynamo.graph_break()\n        return x + 1\nfn(torch.randn(3))\n```\n\nWe can avoid skipping by moving the graph break outside of the context manager:\n\n```{code-cell}\n@torch.compile\ndef fn(x):\n    with CustomCtxManager():\n        x = x + 1\n    torch._dynamo.graph_break()\n    with CustomCtxManager():\n        return x + 1\nfn(torch.randn(3))\n```\n\nThere are some context managers where Dynamo can resume after a graph break.\nSome of these can be found in `supported_ctx_manager_classes` in `torch/_dynamo/variables/torch.py`.\nIn general, any context manager represented by a `ContextWrappingVariable` subclass in\n`torch/_dynamo/variables/ctx_manager.py` support resuming after a graph break. For example:\n\n```{code-cell}\nimport contextlib\n@torch.compile\ndef fn(x):\n    with contextlib.nullcontext():\n        with torch.no_grad():\n            x = x + 1\n            torch._dynamo.graph_break()\n            return x + 1\nfn(torch.randn(3))\n```",
    "1395": "一级标题：Skipped Functions\n二级标题：Graph Break in a Try Block\n内容：\nA graph break in a try block cannot be resumed:\n\n```{code-cell}\n@torch.compile\ndef fn(x):\n    try:\n        x = x + 1\n        torch._dynamo.graph_break()\n        return x + 1\n    except Exception as e:\n        pass\nfn(torch.randn(3))\n```\n\nWe can avoid skipping by moving the graph break outside of the try block:\n\n```{code-cell}\n@torch.compile\ndef fn(x):\n    try:\n        x = x + 1\n    except Exception as e:\n        pass\n    torch._dynamo.graph_break()\n    try:\n        return x + 1\n    except Exception as e:\n        pass\nfn(torch.randn(3))\n```",
    "1396": "一级标题：Skipped Functions\n二级标题：Hitting a Recompilation Limit\n内容：\nSee [Changing the Cache Size Limit.](programming_model.recompilation.changing_cache_size_limit)",
    "1397": "一级标题：Skipped Functions\n二级标题：Compiler Errors\n内容：\nSome compiler errors will result in skipped functions.\nOther compiler errors will result in a hard error rather than a skipped function.",
    "1398": "一级标题：Skipped Functions\n二级标题：Dealing with Skipped Functions\n内容：\nIn general, you can resolve a skipped function by fixing the underlying graph break or error that\nis causing the function to be skipped.\n\nIf the graph break/error causing the skipped function is difficult to fix,\nthen consider isolating the graph break/error in its own function so that minimal things are skipped.\n\n```{code-cell}\ndef inner1(x):\n    return x + 1\ndef inner2(x):\n    return x + 2\n@torch.compile\ndef fn(x):\n    x = inner1(x)\n    def problematic_code():\n        torch._dynamo.skip_frame()\n    problematic_code()\n    x = inner2(x)\nfn(torch.randn(3))\n```",
    "1399": "一级标题：Where to apply torch.compile?\n二级标题：无\n内容：\nWe recommend applying `torch.compile` to the highest-level function that doesn’t cause excessive problems.\nTypically, it is:\n- your `train` or `eval` step with the optimizer but without the loop,\n- your top-level `nn.Module`\n- or some sub-`nn.Module`s.\n\n`torch.compile` specifically doesn’t handle distributed wrapper modules like DDP or FSDP very well,\nso consider applying `torch.compile` to the inner module passed to the wrapper.\n\n```python\n# inference\nmodel = ...\nmodel.compile()\n\nfor _ in range(N_ITERS):\n    inp = ...\n    out = model(inp)\n```\n\n```python\n# training\nmodel = ...\nopt = torch.optim.Adam(model.parameters())\n\n@torch.compile\ndef train(mod, data):\n    opt.zero_grad(True)\n    pred = mod(data[0])\n    loss = torch.nn.CrossEntropyLoss()(pred, data[1])\n    loss.backward()\n    opt.step()\n\nfor _ in range(N_ITERS):\n    inp = ...\n    train(model, inp)\n```\n\n```python\n# DistributedDataParallel\nmodel = ...\nmodel.compile()\nmodel_ddp = DistributedDataParallel(model, ...)\n\nfor _ in range(N_ITERS):\n    inp = ...\n    out = model_ddp(inp)\n```\n\n<!-- TODO add examples for specific model domains, compile(model) vs. model.compile()-->",
    "1400": "一级标题：Where to apply torch.compile?\n二级标题：`compile(model)` vs `model.compile()`\n内容：\nDue to nuances to how `torch.compile` interacts with `nn.Module` instances,\nwe advise using the `.compile()` method of `nn.Module` instances if you wish to compile them as\ntop-level functions. Nested module calls will be traced correctly -\nthere is no need to call `.compile()` in that case.\n\n```python\n# DO NOT DO THIS\nmodel = MyModel()\nmodel = torch.compile(model)\nmodel(inp)\n\n# DO THIS\nmodel = MyModel()\nmodel.compile()\nmodel(inp)\n\n# this is also acceptable\n@torch.compile\ndef fn(model, inp):\n    return model(inp)\nmodel = MyModel()\nfn(model, inp)\n```",
    "1401": "一级标题：Complex Numbers\n二级标题：无\n内容：\nComplex numbers are numbers that can be expressed in the form {math}`a + bj`, where a and b are real numbers,\nand *j* is called the imaginary unit, which satisfies the equation {math}`j^2 = -1`. Complex numbers frequently occur in mathematics and\nengineering, especially in topics like signal processing. Traditionally many users and libraries (e.g., TorchAudio) have\nhandled complex numbers by representing the data in float tensors with shape {math}`(..., 2)` where the last\ndimension contains the real and imaginary values.\n\nTensors of complex dtypes provide a more natural user experience while working with complex numbers. Operations on\ncomplex tensors (e.g., {func}`torch.mv`, {func}`torch.matmul`) are likely to be faster and more memory efficient\nthan operations on float tensors mimicking them. Operations involving complex numbers in PyTorch are optimized\nto use vectorized assembly instructions and specialized kernels (e.g. LAPACK, cuBlas).\n\n```{note}\nSpectral operations in the [torch.fft module](https://pytorch.org/docs/stable/fft.html#torch-fft) support\nnative complex tensors.\n```\n\n```{warning}\nComplex tensors is a beta feature and subject to change.\n```",
    "1402": "一级标题：Complex Numbers\n二级标题：Creating Complex Tensors\n内容：\nWe support two complex dtypes: `torch.cfloat` and `torch.cdouble`\n\n```python\n>>> x = torch.randn(2,2, dtype=torch.cfloat)\n>>> x\ntensor([[-0.4621-0.0303j, -0.2438-0.5874j],\n     [ 0.7706+0.1421j,  1.2110+0.1918j]])\n```\n\n```{note}\nThe default dtype for complex tensors is determined by the default floating point dtype.\nIf the default floating point dtype is `torch.float64` then complex numbers are inferred to\nhave a dtype of `torch.complex128`, otherwise they are assumed to have a dtype of `torch.complex64`.\n```\n\nAll factory functions apart from {func}`torch.linspace`, {func}`torch.logspace`, and {func}`torch.arange` are\nsupported for complex tensors.",
    "1403": "一级标题：Complex Numbers\n二级标题：Transition from the old representation\n内容：\nUsers who currently worked around the lack of complex tensors with real tensors of shape {math}`(..., 2)`\ncan easily to switch using the complex tensors in their code using {func}`torch.view_as_complex`\nand {func}`torch.view_as_real`. Note that these functions don’t perform any copy and return a\nview of the input tensor.\n\n```python\n>>> x = torch.randn(3, 2)\n>>> x\ntensor([[ 0.6125, -0.1681],\n     [-0.3773,  1.3487],\n     [-0.0861, -0.7981]])\n>>> y = torch.view_as_complex(x)\n>>> y\ntensor([ 0.6125-0.1681j, -0.3773+1.3487j, -0.0861-0.7981j])\n>>> torch.view_as_real(y)\ntensor([[ 0.6125, -0.1681],\n     [-0.3773,  1.3487],\n     [-0.0861, -0.7981]])\n```",
    "1404": "一级标题：Complex Numbers\n二级标题：Accessing real and imag\n内容：\nThe real and imaginary values of a complex tensor can be accessed using the {attr}`real` and\n{attr}`imag`.\n\n```{note}\nAccessing `real` and `imag` attributes doesn't allocate any memory, and in-place updates on the\n`real` and `imag` tensors will update the original complex tensor. Also, the\nreturned `real` and `imag` tensors are not contiguous.\n```\n\n```python\n>>> y.real\ntensor([ 0.6125, -0.3773, -0.0861])\n>>> y.imag\ntensor([-0.1681,  1.3487, -0.7981])\n\n>>> y.real.mul_(2)\ntensor([ 1.2250, -0.7546, -0.1722])\n>>> y\ntensor([ 1.2250-0.1681j, -0.7546+1.3487j, -0.1722-0.7981j])\n>>> y.real.stride()\n(2,)\n```",
    "1405": "一级标题：Complex Numbers\n二级标题：Angle and abs\n内容：\nThe angle and absolute values of a complex tensor can be computed using {func}`torch.angle` and\n{func}`torch.abs`.\n\n```python\n>>> x1=torch.tensor([3j, 4+4j])\n>>> x1.abs()\ntensor([3.0000, 5.6569])\n>>> x1.angle()\ntensor([1.5708, 0.7854])\n```",
    "1406": "一级标题：Complex Numbers\n二级标题：Linear Algebra\n内容：\nMany linear algebra operations, like {func}`torch.matmul`, {func}`torch.linalg.svd`, {func}`torch.linalg.solve` etc., support complex numbers.\nIf you'd like to request an operation we don't currently support, please [search](https://github.com/pytorch/pytorch/issues?q=is%3Aissue+is%3Aopen+complex)\nif an issue has already been filed and if not, [file one](https://github.com/pytorch/pytorch/issues/new/choose).",
    "1407": "一级标题：Complex Numbers\n二级标题：Serialization\n内容：\nComplex tensors can be serialized, allowing data to be saved as complex values.\n\n```python\n>>> torch.save(y, 'complex_tensor.pt')\n>>> torch.load('complex_tensor.pt')\ntensor([ 0.6125-0.1681j, -0.3773+1.3487j, -0.0861-0.7981j])\n```",
    "1408": "一级标题：Complex Numbers\n二级标题：Autograd\n内容：\nPyTorch supports autograd for complex tensors. The gradient computed is the Conjugate Wirtinger derivative,\nthe negative of which is precisely the direction of steepest descent used in Gradient Descent algorithm. Thus,\nall the existing optimizers can be implemented to work out of the box with complex parameters. For more details,\ncheck out the note {ref}`complex_autograd-doc`.",
    "1409": "一级标题：Complex Numbers\n二级标题：Optimizers\n内容：\nSemantically, we define stepping through a PyTorch optimizer with complex parameters as being equivalent to stepping\nthrough the same optimizer on the {func}`torch.view_as_real` equivalent of the complex params. More concretely:\n\n```python\n>>> params = [torch.rand(2, 3, dtype=torch.complex64) for _ in range(5)]\n>>> real_params = [torch.view_as_real(p) for p in params]\n\n>>> complex_optim = torch.optim.AdamW(params)\n>>> real_optim = torch.optim.AdamW(real_params)\n```\n\n`real_optim` and `complex_optim` will compute the same updates on the parameters, though there may be slight numerical\ndiscrepancies between the two optimizers, similar to numerical discrepancies between foreach vs forloop optimizers\nand capturable vs default optimizers. For more details, see [numbercial accuracy](https://pytorch.org/docs/stable/notes/numerical_accuracy.html).\n\nSpecifically, while you can think of our optimizer's handling of complex tensors as the same as optimizing over their\n`p.real` and `p.imag` pieces separately, the implementation details are not precisely that. Note that the\n{func}`torch.view_as_real` equivalent will convert a complex tensor to a real tensor with shape {math}`(..., 2)`,\nwhereas splitting a complex tensor into two tensors is 2 tensors of size {math}`(...)`. This distinction has no impact on\npointwise optimizers (like AdamW) but will cause slight discrepancy in optimizers that do global reductions (like LBFGS).\nWe currently do not have optimizers that do per-Tensor reductions and thus do not yet define this behavior. Open an issue\nif you have a use case that requires precisely defining this behavior.\n\nWe do not fully support the following subsystems:\n\n* Quantization\n* JIT\n* Sparse Tensors\n* Distributed\n\nIf any of these would help your use case, please [search](https://github.com/pytorch/pytorch/issues?q=is%3Aissue+is%3Aopen+complex)\nif an issue has already been filed and if not, [file one](https://github.com/pytorch/pytorch/issues/new/choose).",
    "1410": "一级标题：Control Flow - Cond\n二级标题：无\n内容：\n`torch.cond` is a structured control flow operator. It can be used to specify if-else like control flow\nand can logically be seen as implemented as follows.\n\n```python\ndef cond(\n    pred: Union[bool, torch.Tensor],\n    true_fn: Callable,\n    false_fn: Callable,\n    operands: Tuple[torch.Tensor]\n):\n    if pred:\n        return true_fn(*operands)\n    else:\n        return false_fn(*operands)\n```\n\nIts unique power lies in its ability of expressing **data-dependent control flow**: it lowers to a conditional\noperator (`torch.ops.higher_order.cond`), which preserves predicate, true function and false functions.\nThis unlocks great flexibility in writing and deploying models that change model architecture based on\nthe **value** or **shape** of inputs or intermediate outputs of tensor operations.\n\n```{warning}\n`torch.cond` is a prototype feature in PyTorch. It has limited support for input and output types and\ndoesn't support training currently. Please look forward to a more stable implementation in a future version of PyTorch.\nRead more about feature classification at: https://pytorch.org/blog/pytorch-feature-classification-changes/#prototype\n```",
    "1411": "一级标题：Control Flow - Cond\n二级标题：Examples\n内容：\nBelow is an example that uses cond to branch based on input shape:\n\n```python\n    import torch\n\n    def true_fn(x: torch.Tensor):\n        return x.cos() + x.sin()\n\n    def false_fn(x: torch.Tensor):\n        return x.sin()\n\n    class DynamicShapeCondPredicate(torch.nn.Module):\n        \"\"\"\n        A basic usage of cond based on dynamic shape predicate.\n        \"\"\"\n\n        def __init__(self):\n            super().__init__()\n\n        def forward(self, x: torch.Tensor) -> torch.Tensor:\n            def true_fn(x: torch.Tensor):\n                return x.cos()\n\n            def false_fn(x: torch.Tensor):\n                return x.sin()\n\n            return torch.cond(x.shape[0] > 4, true_fn, false_fn, (x,))\n\n    dyn_shape_mod = DynamicShapeCondPredicate()\n```\n\nWe can eagerly run the model and expect the results vary based on input shape:\n\n```python\n    inp = torch.randn(3)\n    inp2 = torch.randn(5)\n    assert torch.equal(dyn_shape_mod(inp), false_fn(inp))\n    assert torch.equal(dyn_shape_mod(inp2), true_fn(inp2))\n```\n\nWe can export the model for further transformations and deployment:\n\n```python\n    inp = torch.randn(4, 3)\n    dim_batch = torch.export.Dim(\"batch\", min=2)\n    ep = torch.export.export(DynamicShapeCondPredicate(), (inp,), {}, dynamic_shapes={\"x\": {0: dim_batch}})\n    print(ep)\n```\n\nThis gives us an exported program as shown below:\n\n```\n    class GraphModule(torch.nn.Module):\n        def forward(self, arg0_1: f32[s0, 3]):\n            sym_size: Sym(s0) = torch.ops.aten.sym_size.int(arg0_1, 0)\n            gt: Sym(s0 > 4) = sym_size > 4;  sym_size = None\n            true_graph_0 = self.true_graph_0\n            false_graph_0 = self.false_graph_0\n            conditional: f32[s0, 3] = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = None\n            return (conditional,)\n\n        class <lambda>(torch.nn.Module):\n            def forward(self, arg0_1: f32[s0, 3]):\n                cos: f32[s0, 3] = torch.ops.aten.cos.default(arg0_1)\n                sin: f32[s0, 3] = torch.ops.aten.sin.default(arg0_1);  arg0_1 = None\n                add: f32[s0, 3] = torch.ops.aten.add.Tensor(cos, sin);  cos = sin = None\n                return add\n\n        class <lambda>(torch.nn.Module):\n            def forward(self, arg0_1: f32[s0, 3]):\n                sin: f32[s0, 3] = torch.ops.aten.sin.default(arg0_1);  arg0_1 = None\n                return sin\n```\n\nNotice that `torch.cond` is lowered to `torch.ops.higher_order.cond`, its predicate becomes a Symbolic expression over the shape of input,\nand branch functions becomes two sub-graph attributes of the top level graph module.\n\nHere is another example that showcases how to express a data-dependent control flow:\n\n```python\n    class DataDependentCondPredicate(torch.nn.Module):\n        \"\"\"\n        A basic usage of cond based on data dependent predicate.\n        \"\"\"\n        def __init__(self):\n            super().__init__()\n\n        def forward(self, x: torch.Tensor) -> torch.Tensor:\n            return torch.cond(x.sum() > 4.0, true_fn, false_fn, (x,))\n```\n\nThe exported program we get after export:\n\n```\n    class GraphModule(torch.nn.Module):\n        def forward(self, arg0_1: f32[s0, 3]):\n            sum_1: f32[] = torch.ops.aten.sum.default(arg0_1)\n            gt: b8[] = torch.ops.aten.gt.Scalar(sum_1, 4.0);  sum_1 = None\n\n            true_graph_0 = self.true_graph_0\n            false_graph_0 = self.false_graph_0\n            conditional: f32[s0, 3] = torch.ops.higher_order.cond(gt, true_graph_0, false_graph_0, [arg0_1]);  gt = true_graph_0 = false_graph_0 = arg0_1 = None\n            return (conditional,)\n\n        class <lambda>(torch.nn.Module):\n            def forward(self, arg0_1: f32[s0, 3]):\n                cos: f32[s0, 3] = torch.ops.aten.cos.default(arg0_1)\n                sin: f32[s0, 3] = torch.ops.aten.sin.default(arg0_1);  arg0_1 = None\n                add: f32[s0, 3] = torch.ops.aten.add.Tensor(cos, sin);  cos = sin = None\n                return add\n\n        class <lambda>(torch.nn.Module):\n            def forward(self, arg0_1: f32[s0, 3]):\n                sin: f32[s0, 3] = torch.ops.aten.sin.default(arg0_1);  arg0_1 = None\n                return sin\n```",
    "1412": "一级标题：Control Flow - Cond\n二级标题：Invariants of torch.ops.higher_order.cond\n内容：\nThere are several useful invariants for `torch.ops.higher_order.cond`:\n\n- For predicate:\n    - Dynamicness of predicate is preserved (e.g. `gt` shown in the above example)\n    - If the predicate in user-program is constant (e.g. a python bool constant), the `pred` of the operator will be a constant.\n\n- For branches:\n    - The input and output signature will be a flattened tuple.\n    - They are `torch.fx.GraphModule`.\n    - Closures in original function becomes explicit inputs. No closures.\n    - No mutations on inputs or globals are allowed.\n\n- For operands:\n    - It will also be a flat tuple.\n\n- Nesting of `torch.cond` in user program becomes nested graph modules.",
    "1413": "一级标题：Control Flow - Cond\n二级标题：API Reference\n内容：\n```{eval-rst}\n.. autofunction:: torch._higher_order_ops.cond.cond\n```",
    "1414": "一级标题：torch.__config__\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.__config__\n.. currentmodule:: torch.__config__\n```\n\n```{eval-rst}\n.. autofunction:: show\n.. autofunction:: parallel_info\n```",
    "1415": "一级标题：torch.cuda\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.cuda\n```\n\n```{eval-rst}\n.. currentmodule:: torch.cuda\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    StreamContext\n    can_device_access_peer\n    current_blas_handle\n    current_device\n    current_stream\n    cudart\n    default_stream\n    device\n    device_count\n    device_memory_used\n    device_of\n    get_arch_list\n    get_device_capability\n    get_device_name\n    get_device_properties\n    get_gencode_flags\n    get_stream_from_external\n    get_sync_debug_mode\n    init\n    ipc_collect\n    is_available\n    is_initialized\n    is_tf32_supported\n    memory_usage\n    set_device\n    set_stream\n    set_sync_debug_mode\n    stream\n    synchronize\n    utilization\n    temperature\n    power_draw\n    clock_rate\n    AcceleratorError\n    OutOfMemoryError\n```",
    "1416": "一级标题：torch.cuda\n二级标题：Random Number Generator\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    get_rng_state\n    get_rng_state_all\n    set_rng_state\n    set_rng_state_all\n    manual_seed\n    manual_seed_all\n    seed\n    seed_all\n    initial_seed\n\n```",
    "1417": "一级标题：torch.cuda\n二级标题：Communication collectives\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    comm.broadcast\n    comm.broadcast_coalesced\n    comm.reduce_add\n    comm.reduce_add_coalesced\n    comm.scatter\n    comm.gather\n```",
    "1418": "一级标题：torch.cuda\n二级标题：Streams and events\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    Stream\n    ExternalStream\n    Event\n```",
    "1419": "一级标题：torch.cuda\n二级标题：Graphs (beta)\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    is_current_stream_capturing\n    graph_pool_handle\n    CUDAGraph\n    graph\n    make_graphed_callables\n```\n\n(cuda-memory-management-api)=\n\n```{eval-rst}\n.. automodule:: torch.cuda.memory\n```\n\n```{eval-rst}\n.. currentmodule:: torch.cuda.memory\n```",
    "1420": "一级标题：torch.cuda\n二级标题：Memory management\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n     empty_cache\n     get_per_process_memory_fraction\n     list_gpu_processes\n     mem_get_info\n     memory_stats\n     memory_stats_as_nested_dict\n     reset_accumulated_memory_stats\n     host_memory_stats\n     host_memory_stats_as_nested_dict\n     reset_accumulated_host_memory_stats\n     memory_summary\n     memory_snapshot\n     memory_allocated\n     max_memory_allocated\n     reset_max_memory_allocated\n     memory_reserved\n     max_memory_reserved\n     set_per_process_memory_fraction\n     memory_cached\n     max_memory_cached\n     reset_max_memory_cached\n     reset_peak_memory_stats\n     reset_peak_host_memory_stats\n     caching_allocator_alloc\n     caching_allocator_delete\n     get_allocator_backend\n     CUDAPluggableAllocator\n     change_current_allocator\n     MemPool\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    caching_allocator_enable\n```\n\n```{eval-rst}\n.. currentmodule:: torch.cuda\n```\n\n```{eval-rst}\n.. autoclass:: torch.cuda.use_mem_pool\n```\n\n% FIXME The following doesn't seem to exist. Is it supposed to?\n% https://github.com/pytorch/pytorch/issues/27785\n% .. autofunction:: reset_max_memory_reserved",
    "1421": "一级标题：torch.cuda\n二级标题：NVIDIA Tools Extension (NVTX)\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    nvtx.mark\n    nvtx.range_push\n    nvtx.range_pop\n    nvtx.range\n```",
    "1422": "一级标题：torch.cuda\n二级标题：Jiterator (beta)\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    jiterator._create_jit_fn\n    jiterator._create_multi_output_jit_fn\n```",
    "1423": "一级标题：torch.cuda\n二级标题：TunableOp\n内容：\nSome operations could be implemented using more than one library or more than\none technique. For example, a GEMM could be implemented for CUDA or ROCm using\neither the cublas/cublasLt libraries or hipblas/hipblasLt libraries,\nrespectively. How does one know which implementation is the fastest and should\nbe chosen? That's what TunableOp provides. Certain operators have been\nimplemented using multiple strategies as Tunable Operators. At runtime, all\nstrategies are profiled and the fastest is selected for all subsequent\noperations.\n\nSee the {doc}`documentation <cuda.tunable>` for information on how to use it.\n\n```{toctree}\n:hidden: true\n\ncuda.tunable\n```",
    "1424": "一级标题：torch.cuda\n二级标题：Stream Sanitizer (prototype)\n内容：\nCUDA Sanitizer is a prototype tool for detecting synchronization errors between streams in PyTorch.\nSee the {doc}`documentation <cuda._sanitizer>` for information on how to use it.\n\n```{toctree}\n:hidden: true\n\ncuda._sanitizer\n```",
    "1425": "一级标题：torch.cuda\n二级标题：GPUDirect Storage (prototype)\n内容：\nThe APIs in `torch.cuda.gds` provide thin wrappers around certain cuFile APIs that allow\ndirect memory access transfers between GPU memory and storage, avoiding a bounce buffer in the CPU. See the\n[cufile api documentation](https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-io-api)\nfor more details.\n\nThese APIs can be used in versions greater than or equal to CUDA 12.6. In order to use these APIs, one must\nensure that their system is appropriately configured to use GPUDirect Storage per the\n[GPUDirect Storage documentation](https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/contents.html).\n\nSee the docs for {class}`~torch.cuda.gds.GdsFile` for an example of how to use these.\n\n```{eval-rst}\n.. currentmodule:: torch.cuda.gds\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    gds_register_buffer\n    gds_deregister_buffer\n    GdsFile\n\n```\n\n% This module needs to be documented. Adding here in the meantime\n\n% for tracking purposes\n\n```{eval-rst}\n.. py:module:: torch.cuda.comm\n```\n\n```{eval-rst}\n.. py:module:: torch.cuda.error\n```\n\n```{eval-rst}\n.. py:module:: torch.cuda.gds\n```\n\n```{eval-rst}\n.. py:module:: torch.cuda.graphs\n```\n\n```{eval-rst}\n.. py:module:: torch.cuda.jiterator\n```\n\n```{eval-rst}\n.. py:module:: torch.cuda.nccl\n```\n\n```{eval-rst}\n.. py:module:: torch.cuda.nvtx\n```\n\n```{eval-rst}\n.. py:module:: torch.cuda.profiler\n```\n\n```{eval-rst}\n.. py:module:: torch.cuda.random\n```\n\n```{eval-rst}\n.. py:module:: torch.cuda.sparse\n```\n\n```{eval-rst}\n.. py:module:: torch.cuda.streams\n```",
    "1426": "一级标题：TunableOp\n二级标题：无\n内容：",
    "1427": "一级标题：TunableOp\n二级标题：Overview\n内容：\n```{eval-rst}\n.. automodule:: torch.cuda.tunable\n```",
    "1428": "一级标题：TunableOp\n二级标题：API Reference\n内容：\n```{eval-rst}\n.. autofunction:: enable\n```\n\n```{eval-rst}\n.. autofunction:: is_enabled\n```\n\n```{eval-rst}\n.. autofunction:: tuning_enable\n```\n\n```{eval-rst}\n.. autofunction:: tuning_is_enabled\n```\n\n```{eval-rst}\n.. autofunction:: record_untuned_enable\n```\n\n```{eval-rst}\n.. autofunction:: record_untuned_is_enabled\n```\n\n```{eval-rst}\n.. autofunction:: set_max_tuning_duration\n```\n\n```{eval-rst}\n.. autofunction:: get_max_tuning_duration\n```\n\n```{eval-rst}\n.. autofunction:: set_max_tuning_iterations\n```\n\n```{eval-rst}\n.. autofunction:: get_max_tuning_iterations\n```\n\n```{eval-rst}\n.. autofunction:: set_filename\n```\n\n```{eval-rst}\n.. autofunction:: get_filename\n```\n\n```{eval-rst}\n.. autofunction:: get_results\n```\n\n```{eval-rst}\n.. autofunction:: get_validators\n```\n\n```{eval-rst}\n.. autofunction:: write_file_on_exit\n```\n\n```{eval-rst}\n.. autofunction:: write_file\n```\n\n```{eval-rst}\n.. autofunction:: read_file\n```\n\n```{eval-rst}\n.. autofunction:: tune_gemm_in_file\n```\n\n```{eval-rst}\n.. autofunction:: mgpu_tune_gemm_in_file\n```\n\n```{eval-rst}\n.. autofunction:: set_rotating_buffer_size\n```\n\n```{eval-rst}\n.. autofunction:: get_rotating_buffer_size\n```",
    "1429": "一级标题：torch.utils.data\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.utils.data\n```\n\nAt the heart of PyTorch data loading utility is the {class}`torch.utils.data.DataLoader`\nclass. It represents a Python iterable over a dataset, with support for\n\n- {ref}`map-style and iterable-style datasets <dataset-types>`,\n- {ref}`customizing data loading order <data-loading-order-and-sampler>`,\n- {ref}`automatic batching <loading-batched-and-non-batched-data>`,\n- {ref}`single- and multi-process data loading <single-and-multi-process-data-loading>`,\n- {ref}`automatic memory pinning <memory-pinning>`.\n\nThese options are configured by the constructor arguments of a\n{class}`~torch.utils.data.DataLoader`, which has signature:\n\n```python\nDataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n           batch_sampler=None, num_workers=0, collate_fn=None,\n           pin_memory=False, drop_last=False, timeout=0,\n           worker_init_fn=None, *, prefetch_factor=2,\n           persistent_workers=False)\n```\n\nThe sections below describe in details the effects and usages of these options.\n\n(dataset-types)=",
    "1430": "一级标题：torch.utils.data\n二级标题：Dataset Types\n内容：\nThe most important argument of {class}`~torch.utils.data.DataLoader`\nconstructor is {attr}`dataset`, which indicates a dataset object to load data\nfrom. PyTorch supports two different types of datasets:\n\n- {ref}`map-style-datasets`,\n- {ref}`iterable-style-datasets`.\n\n(map-style-datasets)=\n### Map-style datasets\n\nA map-style dataset is one that implements the {meth}`__getitem__` and\n{meth}`__len__` protocols, and represents a map from (possibly non-integral)\nindices/keys to data samples.\n\nFor example, such a dataset, when accessed with `dataset[idx]`, could read\nthe `idx`-th image and its corresponding label from a folder on the disk.\n\nSee {class}`~torch.utils.data.Dataset` for more details.\n\n(iterable-style-datasets)=\n### Iterable-style datasets\n\nAn iterable-style dataset is an instance of a subclass of {class}`~torch.utils.data.IterableDataset`\nthat implements the {meth}`__iter__` protocol, and represents an iterable over\ndata samples. This type of datasets is particularly suitable for cases where\nrandom reads are expensive or even improbable, and where the batch size depends\non the fetched data.\n\nFor example, such a dataset, when called `iter(dataset)`, could return a\nstream of data reading from a database, a remote server, or even logs generated\nin real time.\n\nSee {class}`~torch.utils.data.IterableDataset` for more details.\n\n:::{note}\nWhen using a {class}`~torch.utils.data.IterableDataset` with\n{ref}`multi-process data loading <multi-process-data-loading>`. The same\ndataset object is replicated on each worker process, and thus the\nreplicas must be configured differently to avoid duplicated data. See\n{class}`~torch.utils.data.IterableDataset` documentations for how to\nachieve this.\n:::\n\n\n(data-loading-order-and-sampler)=",
    "1431": "一级标题：torch.utils.data\n二级标题：Data Loading Order and {class}`~torch.utils.data.Sampler`\n内容：\nFor {ref}`iterable-style datasets <iterable-style-datasets>`, data loading order\nis entirely controlled by the user-defined iterable. This allows easier\nimplementations of chunk-reading and dynamic batch size (e.g., by yielding a\nbatched sample at each time).\n\nThe rest of this section concerns the case with\n{ref}`map-style datasets <map-style-datasets>`. {class}`torch.utils.data.Sampler`\nclasses are used to specify the sequence of indices/keys used in data loading.\nThey represent iterable objects over the indices to datasets. E.g., in the\ncommon case with stochastic gradient decent (SGD), a\n{class}`~torch.utils.data.Sampler` could randomly permute a list of indices\nand yield each one at a time, or yield a small number of them for mini-batch\nSGD.\n\nA sequential or shuffled sampler will be automatically constructed based on the {attr}`shuffle` argument to a {class}`~torch.utils.data.DataLoader`.\nAlternatively, users may use the {attr}`sampler` argument to specify a\ncustom {class}`~torch.utils.data.Sampler` object that at each time yields\nthe next index/key to fetch.\n\nA custom {class}`~torch.utils.data.Sampler` that yields a list of batch\nindices at a time can be passed as the {attr}`batch_sampler` argument.\nAutomatic batching can also be enabled via {attr}`batch_size` and\n{attr}`drop_last` arguments. See\n{ref}`the next section <loading-batched-and-non-batched-data>` for more details\non this.\n\n:::{note}\nNeither {attr}`sampler` nor {attr}`batch_sampler` is compatible with\niterable-style datasets, since such datasets have no notion of a key or an\nindex.\n:::\n\n(loading-batched-and-non-batched-data)=",
    "1432": "一级标题：torch.utils.data\n二级标题：Loading Batched and Non-Batched Data\n内容：\n{class}`~torch.utils.data.DataLoader` supports automatically collating\nindividual fetched data samples into batches via arguments\n{attr}`batch_size`, {attr}`drop_last`, {attr}`batch_sampler`, and\n{attr}`collate_fn` (which has a default function).\n\n(automatic-batching-default)=\n### Automatic batching (default)\n\nThis is the most common case, and corresponds to fetching a minibatch of\ndata and collating them into batched samples, i.e., containing Tensors with\none dimension being the batch dimension (usually the first).\n\nWhen {attr}`batch_size` (default `1`) is not `None`, the data loader yields\nbatched samples instead of individual samples. {attr}`batch_size` and\n{attr}`drop_last` arguments are used to specify how the data loader obtains\nbatches of dataset keys. For map-style datasets, users can alternatively\nspecify {attr}`batch_sampler`, which yields a list of keys at a time.\n\n:::{note}\nThe {attr}`batch_size` and {attr}`drop_last` arguments essentially are used\nto construct a {attr}`batch_sampler` from {attr}`sampler`. For map-style\ndatasets, the {attr}`sampler` is either provided by user or constructed\nbased on the {attr}`shuffle` argument. For iterable-style datasets, the\n{attr}`sampler` is a dummy infinite one. See\n{ref}`this section <data-loading-order-and-sampler>` on more details on\nsamplers.\n:::\n\n:::{note}\nWhen fetching from\n{ref}`iterable-style datasets <iterable-style-datasets>` with\n{ref}`multi-processing <multi-process-data-loading>` the {attr}`drop_last`\nargument drops the last non-full batch of each worker's dataset replica.\n:::\n\nAfter fetching a list of samples using the indices from sampler, the function\npassed as the {attr}`collate_fn` argument is used to collate lists of samples\ninto batches.\n\nIn this case, loading from a map-style dataset is roughly equivalent with:\n\n```python\nfor indices in batch_sampler:\n    yield collate_fn([dataset[i] for i in indices])\n```\n\nand loading from an iterable-style dataset is roughly equivalent with:\n\n```python\ndataset_iter = iter(dataset)\nfor indices in batch_sampler:\n    yield collate_fn([next(dataset_iter) for _ in indices])\n```\n\nA custom {attr}`collate_fn` can be used to customize collation, e.g., padding\nsequential data to max length of a batch. See\n{ref}`this section <dataloader-collate_fn>` on more about {attr}`collate_fn`.\n\n(disable-automatic-batching)=\n### Disable automatic batching\n\nIn certain cases, users may want to handle batching manually in dataset code,\nor simply load individual samples. For example, it could be cheaper to directly\nload batched data (e.g., bulk reads from a database or reading continuous\nchunks of memory), or the batch size is data dependent, or the program is\ndesigned to work on individual samples. Under these scenarios, it's likely\nbetter to not use automatic batching (where {attr}`collate_fn` is used to\ncollate the samples), but let the data loader directly return each member of\nthe {attr}`dataset` object.\n\nWhen both {attr}`batch_size` and {attr}`batch_sampler` are `None` (default\nvalue for {attr}`batch_sampler` is already `None`), automatic batching is\ndisabled. Each sample obtained from the {attr}`dataset` is processed with the\nfunction passed as the {attr}`collate_fn` argument.\n\n**When automatic batching is disabled**, the default {attr}`collate_fn` simply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched.\n\nIn this case, loading from a map-style dataset is roughly equivalent with:\n\n```python\nfor index in sampler:\n    yield collate_fn(dataset[index])\n```\n\nand loading from an iterable-style dataset is roughly equivalent with:\n\n```python\nfor data in iter(dataset):\n    yield collate_fn(data)\n```\n\nSee {ref}`this section <dataloader-collate_fn>` on more about {attr}`collate_fn`.\n\n(dataloader-collate_fn)=\n### Working with {attr}`collate_fn`\n\nThe use of {attr}`collate_fn` is slightly different when automatic batching is\nenabled or disabled.\n\n**When automatic batching is disabled**, {attr}`collate_fn` is called with\neach individual data sample, and the output is yielded from the data loader\niterator. In this case, the default {attr}`collate_fn` simply converts NumPy\narrays in PyTorch tensors.\n\n**When automatic batching is enabled**, {attr}`collate_fn` is called with a list\nof data samples at each time. It is expected to collate the input samples into\na batch for yielding from the data loader iterator. The rest of this section\ndescribes the behavior of the default {attr}`collate_fn`\n({func}`~torch.utils.data.default_collate`).\n\nFor instance, if each data sample consists of a 3-channel image and an integral\nclass label, i.e., each element of the dataset returns a tuple\n`(image, class_index)`, the default {attr}`collate_fn` collates a list of\nsuch tuples into a single tuple of a batched image tensor and a batched class\nlabel Tensor. In particular, the default {attr}`collate_fn` has the following\nproperties:\n\n- It always prepends a new dimension as the batch dimension.\n- It automatically converts NumPy arrays and Python numerical values into\n  PyTorch Tensors.\n- It preserves the data structure, e.g., if each sample is a dictionary, it\n  outputs a dictionary with the same set of keys but batched Tensors as values\n  (or lists if the values can not be converted into Tensors). Same\n  for `list` s, `tuple` s, `namedtuple` s, etc.\n\nUsers may use customized {attr}`collate_fn` to achieve custom batching, e.g.,\ncollating along a dimension other than the first, padding sequences of\nvarious lengths, or adding support for custom data types.\n\nIf you run into a situation where the outputs of {class}`~torch.utils.data.DataLoader`\nhave dimensions or type that is different from your expectation, you may\nwant to check your {attr}`collate_fn`.\n\n(single-and-multi-process-data-loading)=",
    "1433": "一级标题：torch.utils.data\n二级标题：Single- and Multi-process Data Loading\n内容：\nA {class}`~torch.utils.data.DataLoader` uses single-process data loading by\ndefault.\n\nWithin a Python process, the\n[Global Interpreter Lock (GIL)](https://wiki.python.org/moin/GlobalInterpreterLock)\nprevents true fully parallelizing Python code across threads. To avoid blocking\ncomputation code with data loading, PyTorch provides an easy switch to perform\nmulti-process data loading by simply setting the argument {attr}`num_workers`\nto a positive integer.\n\n(single-process-data-loading-default)=\n### Single-process data loading (default)\n\nIn this mode, data fetching is done in the same process a\n{class}`~torch.utils.data.DataLoader` is initialized. Therefore, data loading\nmay block computing. However, this mode may be preferred when resource(s) used\nfor sharing data among processes (e.g., shared memory, file descriptors) is\nlimited, or when the entire dataset is small and can be loaded entirely in\nmemory. Additionally, single-process loading often shows more readable error\ntraces and thus is useful for debugging.\n\n(multi-process-data-loading)=\n### Multi-process data loading\n\nSetting the argument {attr}`num_workers` as a positive integer will\nturn on multi-process data loading with the specified number of loader worker\nprocesses.\n\n:::{warning}\nAfter several iterations, the loader worker processes will consume\nthe same amount of CPU memory as the parent process for all Python\nobjects in the parent process which are accessed from the worker\nprocesses. This can be problematic if the Dataset contains a lot of\ndata (e.g., you are loading a very large list of filenames at Dataset\nconstruction time) and/or you are using a lot of workers (overall\nmemory usage is `number of workers * size of parent process`). The\nsimplest workaround is to replace Python objects with non-refcounted\nrepresentations such as Pandas, Numpy or PyArrow objects. Check out\n[issue #13246](https://github.com/pytorch/pytorch/issues/13246#issuecomment-905703662)\nfor more details on why this occurs and example code for how to\nworkaround these problems.\n:::\n\nIn this mode, each time an iterator of a {class}`~torch.utils.data.DataLoader`\nis created (e.g., when you call `enumerate(dataloader)`), {attr}`num_workers`\nworker processes are created. At this point, the {attr}`dataset`,\n{attr}`collate_fn`, and {attr}`worker_init_fn` are passed to each\nworker, where they are used to initialize, and fetch data. This means that\ndataset access together with its internal IO, transforms\n(including {attr}`collate_fn`) runs in the worker process.\n\n{func}`torch.utils.data.get_worker_info()` returns various useful information\nin a worker process (including the worker id, dataset replica, initial seed,\netc.), and returns `None` in main process. Users may use this function in\ndataset code and/or {attr}`worker_init_fn` to individually configure each\ndataset replica, and to determine whether the code is running in a worker\nprocess. For example, this can be particularly helpful in sharding the dataset.\n\nFor map-style datasets, the main process generates the indices using\n{attr}`sampler` and sends them to the workers. So any shuffle randomization is\ndone in the main process which guides loading by assigning indices to load.\n\nFor iterable-style datasets, since each worker process gets a replica of the\n{attr}`dataset` object, naive multi-process loading will often result in\nduplicated data. Using {func}`torch.utils.data.get_worker_info()` and/or\n{attr}`worker_init_fn`, users may configure each replica independently. (See\n{class}`~torch.utils.data.IterableDataset` documentations for how to achieve\nthis. ) For similar reasons, in multi-process loading, the {attr}`drop_last`\nargument drops the last non-full batch of each worker's iterable-style dataset\nreplica.\n\nWorkers are shut down once the end of the iteration is reached, or when the\niterator becomes garbage collected.\n\n:::{warning}\nIt is generally not recommended to return CUDA tensors in multi-process\nloading because of many subtleties in using CUDA and sharing CUDA tensors in\nmultiprocessing (see {ref}`multiprocessing-cuda-note`). Instead, we recommend\nusing {ref}`automatic memory pinning <memory-pinning>` (i.e., setting\n{attr}`pin_memory=True`), which enables fast data transfer to CUDA-enabled\nGPUs.\n:::\n\n(platform-specific-behaviors)=\n#### Platform-specific behaviors\n\nSince workers rely on Python {py:mod}`multiprocessing`, worker launch behavior is\ndifferent on Windows compared to Unix.\n\n- On Unix, {func}`fork()` is the default {py:mod}`multiprocessing` start method.\n  Using {func}`fork`, child workers typically can access the {attr}`dataset` and\n  Python argument functions directly through the cloned address space.\n- On Windows or MacOS, {func}`spawn()` is the default {py:mod}`multiprocessing` start method.\n  Using {func}`spawn()`, another interpreter is launched which runs your main script,\n  followed by the internal worker function that receives the {attr}`dataset`,\n  {attr}`collate_fn` and other arguments through {py:mod}`pickle` serialization.\n\nThis separate serialization means that you should take two steps to ensure you\nare compatible with Windows while using multi-process data loading:\n\n- Wrap most of you main script's code within `if __name__ == '__main__':` block,\n  to make sure it doesn't run again (most likely generating error) when each worker\n  process is launched. You can place your dataset and {class}`~torch.utils.data.DataLoader`\n  instance creation logic here, as it doesn't need to be re-executed in workers.\n- Make sure that any custom {attr}`collate_fn`, {attr}`worker_init_fn`\n  or {attr}`dataset` code is declared as top level definitions, outside of the\n  `__main__` check. This ensures that they are available in worker processes.\n  (this is needed since functions are pickled as references only, not `bytecode`.)\n\n\n(data-loading-randomness)=\n#### Randomness in multi-process data loading\n\nBy default, each worker will have its PyTorch seed set to `base_seed + worker_id`,\nwhere `base_seed` is a long generated by main process using its RNG (thereby,\nconsuming a RNG state mandatorily) or a specified {attr}`generator`. However, seeds for other\nlibraries may be duplicated upon initializing workers, causing each worker to return\nidentical random numbers. (See {ref}`this section <dataloader-workers-random-seed>` in FAQ.).\n\nIn {attr}`worker_init_fn`, you may access the PyTorch seed set for each worker\nwith either {func}`torch.utils.data.get_worker_info().seed <torch.utils.data.get_worker_info>`\nor {func}`torch.initial_seed()`, and use it to seed other libraries before data\nloading.\n\n\n(memory-pinning)=",
    "1434": "一级标题：torch.utils.data\n二级标题：Memory Pinning\n内容：\nHost to GPU copies are much faster when they originate from pinned (page-locked)\nmemory. See {ref}`cuda-memory-pinning` for more details on when and how to use\npinned memory generally.\n\nFor data loading, passing {attr}`pin_memory=True` to a\n{class}`~torch.utils.data.DataLoader` will automatically put the fetched data\nTensors in pinned memory, and thus enables faster data transfer to CUDA-enabled\nGPUs.\n\nThe default memory pinning logic only recognizes Tensors and maps and iterables\ncontaining Tensors. By default, if the pinning logic sees a batch that is a\ncustom type (which will occur if you have a {attr}`collate_fn` that returns a\ncustom batch type), or if each element of your batch is a custom type, the\npinning logic will not recognize them, and it will return that batch (or those\nelements) without pinning the memory. To enable memory pinning for custom\nbatch or data type(s), define a {meth}`pin_memory` method on your custom\ntype(s).\n\nSee the example below.\n\nExample:\n\n```python\nclass SimpleCustomBatch:\n    def __init__(self, data):\n        transposed_data = list(zip(*data))\n        self.inp = torch.stack(transposed_data[0], 0)\n        self.tgt = torch.stack(transposed_data[1], 0)\n\n    # custom memory pinning method on custom type\n    def pin_memory(self):\n        self.inp = self.inp.pin_memory()\n        self.tgt = self.tgt.pin_memory()\n        return self\n\ndef collate_wrapper(batch):\n    return SimpleCustomBatch(batch)\n\ninps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\ntgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\ndataset = TensorDataset(inps, tgts)\n\nloader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,\n                    pin_memory=True)\n\nfor batch_ndx, sample in enumerate(loader):\n    print(sample.inp.is_pinned())\n    print(sample.tgt.is_pinned())\n```\n\n```{eval-rst}\n.. autoclass:: DataLoader\n```\n\n```{eval-rst}\n.. autoclass:: Dataset\n```\n\n```{eval-rst}\n.. autoclass:: IterableDataset\n```\n\n```{eval-rst}\n.. autoclass:: TensorDataset\n```\n\n```{eval-rst}\n.. autoclass:: StackDataset\n```\n\n```{eval-rst}\n.. autoclass:: ConcatDataset\n```\n\n```{eval-rst}\n.. autoclass:: ChainDataset\n```\n\n```{eval-rst}\n.. autoclass:: Subset\n```\n\n```{eval-rst}\n.. autofunction:: torch.utils.data._utils.collate.collate\n```\n\n```{eval-rst}\n.. autofunction:: torch.utils.data.default_collate\n```\n\n```{eval-rst}\n.. autofunction:: torch.utils.data.default_convert\n```\n\n```{eval-rst}\n.. autofunction:: torch.utils.data.get_worker_info\n```\n\n```{eval-rst}\n.. autofunction:: torch.utils.data.random_split\n```\n\n```{eval-rst}\n.. autoclass:: torch.utils.data.Sampler\n```\n\n```{eval-rst}\n.. autoclass:: torch.utils.data.SequentialSampler\n```\n\n```{eval-rst}\n.. autoclass:: torch.utils.data.RandomSampler\n```\n\n```{eval-rst}\n.. autoclass:: torch.utils.data.SubsetRandomSampler\n```\n\n```{eval-rst}\n.. autoclass:: torch.utils.data.WeightedRandomSampler\n```\n\n```{eval-rst}\n.. autoclass:: torch.utils.data.BatchSampler\n```\n\n```{eval-rst}\n.. autoclass:: torch.utils.data.distributed.DistributedSampler\n\n```\n\n% These modules are documented as part of torch/data listing them here for\n\n% now until we have a clearer fix\n\n```{eval-rst}\n.. py:module:: torch.utils.data.datapipes\n```\n\n```{eval-rst}\n.. py:module:: torch.utils.data.datapipes.dataframe\n```\n\n```{eval-rst}\n.. py:module:: torch.utils.data.datapipes.iter\n```\n\n```{eval-rst}\n.. py:module:: torch.utils.data.datapipes.map\n```\n\n```{eval-rst}\n.. py:module:: torch.utils.data.datapipes.utils\n```",
    "1435": "一级标题：DDP Communication Hooks\n二级标题：无\n内容：\nDDP communication hook is a generic interface to control how to communicate\ngradients across workers by overriding the vanilla allreduce in\n[DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.).\nA few built-in communication hooks are provided,\nand users can easily apply any of these hooks to optimize communication.\nBesides, the hook interface can also support user-defined communication\nstrategies for more advanced use cases.",
    "1436": "一级标题：DDP Communication Hooks\n二级标题：How to Use a Communication Hook?\n内容：\nTo use a communication hook, the user just needs to let the DDP model register\nthe hook before the training loop as below.\n\n{func}`torch.nn.parallel.DistributedDataParallel.register_comm_hook`",
    "1437": "一级标题：DDP Communication Hooks\n二级标题：What Does a Communication Hook Operate On?\n内容：\nA communication hook provides a flexible way to allreduce gradients.\nTherefore, it mainly operates on the gradients on each replica before allreduce,\nwhich are bucketized to increase the overlap between communication and computation.\nParticularly, {class}`torch.distributed.GradBucket` represents a bucket of gradient tensors to be allreduced.\n\n```{eval-rst}\n.. autoclass:: torch.distributed.GradBucket\n\n.. autofunction:: torch.distributed.GradBucket.index\n.. autofunction:: torch.distributed.GradBucket.buffer\n.. autofunction:: torch.distributed.GradBucket.gradients\n.. autofunction:: torch.distributed.GradBucket.is_last\n.. autofunction:: torch.distributed.GradBucket.set_buffer\n.. autofunction:: torch.distributed.GradBucket.parameters\n```",
    "1438": "一级标题：DDP Communication Hooks\n二级标题：Default Communication Hooks\n内容：\nDefault communication hooks are simple **stateless** hooks, so the input state\nin `register_comm_hook` is either a process group or `None`.\nThe input `bucket` is a {class}`torch.distributed.GradBucket` object.\n\n```{eval-rst}\n.. currentmodule:: torch.distributed.algorithms.ddp_comm_hooks.default_hooks\n.. autofunction:: allreduce_hook\n.. autofunction:: fp16_compress_hook\n.. autofunction:: bf16_compress_hook\n```\n\nAdditionally, a communication hook wrapper is provided to support {meth}`~fp16_compress_hook` or {meth}`~bf16_compress_hook` as a wrapper,\nwhich can be combined with other communication hooks.\n\n```{eval-rst}\n.. autofunction:: fp16_compress_wrapper\n.. autofunction:: bf16_compress_wrapper\n```",
    "1439": "一级标题：DDP Communication Hooks\n二级标题：PowerSGD Communication Hook\n内容：\nPowerSGD ([Vogels et al., NeurIPS 2019](https://arxiv.org/abs/1905.13727))\nis a gradient compression algorithm, which can provide very high compression\nrates and accelerate bandwidth-bound distributed training.\nThis algorithm needs to maintain both some hyperparameters and the internal\nstate. Therefore, PowerSGD communication hook is a **stateful** hook,\nand the user needs to provide a state object defined as below.\n\n### PowerSGD State\n\n```{eval-rst}\n.. currentmodule:: torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook\n.. autoclass:: PowerSGDState\n```\n### PowerSGD Hooks\n\n```{warning}\nPowerSGD typically requires extra memory of the same size as the model's\ngradients to enable error feedback, which can compensate for biased\ncompressed communication and improve accuracy.\n```\n\n```{warning}\nPowerSGD hooks may conflict with [Apex automatic mixed precision package](https://github.com/NVIDIA/apex).\nPlease use PyTorch [native automatic mixed precision package](https://pytorch.org/docs/stable/amp.html)\ninstead.\n```\n\n```{eval-rst}\n.. autofunction:: powerSGD_hook\n.. autofunction:: batched_powerSGD_hook\n```",
    "1440": "一级标题：DDP Communication Hooks\n二级标题：Debugging Communication Hooks\n内容：\nAs the name implies, debugging communication hooks are **only** used for debugging and performance optimization purpose.\n```{eval-rst}\n.. currentmodule:: torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks\n```\n```{warning}\nDebugging communication hooks do not necessarily output the correct results.\n```\n```{eval-rst}\n.. autofunction:: noop_hook\n```",
    "1441": "一级标题：DDP Communication Hooks\n二级标题：Checkpointing of Communication Hooks\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook\n```\nA stateful communication hook can be saved as a part of model checkpointing to enable trainer restarts.\nTo make a hook serializable, ``__setstate__`` and ``__getstate__`` should be defined.\n\n```{warning}\n`__getstate__` should exclude non-serializable attributes from a returned dictionary.\n```\n```{warning}\n`__setstate__` should properly initialize non-serializable attributes, excluded from a provided `state`.\n```\n{class}`PowerSGDState` has `__setstate__` and `__getstate__` implemented and can be used as a reference.\n\n```{eval-rst}\n.. class:: PowerSGDState\n    :noindex:\n\n    .. automethod:: PowerSGDState.__getstate__\n    .. automethod:: PowerSGDState.__setstate__\n```\nHere is a simple, end-to-end example of saving and reloading PowerSGD state and hook.\n\n```python\n\nimport os\nimport sys\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch.distributed.algorithms.ddp_comm_hooks import powerSGD_hook as powerSGD\n\nclass SimpleModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(24,24)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(24,12)\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()\n\ndef run_demo(demo_fn, world_size):\n    mp.spawn(\n        demo_fn,\n        args=(world_size,),\n        nprocs=world_size,\n        join=True)\n\ndef demo_serialization(rank, world_size):\n    setup(rank, world_size)\n\n    CHECKPOINT = tempfile.gettempdir() + \"/checkpoint.pt\"\n\n    model = SimpleModel().to(rank)\n    ddp_model = DistributedDataParallel(model, device_ids=[rank])\n\n    powersgd_hook = powerSGD.powerSGD_hook\n    powersgd_state = powerSGD.PowerSGDState(process_group=None)\n\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n    ddp_model.register_comm_hook(powersgd_state, powersgd_hook)\n\n    state = {\n        'state_dict': ddp_model.state_dict(),\n        'comm_hook': powersgd_hook,\n        'comm_hook_state': powersgd_state}\n\n    if rank == 0:\n        torch.save(state, CHECKPOINT)\n\n    dist.barrier()\n    map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\n    checkpoint = torch.load(CHECKPOINT, map_location=map_location)\n\n    new_ddp_model = DistributedDataParallel(SimpleModel().to(rank), device_ids=[rank])\n    new_ddp_model.load_state_dict(checkpoint['state_dict'])\n    powersgd_hook = checkpoint['comm_hook']\n    powersgd_state = checkpoint['comm_hook_state']\n\n    new_ddp_model.register_comm_hook(powersgd_state, powersgd_hook)\n\n    if rank == 0:\n        os.remove(CHECKPOINT)\n\n    cleanup()\n\nif __name__ == \"__main__\":\n    n_gpus = torch.cuda.device_count()\n    assert n_gpus >= 2, f\"Requires at least 2 GPUs to run, but got {n_gpus}\"\n    world_size = n_gpus\n    run_demo(demo_serialization, world_size)\n```",
    "1442": "一级标题：DDP Communication Hooks\n二级标题：Acknowledgements\n内容：\nMany thanks to PowerSGD paper author **Thijs Vogels** for the code review on\nPowerSGD communication hook, as well as the\n[comparison experiments](https://observablehq.com/@tvogels/powersgd-benchmark),\nwhich show that the performance of PowerSGD communication hook is on par with\nthe implementation in the original [paper](https://arxiv.org/abs/1905.13727).",
    "1443": "一级标题：Debugging Environment Variables\n二级标题：无\n内容：\n:::{list-table}\n  :header-rows: 1\n\n  * - Variable\n    - Description\n  * - ``TORCH_SHOW_CPP_STACKTRACES``\n    - If set to ``1``, makes PyTorch print out a stack trace when it detects a C++ error.\n  * - ``TORCH_CPP_LOG_LEVEL``\n    - Set the log level of c10 logging facility (supports both GLOG and c10 loggers). Valid values are ``INFO``, ``WARNING``, ``ERROR``, and ``FATAL`` or their numerical equivalents ``0``, ``1``, ``2``, and ``3``.\n  * - ``TORCH_LOGS``\n    -  For a more in depth explanation of this environment variable, see {doc}`/logging`.",
    "1444": "一级标题：torch.utils.deterministic\n二级标题：无\n内容：\n```{eval-rst}\n.. py:module:: torch.utils.deterministic\n.. currentmodule:: torch.utils.deterministic\n\n.. attribute:: fill_uninitialized_memory\n\n    A :class:`bool` that, if True, causes uninitialized memory to be filled with\n    a known value when :meth:`torch.use_deterministic_algorithms()` is set to\n    ``True``. Floating point and complex values are set to NaN, and integer\n    values are set to the maximum value.\n\n    Default: ``True``\n\n    Filling uninitialized memory is detrimental to performance. So if your\n    program is valid and does not use uninitialized memory as the input to an\n    operation, then this setting can be turned off for better performance and\n    still be deterministic.\n\n    The following operations will fill uninitialized memory when this setting is\n    turned on:\n\n        * :func:`torch.Tensor.resize_` when called with a tensor that is not\n          quantized\n        * :func:`torch.empty`\n        * :func:`torch.empty_strided`\n        * :func:`torch.empty_permuted`\n        * :func:`torch.empty_like`\n```",
    "1445": "一级标题：Experimental Object Oriented Distributed API\n二级标题：无\n内容：\n```{eval-rst}\n.. role:: hidden\n    :class: hidden-section\n```\n\n```{eval-rst}\n.. automodule:: torch.distributed._dist2\n    :members:\n    :undoc-members:\n    :show-inheritance:\n    :exclude-members: ReduceOp\n```",
    "1446": "一级标题：Generic Join Context Manager\n二级标题：无\n内容：\nThe generic join context manager facilitates distributed training on uneven\ninputs. This page outlines the API of the relevant classes: {class}`Join`,\n{class}`Joinable`, and {class}`JoinHook`. For a tutorial, see\n[Distributed Training with Uneven Inputs Using the Join Context Manager](https://pytorch.org/tutorials/advanced/generic_join.html).\n\n```{eval-rst}\n.. autoclass:: torch.distributed.algorithms.Join\n    :members:\n\n.. autoclass:: torch.distributed.algorithms.Joinable\n    :members:\n\n.. autoclass:: torch.distributed.algorithms.JoinHook\n    :members:\n\n```",
    "1447": "一级标题：Distributed Checkpoint - torch.distributed.checkpoint\n二级标题：无\n内容：\nDistributed Checkpoint (DCP) support loading and saving models from multiple ranks in parallel.\nIt handles load-time resharding which enables saving in one cluster topology and loading into another.\n\nDCP is different than `torch.save` and `torch.load` in a few significant ways:\n\n- It produces multiple files per checkpoint, with at least one per rank.\n- It operates in place, meaning that the model should allocate its data first and DCP uses that storage instead.\n\nThe entrypoints to load and save a checkpoint are the following:",
    "1448": "一级标题：Distributed Checkpoint - torch.distributed.checkpoint\n二级标题：Additional resources:\n内容：\n- [Getting Started with Distributed Checkpoint (DCP)](https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html)\n- [Asynchronous Saving with Distributed Checkpoint (DCP)](https://pytorch.org/tutorials/recipes/distributed_async_checkpoint_recipe.html)\n- [TorchTitan Checkpointing Docs](https://github.com/pytorch/torchtitan/blob/main/docs/checkpoint.md)\n- [TorchTitan DCP Implementation](https://github.com/pytorch/torchtitan/blob/main/torchtitan/components/checkpoint.py)\n\n```{eval-rst}\n.. automodule:: torch.distributed.checkpoint\n```\n\n```{eval-rst}\n.. currentmodule:: torch.distributed.checkpoint.state_dict_saver\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.checkpoint.state_dict_saver.AsyncCheckpointerType\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.checkpoint.state_dict_saver.AsyncSaveResponse\n  :members:\n```\n\n```{eval-rst}\n.. autofunction::  save\n```\n\n```{eval-rst}\n.. autofunction::  async_save\n```\n\n```{eval-rst}\n.. autofunction::  save_state_dict\n```\n\n```{eval-rst}\n.. currentmodule:: torch.distributed.checkpoint.state_dict_loader\n```\n\n```{eval-rst}\n.. autofunction::  load\n```\n\n```{eval-rst}\n.. autofunction::  load_state_dict\n```\n\nThe following module is also useful for additional customization of the staging mechanisms used for asynchronous checkpointing (`torch.distributed.checkpoint.async_save`):\n\n```{eval-rst}\n.. automodule:: torch.distributed.checkpoint.staging\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.checkpoint.staging.AsyncStager\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.checkpoint.staging.DefaultStager\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.checkpoint.staging.StagingOptions\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.checkpoint.staging.BlockingAsyncStager\n  :members:\n```\n\nIn addition to the above entrypoints, `Stateful` objects, as described below, provide additional customization during saving/loading\n\n```{eval-rst}\n.. automodule:: torch.distributed.checkpoint.stateful\n   :noindex:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.checkpoint.stateful.Stateful\n  :members:\n```\n\nThis [example](https://github.com/pytorch/pytorch/blob/main/torch/distributed/checkpoint/examples/fsdp_checkpoint_example.py) shows how to use Pytorch Distributed Checkpoint to save a FSDP model.\n\nThe following types define the IO interface used during checkpoint:\n\n```{eval-rst}\n.. autoclass:: torch.distributed.checkpoint.StorageReader\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.checkpoint.StorageWriter\n  :members:\n```\n\nThe following types define the planner interface used during checkpoint:\n\n```{eval-rst}\n.. autoclass:: torch.distributed.checkpoint.LoadPlanner\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.checkpoint.LoadPlan\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.checkpoint.ReadItem\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.checkpoint.SavePlanner\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.checkpoint.SavePlan\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.checkpoint.planner.WriteItem\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.checkpoint.planner.BytesIOWriteData\n  :members:\n```\n\nWe provide a filesystem based storage layer:\n\n```{eval-rst}\n.. autoclass:: torch.distributed.checkpoint.FileSystemReader\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.checkpoint.FileSystemWriter\n  :members:\n```\n\nWe also provide other storage layers, including ones to interact with HuggingFace safetensors:\n\n.. autoclass:: torch.distributed.checkpoint.HuggingFaceStorageReader\n  :members:\n\n.. autoclass:: torch.distributed.checkpoint.HuggingFaceStorageWriter\n  :members:\n\nWe provide default implementations of `LoadPlanner` and `SavePlanner` that\ncan handle all of torch.distributed constructs such as FSDP, DDP, ShardedTensor and DistributedTensor.\n\n```{eval-rst}\n.. autoclass:: torch.distributed.checkpoint.DefaultSavePlanner\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.checkpoint.DefaultLoadPlanner\n  :members:\n\n```\n\nDue to legacy design decisions, the state dictionaries of `FSDP` and `DDP` may have different keys or fully qualified names (e.g., layer1.weight) even when the original unparallelized model is identical. Moreover, `FSDP` offers various types of model state dictionaries, such as full and sharded state dictionaries. Additionally, optimizer state dictionaries employ parameter IDs instead of fully qualified names to identify parameters, potentially causing issues when parallelisms are used (e.g., pipeline parallelism).\n\nTo tackle these challenges, we offer a collection of APIs for users to easily manage state_dicts. `get_model_state_dict()` returns a model state dictionary with keys consistent with those returned by the unparallelized model state dictionary. Similarly, `get_optimizer_state_dict()` provides the optimizer state dictionary with keys uniform across all parallelisms applied. To achieve this consistency, `get_optimizer_state_dict()` converts parameter IDs to fully qualified names identical to those found in the unparallelized model state dictionary.\n\nNote that results returned by these APIs can be used directly with the `torch.distributed.checkpoint.save()` and `torch.distributed.checkpoint.load()` methods without requiring any additional conversions.\n\n`set_model_state_dict()` and `set_optimizer_state_dict()` are provided to load the model and optimizer state_dict generated by by their respective getter APIs.\n\nNote that `set_optimizer_state_dict()` can only be called before `backward()` or after `step()` is called on optimizers.\n\nNote that this feature is experimental, and API signatures might change in the future.\n\n```{eval-rst}\n.. autofunction:: torch.distributed.checkpoint.state_dict.get_state_dict\n```\n\n```{eval-rst}\n.. autofunction:: torch.distributed.checkpoint.state_dict.get_model_state_dict\n```\n\n```{eval-rst}\n.. autofunction:: torch.distributed.checkpoint.state_dict.get_optimizer_state_dict\n```\n\n```{eval-rst}\n.. autofunction:: torch.distributed.checkpoint.state_dict.set_state_dict\n```\n\n```{eval-rst}\n.. autofunction:: torch.distributed.checkpoint.state_dict.set_model_state_dict\n```\n\n```{eval-rst}\n.. autofunction:: torch.distributed.checkpoint.state_dict.set_optimizer_state_dict\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.checkpoint.state_dict.StateDictOptions\n   :members:\n```\n\nFor users which are used to using and sharing models in the `torch.save` format, the following methods are provided which provide offline utilities for converting betweeing formats.\n\n```{eval-rst}\n.. automodule:: torch.distributed.checkpoint.format_utils\n```\n\n```{eval-rst}\n.. currentmodule:: torch.distributed.checkpoint.format_utils\n```\n\n```{eval-rst}\n.. autofunction:: dcp_to_torch_save\n```\n\n```{eval-rst}\n.. autofunction:: torch_save_to_dcp\n```\n\nThe following classes can also be utilized for online loading and resharding of models from the torch.save format.\n\n```{eval-rst}\n.. autoclass:: torch.distributed.checkpoint.format_utils.BroadcastingTorchSaveReader\n   :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.checkpoint.format_utils.DynamicMetaLoadPlanner\n   :members:\n```\n\nThe following experimental interfaces are provided for improved observability in production environments:\n\n```{eval-rst}\n.. py:module:: torch.distributed.checkpoint.logger\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.checkpoint.logging_handlers\n```",
    "1449": "一级标题：Torch Distributed Elastic\n二级标题：无\n内容：\nMakes distributed PyTorch fault-tolerant and elastic.",
    "1450": "一级标题：Torch Distributed Elastic\n二级标题：Get Started\n内容：\n```{toctree}\n:caption: Usage\n:maxdepth: 1\n\nelastic/quickstart\nelastic/train_script\nelastic/examples\n```",
    "1451": "一级标题：Torch Distributed Elastic\n二级标题：Documentation\n内容：\n```{toctree}\n:caption: API\n:maxdepth: 1\n\nelastic/run\nelastic/agent\nelastic/multiprocessing\nelastic/errors\nelastic/rendezvous\nelastic/timer\nelastic/metrics\nelastic/events\nelastic/subprocess_handler\nelastic/control_plane\nelastic/numa\n```\n\n```{toctree}\n:caption: Advanced\n:maxdepth: 1\n\nelastic/customization\n```\n\n```{toctree}\n:caption: Plugins\n:maxdepth: 1\n\nelastic/kubernetes\n```",
    "1452": "一级标题：torch.distributed.fsdp.fully_shard\n二级标题：无\n内容：",
    "1453": "一级标题：torch.distributed.fsdp.fully_shard\n二级标题：PyTorch FSDP2 (`fully_shard`)\n内容：\nPyTorch FSDP2 ([RFC](<https://github.com/pytorch/pytorch/issues/114299>)) provides\na fully sharded data parallelism (FSDP) implementation targeting performant\neager-mode while using per-parameter sharding for improved usability\n\n- See the [Getting Started with FSDP2](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html)\n  tutorial for more information.\n\n- If you are currently using FSDP1, consider migrating to FSDP2 using our\n  [migration guide](https://docs.pytorch.org/tutorials/intermediate/FSDP_tutorial.html#fsdp1-to-fsdp2-migration-guide).\n\n\nThe user contract for ``fully_shard(model)`` is as follows\n\n- For model initialization, fully_shard converts model.parameters() from\n  plain torch.Tensor to DTensor in-place. The parameters are moved to the\n  appropriate device according to the device mesh.\n\n- Before forward and backward passes, pre-forward/backward hooks are\n  responsible for all-gathering the parameters and converting model.parameters()\n  from DTensor to plain torch.Tensor.\n\n- After forward and backward passes, post-forward/backward hooks free\n  the unsharded parameters (no communication needed) and convert\n  model.parameters() from plain torch.Tensor back to DTensor.\n\n- For the optimizer, it must be initialized with the DTensor model.parameters(),\n  and the optimizer step should be performed on DTensor parameters.\n\n- Call ``model(input)`` instead of ``model.forward(input)`` to trigger pre-forward\n  hooks to all-gather parameters. To make model.forward(input) work, users must\n  either call ``model.unshard()`` explicitly or use ``register_fsdp_forward_method(model, \"forward\")``\n  to register the forward method for hooking.\n\n- fully_shard groups parameters together for a single all-gather. User should apply\n  fully_shard in a bottom-up manner. For example, in a Transformer model, fully_shard\n  should be applied to each layer before applying it to the root model. When applied\n  to the root model, fully_shard excludes model.parameters() from each layer and groups\n  the remaining parameters (e.g., embeddings, output projection) into a single\n  all-gather group.\n\n- ``type(model)`` is \"unioned\" with ``FSDPModule`` in-place. For example, if model\n  is originally of type nn.Linear, then fully_shard changes ``type(model)`` from\n  nn.Linear to ``FSDPLinear`` in-place. ``FSDPLinear`` is an instance of both\n  nn.Linear and ``FSDPModule``. It retains all methods of nn.Linear while also\n  exposing FSDP2-specific APIs under FSDPModule, such as ``reshard()`` and\n  ``unshard()``.\n\n- Fully Qualified Names (FQNs) for parameters remain unchanged. If we call\n  ``model.state_dict()``, the FQNs are the same before and after applying\n  fully_shard. This is because fully_shard does not wrap the module but only\n  registers hooks to the original module.\n\n\nCompared to PyTorch FSDP1 (`FullyShardedDataParallel`):\n\n- FSDP2 uses `DTensor`-based dim-0 per-parameter sharding for a simpler\n  sharding representation compared to FSDP1's flat-parameter sharding, while\n  preserving similar throughput performance. More specifically, FSDP2 chunks\n  each parameter on dim-0 across the data parallel workers (using\n  `torch.chunk(dim=0)`), whereas FSDP1 flattens, concatenates, and chunks a\n  group of tensors together, making reasoning about what data is present on\n  each worker and resharding to different parallelisms complex. Per-parameter\n  sharding provides a more intuitive user experience, relaxes constraints\n  around frozen parameters, and allows for communication-free (sharded) state\n  dicts, which otherwise require all-gathers in FSDP1.\n- FSDP2 implements a different memory management approach to handle the\n  multi-stream usages that avoids `torch.Tensor.record_stream`. This ensures\n  deterministic and expected memory usage and does not require blocking the CPU\n  like in FSDP1's `limit_all_gathers=True`.\n- FSDP2 exposes APIs for manual control over prefetching and collective\n  scheduling, allowing power users more customization. See the methods on\n  `FSDPModule` below for details.\n- FSDP2 simplifies some of the API surface: e.g. FSDP2 does not directly\n  support full state dicts. Instead, users can reshard the sharded state dicts\n  containing `DTensor` s to full state dicts themselves using `DTensor`\n  APIs like `DTensor.full_tensor()` or by using higher-level APIs like\n  [PyTorch Distributed Checkpoint](https://pytorch.org/docs/stable/distributed.checkpoint.html) 's\n  distributed state dict APIs. Also, some other args have been removed; see\n  [here](https://github.com/pytorch/torchtitan/blob/main/docs/fsdp.md) for\n  details.\n\n\n```{eval-rst}\n.. currentmodule:: torch.distributed.fsdp\n```\n\nThe frontend API is `fully_shard` that can be called on a `module`:\n\n```{eval-rst}\n.. autofunction:: fully_shard\n```\n\n```{eval-rst}\n.. autoclass:: FSDPModule\n    :members:\n    :member-order: bysource\n```\n\n```{eval-rst}\n.. autoclass:: UnshardHandle\n    :members:\n```\n\n```{eval-rst}\n.. autofunction:: register_fsdp_forward_method\n```\n\n```{eval-rst}\n.. autoclass:: MixedPrecisionPolicy\n    :members:\n```\n\n```{eval-rst}\n.. autoclass:: OffloadPolicy\n    :members:\n```\n\n```{eval-rst}\n.. autoclass:: CPUOffloadPolicy\n    :members:\n```",
    "1454": "一级标题：Distributed communication package - torch.distributed\n二级标题：无\n内容：\n:::{note}\nPlease refer to [PyTorch Distributed Overview](https://pytorch.org/tutorials/beginner/dist_overview.html)\nfor a brief introduction to all features related to distributed training.\n:::\n\n```{eval-rst}\n.. automodule:: torch.distributed\n```\n\n```{eval-rst}\n.. currentmodule:: torch.distributed\n```",
    "1455": "一级标题：Distributed communication package - torch.distributed\n二级标题：Backends\n内容：\n`torch.distributed` supports four built-in backends, each with\ndifferent capabilities. The table below shows which functions are available\nfor use with a CPU or GPU for each backend. For NCCL, GPU refers to CUDA GPU\nwhile for XCCL to XPU GPU.\n\nMPI supports CUDA only if the implementation used to build PyTorch supports it.\n\n```{eval-rst}\n+----------------+-----------+-----------+-----------+-----------+\n| Backend        | ``gloo``  | ``mpi``   | ``nccl``  | ``xccl``  |\n+----------------+-----+-----+-----+-----+-----+-----+-----+-----+\n| Device         | CPU | GPU | CPU | GPU | CPU | GPU | CPU | GPU |\n+================+=====+=====+=====+=====+=====+=====+=====+=====+\n| send           | ✓   | ✘   | ✓   | ?   | ✘   | ✓   | ✘   | ✓   |\n+----------------+-----+-----+-----+-----+-----+-----+-----+-----+\n| recv           | ✓   | ✘   | ✓   | ?   | ✘   | ✓   | ✘   | ✓   |\n+----------------+-----+-----+-----+-----+-----+-----+-----+-----+\n| broadcast      | ✓   | ✓   | ✓   | ?   | ✘   | ✓   | ✘   | ✓   |\n+----------------+-----+-----+-----+-----+-----+-----+-----+-----+\n| all_reduce     | ✓   | ✓   | ✓   | ?   | ✘   | ✓   | ✘   | ✓   |\n+----------------+-----+-----+-----+-----+-----+-----+-----+-----+\n| reduce         | ✓   | ✓   | ✓   | ?   | ✘   | ✓   | ✘   | ✓   |\n+----------------+-----+-----+-----+-----+-----+-----+-----+-----+\n| all_gather     | ✓   | ✓   | ✓   | ?   | ✘   | ✓   | ✘   | ✓   |\n+----------------+-----+-----+-----+-----+-----+-----+-----+-----+\n| gather         | ✓   | ✓   | ✓   | ?   | ✘   | ✓   | ✘   | ✓   |\n+----------------+-----+-----+-----+-----+-----+-----+-----+-----+\n| scatter        | ✓   | ✓   | ✓   | ?   | ✘   | ✓   | ✘   | ✓   |\n+----------------+-----+-----+-----+-----+-----+-----+-----+-----+\n| reduce_scatter | ✓   | ✓   | ✘   | ✘   | ✘   | ✓   | ✘   | ✓   |\n+----------------+-----+-----+-----+-----+-----+-----+-----+-----+\n| all_to_all     | ✓   | ✓   | ✓   | ?   | ✘   | ✓   | ✘   | ✓   |\n+----------------+-----+-----+-----+-----+-----+-----+-----+-----+\n| barrier        | ✓   | ✘   | ✓   | ?   | ✘   | ✓   | ✘   | ✓   |\n+----------------+-----+-----+-----+-----+-----+-----+-----+-----+\n```\n\n### Backends that come with PyTorch\n\nPyTorch distributed package supports Linux (stable), MacOS (stable), and Windows (prototype).\nBy default for Linux, the Gloo and NCCL backends are built and included in PyTorch\ndistributed (NCCL only when building with CUDA). MPI is an optional backend that can only be\nincluded if you build PyTorch from source. (e.g. building PyTorch on a host that has MPI\ninstalled.)\n\n:::{note}\nAs of PyTorch v1.8, Windows supports all collective communications backend but NCCL,\nIf the `init_method` argument of {func}`init_process_group` points to a file it must adhere\nto the following schema:\n\n- Local file system, `init_method=\"file:///d:/tmp/some_file\"`\n- Shared file system, `init_method=\"file://////{machine_name}/{share_folder_name}/some_file\"`\n\nSame as on Linux platform, you can enable TcpStore by setting environment variables,\nMASTER_ADDR and MASTER_PORT.\n:::\n\n### Which backend to use?\n\nIn the past, we were often asked: \"which backend should I use?\".\n\n- Rule of thumb\n\n  - Use the NCCL backend for distributed training with CUDA **GPU**.\n  - Use the XCCL backend for distributed training with XPU **GPU**.\n  - Use the Gloo backend for distributed training with **CPU**.\n\n- GPU hosts with InfiniBand interconnect\n\n  - Use NCCL, since it's the only backend that currently supports\n    InfiniBand and GPUDirect.\n\n- GPU hosts with Ethernet interconnect\n\n  - Use NCCL, since it currently provides the best distributed GPU\n    training performance, especially for multiprocess single-node or\n    multi-node distributed training. If you encounter any problem with\n    NCCL, use Gloo as the fallback option. (Note that Gloo currently\n    runs slower than NCCL for GPUs.)\n\n- CPU hosts with InfiniBand interconnect\n\n  - If your InfiniBand has enabled IP over IB, use Gloo, otherwise,\n    use MPI instead. We are planning on adding InfiniBand support for\n    Gloo in the upcoming releases.\n\n- CPU hosts with Ethernet interconnect\n\n  - Use Gloo, unless you have specific reasons to use MPI.\n\n### Common environment variables\n\n#### Choosing the network interface to use\n\nBy default, both the NCCL and Gloo backends will try to find the right network interface to use.\nIf the automatically detected interface is not correct, you can override it using the following\nenvironment variables (applicable to the respective backend):\n\n- **NCCL_SOCKET_IFNAME**, for example `export NCCL_SOCKET_IFNAME=eth0`\n- **GLOO_SOCKET_IFNAME**, for example `export GLOO_SOCKET_IFNAME=eth0`\n\nIf you're using the Gloo backend, you can specify multiple interfaces by separating\nthem by a comma, like this: `export GLOO_SOCKET_IFNAME=eth0,eth1,eth2,eth3`.\nThe backend will dispatch operations in a round-robin fashion across these interfaces.\nIt is imperative that all processes specify the same number of interfaces in this variable.\n\n#### Other NCCL environment variables\n\n**Debugging** - in case of NCCL failure, you can set `NCCL_DEBUG=INFO` to print an explicit\nwarning message as well as basic NCCL initialization information.\n\nYou may also use `NCCL_DEBUG_SUBSYS` to get more details about a specific\naspect of NCCL. For example, `NCCL_DEBUG_SUBSYS=COLL` would print logs of\ncollective calls, which may be helpful when debugging hangs, especially those\ncaused by collective type or message size mismatch. In case of topology\ndetection failure, it would be helpful to set `NCCL_DEBUG_SUBSYS=GRAPH`\nto inspect the detailed detection result and save as reference if further help\nfrom NCCL team is needed.\n\n**Performance tuning** - NCCL performs automatic tuning based on its topology detection to save users'\ntuning effort. On some socket-based systems, users may still try tuning\n`NCCL_SOCKET_NTHREADS` and `NCCL_NSOCKS_PERTHREAD` to increase socket\nnetwork bandwidth. These two environment variables have been pre-tuned by NCCL\nfor some cloud providers, such as AWS or GCP.\n\nFor a full list of NCCL environment variables, please refer to\n[NVIDIA NCCL's official documentation](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html)\n\nYou can tune NCCL communicators even further using `torch.distributed.ProcessGroupNCCL.NCCLConfig`\nand `torch.distributed.ProcessGroupNCCL.Options`. Learn more about them using `help`\n(e.g. `help(torch.distributed.ProcessGroupNCCL.NCCLConfig)`) in the interpreter.\n\n(distributed-basics)=",
    "1456": "一级标题：Distributed communication package - torch.distributed\n二级标题：Basics\n内容：\nThe `torch.distributed` package provides PyTorch support and communication primitives\nfor multiprocess parallelism across several computation nodes running on one or more\nmachines. The class {func}`torch.nn.parallel.DistributedDataParallel` builds on this\nfunctionality to provide synchronous distributed training as a wrapper around any\nPyTorch model. This differs from the kinds of parallelism provided by\n{doc}`multiprocessing` and {func}`torch.nn.DataParallel` in that it supports\nmultiple network-connected machines and in that the user must explicitly launch a separate\ncopy of the main training script for each process.\n\nIn the single-machine synchronous case, `torch.distributed` or the\n{func}`torch.nn.parallel.DistributedDataParallel` wrapper may still have advantages over other\napproaches to data-parallelism, including {func}`torch.nn.DataParallel`:\n\n- Each process maintains its own optimizer and performs a complete optimization step with each\n  iteration. While this may appear redundant, since the gradients have already been gathered\n  together and averaged across processes and are thus the same for every process, this means\n  that no parameter broadcast step is needed, reducing time spent transferring tensors between\n  nodes.\n- Each process contains an independent Python interpreter, eliminating the extra interpreter\n  overhead and \"GIL-thrashing\" that comes from driving several execution threads, model\n  replicas, or GPUs from a single Python process. This is especially important for models that\n  make heavy use of the Python runtime, including models with recurrent layers or many small\n  components.",
    "1457": "一级标题：Distributed communication package - torch.distributed\n二级标题：Initialization\n内容：\nThe package needs to be initialized using the {func}`torch.distributed.init_process_group`\nor {func}`torch.distributed.device_mesh.init_device_mesh` function before calling any other methods.\nBoth block until all processes have joined.\n\n:::{warning}\nInitialization is not thread-safe. Process group creation should be performed from a single thread, to prevent\ninconsistent 'UUID' assignment across ranks, and to prevent races during initialization that can lead to hangs.\n:::\n\n```{eval-rst}\n.. autofunction:: is_available\n```\n\n```{eval-rst}\n.. autofunction:: init_process_group\n```\n\n```{eval-rst}\n.. autofunction:: torch.distributed.device_mesh.init_device_mesh\n```\n\n```{eval-rst}\n.. autofunction:: is_initialized\n```\n\n```{eval-rst}\n.. autofunction:: is_mpi_available\n```\n\n```{eval-rst}\n.. autofunction:: is_nccl_available\n```\n\n```{eval-rst}\n.. autofunction:: is_gloo_available\n```\n\n```{eval-rst}\n.. autofunction:: torch.distributed.distributed_c10d.is_xccl_available\n```\n\n```{eval-rst}\n.. autofunction:: is_torchelastic_launched\n```\n\n```{eval-rst}\n.. autofunction:: get_default_backend_for_device\n```\n\n______________________________________________________________________\n\nCurrently three initialization methods are supported:\n\n### TCP initialization\n\nThere are two ways to initialize using TCP, both requiring a network address\nreachable from all processes and a desired `world_size`. The first way\nrequires specifying an address that belongs to the rank 0 process. This\ninitialization method requires that all processes have manually specified ranks.\n\nNote that multicast address is not supported anymore in the latest distributed\npackage. `group_name` is deprecated as well.\n\n```\nimport torch.distributed as dist\n\n# Use address of one of the machines\ndist.init_process_group(backend, init_method='tcp://10.1.1.20:23456',\n                        rank=args.rank, world_size=4)\n```\n\n### Shared file-system initialization\n\nAnother initialization method makes use of a file system that is shared and\nvisible from all machines in a group, along with a desired `world_size`. The URL should start\nwith `file://` and contain a path to a non-existent file (in an existing\ndirectory) on a shared file system. File-system initialization will automatically\ncreate that file if it doesn't exist, but will not delete the file. Therefore, it\nis your responsibility to make sure that the file is cleaned up before the next\n{func}`init_process_group` call on the same file path/name.\n\nNote that automatic rank assignment is not supported anymore in the latest\ndistributed package and `group_name` is deprecated as well.\n\n:::{warning}\nThis method assumes that the file system supports locking using `fcntl` - most\nlocal systems and NFS support it.\n:::\n\n:::{warning}\nThis method will always create the file and try its best to clean up and remove\nthe file at the end of the program. In other words, each initialization with\nthe file init method will need a brand new empty file in order for the initialization\nto succeed. If the same file used by the previous initialization (which happens not\nto get cleaned up) is used again, this is unexpected behavior and can often cause\ndeadlocks and failures. Therefore, even though this method will try its best to clean up\nthe file, if the auto-delete happens to be unsuccessful, it is your responsibility\nto ensure that the file is removed at the end of the training to prevent the same\nfile to be reused again during the next time. This is especially important\nif you plan to call {func}`init_process_group` multiple times on the same file name.\nIn other words, if the file is not removed/cleaned up and you call\n{func}`init_process_group` again on that file, failures are expected.\nThe rule of thumb here is that, make sure that the file is non-existent or\nempty every time {func}`init_process_group` is called.\n:::\n\n```\nimport torch.distributed as dist\n\n# rank should always be specified\ndist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile',\n                        world_size=4, rank=args.rank)\n```\n\n### Environment variable initialization\n\nThis method will read the configuration from environment variables, allowing\none to fully customize how the information is obtained. The variables to be set\nare:\n\n- `MASTER_PORT` - required; has to be a free port on machine with rank 0\n- `MASTER_ADDR` - required (except for rank 0); address of rank 0 node\n- `WORLD_SIZE` - required; can be set either here, or in a call to init function\n- `RANK` - required; can be set either here, or in a call to init function\n\nThe machine with rank 0 will be used to set up all connections.\n\nThis is the default method, meaning that `init_method` does not have to be specified (or\ncan be `env://`).\n\n### Improving initialization time\n\n- `TORCH_GLOO_LAZY_INIT` - establishes connections on demand rather than\n  using a full mesh which can greatly improve initialization time for non all2all\n  operations.",
    "1458": "一级标题：Distributed communication package - torch.distributed\n二级标题：Post-Initialization\n内容：\nOnce {func}`torch.distributed.init_process_group` was run, the following functions can be used. To\ncheck whether the process group has already been initialized use {func}`torch.distributed.is_initialized`.\n\n```{eval-rst}\n.. autoclass:: Backend\n    :members:\n```\n\n```{eval-rst}\n.. autofunction:: get_backend\n```\n\n```{eval-rst}\n.. autofunction:: get_rank\n```\n\n```{eval-rst}\n.. autofunction:: get_world_size\n```",
    "1459": "一级标题：Distributed communication package - torch.distributed\n二级标题：Shutdown\n内容：\nIt is important to clean up resources on exit by calling {func}`destroy_process_group`.\n\nThe simplest pattern to follow is to destroy every process group and backend by calling\n{func}`destroy_process_group()` with the default value of None for the `group` argument, at a\npoint in the training script where communications are no longer needed, usually near the\nend of main(). The call should be made once per trainer-process, not at the outer\nprocess-launcher level.\n\nif {func}`destroy_process_group` is not called by all ranks in a pg within the timeout duration,\nespecially when there are multiple process-groups in the application e.g. for N-D parallelism,\nhangs on exit are possible. This is because the destructor for ProcessGroupNCCL calls ncclCommAbort,\nwhich must be called collectively, but the order of calling ProcessGroupNCCL's destructor if called\nby python's GC is not deterministic. Calling {func}`destroy_process_group` helps by ensuring\nncclCommAbort is called in a consistent order across ranks, and avoids calling ncclCommAbort\nduring ProcessGroupNCCL's destructor.\n\n### Reinitialization\n\n`destroy_process_group` can also be used to destroy individual process groups. One use\ncase could be fault tolerant training, where a process group may be destroyed and then\na new one initialized during runtime. In this case, it's critical to synchronize the trainer\nprocesses using some means other than torch.distributed primitives \\_after\\_ calling destroy and\nbefore subsequently initializing. This behavior is currently unsupported/untested, due to\nthe difficulty of achieving this synchronization, and is considered a known issue. Please file\na github issue or RFC if this is a use case that's blocking you.\n\n______________________________________________________________________",
    "1460": "一级标题：Distributed communication package - torch.distributed\n二级标题：Groups\n内容：\nBy default collectives operate on the default group (also called the world) and\nrequire all processes to enter the distributed function call. However, some workloads can benefit\nfrom more fine-grained communication. This is where distributed groups come\ninto play. {func}`~torch.distributed.new_group` function can be\nused to create new groups, with arbitrary subsets of all processes. It returns\nan opaque group handle that can be given as a `group` argument to all collectives\n(collectives are distributed functions to exchange information in certain well-known programming patterns).\n\n```{eval-rst}\n.. autofunction:: new_group\n```\n\n```{eval-rst}\n.. autofunction:: get_group_rank\n```\n\n```{eval-rst}\n.. autofunction:: get_global_rank\n```\n\n```{eval-rst}\n.. autofunction:: get_process_group_ranks\n\n```",
    "1461": "一级标题：Distributed communication package - torch.distributed\n二级标题：DeviceMesh\n内容：\nDeviceMesh is a higher level abstraction that manages process groups (or NCCL communicators).\nIt allows user to easily create inter node and intra node process groups without worrying about\nhow to set up the ranks correctly for different sub process groups, and it helps manage those\ndistributed process group easily. {func}`~torch.distributed.device_mesh.init_device_mesh` function can be\nused to create new DeviceMesh, with a mesh shape describing the device topology.\n\n```{eval-rst}\n.. autoclass:: torch.distributed.device_mesh.DeviceMesh\n    :members:\n```",
    "1462": "一级标题：Distributed communication package - torch.distributed\n二级标题：Point-to-point communication\n内容：\n```{eval-rst}\n.. autofunction:: send\n```\n\n```{eval-rst}\n.. autofunction:: recv\n```\n\n{func}`~torch.distributed.isend` and {func}`~torch.distributed.irecv`\nreturn distributed request objects when used. In general, the type of this object is unspecified\nas they should never be created manually, but they are guaranteed to support two methods:\n\n- `is_completed()` - returns True if the operation has finished\n- `wait()` - will block the process until the operation is finished.\n  `is_completed()` is guaranteed to return True once it returns.\n\n```{eval-rst}\n.. autofunction:: isend\n```\n\n```{eval-rst}\n.. autofunction:: irecv\n```\n\n```{eval-rst}\n.. autofunction:: send_object_list\n```\n\n```{eval-rst}\n.. autofunction:: recv_object_list\n```\n\n```{eval-rst}\n.. autofunction:: batch_isend_irecv\n```\n\n```{eval-rst}\n.. autoclass:: P2POp\n```",
    "1463": "一级标题：Distributed communication package - torch.distributed\n二级标题：Synchronous and asynchronous collective operations\n内容：\nEvery collective operation function supports the following two kinds of operations,\ndepending on the setting of the `async_op` flag passed into the collective:\n\n**Synchronous operation** - the default mode, when `async_op` is set to `False`.\nWhen the function returns, it is guaranteed that\nthe collective operation is performed. In the case of CUDA operations, it is not guaranteed\nthat the CUDA operation is completed, since CUDA operations are asynchronous. For CPU collectives, any\nfurther function calls utilizing the output of the collective call will behave as expected. For CUDA collectives,\nfunction calls utilizing the output on the same CUDA stream will behave as expected. Users must take care of\nsynchronization under the scenario of running under different streams. For details on CUDA semantics such as stream\nsynchronization, see [CUDA Semantics](https://pytorch.org/docs/stable/notes/cuda.html).\nSee the below script to see examples of differences in these semantics for CPU and CUDA operations.\n\n**Asynchronous operation** - when `async_op` is set to True. The collective operation function\nreturns a distributed request object. In general, you don't need to create it manually and it\nis guaranteed to support two methods:\n\n- `is_completed()` - in the case of CPU collectives, returns `True` if completed. In the case of CUDA operations,\n  returns `True` if the operation has been successfully enqueued onto a CUDA stream and the output can be utilized on the\n  default stream without further synchronization.\n- `wait()` - in the case of CPU collectives, will block the process until the operation is completed. In the case\n  of CUDA collectives, will block the currently active CUDA stream until the operation is completed (but will not block the CPU).\n- `get_future()` - returns `torch._C.Future` object. Supported for NCCL, also supported for most operations on GLOO\n  and MPI, except for peer to peer operations.\n  Note: as we continue adopting Futures and merging APIs, `get_future()` call might become redundant.\n\n**Example**\n\nThe following code can serve as a reference regarding semantics for CUDA operations when using distributed collectives.\nIt shows the explicit need to synchronize when using collective outputs on different CUDA streams:\n\n```\n# Code runs on each rank.\ndist.init_process_group(\"nccl\", rank=rank, world_size=2)\noutput = torch.tensor([rank]).cuda(rank)\ns = torch.cuda.Stream()\nhandle = dist.all_reduce(output, async_op=True)\n# Wait ensures the operation is enqueued, but not necessarily complete.\nhandle.wait()\n# Using result on non-default stream.\nwith torch.cuda.stream(s):\n    s.wait_stream(torch.cuda.default_stream())\n    output.add_(100)\nif rank == 0:\n    # if the explicit call to wait_stream was omitted, the output below will be\n    # non-deterministically 1 or 101, depending on whether the allreduce overwrote\n    # the value after the add completed.\n    print(output)\n```",
    "1464": "一级标题：Distributed communication package - torch.distributed\n二级标题：Collective functions\n内容：\n```{eval-rst}\n.. autofunction:: broadcast\n```\n\n```{eval-rst}\n.. autofunction:: broadcast_object_list\n```\n\n```{eval-rst}\n.. autofunction:: all_reduce\n```\n\n```{eval-rst}\n.. autofunction:: reduce\n```\n\n```{eval-rst}\n.. autofunction:: all_gather\n```\n\n```{eval-rst}\n.. autofunction:: all_gather_into_tensor\n```\n\n```{eval-rst}\n.. autofunction:: all_gather_object\n```\n\n```{eval-rst}\n.. autofunction:: gather\n```\n\n```{eval-rst}\n.. autofunction:: gather_object\n```\n\n```{eval-rst}\n.. autofunction:: scatter\n```\n\n```{eval-rst}\n.. autofunction:: scatter_object_list\n```\n\n```{eval-rst}\n.. autofunction:: reduce_scatter\n```\n\n```{eval-rst}\n.. autofunction:: reduce_scatter_tensor\n```\n\n```{eval-rst}\n.. autofunction:: all_to_all_single\n```\n\n```{eval-rst}\n.. autofunction:: all_to_all\n```\n\n```{eval-rst}\n.. autofunction:: barrier\n```\n\n```{eval-rst}\n.. autofunction:: monitored_barrier\n```\n\n```{eval-rst}\n.. autoclass:: Work\n    :members:\n```\n\n```{eval-rst}\n.. autoclass:: ReduceOp\n```\n\n```{eval-rst}\n.. class:: reduce_op\n\n    Deprecated enum-like class for reduction operations: ``SUM``, ``PRODUCT``,\n    ``MIN``, and ``MAX``.\n\n    :class:`~torch.distributed.ReduceOp` is recommended to use instead.\n\n```",
    "1465": "一级标题：Distributed communication package - torch.distributed\n二级标题：Distributed Key-Value Store\n内容：\nThe distributed package comes with a distributed key-value store, which can be\nused to share information between processes in the group as well as to\ninitialize the distributed package in\n{func}`torch.distributed.init_process_group` (by explicitly creating the store\nas an alternative to specifying `init_method`.) There are 3 choices for\nKey-Value Stores: {class}`~torch.distributed.TCPStore`,\n{class}`~torch.distributed.FileStore`, and {class}`~torch.distributed.HashStore`.\n\n```{eval-rst}\n.. autoclass:: Store\n    :members:\n    :special-members:\n```\n\n```{eval-rst}\n.. autoclass:: TCPStore\n    :members:\n    :special-members: __init__\n```\n\n```{eval-rst}\n.. autoclass:: HashStore\n    :members:\n    :special-members: __init__\n```\n\n```{eval-rst}\n.. autoclass:: FileStore\n    :members:\n    :special-members: __init__\n```\n\n```{eval-rst}\n.. autoclass:: PrefixStore\n    :members:\n    :special-members: __init__\n\n```",
    "1466": "一级标题：Distributed communication package - torch.distributed\n二级标题：Profiling Collective Communication\n内容：\nNote that you can use `torch.profiler` (recommended, only available after 1.8.1) or `torch.autograd.profiler` to profile collective communication and point-to-point communication APIs mentioned here. All out-of-the-box backends (`gloo`,\n`nccl`, `mpi`) are supported and collective communication usage will be rendered as expected in profiling output/traces. Profiling your code is the same as any regular torch operator:\n\n```\nimport torch\nimport torch.distributed as dist\nwith torch.profiler():\n    tensor = torch.randn(20, 10)\n    dist.all_reduce(tensor)\n```\n\nPlease refer to the [profiler documentation](https://pytorch.org/docs/main/profiler.html) for a full overview of profiler features.",
    "1467": "一级标题：Distributed communication package - torch.distributed\n二级标题：Multi-GPU collective functions\n内容：\n:::{warning}\nThe multi-GPU functions (which stand for multiple GPUs per CPU thread) are\ndeprecated. As of today, PyTorch Distributed's preferred programming model\nis one device per thread, as exemplified by the APIs in this document. If\nyou are a backend developer and want to support multiple devices per thread,\nplease contact PyTorch Distributed's maintainers.\n:::\n\n(object_collectives)=",
    "1468": "一级标题：Distributed communication package - torch.distributed\n二级标题：Object collectives\n内容：\n:::{warning}\nObject collectives have a number of serious limitations. Read further to determine\nif they are safe to use for your use case.\n:::\n\nObject collectives are a set of collective-like operations that work on arbitrary\nPython objects, as long as they can be pickled. There are various collective patterns\nimplemented (e.g. broadcast, all_gather, ...) but they each roughly follow this pattern:\n\n1. convert the input object into a pickle (raw bytes), then shove it into a byte tensor\n2. communicate the size of this byte tensor to peers (first collective operation)\n3. allocate appropriately sized tensor to perform the real collective\n4. communicate the object data (second collective operation)\n5. convert raw data back into Python (unpickle)\n\nObject collectives sometimes have surprising performance or memory characteristics that lead to\nlong runtimes or OOMs, and thus they should be used with caution. Here are some common issues.\n\n**Asymmetric pickle/unpickle time** - Pickling objects can be slow, depending on the number, type and size of the objects.\nWhen the collective has a fan-in (e.g. gather_object), the receiving rank(s) must unpickle N times more objects than\nthe sending rank(s) had to pickle, which can cause other ranks to time out on their next collective.\n\n**Inefficient tensor communication** - Tensors should be sent via regular collective APIs, not object collective APIs.\nIt is possible to send Tensors via object collective APIs, but they will be serialized and deserialized (including a\nCPU-sync and device-to-host copy in the case of non-CPU tensors), and in almost every case other than debugging or\ntroubleshooting code, it would be worth the trouble to refactor the code to use non-object collectives instead.\n\n**Unexpected tensor devices** - If you still want to send tensors via object collectives, there is another aspect\nspecific to cuda (and possibly other accelerators) tensors. If you pickle a tensor that is currently on `cuda:3`, and\nthen unpickle it, you will get another tensor on `cuda:3` *regardless of which process you are on, or which CUDA device\nis the 'default' device for that process*. With regular tensor collective APIs, 'output tensors' will always be on the\nsame, local device, which is generally what you'd expect.\n\nUnpickling a tensor will implicitly activate a CUDA context if it is the first\ntime a GPU is used by the process, which can waste significant amounts of GPU memory. This issue can be avoided by\nmoving tensors to CPU before passing them as inputs to an object collective.",
    "1469": "一级标题：Distributed communication package - torch.distributed\n二级标题：Third-party backends\n内容：\nBesides the builtin GLOO/MPI/NCCL backends, PyTorch distributed supports\nthird-party backends through a run-time register mechanism.\nFor references on how to develop a third-party backend through C++ Extension,\nplease refer to [Tutorials - Custom C++ and CUDA Extensions](https://pytorch.org/tutorials/advanced/cpp_extension.html) and\n`test/cpp_extensions/cpp_c10d_extension.cpp`. The capability of third-party\nbackends are decided by their own implementations.\n\nThe new backend derives from `c10d::ProcessGroup` and registers the backend\nname and the instantiating interface through {func}`torch.distributed.Backend.register_backend`\nwhen imported.\n\nWhen manually importing this backend and invoking {func}`torch.distributed.init_process_group`\nwith the corresponding backend name, the `torch.distributed` package runs on\nthe new backend.\n\n:::{warning}\nThe support of third-party backend is experimental and subject to change.\n:::\n\n(distributed-launch)=",
    "1470": "一级标题：Distributed communication package - torch.distributed\n二级标题：Launch utility\n内容：\nThe `torch.distributed` package also provides a launch utility in\n`torch.distributed.launch`. This helper utility can be used to launch\nmultiple processes per node for distributed training.\n\n```{eval-rst}\n.. automodule:: torch.distributed.launch\n\n```",
    "1471": "一级标题：Distributed communication package - torch.distributed\n二级标题：Spawn utility\n内容：\nThe {ref}`multiprocessing-doc` package also provides a `spawn`\nfunction in {func}`torch.multiprocessing.spawn`. This helper function\ncan be used to spawn multiple processes. It works by passing in the\nfunction that you want to run and spawns N processes to run it. This\ncan be used for multiprocess distributed training as well.\n\nFor references on how to use it, please refer to [PyTorch example - ImageNet\nimplementation](https://github.com/pytorch/examples/tree/master/imagenet)\n\nNote that this function requires Python 3.4 or higher.",
    "1472": "一级标题：Distributed communication package - torch.distributed\n二级标题：Debugging `torch.distributed` applications\n内容：\nDebugging distributed applications can be challenging due to hard to understand hangs, crashes, or inconsistent behavior across ranks. `torch.distributed` provides\na suite of tools to help debug training applications in a self-serve fashion:\n\n### Python Breakpoint\n\nIt is extremely convenient to use python's debugger in a distributed environment, but because it does not work out of the box many people do not use it at all.\nPyTorch offers a customized wrapper around pdb that streamlines the process.\n\n`torch.distributed.breakpoint` makes this process easy. Internally, it customizes `pdb`'s breakpoint behavior in two ways but otherwise behaves as normal `pdb`.\n1. Attaches the debugger only on one rank (specified by the user).\n2. Ensures all other ranks stop, by using a `torch.distributed.barrier()` that will release once the debugged rank issues a `continue`\n3. Reroutes stdin from the child process such that it connects to your terminal.\n\nTo use it, simply issue `torch.distributed.breakpoint(rank)` on all ranks, using the same value for `rank` in each case.\n\n### Monitored Barrier\n\nAs of v1.10, {func}`torch.distributed.monitored_barrier` exists as an alternative to {func}`torch.distributed.barrier` which fails with helpful information about which rank may be faulty\nwhen crashing, i.e. not all ranks calling into {func}`torch.distributed.monitored_barrier` within the provided timeout. {func}`torch.distributed.monitored_barrier` implements a host-side\nbarrier using `send`/`recv` communication primitives in a process similar to acknowledgements, allowing rank 0 to report which rank(s) failed to acknowledge\nthe barrier in time. As an example, consider the following function where rank 1 fails to call into {func}`torch.distributed.monitored_barrier` (in practice this could be due\nto an application bug or hang in a previous collective):\n\n```\nimport os\nfrom datetime import timedelta\n\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\n\ndef worker(rank):\n    dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n    # monitored barrier requires gloo process group to perform host-side sync.\n    group_gloo = dist.new_group(backend=\"gloo\")\n    if rank not in [1]:\n        dist.monitored_barrier(group=group_gloo, timeout=timedelta(seconds=2))\n\n\nif __name__ == \"__main__\":\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29501\"\n    mp.spawn(worker, nprocs=2, args=())\n```\n\nThe following error message is produced on rank 0, allowing the user to determine which rank(s) may be faulty and investigate further:\n\n```\nRuntimeError: Rank 1 failed to pass monitoredBarrier in 2000 ms\n Original exception:\n[gloo/transport/tcp/pair.cc:598] Connection closed by peer [2401:db00:eef0:1100:3560:0:1c05:25d]:8594\n```\n\n### `TORCH_DISTRIBUTED_DEBUG`\n\nWith `TORCH_CPP_LOG_LEVEL=INFO`, the environment variable `TORCH_DISTRIBUTED_DEBUG` can be used to trigger additional useful logging and collective synchronization checks to ensure all ranks\nare synchronized appropriately. `TORCH_DISTRIBUTED_DEBUG` can be set to either `OFF` (default), `INFO`, or `DETAIL` depending on the debugging level\nrequired. Please note that the most verbose option, `DETAIL` may impact the application performance and thus should only be used when debugging issues.\n\nSetting `TORCH_DISTRIBUTED_DEBUG=INFO` will result in additional debug logging when models trained with {func}`torch.nn.parallel.DistributedDataParallel` are initialized, and\n`TORCH_DISTRIBUTED_DEBUG=DETAIL` will additionally log runtime performance statistics a select number of iterations. These runtime statistics\ninclude data such as forward time, backward time, gradient communication time, etc. As an example, given the following application:\n\n```\nimport os\n\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\n\nclass TwoLinLayerNet(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.a = torch.nn.Linear(10, 10, bias=False)\n        self.b = torch.nn.Linear(10, 1, bias=False)\n\n    def forward(self, x):\n        a = self.a(x)\n        b = self.b(x)\n        return (a, b)\n\n\ndef worker(rank):\n    dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n    torch.cuda.set_device(rank)\n    print(\"init model\")\n    model = TwoLinLayerNet().cuda()\n    print(\"init ddp\")\n    ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n\n    inp = torch.randn(10, 10).cuda()\n    print(\"train\")\n\n    for _ in range(20):\n        output = ddp_model(inp)\n        loss = output[0] + output[1]\n        loss.sum().backward()\n\n\nif __name__ == \"__main__\":\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29501\"\n    os.environ[\"TORCH_CPP_LOG_LEVEL\"]=\"INFO\"\n    os.environ[\n        \"TORCH_DISTRIBUTED_DEBUG\"\n    ] = \"DETAIL\"  # set to DETAIL for runtime logging.\n    mp.spawn(worker, nprocs=2, args=())\n```\n\nThe following logs are rendered at initialization time:\n\n```\nI0607 16:10:35.739390 515217 logger.cpp:173] [Rank 0]: DDP Initialized with:\nbroadcast_buffers: 1\nbucket_cap_bytes: 26214400\nfind_unused_parameters: 0\ngradient_as_bucket_view: 0\nis_multi_device_module: 0\niteration: 0\nnum_parameter_tensors: 2\noutput_device: 0\nrank: 0\ntotal_parameter_size_bytes: 440\nworld_size: 2\nbackend_name: nccl\nbucket_sizes: 440\ncuda_visible_devices: N/A\ndevice_ids: 0\ndtypes: float\nmaster_addr: localhost\nmaster_port: 29501\nmodule_name: TwoLinLayerNet\nnccl_async_error_handling: N/A\nnccl_blocking_wait: N/A\nnccl_debug: WARN\nnccl_ib_timeout: N/A\nnccl_nthreads: N/A\nnccl_socket_ifname: N/A\ntorch_distributed_debug: INFO\n```\n\nThe following logs are rendered during runtime (when `TORCH_DISTRIBUTED_DEBUG=DETAIL` is set):\n\n```\nI0607 16:18:58.085681 544067 logger.cpp:344] [Rank 1 / 2] Training TwoLinLayerNet unused_parameter_size=0\n Avg forward compute time: 40838608\n Avg backward compute time: 5983335\nAvg backward comm. time: 4326421\n Avg backward comm/comp overlap time: 4207652\nI0607 16:18:58.085693 544066 logger.cpp:344] [Rank 0 / 2] Training TwoLinLayerNet unused_parameter_size=0\n Avg forward compute time: 42850427\n Avg backward compute time: 3885553\nAvg backward comm. time: 2357981\n Avg backward comm/comp overlap time: 2234674\n```\n\nIn addition, `TORCH_DISTRIBUTED_DEBUG=INFO` enhances crash logging in {func}`torch.nn.parallel.DistributedDataParallel` due to unused parameters in the model. Currently, `find_unused_parameters=True`\nmust be passed into {func}`torch.nn.parallel.DistributedDataParallel` initialization if there are parameters that may be unused in the forward pass, and as of v1.10, all model outputs are required\nto be used in loss computation as {func}`torch.nn.parallel.DistributedDataParallel` does not support unused parameters in the backwards pass. These constraints are challenging especially for larger\nmodels, thus when crashing with an error, {func}`torch.nn.parallel.DistributedDataParallel` will log the fully qualified name of all parameters that went unused. For example, in the above application,\nif we modify `loss` to be instead computed as `loss = output[1]`, then `TwoLinLayerNet.a` does not receive a gradient in the backwards pass, and\nthus results in `DDP` failing. On a crash, the user is passed information about parameters which went unused, which may be challenging to manually find for large models:\n\n```\nRuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing\n the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by\nmaking sure all `forward` function outputs participate in calculating loss.\nIf you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return va\nlue of `forward` of your module when reporting this issue (e.g. list, dict, iterable).\nParameters which did not receive grad for rank 0: a.weight\nParameter indices which did not receive grad for rank 0: 0\n```\n\nSetting `TORCH_DISTRIBUTED_DEBUG=DETAIL` will trigger additional consistency and synchronization checks on every collective call issued by the user\neither directly or indirectly (such as DDP `allreduce`). This is done by creating a wrapper process group that wraps all process groups returned by\n{func}`torch.distributed.init_process_group` and {func}`torch.distributed.new_group` APIs. As a result, these APIs will return a wrapper process group that can be used exactly like a regular process\ngroup, but performs consistency checks before dispatching the collective to an underlying process group. Currently, these checks include a {func}`torch.distributed.monitored_barrier`,\nwhich ensures all ranks complete their outstanding collective calls and reports ranks which are stuck. Next, the collective itself is checked for consistency by\nensuring all collective functions match and are called with consistent tensor shapes. If this is not the case, a detailed error report is included when the\napplication crashes, rather than a hang or uninformative error message. As an example, consider the following function which has mismatched input shapes into\n{func}`torch.distributed.all_reduce`:\n\n```\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\n\ndef worker(rank):\n    dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n    torch.cuda.set_device(rank)\n    tensor = torch.randn(10 if rank == 0 else 20).cuda()\n    dist.all_reduce(tensor)\n    torch.cuda.synchronize(device=rank)\n\n\nif __name__ == \"__main__\":\n    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n    os.environ[\"MASTER_PORT\"] = \"29501\"\n    os.environ[\"TORCH_CPP_LOG_LEVEL\"]=\"INFO\"\n    os.environ[\"TORCH_DISTRIBUTED_DEBUG\"] = \"DETAIL\"\n    mp.spawn(worker, nprocs=2, args=())\n```\n\nWith the `NCCL` backend, such an application would likely result in a hang which can be challenging to root-cause in nontrivial scenarios. If the user enables\n`TORCH_DISTRIBUTED_DEBUG=DETAIL` and reruns the application, the following error message reveals the root cause:\n\n```\nwork = default_pg.allreduce([tensor], opts)\nRuntimeError: Error when verifying shape tensors for collective ALLREDUCE on rank 0. This likely indicates that input shapes into the collective are mismatched across ranks. Got shapes:  10\n[ torch.LongTensor{1} ]\n```\n\n:::{note}\nFor fine-grained control of the debug level during runtime the functions {func}`torch.distributed.set_debug_level`, {func}`torch.distributed.set_debug_level_from_env`, and\n{func}`torch.distributed.get_debug_level` can also be used.\n:::\n\nIn addition, `TORCH_DISTRIBUTED_DEBUG=DETAIL` can be used in conjunction with `TORCH_SHOW_CPP_STACKTRACES=1` to log the entire callstack when a collective desynchronization is detected. These\ncollective desynchronization checks will work for all applications that use `c10d` collective calls backed by process groups created with the\n{func}`torch.distributed.init_process_group` and {func}`torch.distributed.new_group` APIs.",
    "1473": "一级标题：Distributed communication package - torch.distributed\n二级标题：Logging\n内容：\nIn addition to explicit debugging support via {func}`torch.distributed.monitored_barrier` and `TORCH_DISTRIBUTED_DEBUG`, the underlying C++ library of `torch.distributed` also outputs log\nmessages at various levels. These messages can be helpful to understand the execution state of a distributed training job and to troubleshoot problems such as network connection failures. The\nfollowing matrix shows how the log level can be adjusted via the combination of `TORCH_CPP_LOG_LEVEL` and `TORCH_DISTRIBUTED_DEBUG` environment variables.\n\n| `TORCH_CPP_LOG_LEVEL` | `TORCH_DISTRIBUTED_DEBUG` | Effective Log Level |\n| --------------------- | ------------------------- | ------------------- |\n| `ERROR`               | ignored                   | Error               |\n| `WARNING`             | ignored                   | Warning             |\n| `INFO`                | ignored                   | Info                |\n| `INFO`                | `INFO`                    | Debug               |\n| `INFO`                | `DETAIL`                  | Trace (a.k.a. All)  |\n\nDistributed components raise custom Exception types derived from `RuntimeError`:\n\n- `torch.distributed.DistError`: This is the base type of all distributed exceptions.\n- `torch.distributed.DistBackendError`: This exception is thrown when a backend-specific error occurs. For example, if\n  the `NCCL` backend is used and the user attempts to use a GPU that is not available to the `NCCL` library.\n- `torch.distributed.DistNetworkError`: This exception is thrown when networking\n  libraries encounter errors (ex: Connection reset by peer)\n- `torch.distributed.DistStoreError`: This exception is thrown when the Store encounters\n  an error (ex: TCPStore timeout)\n\n```{eval-rst}\n.. autoclass:: torch.distributed.DistError\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.DistBackendError\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.DistNetworkError\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.DistStoreError\n```\n\nIf you are running single node training, it may be convenient to interactively breakpoint your script. We offer a way to conveniently breakpoint a single rank:\n\n```{eval-rst}\n.. autofunction:: torch.distributed.breakpoint\n```\n\n% Distributed modules that are missing specific entries.\n\n% Adding them here for tracking purposes until they are more permanently fixed.\n\n```{eval-rst}\n.. py:module:: torch.distributed.algorithms\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.algorithms.ddp_comm_hooks\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.algorithms.model_averaging\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.utils\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.utils.data\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.launcher\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.nn\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.nn.api\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.nn.jit\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.nn.jit.templates\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.algorithms.ddp_comm_hooks.default_hooks\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.algorithms.ddp_comm_hooks.mixed_precision_hooks\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.algorithms.join\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.algorithms.model_averaging.averagers\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.algorithms.model_averaging.hierarchical_model_averager\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.algorithms.model_averaging.utils\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.argparse_util\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.c10d_logger\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.checkpoint.api\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.checkpoint.default_planner\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.checkpoint.filesystem\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.checkpoint.hf_storage\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.checkpoint.metadata\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.checkpoint.optimizer\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.checkpoint.planner\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.checkpoint.planner_helpers\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.checkpoint.resharding\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.checkpoint.state_dict_loader\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.checkpoint.state_dict_saver\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.checkpoint.stateful\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.checkpoint.storage\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.checkpoint.utils\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.collective_utils\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.constants\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.device_mesh\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.distributed_c10d\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.agent.server.api\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.agent.server.local_elastic_agent\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.events.api\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.events.handlers\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.metrics.api\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.multiprocessing.api\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.multiprocessing.errors.error_handler\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.multiprocessing.errors.handlers\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.multiprocessing.redirects\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.multiprocessing.tail_log\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.rendezvous.api\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.rendezvous.c10d_rendezvous_backend\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.rendezvous.dynamic_rendezvous\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.rendezvous.etcd_rendezvous\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.rendezvous.etcd_rendezvous_backend\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.rendezvous.etcd_server\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.rendezvous.etcd_store\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.rendezvous.static_tcp_rendezvous\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.rendezvous.utils\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.timer.api\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.timer.file_based_local_timer\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.timer.local_timer\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.utils.api\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.utils.data.cycling_iterator\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.utils.data.elastic_distributed_sampler\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.utils.distributed\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.utils.log_level\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.utils.logging\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.elastic.utils.store\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.fsdp.api\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.fsdp.fully_sharded_data_parallel\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.fsdp.sharded_grad_scaler\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.fsdp.wrap\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.launcher.api\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.logging_handlers\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.nn.api.remote_module\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.nn.functional\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.nn.jit.instantiator\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.nn.jit.templates.remote_module_template\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.optim.apply_optimizer_in_backward\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.optim.functional_adadelta\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.optim.functional_adagrad\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.optim.functional_adam\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.optim.functional_adamax\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.optim.functional_adamw\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.optim.functional_rmsprop\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.optim.functional_rprop\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.optim.functional_sgd\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.optim.named_optimizer\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.optim.optimizer\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.optim.post_localSGD_optimizer\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.optim.utils\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.optim.zero_redundancy_optimizer\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.remote_device\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.rendezvous\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.rpc.api\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.rpc.backend_registry\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.rpc.constants\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.rpc.functions\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.rpc.internal\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.rpc.options\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.rpc.rref_proxy\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.rpc.server_process_global_profiler\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.tensor.parallel.api\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.tensor.parallel.ddp\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.tensor.parallel.fsdp\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.tensor.parallel.input_reshard\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.tensor.parallel.loss\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.tensor.parallel.style\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.utils\n```\n\n```{eval-rst}\n.. py:module:: torch.distributed.checkpoint.state_dict\n```\n\n```{toctree}\n:hidden:\n\ndistributed._dist2\n```",
    "1474": "一级标题：Distributed Optimizers\n二级标题：无\n内容：\n:::{warning}\nDistributed optimizer is not currently supported when using CUDA tensors\n:::\n\n```{eval-rst}\n.. automodule:: torch.distributed.optim\n    :members: DistributedOptimizer, PostLocalSGDOptimizer, ZeroRedundancyOptimizer\n```",
    "1475": "一级标题：Pipeline Parallelism\n二级标题：无\n内容：\n:::{note}\n`torch.distributed.pipelining` is currently in alpha state and under\ndevelopment. API changes may be possible. It was migrated from the [PiPPy](https://github.com/pytorch/PiPPy) project.\n:::",
    "1476": "一级标题：Pipeline Parallelism\n二级标题：Why Pipeline Parallel?\n内容：\nPipeline Parallelism is one of the **primitive** parallelism for deep learning.\nIt allows the **execution** of a model to be partitioned such that multiple\n**micro-batches** can execute different parts of the model code concurrently.\nPipeline parallelism can be an effective technique for:\n\n- large-scale training\n- bandwidth-limited clusters\n- large model inference\n\nThe above scenarios share a commonality that the computation per device cannot\nhide the communication of conventional parallelism, for example, the weight\nall-gather of FSDP.",
    "1477": "一级标题：Pipeline Parallelism\n二级标题：What is `torch.distributed.pipelining`?\n内容：\nWhile promising for scaling, pipelining is often difficult to implement because\nit needs to **partition the execution** of a model in addition to model weights.\nThe partitioning of execution often requires intrusive code changes to your\nmodel. Another aspect of complexity comes from **scheduling micro-batches in a\ndistributed environment**, with **data flow dependency** considered.\n\nThe `pipelining` package provides a toolkit that does said things\n**automatically** which allows easy implementation of pipeline parallelism\non **general** models.\n\nIt consists of two parts: a\n**splitting frontend** and a **distributed runtime**.\nThe splitting frontend takes your model code as-is, splits it up into \"model\npartitions\", and captures the data-flow relationship. The distributed runtime\nexecutes the pipeline stages on different devices in parallel, handling things\nlike micro-batch splitting, scheduling, communication, and gradient propagation,\netc.\n\nOverall, the `pipelining` package provides the following features:\n\n- Splitting of model code based on simple specification.\n- Rich support for pipeline schedules, including GPipe, 1F1B,\n  Interleaved 1F1B and Looped BFS, and providing the infrastructure for writing\n  customized schedules.\n- First-class support for cross-host pipeline parallelism, as this is where PP\n  is typically used (over slower interconnects).\n- Composability with other PyTorch parallel techniques such as data parallel\n  (DDP, FSDP) or tensor parallel. The [TorchTitan](https://github.com/pytorch/torchtitan) project demonstrates a \"3D parallel\"\n  application on the Llama model.",
    "1478": "一级标题：Pipeline Parallelism\n二级标题：Step 1: build `PipelineStage`\n内容：\nBefore we can use a `PipelineSchedule`, we need to create `PipelineStage`\nobjects that wrap the part of the model running in that stage. The\n`PipelineStage` is responsible for allocating communication buffers and\ncreating send/recv ops to communicate with its peers. It manages intermediate\nbuffers e.g. for the outputs of forward that have not been consumed yet, and it\nprovides a utility for running the backwards for the stage model.\n\nA `PipelineStage` needs to know the input and output shapes for the stage\nmodel, so that it can correctly allocate communication buffers. The shapes must\nbe static, e.g. at runtime the shapes can not change from step to step. A class\n`PipeliningShapeError` will be raised if runtime shapes do not match the\nexpected shapes. When composing with other paralleisms or applying mixed\nprecision, these techniques must be taken into account so the `PipelineStage`\nknows the correct shape (and dtype) for the output of the stage module at\nruntime.\n\nUsers may construct a `PipelineStage` instance directly, by passing in an\n`nn.Module` representing the portion of the model that should run on the\nstage. This may require changes to the original model code. See the example\nin {ref}`option_1_manual`.\n\nAlternatively, the splitting frontend can use graph partitioning to split your\nmodel into a series of `nn.Module` automatically. This technique requires the\nmodel is traceable with `torch.Export`. Composability of the resulting\n`nn.Module` with other parallelism techniques is experimental, and may require\nsome workarounds. Usage of this frontend may be more appealing if the user\ncannot easily change the model code. See {ref}`option_2_tracer` for more\ninformation.",
    "1479": "一级标题：Pipeline Parallelism\n二级标题：Step 2: use `PipelineSchedule` for execution\n内容：\nWe can now attach the `PipelineStage` to a pipeline schedule, and run the\nschedule with input data. Here is a GPipe example:\n\n```python\nfrom torch.distributed.pipelining import ScheduleGPipe\n\n# Create a schedule\nschedule = ScheduleGPipe(stage, n_microbatches)\n\n# Input data (whole batch)\nx = torch.randn(batch_size, in_dim, device=device)\n\n# Run the pipeline with input `x`\n# `x` will be divided into microbatches automatically\nif rank == 0:\n    schedule.step(x)\nelse:\n    output = schedule.step()\n```\n\nNote that the above code needs to be launched for each worker, thus we use a\nlauncher service to launch multiple processes:\n\n```bash\ntorchrun --nproc_per_node=2 example.py\n```",
    "1480": "一级标题：Pipeline Parallelism\n二级标题：Options for Splitting a Model\n内容：\n(option_1_manual)=\n\n### Option 1: splitting a model manually\n\nTo directly construct a `PipelineStage`, the user is responsible for providing\na single `nn.Module` instance that owns the relevant `nn.Parameters` and\n`nn.Buffers`, and defines a `forward()` method that executes the operations\nrelevant for that stage. For example, a condensed version of the Transformer\nclass defined in Torchtitan shows a pattern of building an easily partitionable\nmodel.\n\n```python\nclass Transformer(nn.Module):\n    def __init__(self, model_args: ModelArgs):\n        super().__init__()\n\n        self.tok_embeddings = nn.Embedding(...)\n\n        # Using a ModuleDict lets us delete layers without affecting names,\n        # ensuring checkpoints will correctly save and load.\n        self.layers = torch.nn.ModuleDict()\n        for layer_id in range(model_args.n_layers):\n            self.layers[str(layer_id)] = TransformerBlock(...)\n\n        self.output = nn.Linear(...)\n\n    def forward(self, tokens: torch.Tensor):\n        # Handling layers being 'None' at runtime enables easy pipeline splitting\n        h = self.tok_embeddings(tokens) if self.tok_embeddings else tokens\n\n        for layer in self.layers.values():\n            h = layer(h, self.freqs_cis)\n\n        h = self.norm(h) if self.norm else h\n        output = self.output(h).float() if self.output else h\n        return output\n```\n\nA model defined in this manner can be easily configured per stage by first\ninitializing the whole model (using meta-device to avoid OOM errors), deleting\nundesired layers for that stage, and then creating a PipelineStage that wraps\nthe model. For example:\n\n```python\nwith torch.device(\"meta\"):\n    assert num_stages == 2, \"This is a simple 2-stage example\"\n\n    # we construct the entire model, then delete the parts we do not need for this stage\n    # in practice, this can be done using a helper function that automatically divides up layers across stages.\n    model = Transformer()\n\n    if stage_index == 0:\n        # prepare the first stage model\n        del model.layers[\"1\"]\n        model.norm = None\n        model.output = None\n\n    elif stage_index == 1:\n        # prepare the second stage model\n        model.tok_embeddings = None\n        del model.layers[\"0\"]\n\n    from torch.distributed.pipelining import PipelineStage\n    stage = PipelineStage(\n        model,\n        stage_index,\n        num_stages,\n        device,\n    )\n```\n\nWhen composing with other Data or Model parallelism techniques, `output_args`\nmay also be required, if the output shape/dtype of the model chunk will be\naffected.\n\n(option_2_tracer)=\n\n### Option 2: splitting a model automatically\n\nIf you have a full model and do not want to spend time on modifying it into a\nsequence of \"model partitions\", the `pipeline` API is here to help.\nHere is a brief example:\n\n```python\nclass Model(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.emb = torch.nn.Embedding(10, 3)\n        self.layers = torch.nn.ModuleList(\n            Layer() for _ in range(2)\n        )\n        self.lm = LMHead()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.emb(x)\n        for layer in self.layers:\n            x = layer(x)\n        x = self.lm(x)\n        return x\n```\n\nIf we print the model, we can see multiple hierarchies, which makes it hard to split by hand:\n\n```python\nModel(\n  (emb): Embedding(10, 3)\n  (layers): ModuleList(\n    (0-1): 2 x Layer(\n      (lin): Linear(in_features=3, out_features=3, bias=True)\n    )\n  )\n  (lm): LMHead(\n    (proj): Linear(in_features=3, out_features=3, bias=True)\n  )\n)\n```\n\nLet us see how the `pipeline` API works:\n\n```python\nfrom torch.distributed.pipelining import pipeline, SplitPoint\n\n# An example micro-batch input\nx = torch.LongTensor([1, 2, 4, 5])\n\npipe = pipeline(\n    module=mod,\n    mb_args=(x,),\n    split_spec={\n        \"layers.1\": SplitPoint.BEGINNING,\n    }\n)\n```\n\nThe `pipeline` API splits your model given a `split_spec`, where\n`SplitPoint.BEGINNING` stands for adding a split point\n*before* execution of certain submodule in the `forward` function, and\nsimilarly, `SplitPoint.END` for split point *after* such.\n\nIf we `print(pipe)`, we can see:\n\n```python\nGraphModule(\n  (submod_0): GraphModule(\n    (emb): InterpreterModule()\n    (layers): Module(\n      (0): InterpreterModule(\n        (lin): InterpreterModule()\n      )\n    )\n  )\n  (submod_1): GraphModule(\n    (layers): Module(\n      (1): InterpreterModule(\n        (lin): InterpreterModule()\n      )\n    )\n    (lm): InterpreterModule(\n      (proj): InterpreterModule()\n    )\n  )\n)\n\ndef forward(self, x):\n    submod_0 = self.submod_0(x);  x = None\n    submod_1 = self.submod_1(submod_0);  submod_0 = None\n    return (submod_1,)\n```\n\nThe \"model partitions\" are represented by submodules (`submod_0`,\n`submod_1`), each of which is reconstructed with original model operations, weights\nand hierarchies. In addition, a \"root-level\" `forward` function is\nreconstructed to capture the data flow between those partitions. Such data flow\nwill be replayed by the pipeline runtime later, in a distributed fashion.\n\nThe `Pipe` object provides a method for retrieving the \"model partitions\":\n\n```python\nstage_mod : nn.Module = pipe.get_stage_module(stage_idx)\n```\n\nThe returned `stage_mod` is a `nn.Module`, with which you can create an\noptimizer, save or load checkpoints, or apply other parallelisms.\n\n`Pipe` also allows you to create a distributed stage runtime on a device given\na `ProcessGroup`:\n\n```python\nstage = pipe.build_stage(stage_idx, device, group)\n```\n\nAlternatively, if you would like to build the stage runtime later after some\nmodification to the `stage_mod`, you can use a functional version of the\n`build_stage` API. For example:\n\n```python\nfrom torch.distributed.pipelining import build_stage\nfrom torch.nn.parallel import DistributedDataParallel\n\ndp_mod = DistributedDataParallel(stage_mod)\ninfo = pipe.info()\nstage = build_stage(dp_mod, stage_idx, info, device, group)\n```\n\n:::{note}\nThe `pipeline` frontend uses a tracer (`torch.export`) to capture your\nmodel into a single graph. If your model is not full-graph'able, you can use\nour manual frontend below.\n:::",
    "1481": "一级标题：Pipeline Parallelism\n二级标题：Hugging Face Examples\n内容：\nIn the [PiPPy](https://github.com/pytorch/PiPPy) repo where this package was\noriginal created, we kept examples based on unmodified Hugging Face models.\nSee the [examples/huggingface](https://github.com/pytorch/PiPPy/tree/main/examples/huggingface) directory.\n\nExamples include:\n\n- [GPT2](https://github.com/pytorch/PiPPy/tree/main/examples/huggingface/pippy_gpt2.py)\n- [Llama](https://github.com/pytorch/PiPPy/tree/main/examples/llama)",
    "1482": "一级标题：Pipeline Parallelism\n二级标题：Technical Deep Dive\n内容：\n### How does the `pipeline` API split a model?\n\nFirst, the `pipeline` API turns our model into a directed acyclic graph (DAG)\nby tracing the model. It traces the model using `torch.export` -- a PyTorch 2\nfull-graph capturing tool.\n\nThen, it groups together the **operations and parameters** needed by a stage\ninto a reconstructed submodule: `submod_0`, `submod_1`, ...\n\nDifferent from conventional submodule access methods like `Module.children()`,\nthe `pipeline` API does not only cut the module structure of your model, but\nalso the **forward** function of your model.\n\nThis is necessary because model structure like `Module.children()` merely\ncaptures information during `Module.__init__()`, and does not capture any\ninformation about `Module.forward()`. Said differently, `Module.children()`\nlacks information about the following aspects key to pipelininig:\n\n- Execution order of child modules in `forward`\n- Activation flows between child modules\n- Whether there are any functional operators between child modules (for example,\n  `relu` or `add` operations will not be captured by `Module.children()`).\n\nThe `pipeline` API, on the contrary, makes sure that the `forward` behavior\nis truly preserved. It also captures the activation flow between the partitions,\nhelping the distributed runtime to make correct send/receive calls without human\nintervention.\n\nAnother flexibility of the `pipeline` API is that split points can be at\narbitrary levels within your model hierarchy. In the split partitions, the original model\nhierarchy related to that partition will be reconstructed at no cost to you.\nAt a result, fully-qualified names (FQNs) pointing to a submodule or parameter\nwould be still valid, and services that relies on FQNs (such as FSDP, TP or\ncheckpointing) can still run with your partitioned modules with almost zero code\nchange.",
    "1483": "一级标题：Pipeline Parallelism\n二级标题：Implementing Your Own Schedule\n内容：\nYou can implement your own pipeline schedule by extending one of the following two class:\n\n- `PipelineScheduleSingle`\n- `PipelineScheduleMulti`\n\n`PipelineScheduleSingle` is for schedules that assigns *only one* stage per rank.\n`PipelineScheduleMulti` is for schedules that assigns multiple stages per rank.\n\nFor example, `ScheduleGPipe` and `Schedule1F1B` are subclasses of `PipelineScheduleSingle`.\nWhereas, `ScheduleInterleaved1F1B`, `ScheduleLoopedBFS`, `ScheduleInterleavedZeroBubble`, and `ScheduleZBVZeroBubble`\nare subclasses of `PipelineScheduleMulti`.",
    "1484": "一级标题：Pipeline Parallelism\n二级标题：Logging\n内容：\nYou can turn on additional logging using the `TORCH_LOGS` environment variable from [torch.\\_logging](https://pytorch.org/docs/main/logging.html#module-torch._logging):\n\n- `TORCH_LOGS=+pp` will display `logging.DEBUG` messages and all levels above it.\n- `TORCH_LOGS=pp` will display `logging.INFO` messages and above.\n- `TORCH_LOGS=-pp` will display `logging.WARNING` messages and above.",
    "1485": "一级标题：Pipeline Parallelism\n二级标题：API Reference\n内容：\n```{eval-rst}\n.. automodule:: torch.distributed.pipelining\n```\n\n### Model Split APIs\n\nThe following set of APIs transform your model into a pipeline representation.\n\n```{eval-rst}\n.. currentmodule:: torch.distributed.pipelining\n```\n\n```{eval-rst}\n.. autoclass:: SplitPoint\n```\n\n```{eval-rst}\n.. autofunction:: pipeline\n```\n\n```{eval-rst}\n.. autoclass:: Pipe\n```\n\n```{eval-rst}\n.. autofunction:: pipe_split\n```\n\n### Microbatch Utilities\n\n```{eval-rst}\n.. automodule:: torch.distributed.pipelining.microbatch\n```\n\n```{eval-rst}\n.. currentmodule:: torch.distributed.pipelining.microbatch\n```\n\n```{eval-rst}\n.. autoclass:: TensorChunkSpec\n```\n\n```{eval-rst}\n.. autofunction:: split_args_kwargs_into_chunks\n```\n\n```{eval-rst}\n.. autofunction:: merge_chunks\n```\n\n### Pipeline Stages\n\n```{eval-rst}\n.. automodule:: torch.distributed.pipelining.stage\n```\n\n```{eval-rst}\n.. currentmodule:: torch.distributed.pipelining.stage\n```\n\n```{eval-rst}\n.. autoclass:: PipelineStage\n```\n\n```{eval-rst}\n.. autofunction:: build_stage\n```\n\n### Pipeline Schedules\n\n```{eval-rst}\n.. automodule:: torch.distributed.pipelining.schedules\n```\n\n```{eval-rst}\n.. currentmodule:: torch.distributed.pipelining.schedules\n```\n\n```{eval-rst}\n.. autoclass:: ScheduleGPipe\n```\n\n```{eval-rst}\n.. autoclass:: Schedule1F1B\n```\n\n```{eval-rst}\n.. autoclass:: ScheduleInterleaved1F1B\n```\n\n```{eval-rst}\n.. autoclass:: ScheduleLoopedBFS\n```\n\n```{eval-rst}\n.. autoclass:: ScheduleInterleavedZeroBubble\n```\n\n```{eval-rst}\n.. autoclass:: ScheduleZBVZeroBubble\n```\n\n```{eval-rst}\n.. autoclass:: ScheduleDualPipeV\n```\n\n```{eval-rst}\n.. autoclass:: PipelineScheduleSingle\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: PipelineScheduleMulti\n  :members:\n```",
    "1486": "一级标题：torch.distributed.tensor\n二级标题：无\n内容：\n:::{note}\n`torch.distributed.tensor` is currently in alpha state and under\ndevelopment, we are committing backward compatibility for the most APIs listed\nin the doc, but there might be API changes if necessary.\n:::",
    "1487": "一级标题：torch.distributed.tensor\n二级标题：PyTorch DTensor (Distributed Tensor)\n内容：\nPyTorch DTensor offers simple and flexible tensor sharding primitives that transparently handles distributed\nlogic, including sharded storage, operator computation and collective communications across devices/hosts.\n`DTensor` could be used to build different parallelism solutions and support sharded state_dict representation\nwhen working with multi-dimensional sharding.\n\nPlease see examples from the PyTorch native parallelism solutions that are built on top of `DTensor`:\n\n- [Tensor Parallel](https://pytorch.org/docs/main/distributed.tensor.parallel.html)\n- [FSDP2](https://github.com/pytorch/torchtitan/blob/main/docs/fsdp.md)\n\n```{eval-rst}\n.. automodule:: torch.distributed.tensor\n```\n\n{class}`DTensor` follows the SPMD (single program, multiple data) programming model to empower users to\nwrite distributed program as if it's a **single-device program with the same convergence property**. It\nprovides a uniform tensor sharding layout (DTensor Layout) through specifying the {class}`DeviceMesh`\nand {class}`Placement`:\n\n- {class}`DeviceMesh` represents the device topology and the communicators of the cluster using\n  an n-dimensional array.\n- {class}`Placement` describes the sharding layout of the logical tensor on the {class}`DeviceMesh`.\n  DTensor supports three types of placements: {class}`Shard`, {class}`Replicate` and {class}`Partial`.\n\n### DTensor Class APIs\n\n```{eval-rst}\n.. currentmodule:: torch.distributed.tensor\n```\n\n{class}`DTensor` is a `torch.Tensor` subclass. This means once a {class}`DTensor` is created, it could be\nused in very similar way to `torch.Tensor`, including running different types of PyTorch operators as if\nrunning them in a single device, allowing proper distributed computation for PyTorch operators.\n\nIn addition to existing `torch.Tensor` methods, it also offers a set of additional methods to interact with\n`torch.Tensor`, `redistribute` the DTensor Layout to a new DTensor, get the full tensor content\non all devices, etc.\n\n```{eval-rst}\n.. autoclass:: DTensor\n    :members: from_local, to_local, full_tensor, redistribute, device_mesh, placements\n    :member-order: groupwise\n    :special-members: __create_chunk_list__\n\n```\n\n### DeviceMesh as the distributed communicator\n\n```{eval-rst}\n.. currentmodule:: torch.distributed.device_mesh\n```\n\n{class}`DeviceMesh` was built from DTensor as the abstraction to describe cluster's device topology and represent\nmulti-dimensional communicators (on top of `ProcessGroup`). To see the details of how to create/use a DeviceMesh,\nplease refer to the [DeviceMesh recipe](https://pytorch.org/tutorials/recipes/distributed_device_mesh.html).\n\n### DTensor Placement Types\n\n```{eval-rst}\n.. automodule:: torch.distributed.tensor.placement_types\n```\n\n```{eval-rst}\n.. currentmodule:: torch.distributed.tensor.placement_types\n```\n\nDTensor supports the following types of {class}`Placement` on each {class}`DeviceMesh` dimension:\n\n```{eval-rst}\n.. autoclass:: Shard\n  :members:\n  :undoc-members:\n```\n\n```{eval-rst}\n.. autoclass:: Replicate\n  :members:\n  :undoc-members:\n```\n\n```{eval-rst}\n.. autoclass:: Partial\n  :members:\n  :undoc-members:\n```\n\n```{eval-rst}\n.. autoclass:: Placement\n  :members:\n  :undoc-members:\n```\n\n(create_dtensor)=",
    "1488": "一级标题：torch.distributed.tensor\n二级标题：Different ways to create a DTensor\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributed.tensor\n```\n\nThere're three ways to construct a {class}`DTensor`:\n: - {meth}`distribute_tensor` creates a {class}`DTensor` from a logical or \"global\" `torch.Tensor` on\n    each rank. This could be used to shard the leaf `torch.Tensor` s (i.e. model parameters/buffers\n    and inputs).\n  - {meth}`DTensor.from_local` creates a {class}`DTensor` from a local `torch.Tensor` on each rank, which can\n    be used to create {class}`DTensor` from a non-leaf `torch.Tensor` s (i.e. intermediate activation\n    tensors during forward/backward).\n  - DTensor provides dedicated tensor factory functions (e.g. {meth}`empty`, {meth}`ones`, {meth}`randn`, etc.)\n    to allow different {class}`DTensor` creations by directly specifying the {class}`DeviceMesh` and\n    {class}`Placement`. Compare to {meth}`distribute_tensor`, this could directly materializing the sharded memory\n    on device, instead of performing sharding after initializing the logical Tensor memory.\n\n### Create DTensor from a logical torch.Tensor\n\nThe SPMD (single program, multiple data) programming model in `torch.distributed` launches multiple processes\n(i.e. via `torchrun`) to execute the same program, this means that the model inside the program would be\ninitialized on different processes first (i.e. the model might be initialized on CPU, or meta device, or directly\non GPU if enough memory).\n\n`DTensor` offers a {meth}`distribute_tensor` API that could shard the model weights or Tensors to `DTensor` s,\nwhere it would create a DTensor from the \"logical\" Tensor on each process. This would empower the created\n`DTensor` s to comply with the single device semantic, which is critical for **numerical correctness**.\n\n```{eval-rst}\n.. autofunction::  distribute_tensor\n```\n\nAlong with {meth}`distribute_tensor`, DTensor also offers a {meth}`distribute_module` API to allow easier\nsharding on the {class}`nn.Module` level\n\n```{eval-rst}\n.. autofunction::  distribute_module\n\n```\n\n### DTensor Factory Functions\n\nDTensor also provides dedicated tensor factory functions to allow creating {class}`DTensor` directly\nusing torch.Tensor like factory function APIs (i.e. torch.ones, torch.empty, etc), by additionally\nspecifying the {class}`DeviceMesh` and {class}`Placement` for the {class}`DTensor` created:\n\n```{eval-rst}\n.. autofunction:: zeros\n```\n\n```{eval-rst}\n.. autofunction:: ones\n```\n\n```{eval-rst}\n.. autofunction:: empty\n```\n\n```{eval-rst}\n.. autofunction:: full\n```\n\n```{eval-rst}\n.. autofunction:: rand\n```\n\n```{eval-rst}\n.. autofunction:: randn\n\n```",
    "1489": "一级标题：torch.distributed.tensor\n二级标题：Debugging\n内容：\n```{eval-rst}\n.. automodule:: torch.distributed.tensor.debug\n```\n\n```{eval-rst}\n.. currentmodule:: torch.distributed.tensor.debug\n```\n\n### Logging\n\nWhen launching the program, you can turn on additional logging using the `TORCH_LOGS` environment variable from\n[torch._logging](https://pytorch.org/docs/main/logging.html#module-torch._logging) :\n\n- `TORCH_LOGS=+dtensor` will display `logging.DEBUG` messages and all levels above it.\n- `TORCH_LOGS=dtensor` will display `logging.INFO` messages and above.\n- `TORCH_LOGS=-dtensor` will display `logging.WARNING` messages and above.\n\n### Debugging Tools\n\nTo debug the program that applied DTensor, and understand more details about what collectives happened under the\nhood, DTensor provides a {class}`CommDebugMode`:\n\n```{eval-rst}\n.. autoclass:: CommDebugMode\n    :members:\n    :undoc-members:\n```\n\nTo visualize the sharding of a DTensor that have less than 3 dimensions, DTensor provides {meth}`visualize_sharding`:\n\n```{eval-rst}\n.. autofunction:: visualize_sharding\n\n```",
    "1490": "一级标题：torch.distributed.tensor\n二级标题：Experimental Features\n内容：\n`DTensor` also provides a set of experimental features. These features are either in prototyping stage, or the basic\nfunctionality is done and but looking for user feedbacks. Please submit a issue to PyTorch if you have feedbacks to\nthese features.\n\n```{eval-rst}\n.. automodule:: torch.distributed.tensor.experimental\n```\n\n```{eval-rst}\n.. currentmodule:: torch.distributed.tensor.experimental\n```\n\n```{eval-rst}\n.. autofunction:: context_parallel\n```\n\n```{eval-rst}\n.. autofunction:: local_map\n```\n\n```{eval-rst}\n.. autofunction:: register_sharding\n\n```\n\n% modules that are missing docs, add the doc later when necessary\n\n```{eval-rst}\n.. py:module:: torch.distributed.tensor.device_mesh\n```",
    "1491": "一级标题：Tensor Parallelism - torch.distributed.tensor.parallel\n二级标题：无\n内容：\nTensor Parallelism(TP) is built on top of the PyTorch DistributedTensor\n(DTensor)[https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/README.md]\nand provides different parallelism styles: Colwise, Rowwise, and Sequence Parallelism.\n\n:::{warning}\nTensor Parallelism APIs are experimental and subject to change.\n:::\n\nThe entrypoint to parallelize your `nn.Module` using Tensor Parallelism is:\n\n```{eval-rst}\n.. automodule:: torch.distributed.tensor.parallel\n```\n\n```{eval-rst}\n.. currentmodule:: torch.distributed.tensor.parallel\n```\n\n```{eval-rst}\n.. autofunction::  parallelize_module\n```\n\nTensor Parallelism supports the following parallel styles:\n\n```{eval-rst}\n.. autoclass:: torch.distributed.tensor.parallel.ColwiseParallel\n  :members:\n  :undoc-members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.tensor.parallel.RowwiseParallel\n  :members:\n  :undoc-members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.tensor.parallel.SequenceParallel\n  :members:\n  :undoc-members:\n```\n\nTo simply configure the nn.Module's inputs and outputs with DTensor layouts\nand perform necessary layout redistributions, without distribute the module\nparameters to DTensors, the following `ParallelStyle` s can be used in\nthe `parallelize_plan` when calling `parallelize_module`:\n\n\n```{eval-rst}\n.. autoclass:: torch.distributed.tensor.parallel.PrepareModuleInput\n  :members:\n  :undoc-members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.tensor.parallel.PrepareModuleOutput\n  :members:\n  :undoc-members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.tensor.parallel.PrepareModuleInputOutput\n  :members:\n  :undoc-members:\n```\n\n:::{note}\nwhen using the `Shard(dim)` as the input/output layouts for the above\n`ParallelStyle` s, we assume the input/output activation tensors are evenly sharded on\nthe tensor dimension `dim` on the `DeviceMesh` that TP operates on. For instance,\nsince `RowwiseParallel` accepts input that is sharded on the last dimension, it assumes\nthe input tensor has already been evenly sharded on the last dimension. For the case of uneven sharded activation tensors, one could pass in DTensor directly to the partitioned modules, and use `use_local_output=False` to return DTensor after each `ParallelStyle`, where DTensor could track the uneven sharding information.\n:::\n\nFor models like Transformer, we recommend users to use `ColwiseParallel`\nand `RowwiseParallel` together in the parallelize_plan for achieve the desired\nsharding for the entire model (i.e. Attention and MLP).\n\nParallelized cross-entropy loss computation (loss parallelism), is supported via the following context manager:\n\n```{eval-rst}\n.. autofunction:: torch.distributed.tensor.parallel.loss_parallel\n```\n:::{warning}\n    The loss_parallel API is experimental and subject to change.\n:::",
    "1492": "一级标题：Probability distributions - torch.distributions\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.distributions\n```\n\n```{eval-rst}\n.. currentmodule:: torch.distributions\n```",
    "1493": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`Distribution`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.distribution\n```\n\n```{eval-rst}\n.. autoclass:: Distribution\n    :members:\n    :show-inheritance:\n```",
    "1494": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`ExponentialFamily`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.exp_family\n```\n\n```{eval-rst}\n.. autoclass:: ExponentialFamily\n    :members:\n    :show-inheritance:\n```",
    "1495": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`Bernoulli`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.bernoulli\n```\n\n```{eval-rst}\n.. autoclass:: Bernoulli\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1496": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`Beta`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.beta\n```\n\n```{eval-rst}\n.. autoclass:: Beta\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1497": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`Binomial`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.binomial\n```\n\n```{eval-rst}\n.. autoclass:: Binomial\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1498": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`Categorical`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.categorical\n```\n\n```{eval-rst}\n.. autoclass:: Categorical\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1499": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`Cauchy`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.cauchy\n```\n\n```{eval-rst}\n.. autoclass:: Cauchy\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1500": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`Chi2`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.chi2\n```\n\n```{eval-rst}\n.. autoclass:: Chi2\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1501": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`ContinuousBernoulli`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.continuous_bernoulli\n```\n\n```{eval-rst}\n.. autoclass:: ContinuousBernoulli\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1502": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`Dirichlet`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.dirichlet\n```\n\n```{eval-rst}\n.. autoclass:: Dirichlet\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1503": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`Exponential`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.exponential\n```\n\n```{eval-rst}\n.. autoclass:: Exponential\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1504": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`FisherSnedecor`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.fishersnedecor\n```\n\n```{eval-rst}\n.. autoclass:: FisherSnedecor\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1505": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`Gamma`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.gamma\n```\n\n```{eval-rst}\n.. autoclass:: Gamma\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1506": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`GeneralizedPareto`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.generalized_pareto\n```\n\n```{eval-rst}\n.. autoclass:: GeneralizedPareto\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1507": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`Geometric`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.geometric\n```\n\n```{eval-rst}\n.. autoclass:: Geometric\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1508": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`Gumbel`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.gumbel\n```\n\n```{eval-rst}\n.. autoclass:: Gumbel\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1509": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`HalfCauchy`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.half_cauchy\n```\n\n```{eval-rst}\n.. autoclass:: HalfCauchy\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1510": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`HalfNormal`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.half_normal\n```\n\n```{eval-rst}\n.. autoclass:: HalfNormal\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1511": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`Independent`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.independent\n```\n\n```{eval-rst}\n.. autoclass:: Independent\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1512": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`InverseGamma`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.inverse_gamma\n```\n\n```{eval-rst}\n.. autoclass:: InverseGamma\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1513": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`Kumaraswamy`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.kumaraswamy\n```\n\n```{eval-rst}\n.. autoclass:: Kumaraswamy\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1514": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`LKJCholesky`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.lkj_cholesky\n```\n\n```{eval-rst}\n.. autoclass:: LKJCholesky\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1515": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`Laplace`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.laplace\n```\n\n```{eval-rst}\n.. autoclass:: Laplace\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1516": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`LogNormal`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.log_normal\n```\n\n```{eval-rst}\n.. autoclass:: LogNormal\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1517": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`LowRankMultivariateNormal`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.lowrank_multivariate_normal\n```\n\n```{eval-rst}\n.. autoclass:: LowRankMultivariateNormal\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1518": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`MixtureSameFamily`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.mixture_same_family\n```\n\n```{eval-rst}\n.. autoclass:: MixtureSameFamily\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1519": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`Multinomial`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.multinomial\n```\n\n```{eval-rst}\n.. autoclass:: Multinomial\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1520": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`MultivariateNormal`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.multivariate_normal\n```\n\n```{eval-rst}\n.. autoclass:: MultivariateNormal\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1521": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`NegativeBinomial`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.negative_binomial\n```\n\n```{eval-rst}\n.. autoclass:: NegativeBinomial\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1522": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`Normal`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.normal\n```\n\n```{eval-rst}\n.. autoclass:: Normal\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1523": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`OneHotCategorical`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.one_hot_categorical\n```\n\n```{eval-rst}\n.. autoclass:: OneHotCategorical\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1524": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`Pareto`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.pareto\n```\n\n```{eval-rst}\n.. autoclass:: Pareto\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1525": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`Poisson`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.poisson\n```\n\n```{eval-rst}\n.. autoclass:: Poisson\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1526": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`RelaxedBernoulli`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.relaxed_bernoulli\n```\n\n```{eval-rst}\n.. autoclass:: RelaxedBernoulli\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1527": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`LogitRelaxedBernoulli`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.relaxed_bernoulli\n```\n\n```{eval-rst}\n.. autoclass:: LogitRelaxedBernoulli\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1528": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`RelaxedOneHotCategorical`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.relaxed_categorical\n```\n\n```{eval-rst}\n.. autoclass:: RelaxedOneHotCategorical\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1529": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`StudentT`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.studentT\n```\n\n```{eval-rst}\n.. autoclass:: StudentT\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1530": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`TransformedDistribution`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.transformed_distribution\n```\n\n```{eval-rst}\n.. autoclass:: TransformedDistribution\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1531": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`Uniform`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.uniform\n```\n\n```{eval-rst}\n.. autoclass:: Uniform\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1532": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`VonMises`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.von_mises\n```\n\n```{eval-rst}\n.. autoclass:: VonMises\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1533": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`Weibull`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.weibull\n```\n\n```{eval-rst}\n.. autoclass:: Weibull\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1534": "一级标题：Probability distributions - torch.distributions\n二级标题：{hidden}`Wishart`\n内容：\n```{eval-rst}\n.. currentmodule:: torch.distributions.wishart\n```\n\n```{eval-rst}\n.. autoclass:: Wishart\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```",
    "1535": "一级标题：Probability distributions - torch.distributions\n二级标题：`KL Divergence`\n内容：\n```{eval-rst}\n.. automodule:: torch.distributions.kl\n```\n\n```{eval-rst}\n.. currentmodule:: torch.distributions.kl\n```\n\n```{eval-rst}\n.. autofunction:: kl_divergence\n```\n\n```{eval-rst}\n.. autofunction:: register_kl\n```",
    "1536": "一级标题：Probability distributions - torch.distributions\n二级标题：`Transforms`\n内容：\n```{eval-rst}\n.. automodule:: torch.distributions.transforms\n    :members:\n    :member-order: bysource\n```",
    "1537": "一级标题：Probability distributions - torch.distributions\n二级标题：`Constraints`\n内容：\n```{eval-rst}\n.. automodule:: torch.distributions.constraints\n    :members:\n    :member-order: bysource\n```",
    "1538": "一级标题：Probability distributions - torch.distributions\n二级标题：`Constraint Registry`\n内容：\n```{eval-rst}\n.. automodule:: torch.distributions.constraint_registry\n    :members:\n    :member-order: bysource\n```\n\n% This module needs to be documented. Adding here in the meantime\n\n% for tracking purposes\n\n```{eval-rst}\n.. py:module:: torch.distributions.bernoulli\n\n.. py:module:: torch.distributions.beta\n\n.. py:module:: torch.distributions.binomial\n\n.. py:module:: torch.distributions.categorical\n\n.. py:module:: torch.distributions.cauchy\n\n.. py:module:: torch.distributions.chi2\n\n.. py:module:: torch.distributions.continuous_bernoulli\n\n.. py:module:: torch.distributions.dirichlet\n\n.. py:module:: torch.distributions.distribution\n\n.. py:module:: torch.distributions.exp_family\n\n.. py:module:: torch.distributions.exponential\n\n.. py:module:: torch.distributions.fishersnedecor\n\n.. py:module:: torch.distributions.gamma\n\n.. py:module:: torch.distributions.generalized_pareto\n\n.. py:module:: torch.distributions.geometric\n\n.. py:module:: torch.distributions.gumbel\n\n.. py:module:: torch.distributions.half_cauchy\n\n.. py:module:: torch.distributions.half_normal\n\n.. py:module:: torch.distributions.independent\n\n.. py:module:: torch.distributions.inverse_gamma\n\n.. py:module:: torch.distributions.kumaraswamy\n\n.. py:module:: torch.distributions.laplace\n\n.. py:module:: torch.distributions.lkj_cholesky\n\n.. py:module:: torch.distributions.log_normal\n\n.. py:module:: torch.distributions.logistic_normal\n\n.. py:module:: torch.distributions.lowrank_multivariate_normal\n\n.. py:module:: torch.distributions.mixture_same_family\n\n.. py:module:: torch.distributions.multinomial\n\n.. py:module:: torch.distributions.multivariate_normal\n\n.. py:module:: torch.distributions.negative_binomial\n\n.. py:module:: torch.distributions.normal\n\n.. py:module:: torch.distributions.one_hot_categorical\n\n.. py:module:: torch.distributions.pareto\n\n.. py:module:: torch.distributions.poisson\n\n.. py:module:: torch.distributions.relaxed_bernoulli\n\n.. py:module:: torch.distributions.relaxed_categorical\n\n.. py:module:: torch.distributions.studentT\n\n.. py:module:: torch.distributions.transformed_distribution\n\n.. py:module:: torch.distributions.uniform\n\n.. py:module:: torch.distributions.utils\n\n.. py:module:: torch.distributions.von_mises\n\n.. py:module:: torch.distributions.weibull\n\n.. py:module:: torch.distributions.wishart\n```",
    "1539": "一级标题：torch.utils.dlpack\n二级标题：无\n内容：\n```{eval-rst}\n.. currentmodule:: torch.utils.dlpack\n```\n\n```{eval-rst}\n.. autofunction:: from_dlpack\n```\n\n```{eval-rst}\n.. autofunction:: to_dlpack\n```",
    "1540": "一级标题：torch.export\n二级标题：无\n内容：",
    "1541": "一级标题：torch.export\n二级标题：Overview\n内容：\n{func}`torch.export.export` takes a {class}`torch.nn.Module` and produces a traced graph\nrepresenting only the Tensor computation of the function in an Ahead-of-Time\n(AOT) fashion, which can subsequently be executed with different outputs or\nserialized.\n\n```{code-cell}\nimport torch\nfrom torch.export import export, ExportedProgram\n\nclass Mod(torch.nn.Module):\n    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n        a = torch.sin(x)\n        b = torch.cos(y)\n        return a + b\n\nexample_args = (torch.randn(10, 10), torch.randn(10, 10))\n\nexported_program: ExportedProgram = export(Mod(), args=example_args)\nprint(exported_program)\n```\n\n`torch.export` produces a clean intermediate representation (IR) with the\nfollowing invariants. More specifications about the IR can be found\n{ref}`here <export.ir_spec>`.\n\n- **Soundness**: It is guaranteed to be a sound representation of the original\n  program, and maintains the same calling conventions of the original program.\n- **Normalized**: There are no Python semantics within the graph. Submodules\n  from the original programs are inlined to form one fully flattened\n  computational graph.\n- **Graph properties**: The graph is purely functional, meaning it does not\n  contain operations with side effects such as mutations or aliasing. It does\n  not mutate any intermediate values, parameters, or buffers.\n- **Metadata**: The graph contains metadata captured during tracing, such as a\n  stacktrace from user's code.\n\nUnder the hood, `torch.export` leverages the following latest technologies:\n\n- **TorchDynamo (torch._dynamo)** is an internal API that uses a CPython feature\n  called the Frame Evaluation API to safely trace PyTorch graphs. This\n  provides a massively improved graph capturing experience, with much fewer\n  rewrites needed in order to fully trace the PyTorch code.\n- **AOT Autograd** provides a functionalized PyTorch graph and ensures the graph\n  is decomposed/lowered to the ATen operator set.\n- **Torch FX (torch.fx)** is the underlying representation of the graph,\n  allowing flexible Python-based transformations.\n\n### Existing frameworks\n\n{func}`torch.compile` also utilizes the same PT2 stack as `torch.export`, but\nis slightly different:\n\n- **JIT vs. AOT**: {func}`torch.compile` is a JIT compiler whereas\n  which is not intended to be used to produce compiled artifacts outside of\n  deployment.\n- **Partial vs. Full Graph Capture**: When {func}`torch.compile` runs into an\n  untraceable part of a model, it will \"graph break\" and fall back to running\n  the program in the eager Python runtime. In comparison, `torch.export` aims\n  to get a full graph representation of a PyTorch model, so it will error out\n  when something untraceable is reached. Since `torch.export` produces a full\n  graph disjoint from any Python features or runtime, this graph can then be\n  saved, loaded, and run in different environments and languages.\n- **Usability tradeoff**: Since {func}`torch.compile` is able to fallback to the\n  Python runtime whenever it reaches something untraceable, it is a lot more\n  flexible. `torch.export` will instead require users to provide more\n  information or rewrite their code to make it traceable.\n\nCompared to {func}`torch.fx.symbolic_trace`, `torch.export` traces using\nTorchDynamo which operates at the Python bytecode level, giving it the ability\nto trace arbitrary Python constructs not limited by what Python operator\noverloading supports. Additionally, `torch.export` keeps fine-grained track of\ntensor metadata, so that conditionals on things like tensor shapes do not\nfail tracing. In general, `torch.export` is expected to work on more user\nprograms, and produce lower-level graphs (at the `torch.ops.aten` operator\nlevel). Note that users can still use {func}`torch.fx.symbolic_trace` as a\npreprocessing step before `torch.export`.\n\nCompared to {func}`torch.jit.script`, `torch.export` does not capture Python\ncontrol flow or data structures, unless using explicit {ref}`control flow operators <cond>`,\nbut it supports more Python language features due to its comprehensive coverage\nover Python bytecodes. The resulting graphs are simpler and only have straight\nline control flow, except for explicit control flow operators.\n\nCompared to {func}`torch.jit.trace`, `torch.export` is sound:\nit can trace code that performs integer computation on sizes and records\nall of the side-conditions necessary to ensure that a particular\ntrace is valid for other inputs.",
    "1542": "一级标题：torch.export\n二级标题：Exporting a PyTorch Model\n内容：\nThe main entrypoint is through {func}`torch.export.export`, which takes a\n{class}`torch.nn.Module` and sample inputs, and\ncaptures the computation graph into an {class}`torch.export.ExportedProgram`. An\nexample:\n\n```{code-cell}\nimport torch\nfrom torch.export import export, ExportedProgram\n\n# Simple module for demonstration\nclass M(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv = torch.nn.Conv2d(\n            in_channels=3, out_channels=16, kernel_size=3, padding=1\n        )\n        self.relu = torch.nn.ReLU()\n        self.maxpool = torch.nn.MaxPool2d(kernel_size=3)\n\n    def forward(self, x: torch.Tensor, *, constant=None) -> torch.Tensor:\n        a = self.conv(x)\n        a.add_(constant)\n        return self.maxpool(self.relu(a))\n\nexample_args = (torch.randn(1, 3, 256, 256),)\nexample_kwargs = {\"constant\": torch.ones(1, 16, 256, 256)}\n\nexported_program: ExportedProgram = export(\n    M(), args=example_args, kwargs=example_kwargs\n)\nprint(exported_program)\n\n# To run the exported program, we can use the `module()` method\nprint(exported_program.module()(torch.randn(1, 3, 256, 256), constant=torch.ones(1, 16, 256, 256)))\n```\n\nInspecting the `ExportedProgram`, we can note the following:\n\n- The {class}`torch.fx.Graph` contains the computation graph of the original\n  program, along with records of the original code for easy debugging.\n- The graph contains only `torch.ops.aten` operators found [here](https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/native_functions.yaml)\n  and custom operators.\n- The parameters (weight and bias to conv) are lifted as inputs to the graph,\n  resulting in no `get_attr` nodes in the graph, which previously existed in\n  the result of {func}`torch.fx.symbolic_trace`.\n- The {class}`torch.export.ExportGraphSignature` models the input and output\n  signature, along with specifying which inputs are parameters.\n- The resulting shape and dtype of tensors produced by each node in the graph is\n  noted. For example, the `conv2d` node will result in a tensor of dtype\n  `torch.float32` and shape (1, 16, 256, 256).",
    "1543": "一级标题：torch.export\n二级标题：Expressing Dynamism\n内容：\nBy default `torch.export` will trace the program assuming all input shapes are\n**static**, and specializing the exported program to those dimensions. One\nconsequence of this is that at runtime, the program won’t work on inputs with\ndifferent shapes, even if they’re valid in eager mode.\n\nAn example:\n\n```{code-cell}\nimport torch\nimport traceback as tb\n\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.branch1 = torch.nn.Sequential(\n            torch.nn.Linear(64, 32), torch.nn.ReLU()\n        )\n        self.branch2 = torch.nn.Sequential(\n            torch.nn.Linear(128, 64), torch.nn.ReLU()\n        )\n        self.buffer = torch.ones(32)\n\n    def forward(self, x1, x2):\n        out1 = self.branch1(x1)\n        out2 = self.branch2(x2)\n        return (out1 + self.buffer, out2)\n\nexample_args = (torch.randn(32, 64), torch.randn(32, 128))\n\nep = torch.export.export(M(), example_args)\nprint(ep)\n\nexample_args2 = (torch.randn(64, 64), torch.randn(64, 128))\ntry:\n    ep.module()(*example_args2)  # fails\nexcept Exception:\n    tb.print_exc()\n```\n\n\nHowever, some dimensions, such as a batch dimension, can be dynamic and vary\nfrom run to run. Such dimensions must be specified by using the\n{func}`torch.export.Dim()` API to create them and by passing them into\n{func}`torch.export.export()` through the `dynamic_shapes` argument.\n\n```{code-cell}\nimport torch\n\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.branch1 = torch.nn.Sequential(\n            torch.nn.Linear(64, 32), torch.nn.ReLU()\n        )\n        self.branch2 = torch.nn.Sequential(\n            torch.nn.Linear(128, 64), torch.nn.ReLU()\n        )\n        self.buffer = torch.ones(32)\n\n    def forward(self, x1, x2):\n        out1 = self.branch1(x1)\n        out2 = self.branch2(x2)\n        return (out1 + self.buffer, out2)\n\nexample_args = (torch.randn(32, 64), torch.randn(32, 128))\n\n# Create a dynamic batch size\nbatch = torch.export.Dim(\"batch\")\n# Specify that the first dimension of each input is that batch size\ndynamic_shapes = {\"x1\": {0: batch}, \"x2\": {0: batch}}\n\nep = torch.export.export(\n    M(), args=example_args, dynamic_shapes=dynamic_shapes\n)\nprint(ep)\n\nexample_args2 = (torch.randn(64, 64), torch.randn(64, 128))\nep.module()(*example_args2)  # success\n```\n\nSome additional things to note:\n\n- Through the {func}`torch.export.Dim` API and the `dynamic_shapes` argument, we specified the first\n  dimension of each input to be dynamic. Looking at the inputs `x1` and\n  `x2`, they have a symbolic shape of `(s0, 64)` and `(s0, 128)`, instead of\n  the `(32, 64)` and `(32, 128)` shaped tensors that we passed in as example inputs.\n  `s0` is a symbol representing that this dimension can be a range\n  of values.\n- `exported_program.range_constraints` describes the ranges of each symbol\n  appearing in the graph. In this case, we see that `s0` has the range\n  [0, int_oo]. For technical reasons that are difficult to explain here, they are\n  assumed to be not 0 or 1. This is not a bug, and does not necessarily mean\n  that the exported program will not work for dimensions 0 or 1. See\n  [The 0/1 Specialization Problem](https://docs.google.com/document/d/16VPOa3d-Liikf48teAOmxLc92rgvJdfosIy-yoT38Io/edit?fbclid=IwAR3HNwmmexcitV0pbZm_x1a4ykdXZ9th_eJWK-3hBtVgKnrkmemz6Pm5jRQ#heading=h.ez923tomjvyk)\n  for an in-depth discussion of this topic.\n\n\nIn the example, we used `Dim(\"batch\")` to create a dynamic dimension. This is\nthe most explicit way to specify dynamism. We can also use `Dim.DYNAMIC` and\n`Dim.AUTO` to specify dynamism. We will go over both methods in the next section.\n\n### Named Dims\n\nFor every dimension specified with `Dim(\"name\")`, we will allocate a symbolic\nshape. Specifying a `Dim` with the same name will result in the same symbol\nto be generated. This allows users to specify what symbols are allocated for\neach input dimension.\n\n```python\nbatch = Dim(\"batch\")\ndynamic_shapes = {\"x1\": {0: dim}, \"x2\": {0: batch}}\n```\n\nFor each `Dim`, we can specify minimum and maximum values. We also allow\nspecifying relations between `Dim`s in univariate linear expressions: `A * dim + B`.\nThis allows users to specify more complex constraints like integer divisibility\nfor dynamic dimensions. These features allow for users to place explicit\nrestrictions on the dynamic behavior of the `ExportedProgram` produced.\n\n```python\ndx = Dim(\"dx\", min=4, max=256)\ndh = Dim(\"dh\", max=512)\ndynamic_shapes = {\n    \"x\": (dx, None),\n    \"y\": (2 * dx, dh),\n}\n```\n\nHowever, `ConstraintViolationErrors` will be raised if the while tracing, we emit guards\nthat conflict with the relations or static/dynamic specifications given. For\nexample, in the above specification, the following is asserted:\n\n* `x.shape[0]` is to have range `[4, 256]`, and related to `y.shape[0]` by `y.shape[0] == 2 * x.shape[0]`.\n* `x.shape[1]` is static.\n* `y.shape[1]` has range `[0, 512]`, and is unrelated to any other dimension.\n\nIf any of these assertions are found to be incorrect while tracing (ex.\n`x.shape[0]` is static, or `y.shape[1]` has a smaller range, or\n`y.shape[0] != 2 * x.shape[0]`), then a `ConstraintViolationError` will be\nraised, and the user will need to change their `dynamic_shapes` specification.\n\n### Dim Hints\n\nInstead of explicitly specifying dynamism using `Dim(\"name\")`, we can let\n`torch.export` infer the ranges and relationships of the dynamic values using\n`Dim.DYNAMIC`. This is also a more convenient way to specify dynamism when you\ndon't know specifically *how* dynamic your dynamic values are.\n\n```python\ndynamic_shapes = {\n    \"x\": (Dim.DYNAMIC, None),\n    \"y\": (Dim.DYNAMIC, Dim.DYNAMIC),\n}\n```\n\nWe can also specify min/max values for `Dim.DYNAMIC`, which will serve as hints\nto export. But if while tracing export found the range to be different, it will\nautomatically update the range without raising an error. We also cannot specify\nrelationships between dynamic values. Instead, this will be inferred by export,\nand exposed to users through an inspection of assertions within the graph.  In\nthis method of specifying dynamism, `ConstraintViolationErrors` will **only** be\nraised if the specified value is inferred to be **static**.\n\nAn even more convenient way to specify dynamism is to use `Dim.AUTO`, which will\nbehave like `Dim.DYNAMIC`, but will **not** raise an error if the dimension is\ninferred to be static. This is useful for when you have no idea what the dynamic\nvalues are, and want to export the program with a \"best effort\" dynamic approach.\n\n### ShapesCollection\n\nWhen specifying which inputs are dynamic via `dynamic_shapes`, we must specify\nthe dynamism of every input. For example, given the following inputs:\n\n```python\nargs = {\"x\": tensor_x, \"others\": [tensor_y, tensor_z]}\n```\n\nwe would need to specify the dynamism of `tensor_x`, `tensor_y`, and `tensor_z`\nalong with the dynamic shapes:\n\n```python\n# With named-Dims\ndim = torch.export.Dim(...)\ndynamic_shapes = {\"x\": {0: dim, 1: dim + 1}, \"others\": [{0: dim * 2}, None]}\n\ntorch.export(..., args, dynamic_shapes=dynamic_shapes)\n```\n\nHowever, this is particularly complicated as we need to specify the\n`dynamic_shapes` specification in the same nested input structure as the input\narguments. Instead, an easier way to specify dynamic shapes is with the helper\nutility {class}`torch.export.ShapesCollection`, where instead of specifying the\ndynamism of every single input, we can just assign directly which input\ndimensions are dynamic.\n\n```{code-cell}\nimport torch\n\nclass M(torch.nn.Module):\n    def forward(self, inp):\n        x = inp[\"x\"] * 1\n        y = inp[\"others\"][0] * 2\n        z = inp[\"others\"][1] * 3\n        return x, y, z\n\ntensor_x = torch.randn(3, 4, 8)\ntensor_y = torch.randn(6)\ntensor_z = torch.randn(6)\nargs = {\"x\": tensor_x, \"others\": [tensor_y, tensor_z]}\n\ndim = torch.export.Dim(\"dim\")\nsc = torch.export.ShapesCollection()\nsc[tensor_x] = (dim, dim + 1, 8)\nsc[tensor_y] = {0: dim * 2}\n\nprint(sc.dynamic_shapes(M(), (args,)))\nep = torch.export.export(M(), (args,), dynamic_shapes=sc)\nprint(ep)\n```\n\n### AdditionalInputs\n\nIn the case where you don't know how dynamic your inputs are, but you have an\nample set of testing or profiling data that can provide a fair sense of\nrepresentative inputs for a model, you can use\n{class}`torch.export.AdditionalInputs` in place of `dynamic_shapes`. You can\nspecify all the possible inputs used to trace the program, and\n`AdditionalInputs` will infer which inputs are dynamic based on which input\nshapes are changing.\n\nExample:\n\n```{code-cell}\nimport dataclasses\nimport torch\nimport torch.utils._pytree as pytree\n\n@dataclasses.dataclass\nclass D:\n    b: bool\n    i: int\n    f: float\n    t: torch.Tensor\n\npytree.register_dataclass(D)\n\nclass M(torch.nn.Module):\n    def forward(self, d: D):\n        return d.i + d.f + d.t\n\ninput1 = (D(True, 3, 3.0, torch.ones(3)),)\ninput2 = (D(True, 4, 3.0, torch.ones(4)),)\nai = torch.export.AdditionalInputs()\nai.add(input1)\nai.add(input2)\n\nprint(ai.dynamic_shapes(M(), input1))\nep = torch.export.export(M(), input1, dynamic_shapes=ai)\nprint(ep)\n```",
    "1544": "一级标题：torch.export\n二级标题：Serialization\n内容：\nTo save the `ExportedProgram`, users can use the {func}`torch.export.save` and\n{func}`torch.export.load` APIs. The resulting file is a zipfile with a specific\nstructure. The details of the structure are defined in the\n{ref}`PT2 Archive Spec <export.pt2_archive>`.\n\nAn example:\n\n```python\nimport torch\n\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return x + 10\n\nexported_program = torch.export.export(MyModule(), (torch.randn(5),))\n\ntorch.export.save(exported_program, 'exported_program.pt2')\nsaved_exported_program = torch.export.load('exported_program.pt2')\n```\n\n(training-export)=",
    "1545": "一级标题：torch.export\n二级标题：Export IR, Decompositions\n内容：\nThe graph produced by `torch.export` returns a graph containing only\n[ATen operators](https://pytorch.org/cppdocs/#aten), which are the basic unit of\ncomputation in PyTorch. As there are over\n3000 ATen operators, export provides a way to narrow down the operator set used\nin the graph based on certain characteristics, creating different IRs.\n\nBy default, export produces the most generic IR which contains all ATen\noperators, including both functional and non-functional operators. A functional\noperator is one that does not contain any mutations or aliasing of the inputs.\nYou can find a list of all ATen operators\n[here](https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/native_functions.yaml)\nand you can inspect if an operator is functional by checking\n`op._schema.is_mutable`.\n\nThis generic IR can be used to train in eager PyTorch Autograd.\n\n```{code-cell}\nimport torch\n\nclass M(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 3, 1, 1)\n        self.bn = torch.nn.BatchNorm2d(3)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return (x,)\n\nep_for_training = torch.export.export(M(), (torch.randn(1, 1, 3, 3),))\nprint(ep_for_training.graph_module.print_readable(print_output=False))\n```\n\nHowever, if you want to use the IR for inference, or decrease the amount of\noperators being used, you can lower the graph through the\n{func}`ExportedProgram.run_decompositions` API. This method decomposes the\nATen operators into the ones specified in the decomposition table, and\nfunctionalizes the graph.\n\nBy specifying an empty set, we're only performing functionalization, and does\nnot do any additional decompositions. This results in an IR which contains ~2000\noperators (instead of the 3000 operators above), and is ideal for inference cases.\n\n```{code-cell}\nimport torch\n\nclass M(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 3, 1, 1)\n        self.bn = torch.nn.BatchNorm2d(3)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return (x,)\n\nep_for_training = torch.export.export(M(), (torch.randn(1, 1, 3, 3),))\nwith torch.no_grad():\n    ep_for_inference = ep_for_training.run_decompositions(decomp_table={})\nprint(ep_for_inference.graph_module.print_readable(print_output=False))\n```\n\nAs we can see, the previously in-place operator,\n`torch.ops.aten.add_.default` has now been replaced with\n`torch.ops.aten.add.default`, a functional operator.\n\nWe can also further lower this exported program to an operator set which only\ncontains the\n`Core ATen Operator Set <https://pytorch.org/docs/main/torch.compiler_ir.html#core-aten-ir>`__,\nwhich is a collection of only ~180 operators. This IR is optimal for backends\nwho do not want to reimplement all ATen operators.\n\n```{code-cell}\nimport torch\n\nclass M(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 3, 1, 1)\n        self.bn = torch.nn.BatchNorm2d(3)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return (x,)\n\nep_for_training = torch.export.export(M(), (torch.randn(1, 1, 3, 3),))\nwith torch.no_grad():\n    core_aten_ir = ep_for_training.run_decompositions(decomp_table=None)\nprint(core_aten_ir.graph_module.print_readable(print_output=False))\n```\n\nWe now see that `torch.ops.aten.conv2d.default` has been decomposed\ninto `torch.ops.aten.convolution.default`. This is because `convolution`\nis a more \"core\" operator, as operations like `conv1d` and `conv2d` can be\nimplemented using the same op.\n\nWe can also specify our own decomposition behaviors:\n\n```{code-cell}\nclass M(torch.nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        self.conv = torch.nn.Conv2d(1, 3, 1, 1)\n        self.bn = torch.nn.BatchNorm2d(3)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return (x,)\n\nep_for_training = torch.export.export(M(), (torch.randn(1, 1, 3, 3),))\n\nmy_decomp_table = torch.export.default_decompositions()\n\ndef my_awesome_custom_conv2d_function(x, weight, bias, stride=[1, 1], padding=[0, 0], dilation=[1, 1], groups=1):\n    return 2 * torch.ops.aten.convolution(x, weight, bias, stride, padding, dilation, False, [0, 0], groups)\n\nmy_decomp_table[torch.ops.aten.conv2d.default] = my_awesome_custom_conv2d_function\nmy_ep = ep_for_training.run_decompositions(my_decomp_table)\nprint(my_ep.graph_module.print_readable(print_output=False))\n```\n\nNotice that instead of `torch.ops.aten.conv2d.default` being decomposed\ninto `torch.ops.aten.convolution.default`, it is now decomposed into\n`torch.ops.aten.convolution.default` and `torch.ops.aten.mul.Tensor`,\nwhich matches our custom decomposition rule.\n\n(limitations-of-torch-export)=",
    "1546": "一级标题：torch.export\n二级标题：Limitations of torch.export\n内容：\nAs `torch.export` is a one-shot process for capturing a computation graph from\na PyTorch program, it might ultimately run into untraceable parts of programs as\nit is nearly impossible to support tracing all PyTorch and Python features. In\nthe case of `torch.compile`, an unsupported operation will cause a \"graph\nbreak\" and the unsupported operation will be run with default Python evaluation.\nIn contrast, `torch.export` will require users to provide additional\ninformation or rewrite parts of their code to make it traceable.\n\n{ref}`Draft-export <export.draft_export>` is a great resource for listing out\ngraphs breaks that will be encountered when tracing the program, along with\nadditional debug information to solve those errors.\n\n{ref}`ExportDB <torch.export_db>` is also great resource for learning about the\nkinds of programs that are supported and unsupported, along with ways to rewrite\nprograms to make them traceable.\n\n### TorchDynamo unsupported\n\nWhen using `torch.export` with `strict=True`, this will use TorchDynamo to\nevaluate the program at the Python bytecode level to trace the program into a\ngraph. Compared to previous tracing frameworks, there will be significantly\nfewer rewrites required to make a program traceable, but there will still be\nsome Python features that are unsupported. An option to get past dealing with\nthis graph breaks is by using\n{ref}`non-strict export <non-strict-export>` through changing the `strict` flag\nto `strict=False`.\n\n(data-shape-dependent-control-flow)=\n\n### Data/Shape-Dependent Control Flow\n\nGraph breaks can also be encountered on data-dependent control flow (`if\nx.shape[0] > 2`) when shapes are not being specialized, as a tracing compiler cannot\npossibly deal with without generating code for a combinatorially exploding\nnumber of paths. In such cases, users will need to rewrite their code using\nspecial control flow operators. Currently, we support {ref}`torch.cond <cond>`\nto express if-else like control flow (more coming soon!).\n\nYou can also refer to this\n[tutorial](https://docs.pytorch.org/tutorials/intermediate/torch_export_tutorial.html#data-dependent-errors)\nfor more ways of addressing data-dependent errors.\n\n### Missing Fake/Meta Kernels for Operators\n\nWhen tracing, a FakeTensor kernel (aka meta kernel) is required for all\noperators. This is used to reason about the input/output shapes for this\noperator.\n\nPlease see this [tutorial](https://docs.pytorch.org/tutorials/advanced/custom_ops_landing_page.html)\nfor more details.\n\nIn the unfortunate case where your model uses an ATen operator that is does not\nhave a FakeTensor kernel implementation yet, please file an issue.",
    "1547": "一级标题：torch.export\n二级标题：Read More\n内容：\n```{toctree}\n:caption: Additional Links for Export Users\n:maxdepth: 1\n\nexport/api_reference\nexport/programming_model\nexport/ir_spec\nexport/pt2_archive\nexport/draft_export\ncond\ngenerated/exportdb/index\ntorch.compiler_aot_inductor\ntorch.compiler_ir\n```\n\n```{toctree}\n:caption: Deep Dive for PyTorch Developers\n:maxdepth: 1\n\ntorch.compiler_dynamic_shapes\ntorch.compiler_fake_tensor\ntorch.compiler_transformations\n```",
    "1548": "一级标题：torch.export API Reference\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.export\n\n.. autofunction:: torch.export.export\n\n.. autoclass:: torch.export.ExportedProgram\n   :members:\n   :exclude-members: __init__\n\n.. automodule:: torch.export.dynamic_shapes\n   :members: Dim, ShapesCollection, AdditionalInputs, refine_dynamic_shapes_from_suggested_fixes\n\n.. autofunction:: torch.export.save\n\n.. autofunction:: torch.export.load\n\n.. autofunction:: torch.export.pt2_archive._package.package_pt2\n\n.. autofunction:: torch.export.pt2_archive._package.load_pt2\n\n.. autofunction:: torch.export.draft_export\n\n.. automodule:: torch.export.unflatten\n    :members:\n\n.. autofunction:: torch.export.register_dataclass\n\n.. automodule:: torch.export.decomp_utils\n   :members:\n   :ignore-module-all:\n   :undoc-members:\n\n.. automodule:: torch.export.experimental\n   :members:\n   :ignore-module-all:\n\n.. automodule:: torch.export.passes\n   :members:\n\n.. automodule:: torch.export.pt2_archive\n   :members:\n   :ignore-module-all:\n\n.. automodule:: torch.export.pt2_archive.constants\n   :members:\n   :ignore-module-all:\n\n.. automodule:: torch.export.exported_program\n   :members:\n   :ignore-module-all:\n   :exclude-members: ExportedProgram\n\n.. automodule:: torch.export.custom_ops\n   :members:\n   :ignore-module-all:\n\n.. automodule:: torch.export.custom_obj\n   :members:\n   :ignore-module-all:\n\n.. automodule:: torch.export.graph_signature\n   :members:\n   :ignore-module-all:\n   :undoc-members:\n```",
    "1549": "一级标题：Draft Export\n二级标题：无\n内容：\n:::{warning}\nThis feature is not meant to be used in production and is designed to be\nused as a tool for debugging torch.export tracing errors.\n:::\n\nDraft-export is a new version of export, which is designed to consistently\nproduce a graph, even if there are potential soundness issues, and to generate a\nreport listing out all of the issues export encountered during\ntracing and providing additional debugging information. For custom operators that\ndon't have fake kernels, it will also generate a profile which you can register\nto automatically generate a fake kernel.\n\nHave you ever tried to export a model using {func}`torch.export.export`, only to\nencounter a data-dependent issue? You fix it, but then run into a missing fake\nkernel problem. And after resolving that, you get hit with another\ndata-dependent issue. You wonder to yourself, I wish there was a way I could\njust get a graph to play around with, and be able to view all the issues in one\nplace so that I can fix them later…\n\n`draft_export` to the rescue!\n\n`draft_export` is a version of export which will always successfully export a\ngraph, even if there are potential soundness issues. These issues will then be\ncompiled into a report for clearer visualization, which can be fixed later on.",
    "1550": "一级标题：Draft Export\n二级标题：What sort of errors does it catch?\n内容：\nDraft-export helps to catch and debug the following errors:\n\n- Guard on data-dependent errors\n- Constraint violation errors\n- Missing fake kernels\n- Incorrectly written fake kernels",
    "1551": "一级标题：Draft Export\n二级标题：How does it work?\n内容：\nIn normal export, we will convert the sample inputs into FakeTensors and use\nthem to record operations and trace the program into a graph. Input tensor\nshapes that can change (which are marked through `dynamic_shapes`), or values\nwithin tensors (typically from an `.item()` call) will be represented as a symbolic\nshape (`SymInt`) instead of a concrete integer. However some issues may occur\nwhile tracing - we may run into guards that we cannot evaluate, like if we want\nto check if some item in a tensor is greater than 0 (`u0 >= 0`). Since the tracer\ndoesn't know anything about the value of `u0`, it will throw a data-dependent\nerror. If the model uses a custom operator but a fake kernel hasn't been\ndefined for it, then we will error with `fake_tensor.UnsupportedOperatorException`\nbecause export doesn't know how to apply this on `FakeTensors`. If a custom\noperator has a fake kernel implemented incorrectly, export will silently produce\nan incorrect graph that doesn't match the eager behavior.\n\nTo fix the above errors, draft-export uses *real tensor tracing* to guide us on\nhow to proceed when tracing. As we trace the model with fake tensors, for every\noperation that happens on a fake tensor, draft-export will also run the operator\non stored real tensors which come from the example inputs passed to export. This\nallows us to address the above errors: When we reach a guard that we cannot\nevaluate, like `u0 >= 0`, we will use the stored real tensor values to\nevaluate this guard. Runtime asserts will be added into the graph to ensure that\nthe graph asserts the same guard that we assumed while tracing. If we run into\na custom operator without a fake kernel, we will run the operator's normal\nkernel with the stored real tensors, and return a fake tensor with the same rank\nbut unbacked shapes. Since we have the real tensor output for every operation,\nwe will compare this with the fake tensor output from the fake kernel. If the\nfake kernel is implemented incorrectly, we will then catch this behavior and\ngenerate a more correct fake kernel.",
    "1552": "一级标题：Draft Export\n二级标题：How can I use draft export?\n内容：\nLet's say you're trying to export this piece of code:\n\n```python\nclass M(torch.nn.Module):\n    def forward(self, x, y, z):\n        res = torch.ops.mylib.foo2(x, y)\n\n        a = res.item()\n        a = -a\n        a = a // 3\n        a = a + 5\n\n        z = torch.cat([z, z])\n\n        torch._check_is_size(a)\n        torch._check(a < z.shape[0])\n\n        return z[:a]\n\ninp = (torch.tensor(3), torch.tensor(4), torch.ones(3, 3))\n\nep = torch.export.export(M(), inp)\n```\n\nThis runs into a “missing fake kernel” error for `mylib.foo2` and then a\n`GuardOnDataDependentExpression` because of the slicing of `z` with `a`,\nan unbacked symint.\n\nTo call `draft-export`, we can replace the `torch.export` line with the following:\n\n```python\nep = torch.export.draft_export(M(), inp)\n```\n\n`ep` is a valid ExportedProgram which can now be passed through further environments!",
    "1553": "一级标题：Draft Export\n二级标题：Debugging with draft-export\n内容：\nIn the terminal output from draft-export, you should see the following message:\n\n```",
    "1554": "#########\nWARNING: 2 issue(s) found during export, and it was not able to soundly produce a graph.\nTo view the report of failures in an html page, please run the command:\n    `tlparse /tmp/export_angelayi/dedicated_log_torch_trace_axpofwe2.log --export`\nOr, you can view the errors in python by inspecting `print(ep._report)`.",
    "1555": "########\n```\n\nDraft-export automatically dumps logs for `tlparse`. You can view the tracing\nerrors by using `print(ep._report)`, or you can pass the logs into `tlparse`\nto generate an html report.\n\nRunning the `tlparse` command in the terminal will generate a\n[tlparse](https://github.com/pytorch/tlparse)\nHTML report. Here is an example of the `tlparse` report:\n\n```{image} ../_static/img/export/draft_export_report.png\n```\n\nClicking into the Data Dependent Error, we will see the following page which\ncontains information to help debug this error. Specifically, it contains:\n\n- The stacktrace at which this error occurs\n- A list of local variables and their shapes\n- Information for how this guard was created\n\n```{image} ../_static/img/export/draft_export_report_dde.png\n```",
    "1556": "一级标题：Draft Export\n二级标题：The returned Exported Program\n内容：\nBecause draft-export specializes on code paths based on the example inputs, the\nexported program resulting from draft-export is guaranteed to be runnable and\nreturn correct results for **at least** the given example inputs. Other inputs can\nwork, as long as they match the same guards that were taken when we were\ndraft-exporting.\n\nFor example, if we have a graph branching on if a value is greater than 5, if in\ndraft-export our example inputs were greater than 5, then the returned\n`ExportedProgram` will specialize on that branch, and will assert that the value\nis greater than 5. This means that the program will succeed if you pass in\nanother value greater than 5, but will fail if you pass in a value less than 5.\nThis is more sound than `torch.jit.trace`, which will silently specialize on the\nbranch. The proper way for `torch.export` to support both branches would be to\nrewrite the code using `torch.cond`, which will then capture both branches.\n\nBecause of the runtime assertions in the graph, the returned exported-program is\nalso retraceable with `torch.export` or `torch.compile`, with a minor addition in\nthe case where a custom operator is missing a fake kernel.",
    "1557": "一级标题：Draft Export\n二级标题：Generating Fake Kernels\n内容：\nIf a custom operator does not contain a fake implementation, currently\ndraft-export will use the real-tensor propagation to get an output for the\noperator and continue tracing. However, if we run the exported program with fake\ntensors or retrace the exported model, we will still fail because there is still\nno fake kernel implementation.\n\nTo address this, after draft-export, we will generate an operator profile for\neach custom operator call that we encounter, and store this on the report\nattached to the exported program: `ep._report.op_profiles`. Users can then use the\ncontext manager `torch._library.fake_profile.unsafe_generate_fake_kernels` to\ngenerate and register a fake implementation based on these operator profiles.\nThis way future fake tensor retracing will work.\n\nThe workflow would look something like:\n\n```python\nclass M(torch.nn.Module):\n    def forward(self, a, b):\n        res = torch.ops.mylib.foo(a, b)  # no fake impl\n        return res\n\nep = draft_export(M(), (torch.ones(3, 4), torch.ones(3, 4)))\n\nwith torch._library.fake_profile.unsafe_generate_fake_kernels(ep._report.op_profiles):\n    decomp = ep.run_decompositions()\n\nnew_inp = (\n    torch.ones(2, 3, 4),\n    torch.ones(2, 3, 4),\n)\n\n# Save the profile to a yaml and check it into a codebase\nsave_op_profiles(ep._report.op_profiles, \"op_profile.yaml\")\n# Load the yaml\nloaded_op_profile = load_op_profiles(\"op_profile.yaml\")\n```\n\nThe operator profile is a dictionary mapping operator name to a set of profiles\nwhich describe the input and outputs of the operator, and could be manually\nwritten, saved into a yaml file, and checked into a codebase. Here's an example\nof a profile for `mylib.foo.default`:\n\n```python\n\"mylib.foo.default\": {\n    OpProfile(\n        args_profile=(\n            TensorMetadata(\n                rank=2,\n                dtype=torch.float32,\n                device=torch.device(\"cpu\"),\n                layout=torch.strided,\n            ),\n            TensorMetadata(\n                rank=2,\n                dtype=torch.float32,\n                device=torch.device(\"cpu\"),\n                layout=torch.strided,\n            ),\n        ),\n        out_profile=TensorMetadata(\n            rank=2,\n            dtype=torch.float32,\n            device=torch.device(\"cpu\"),\n            layout=torch.strided,\n        ),\n    )\n}\n```\n\n`mylib.foo.default`'s profile contains only one profile, which says that for 2\ninput tensors of rank 2, dtype `torch.float32`, device `cpu`, we will return\none tensor of rank 2, dtype `torch.float32`, and device `cpu`. Using the\ncontext manager, will then generate a fake kernel where given 2 input tensors of\nrank 2 (and the other tensor metadata), we will output one tensor of rank 2 (and\nthe other tensor metadata).\n\nIf the operator also supports other input ranks, then we can add the profile to\nthis list of profiles, either by manually adding it into the existing profile or\nrerunning draft-export with new inputs to get new profiles, so that the\ngenerated fake kernel will support more input types. Otherwise it will error.",
    "1558": "一级标题：Draft Export\n二级标题：Where to go from here?\n内容：\nNow that we have successfully created an `ExportedProgram` using draft-export,\nwe can use further compilers such as `AOTInductor` to optimize its performance\nand produce a runnable artifact. This optimized version can then be used for\ndeployment. In parallel, we can utilize the report generated by draft-export to\nidentify and fix `torch.export` errors that were encountered so that the\noriginal model can be directly traceable with `torch.export`.",
    "1559": "一级标题：torch.export IR Specification\n二级标题：无\n内容：\nExport IR is an intermediate representation (IR) for compilers, which bears\nsimilarities to [MLIR](https://mlir.llvm.org/) and TorchScript. It is specifically designed to express the\nsemantics of PyTorch programs. Export IR primarily represents computation in a\nstreamlined list of operations, with limited support for dynamism such as\ncontrol flows.\n\nTo create an Export IR graph, a frontend can be used that soundly captures a\nPyTorch program via a trace-specializing mechanism. The resulting Export IR can\nthen be optimized and executed by a backend. This can be done today through\n{func}`torch.export.export`.\n\nThe key concepts that will be covered in this document include:\n\n- ExportedProgram: the data structure containing the Export IR program\n- Graph: which consists of a list of nodes.\n- Nodes: which represents operations, control flow, and metadata stored on this node.\n- Values are produced and consumed by nodes.\n- Types are associated with values and nodes.\n- The size and memory layout of values are also defined.",
    "1560": "一级标题：torch.export IR Specification\n二级标题：Assumptions\n内容：\nThis doc assumes that the audience is sufficiently familiar with PyTorch,\nspecifically with {class}`torch.fx` and its related toolings. Thus it will stop\ndescribing contents present in {class}`torch.fx` documentation and paper.",
    "1561": "一级标题：torch.export IR Specification\n二级标题：What is Export IR\n内容：\nExport IR is a graph-based intermediate representation IR of PyTorch programs.\nExport IR is realized on top of {class}`torch.fx.Graph`. In other words, **all\nExport IR graphs are also valid FX graphs**, and if interpreted using standard\nFX semantics, Export IR can be interpreted soundly. One implication is that an\nexported graph can be converted to a valid Python program via standard FX\ncodegen.\n\nThis documentation will primarily focus on highlighting areas where Export IR\ndiffers from FX in terms of its strictness, while skipping parts where it shares\nsimilarities with FX.",
    "1562": "一级标题：torch.export IR Specification\n二级标题：ExportedProgram\n内容：\nThe top-level Export IR construct is an {class}`torch.export.ExportedProgram`\nclass. It bundles the computational graph of a PyTorch model (which is usually a\n{class}`torch.nn.Module`) with the parameters or weights that this model\nconsumes.\n\nSome notable attributes of the {class}`torch.export.ExportedProgram` class are:\n\n- `graph_module` ({class}`torch.fx.GraphModule`): Data structure containing\n  the flattened computational graph of the PyTorch model. The graph can be\n  directly accessed through `ExportedProgram.graph`.\n- `graph_signature` ({class}`torch.export.ExportGraphSignature`): The graph\n  signature, which specifies the parameters and buffer names used and mutated\n  within the graph. Instead of storing parameters and buffers as attributes of\n  the graph, they are lifted as inputs to the graph. The graph_signature is\n  utilized to keep track of additional information on these parameters and\n  buffers.\n- `state_dict` (`Dict[str, Union[torch.Tensor, torch.nn.Parameter]]`): Data\n  structure containing the parameters and buffers.\n- `range_constraints` (`Dict[sympy.Symbol, RangeConstraint]`): For programs\n  that are exported with data dependent behavior, the metadata on each node will\n  contain symbolic shapes (which look like `s0`, `i0`). This attribute maps\n  the symbolic shapes to their lower/upper ranges.",
    "1563": "一级标题：torch.export IR Specification\n二级标题：Graph\n内容：\nAn Export IR Graph is a PyTorch program represented in the form of a DAG\n(directed acyclic graph). Each node in this graph represents a particular\ncomputation or operation, and edges of this graph consist of references between\nnodes.\n\nWe can view Graph having this schema:\n\n```python\nclass Graph:\n  nodes: List[Node]\n```\n\nIn practice, Export IR's graph is realized as {class}`torch.fx.Graph` Python class.\n\nAn Export IR graph contains the following nodes (Nodes will be described in more\ndetails in the next section):\n\n- 0 or more nodes of op type `placeholder`\n- 0 or more nodes of op type `call_function`\n- exactly 1 node of op type `output`\n\n**Collorary:** The smallest valid Graph will be of one node. i.e. nodes is never empty.\n\n**Definition:**\nThe set of `placeholder` nodes of a Graph represents the **inputs** of the\nGraph of GraphModule. The `output` node of a Graph represents the **outputs**\nof the Graph of GraphModule.\n\nExample:\n\n```python\nimport torch\nfrom torch import nn\n\nclass MyModule(nn.Module):\n\n    def forward(self, x, y):\n      return x + y\n\nexample_args = (torch.randn(1), torch.randn(1))\nmod = torch.export.export(MyModule(), example_args)\nprint(mod.graph)\n```\n\n```python\ngraph():\n  %x : [num_users=1] = placeholder[target=x]\n  %y : [num_users=1] = placeholder[target=y]\n  %add : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%x, %y), kwargs = {})\n  return (add,)\n```\n\nThe above is the textual representation of a Graph, with each line being a node.",
    "1564": "一级标题：torch.export IR Specification\n二级标题：Node\n内容：\nA Node represents a particular computation or operation and is represented in\nPython using the {class}`torch.fx.Node` class. Edges between nodes are\nrepresented as direct references to other nodes via the `args` property of the\nNode class. Using the same FX machinery, we can represent the following\noperations that a computational graph typically needs, such as operator calls,\nplaceholders (aka inputs), conditionals, and loops.\n\nThe Node has the following schema:\n\n```python\nclass Node:\n  name: str # name of node\n  op_name: str  # type of operation\n\n  # interpretation of the fields below depends on op_name\n  target: [str|Callable]\n  args: List[object]\n  kwargs: Dict[str, object]\n  meta: Dict[str, object]\n```\n\n**FX Text Format**\n\nAs in the example above, notice that each line has this format:\n\n```\n%<name>:[...] = <op_name>[target=<target>](args = (%arg1, %arg2, arg3, arg4, …)), kwargs = {\"keyword\": arg5})\n```\n\nThis format captures everything present in the Node class, with the exception of\n`meta`, in a compact format.\n\nConcretely:\n\n- **<name>** is the name of the node as it would appear in `node.name`.\n- **<op_name>** is the `node.op` field, which must be one of these:\n  `<call_function>`, `<placeholder>`,\n  `<get_attr>`, or `<output>`.\n- **<target>** is the target of the node as `node.target`. The meaning of this\n  field depends on `op_name`.\n- **args1, … args 4…** are what is listed in the `node.args` tuple. If a\n  value in the list is an {class}`torch.fx.Node`, then it will be especially\n  indicated with a leading **%.**\n\nFor example, a call to the add operator would appear as:\n\n```\n%add1 = call_function[target = torch.op.aten.add.Tensor](args = (%x, %y), kwargs = {})\n```\n\nWhere `%x`, `%y` are two other Nodes that have names x and y. Worth noting\nthat the string `torch.op.aten.add.Tensor` represents the callable object that\nis actually stored in the target field, not merely its string name.\n\nThe final line of this text format is:\n\n```\nreturn [add]\n```\n\nwhich is a Node with `op_name = output`, indicating that we are returning this\none element.\n\n### call_function\n\nA `call_function` node represents a call to an operator.\n\n**Definitions**\n\n- **Functional:** We say a callable is “functional” if it satisfies all the\n  following requirements:\n\n  - Non-mutating: The operator does not mutate the value of its input (for\n    tensors, this includes both metadata and data).\n  - No side effects: The operator does not mutate states that are visible\n    from outside, like changing values of module parameters.\n\n- **Operator:** is a functional callable with a predefined schema. Examples of\n  such operators include functional ATen operators.\n\n**Representation in FX**\n\n```\n%name = call_function[target = operator](args = (%x, %y, …), kwargs = {})\n```\n\n**Differences from vanilla FX call_function**\n\n1. In FX graph, a call_function can refer to any callable, in Export IR, we\n   restrict it to only a select subset of ATen operators, custom operators, and\n   control flow operators.\n2. In Export IR, constant arguments will be embedded within the graph.\n3. In FX graph, a get_attr node can represent reading any attribute stored in\n   the graph module. However, in Export IR this is restricted to reading only\n   submodules as all parameters/buffers will be passed in as inputs to the graph\n   module.\n\n#### Metadata\n\n`Node.meta` is a dict attached to every FX node. However, the FX spec does not\nspecify what metadata can or will be there. Export IR provides a stronger\ncontract, specifically all `call_function` nodes will guarantee having and\nonly having the following metadata fields:\n\n- `node.meta[\"stack_trace\"]` is a string containing the Python stack trace\n  referencing the original Python source code. An example stack trace looks\n  like:\n\n  ```\n  File \"my_module.py\", line 19, in forward\n  return x + dummy_helper(y)\n  File \"helper_utility.py\", line 89, in dummy_helper\n  return y + 1\n  ```\n\n- `node.meta[\"val\"]` describes the output of running the operation. It can be\n  of type `<symint>`, `<FakeTensor>`, a\n  `List[Union[FakeTensor, SymInt]]`, or `None`.\n\n- `node.meta[\"nn_module_stack\"]` describes the \"stacktrace\" of the\n  {class}`torch.nn.Module` from which the node came, if it was from a\n  {class}`torch.nn.Module` call. For example, if a node containing the `addmm`\n  op called from a {class}`torch.nn.Linear` module inside of a\n  {class}`torch.nn.Sequential` module, the `nn_module_stack` would look\n  something like:\n\n  ```\n  {'self_linear': ('self.linear', <class 'torch.nn.Linear'>), 'self_sequential': ('self.sequential', <class 'torch.nn.Sequential'>)}\n  ```\n\n- `node.meta[\"source_fn_stack\"]` contains the torch function or the leaf\n  {class}`torch.nn.Module` class this node was called from before decomposition.\n  For example, a node containing the `addmm` op from a\n  {class}`torch.nn.Linear` module call would contain {class}`torch.nn.Linear` in\n  their `source_fn`, and a node containing the `addmm` op from a\n  {class}`torch.nn.functional.Linear` module call would contain\n  {class}`torch.nn.functional.Linear` in their `source_fn`.\n\n### placeholder\n\nPlaceholder represents an input to a graph. Its semantics are exactly the same as in FX.\nPlaceholder nodes must be the first N nodes in the nodes list of a graph. N can be zero.\n\n**Representation in FX**\n\n```python\n%name = placeholder[target = name](args = ())\n```\n\nThe target field is a string which is the name of input.\n\n`args`, if non-empty, should be of size 1 representing the default value of this input.\n\n**Metadata**\n\nPlaceholder nodes also have `meta[‘val’]`, like `call_function` nodes. The\n`val` field in this case represents the input shape/dtype that the graph is\nexpected to receive for this input parameter.\n\n### output\n\nAn output call represents a return statement in a function; it thus terminates the\ncurrent graph. There is one and only one output node, and it will always be the\nlast node of the graph.\n\n**Representation in FX**\n\n```\noutput[](args = (%something, …))\n```\n\nThis has the exact semantics as in {class}`torch.fx`. `args` represents the node\nto be returned.\n\n**Metadata**\n\nOutput node has the same metadata as `call_function` nodes.\n\n### get_attr\n\n`get_attr` nodes represent reading a submodule from the encapsulating\n{class}`torch.fx.GraphModule`. Unlike a vanilla FX graph from\n{func}`torch.fx.symbolic_trace` in which `get_attr` nodes are used to read\nattributes such as parameters and buffers from the top-level\n{class}`torch.fx.GraphModule`, parameters and buffers are passed in as\ninputs to the graph module, and stored in the top-level\n{class}`torch.export.ExportedProgram`.\n\n**Representation in FX**\n\n```python\n%name = get_attr[target = name](args = ())\n```\n\n**Example**\n\nConsider the following model:\n\n```python\nfrom functorch.experimental.control_flow import cond\n\ndef true_fn(x):\n    return x.sin()\n\ndef false_fn(x):\n    return x.cos()\n\ndef f(x, y):\n    return cond(y, true_fn, false_fn, [x])\n```\n\nGraph:\n\n```\ngraph():\n    %x_1 : [num_users=1] = placeholder[target=x_1]\n    %y_1 : [num_users=1] = placeholder[target=y_1]\n    %true_graph_0 : [num_users=1] = get_attr[target=true_graph_0]\n    %false_graph_0 : [num_users=1] = get_attr[target=false_graph_0]\n    %conditional : [num_users=1] = call_function[target=torch.ops.higher_order.cond](args = (%y_1, %true_graph_0, %false_graph_0, [%x_1]), kwargs = {})\n    return conditional\n```\n\nThe line, `%true_graph_0 : [num_users=1] = get_attr[target=true_graph_0]`,\nreads the submodule `true_graph_0` which contains the `sin` operator.",
    "1565": "一级标题：torch.export IR Specification\n二级标题：References\n内容：\n### SymInt\n\nA SymInt is an object that can either be a literal integer or a symbol that represents\nan Integer (represented in Python by `sympy.Symbol` class). When SymInt is a\nsymbol, it describes a variable of type integer that is unknown to the graph at\ncompile time, that is, its value is only known at runtime.\n\n### FakeTensor\n\nA FakeTensor is an object that contains the metadata of a tensor. It can be\nviewed as having the following metadata.\n\n```python\nclass FakeTensor:\n  size: List[SymInt]\n  dtype: torch.dtype\n  device: torch.device\n  dim_order: List[int]  # This doesn't exist yet\n```\n\nThe size field of FakeTensor is a list of integers or SymInts. If SymInts are\npresent, this means this tensor has a dynamic shape. If integers are present, it\nis assumed that the tensor will have that exact static shape. The rank of the\nTensorMeta is never dynamic. The dtype field represents the dtype of the\noutput of that node. There are no implicit type promotions in Edge IR. There\nare no strides in FakeTensor.\n\nIn other words:\n\n- If the operator in node.target returns a Tensor, then `node.meta['val']` is a\n  FakeTensor describing that tensor.\n- If the operator in node.target returns an n-tuple of Tensors, then\n  `node.meta['val']` is an n-tuple of FakeTensors describing each tensor.\n- If the operator in node.target returns an int/float/scalar that is known at\n  compile time, then `node.meta['val']` is None.\n- If the operator in node.target returns an int/float/scalar that is not known\n  at compile time, then `node.meta['val']` is of type SymInt.\n\nFor example:\n\n- `aten::add` returns a Tensor; so its spec will be a FakeTensor with dtype\n  and size of the tensor returned by this operator.\n- `aten::sym_size` returns an integer; so its val will be a SymInt because its\n  value is only available at runtime.\n- `max_pool2d_with_indexes` returns a tuple of (Tensor, Tensor); so the spec\n  will also be a 2-tuple of FakeTensor objects, the first TensorMeta describes\n  the first element of the return value etc.\n\nPython code:\n\n```python\ndef add_one(x):\n  return torch.ops.aten(x, 1)\n```\n\nGraph:\n\n```\ngraph():\n  %ph_0 : [#users=1] = placeholder[target=ph_0]\n  %add_tensor : [#users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%ph_0, 1), kwargs = {})\n  return [add_tensor]\n```\n\nFakeTensor:\n\n```python\nFakeTensor(dtype=torch.int, size=[2,], device=CPU)\n```\n\n### Pytree-able Types\n\nWe define a type “Pytree-able”, if it is either a leaf type or a container type\nthat contains other Pytree-able types.\n\nNote:\n\n> The concept of pytree is the same as the one documented\n> [here](https://jax.readthedocs.io/en/latest/pytrees.html) for JAX:\n\nThe following types are defined as **leaf type**:\n\n```{eval-rst}\n.. list-table::\n   :widths: 50 50\n   :header-rows: 1\n\n   * - Type\n     - Definition\n   * - Tensor\n     - :class:`torch.Tensor`\n   * - Scalar\n     - Any numerical types from Python, including integral types, floating point types, and zero dimensional tensors.\n   * - int\n     - Python int (bound as int64_t in C++)\n   * - float\n     - Python float (bound as double in C++)\n   * - bool\n     - Python bool\n   * - str\n     - Python string\n   * - ScalarType\n     - :class:`torch.dtype`\n   * - Layout\n     - :class:`torch.layout`\n   * - MemoryFormat\n     - :class:`torch.memory_format`\n   * - Device\n     - :class:`torch.device`\n```\n\nThe following types are defined as **container type**:\n\n```{eval-rst}\n.. list-table::\n   :widths: 50 50\n   :header-rows: 1\n\n   * - Type\n     - Definition\n   * - Tuple\n     - Python tuple\n   * - List\n     - Python list\n   * - Dict\n     - Python dict with Scalar keys\n   * - NamedTuple\n     - Python namedtuple\n   * - Dataclass\n     - Must be registered through `register_dataclass <https://github.com/pytorch/pytorch/blob/901aa85b58e8f490631ce1db44e6555869a31893/torch/export/__init__.py#L693>`__\n   * - Custom class\n     - Any custom class defined with `_register_pytree_node <https://github.com/pytorch/pytorch/blob/901aa85b58e8f490631ce1db44e6555869a31893/torch/utils/_pytree.py#L72>`__\n```",
    "1566": "一级标题：torch.export Programming Model\n二级标题：无\n内容：\nThis document aims to explain the behaviors and capabilities of\n{func}`torch.export.export`. It is intended to help build your intuition\nfor how {func}`torch.export.export` handles code.",
    "1567": "一级标题：torch.export Programming Model\n二级标题：Basics of Tracing\n内容：\n{func}`torch.export.export` captures a graph representing your model by\ntracing its execution on \"example\" inputs and recording the PyTorch operations\nand conditions observed along the traced path. This graph can then be run\non different inputs as long as they satisfy the same conditions.\n\nThe basic output of {func}`torch.export.export` is a single graph of PyTorch\noperations, with associated metadata. The exact format of this output is\ncovered in the {ref}`export IR spec <export.ir_spec>`.\n\n(non-strict-export)=\n\n### Strict vs. Non-Strict Tracing\n\n{func}`torch.export.export` provides two modes of tracing.\n\nIn *non-strict mode*, we trace through the program using the normal Python\ninterpreter. Your code executes exactly as it would in eager mode; the only\ndifference is that all Tensors are replaced by\n[fake Tensors](https://pytorch.org/docs/main/torch.compiler_fake_tensor.html),\n**which have shapes and other forms of metadata but no data**, wrapped in\n[Proxy objects](https://pytorch.org/docs/main/fx.html) that record all\noperations on them into a graph. We also capture\n[conditions on Tensor shapes](https://pytorch.org/docs/main/torch.compiler_dynamic_shapes.html#the-guard-model)\n**that guard the correctness of the generated code**.\n\nIn *strict mode*, we first trace through the program using\n{ref}`TorchDynamo <torch.compiler_dynamo_deepdive>`, a Python bytecode\nanalysis engine. TorchDynamo does not actually execute your Python code.\nInstead, it symbolically analyzes it and builds a graph based on the results.\nOn the one hand, this analysis allows {func}`torch.export.export` to provide\nadditional guarantees on Python-level safety (beyond capturing conditions on\nTensor shapes, as in non-strict mode). On the other hand, not all Python\nfeatures are supported by this analysis.\n\nAlthough currently the default mode of tracing is strict, **we strongly\nrecommend using non-strict**, which will soon become the default.\nFor most models, conditions on Tensor shapes are enough for soundness, and\nthe additional guarantees on Python-level safety have no impact; at the same\ntime, the possibility of hitting unsupported Python features in TorchDynamo\npresents an unnecessary risk.\n\nIn the rest of this document we assume we are tracing in\n[non-strict mode](https://pytorch.org/docs/main/export.html#non-strict-export);\nin particular, we assume that **all Python features are supported**.",
    "1568": "一级标题：torch.export Programming Model\n二级标题：Values: Static vs. Dynamic\n内容：\nA key concept in understanding the behavior of {func}`torch.export.export` is\nthe difference between *static* and *dynamic* values.\n\n### Static Values\n\nA *static* value is a value that is **fixed at export time and cannot change\nbetween executions of the exported program**. When the value is encountered\nduring tracing, we treat it as a constant and hard-code it into the graph.\n\nWhen an operation is performed (e.g. `x + y`) and all inputs are static,\nthe output of the operation is directly hard-coded into the graph and the\noperation does not show up (i.e. it gets \"constant-folded\").\n\nWhen a value has been hard-coded into the graph, we say that the graph has\nbeen *specialized* to that value. For example:\n\n```python\nimport torch\n\nclass MyMod(torch.nn.Module):\n    def forward(self, x, y):\n        z = y + 7\n        return x + z\n\nm = torch.export.export(MyMod(), (torch.randn(1), 3))\nprint(m.graph_module.code)\n\n\"\"\"\ndef forward(self, arg0_1, arg1_1):\n    add = torch.ops.aten.add.Tensor(arg0_1, 10);  arg0_1 = None\n    return (add,)\n\n\"\"\"\n```\n\nHere, we provide `3` as the traced value for `y`; it is treated as a static\nvalue and added to `7`, burning in the static value `10` in the graph.\n\n### Dynamic Values\n\nA *dynamic* value is one that **can change from run to run**. It behaves just\nlike a \"normal\" function argument: you can pass different inputs and expect\nyour function to do the right thing.\n\n### Which values are static vs. dynamic?\n\nWhether a value is static or dynamic depends on its type:\n\n- For Tensor:\n\n  - Tensor *data* is treated as dynamic.\n\n  - Tensor *shapes* can be treated by the system as static or dynamic.\n\n    - By default, shapes of all input Tensors are considered static.\n      The user can override this behavior for any input Tensor by specifying\n      a [dynamic shape](https://pytorch.org/docs/main/export.html#expressing-dynamism)\n      for it.\n    - Tensors that are part of module state, i.e., parameters and buffers,\n      always have static shapes.\n\n  - Other forms of Tensor *metadata* (e.g. `device`, `dtype`) are static.\n\n- Python *primitives* (`int`, `float`, `bool`, `str`, `None`) are static.\n\n  - There are dynamic variants for some primitive types (`SymInt`,\n    `SymFloat`, `SymBool`). Typically users do not have to deal with them.\n  - Users can specify integer inputs as dynamic by specifying\n    a [dynamic shape](https://pytorch.org/docs/main/export.html#expressing-dynamism)\n    for it.\n\n- For Python *standard containers* (`list`, `tuple`, `dict`, `namedtuple`):\n\n  - The structure (i.e., length for `list` and `tuple` values, and key\n    sequence for `dict` and `namedtuple` values) is static.\n  - The contained elements have these rules applied to them recursively\n    (basically the\n    [PyTree](https://jax.readthedocs.io/en/latest/pytrees.html) scheme)\n    with leaves that are either Tensor or primitive types.\n\n- Other *classes* (including data classes) can be registered with PyTree\n  (see below), and follow the same rules as the standard containers.",
    "1569": "一级标题：torch.export Programming Model\n二级标题：Input types\n内容：\nInputs will be treated as either static or dynamic, based on their type\n(as explained above).\n\n- A static input will get hard-coded into the graph, and passing a different\n  value at run time will result in an error. Recall that these are mostly\n  values of primitive types.\n- A dynamic input behaves like a \"normal\" function input. Recall that these\n  are mostly values of Tensor types.\n\nBy default, the types of inputs you can use for your program are:\n\n- Tensor\n- Python primitives (`int`, `float`, `bool`, `str`, `None`)\n- Python standard containers (`list`, `tuple`, `dict`, `namedtuple`)\n\n### Custom Input Types (PyTree)\n\nIn addition, you can also define your own (custom) class and use it as an\ninput type, but you will need to register such a class as a PyTree.\n\nHere's an example of using an utility to register a dataclass that is used as\nan input type.\n\n```python\n@dataclass\nclass Input:\n    f: torch.Tensor\n    p: torch.Tensor\n\nimport torch.utils._pytree as pytree\npytree.register_dataclass(Input)\n\nclass M(torch.nn.Module):\n    def forward(self, x: Input):\n        return x.f + 1\n\ntorch.export.export(M(), (Input(f=torch.ones(10, 4), p=torch.zeros(10, 4)),))\n```\n\n### Optional input types\n\nFor optional inputs to the program that are not passed in,\n{func}`torch.export.export` will specialize to their default values. As a\nresult, the exported program will require users to explicitly pass in all\narguments, and will lose the defaulting behavior. For example:\n\n```python\nclass M(torch.nn.Module):\n    def forward(self, x, y=None):\n        if y is not None:\n            return y * x\n        return x + x\n\n# Optional input is passed in\nep = torch.export.export(M(), (torch.randn(3, 3), torch.randn(3, 3)))\nprint(ep)\n\"\"\"\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 3]\", y: \"f32[3, 3]\"):\n            # File: /data/users/angelayi/pytorch/moo.py:15 in forward, code: return y * x\n            mul: \"f32[3, 3]\" = torch.ops.aten.mul.Tensor(y, x);  y = x = None\n            return (mul,)\n\"\"\"\n\n# Optional input is not passed in\nep = torch.export.export(M(), (torch.randn(3, 3),))\nprint(ep)\n\"\"\"\nExportedProgram:\n    class GraphModule(torch.nn.Module):\n        def forward(self, x: \"f32[3, 3]\", y):\n            # File: /data/users/angelayi/pytorch/moo.py:16 in forward, code: return x + x\n            add: \"f32[3, 3]\" = torch.ops.aten.add.Tensor(x, x);  x = None\n            return (add,)\n\"\"\"\n```",
    "1570": "一级标题：torch.export Programming Model\n二级标题：Control Flow: Static vs. Dynamic\n内容：\nControl flow is supported by {func}`torch.export.export`. The behavior of\ncontrol flow depends on whether the value you are branching on is static or\ndynamic.\n\n### Static Control Flow\n\n**Python control flow over static values is supported transparently**. (Recall\nthat static values include static shapes, so control flow over static shapes\nis also covered by this case.)\n\nAs mentioned above, we \"burn in\" static values, so the exported graph will\nnever see any control flow over static values.\n\nIn the case of an `if` statement, we will continue tracing the branch taken\nat export time. In the case of a `for` or `while` statement, we will continue\ntracing by unrolling the loop.\n\n### Dynamic Control Flow: Shape-Dependent vs. Data-Dependent\n\nWhen the value involved in a control flow is dynamic, it could depend on\ndynamic shapes or dynamic data. Given that the compiler traces with\ninformation on shapes rather than data, the implications on the programming\nmodel are different in these cases.\n\n#### Dynamic Shape-Dependent Control Flow\n\nWhen the value involved in a control flow is a\n[dynamic shape](https://pytorch.org/docs/main/torch.compiler_dynamic_shapes.html),\nin most cases **we will also know the concrete value of the dynamic shape\nduring tracing**: see the following section for more details on how the\ncompiler tracks this information.\n\nIn these cases we say that the control flow is shape-dependent. **We use the\nconcrete value of the dynamic shape to evaluate the condition** to either\n`True` or `False` and continue tracing (as discussed above), additionally\nemitting a guard corresponding to the condition just evaluated.\n\nOtherwise the control flow is considered data-dependent. We cannot evaluate\nthe condition to either `True` or `False`, so cannot continue tracing and have to\nraise an error at export time. See next section.\n\n#### Dynamic Data-Dependent Control Flow\n\n**Data-dependent control flow over dynamic values is supported, but you must\nuse one of PyTorch's explicit operators** to continue tracing. Using Python\ncontrol flow statements over dynamic values is not permitted, because the\ncompiler cannot evaluate the conditions necessary to continue tracing and\nthus an error must be raised at export time.\n\nWe provide **operators to express general conditionals and loops over dynamic\nvalues**, e.g., `torch.cond`, `torch.map`. Note that you only need to use these\nif you truly want *data-dependent control flow*.\n\nHere's an example of an `if` statement on a data-dependent condition,\n`x.sum() > 0`, where `x` is an input Tensor, rewritten using `torch.cond`.\nInstead of having to decide which branch to trace, now both branches are\ntraced.\n\n```python\nclass M_old(torch.nn.Module):\n    def forward(self, x):\n        if x.sum() > 0:\n            return x.sin()\n        else:\n            return x.cos()\n\nclass M_new(torch.nn.Module):\n    def forward(self, x):\n        return torch.cond(\n            pred=x.sum() > 0,\n            true_fn=lambda x: x.sin(),\n            false_fn=lambda x: x.cos(),\n            operands=(x,),\n        )\n```\n\nA special case of data-dependent control flow is where it involves a\n[data-dependent dynamic shape](https://pytorch.org/docs/main/torch.compiler_dynamic_shapes.html#unbacked-symints):\ntypically, the shape of some intermediate Tensor that depends on input data\nrather than on input shapes (thus not shape-dependent). Instead of using a\ncontrol flow operator, in this case you can provide an assertion that decides\nwhether the condition is `True` or `False`. Given such an assertion, we can\ncontinue tracing, emitting a guard as above.\n\nWe provide **operators to express assertions on dynamic shapes**, e.g.,\n`torch._check`. Note that you only need to use this when there is control\nflow on data-dependent dynamic shapes.\n\nHere's an example of an `if` statement on a condition involving a\ndata-dependent dynamic shape, `nz.shape[0] > 0`, where `nz` is the result of\ncalling {func}`torch.nonzero`, an operator whose output shape depends on input\ndata. Instead of rewriting it, you can add an assertion using `torch._check`\nto effectively decide which branch to trace.\n\n```python\nclass M_old(torch.nn.Module):\n    def forward(self, x):\n        nz = x.nonzero()\n        if nz.shape[0] > 0:\n            return x.sin()\n        else:\n            return x.cos()\n\nclass M_new(torch.nn.Module):\n    def forward(self, x):\n        nz = x.nonzero()\n        torch._check(nz.shape[0] > 0)\n        if nz.shape[0] > 0:\n            return x.sin()\n        else:\n            return x.cos()\n```",
    "1571": "一级标题：torch.export Programming Model\n二级标题：Basics of Symbolic Shapes\n内容：\nDuring tracing, dynamic Tensor shapes and conditions over them are encoded as\n\"symbolic expressions.\" (In contrast, static Tensor shapes and conditions\nover them are simply `int` and `bool` values.)\n\nA *symbol* is like a variable; it describes a dynamic Tensor shape.\n\nAs tracing proceeds, shapes of intermediate Tensors may be described by more\ngeneral expressions, typically involving integer arithmetic operators. This\nis because **for most PyTorch operators, shapes of output Tensors can be\ndescribed as functions of shapes of input Tensors**. For example, the shape of\nthe output of {func}`torch.cat` is the sum of the shapes of its inputs.\n\nMoreover, as we encounter control flow in the program, we create boolean\nexpressions, typically involving relational operators, describing conditions\nalong the traced path. These **expressions are evaluated to decide which path\nto trace through the program**, and recorded in a\n[shape environment](https://pytorch.org/docs/main/torch.compiler_dynamic_shapes.html#overall-architecture)\nto guard the correctness of the traced path and to evaluate subsequently\ncreated expressions.\n\nWe briefly introduce these subsystems next.\n\n### Fake Implementations of PyTorch Operators\n\nRecall that during tracing, we are executing the program with\n[fake Tensors](https://pytorch.org/docs/main/torch.compiler_fake_tensor.html),\nwhich have no data. In general we cannot call the actual implementations of\nPyTorch operators with fake Tensors. Thus each operator needs to have an\nadditional fake (a.k.a. \"meta\") implementation, which inputs and outputs fake\nTensors, that matches the behavior of the actual implementation in terms of\nshapes and other forms of metadata carried by fake Tensors.\n\nFor example, note how the fake implementation of {func}`torch.index_select`\ncomputes the shape of the output using the shape of the input (while ignoring\ninput data and returning empty output data).\n\n```python\ndef meta_index_select(self, dim, index):\n    result_size = list(self.size())\n    if self.dim() > 0:\n        result_size[dim] = index.numel()\n    return self.new_empty(result_size)\n```\n\n#### Shape Propagation: Backed vs. Unbacked Dynamic Shapes\n\nShapes are propagated using fake implementations of PyTorch operators.\n\nA key concept to understand the propagation of dynamic shapes in particular\nis the difference between *backed* and *unbacked* dynamic shapes: we know the\nconcrete values of the former but not the latter.\n\nPropagation of shapes, including tracking backed and unbacked dynamic shapes,\nproceeds as follows:\n\n- The shapes of Tensors representing inputs can be static or dynamic. When\n  dynamic, they are described by symbols; moreover, **such symbols are backed\n  since we also know their concrete values given the \"real\" example inputs\n  provided by the user at export time**.\n\n- The output shape of an operator is computed by its fake implementation, and\n  is either static or dynamic. When dynamic, in general it is described by a\n  symbolic expression. Moreover:\n\n  - If the output shape depends only on input shapes, it is either static or\n    backed dynamic whenever the input shapes are all static or backed dynamic.\n  - On the other hand, **if the output shape depends on input data**, it is\n    necessarily dynamic, and moreover, **because we cannot know its concrete\n    value it is unbacked**.\n\n### Control Flow: Guards and Assertions\n\nWhen a condition on shapes is encountered, it either involves only static\nshapes, in which case it is a `bool`, or it involves dynamic shapes, in which\ncase it is a symbolic boolean expression. For the latter:\n\n- When the condition involves only backed dynamic shapes, we can use the\n  concrete values of those dynamic shapes to evaluate the condition to `True`\n  or `False`. We can then add a guard to the shape environment that states\n  that the corresponding symbolic boolean expression is `True` or `False`,\n  and continue tracing.\n- Otherwise the condition involves unbacked dynamic shapes. In general we\n  cannot evaluate such a condition without additional information; thus we\n  cannot continue tracing, and we must raise an error at export time. The\n  user is expected to use an explicit PyTorch operator for tracing to\n  continue. This information is added as a guard in the shape environment,\n  and can also possibly help evaluate other subsequently encountered\n  conditions to `True` or `False`.\n\nOnce the model is exported, **any guards on backed dynamic shapes can be\nunderstood as conditions on input dynamic shapes**. These are verified against\na dynamic shape specification that must have been provided to export,\ndescribing conditions on dynamic shapes that not only example inputs but also\nall future inputs are expected to satisfy for the generated code to be\ncorrect. More precisely, the dynamic shape specification must logically imply\nthe generated guards, otherwise an error is raised at export time (along with\nsuggested fixes to the dynamic shape specification). On the other hand, when\nthere are no generated guards on backed dynamic shapes (in particular, when\nall shapes are static) no dynamic shape specification needs to be provided to\nexport. In general, the dynamic shape specification is converted to runtime\nassertions on the inputs of the generated code.\n\nFinally, **any guards on unbacked dynamic shapes are converted to \"inline\"\nruntime assertions**. These are added in the generated code at the locations\nwhere those unbacked dynamic shapes were created: typically, right after\ndata-dependent operator calls.",
    "1572": "一级标题：torch.export Programming Model\n二级标题：Allowed PyTorch operators\n内容：\nAll PyTorch operators are permitted.\n\n### Custom operators\n\nIn addition, you can define and use\n[custom operators](https://pytorch.org/tutorials/advanced/python_custom_ops#python-custom-ops-tutorial).\nDefining a custom operator includes defining a fake implementation for it,\njust like any other PyTorch operator (see previous section).\n\nHere's an example of a custom `sin` operator that wraps NumPy, and its\nregistered (trivial) fake implementation.\n\n```python\n@torch.library.custom_op(\"mylib::sin\", mutates_args=())\ndef sin(x: Tensor) -> Tensor:\n    x_np = x.numpy()\n    y_np = np.sin(x_np)\n    return torch.from_numpy(y_np)\n\n@torch.library.register_fake(\"mylib::sin\")\ndef _(x: Tensor) -> Tensor:\n    return torch.empty_like(x)\n```\n\n**Sometimes your custom operator's fake implementation will involve\ndata-dependent shapes**. Here's how a fake implementation for a custom\n`nonzero` might look like.\n\n```python\n...\n\n@torch.library.register_fake(\"mylib::custom_nonzero\")\ndef _(x):\n    nnz = torch.library.get_ctx().new_dynamic_size()\n    shape = [nnz, x.dim()]\n    return x.new_empty(shape, dtype=torch.int64)\n```",
    "1573": "一级标题：torch.export Programming Model\n二级标题：Module State: Reads vs. Updates\n内容：\nModule states include parameters, buffers, and regular attributes.\n\n- A regular attribute can be of any type.\n- On the other hand, parameters and buffers are always Tensors.\n\nModule states can be dynamic or static, based on their types as outlined\nabove. For example, `self.training` is a `bool`, which means it is static; on\nthe other hand, any parameter or buffer is dynamic.\n\nThe *shapes* of any Tensors contained in module states cannot be dynamic, i.e.,\nthose shapes are fixed at export time, and cannot change between executions\nof the exported program.\n\n### Access rules\n\n**All module states must be initialized**. Accessing a module state that is\nnot already initialized causes an error to be raised at export time.\n\n**Reading module states is always permitted**.\n\nUpdating module states is possible, but must follow the rules below:\n\n- **A static regular attribute** (e.g., of primitive type) **can be updated**.\n  Reads and updates can be freely interleaved, and as expected, any reads\n  will always see the values of the latest updates. Because these attributes\n  are static, we will also burn the values in, so the generated code will not\n  have any instructions to actually \"get\" or \"set\" such attributes.\n- **A dynamic regular attribute** (e.g., of Tensor type) **cannot be updated**.\n  To do so, it must be registered as a buffer during module initialization.\n- **A buffer can be updated**, where the updating can be in-place (e.g.,\n  `self.buffer[:] = ...`) or not (e.g., `self.buffer = ...`).\n- **A parameter cannot be updated**. Typically parameters are updated only\n  during training, not during inference. We recommend exporting with\n  {func}`torch.no_grad` to avoid parameter updates at export time.\n\n### Effects of functionalization\n\nAny dynamic module state that is read and/or updated is \"lifted\"\n(respectively) as an input and/or output of the generated code.\n\nThe exported program stores, along with the generated code, the initial\nvalues of parameters and buffers and the constant values of other Tensor\nattributes.",
    "1574": "一级标题：PT2 Archive Spec\n二级标题：无\n内容：\nThe following specification defines the archive format which can be produced\nthrough the following methods:\n\n* {ref}`torch.export <torch.export>` through calling {func}`torch.export.save`\n* {ref}`AOTInductor <torch.compiler_aot_inductor>` through calling {func}`torch._inductor.aoti_compile_and_package`\n\nThe archive is a zipfile, and can be manipulated using standard zipfile APIs.\n\nThe following is a sample archive. We will walk through the archive folder by folder.\n\n```\n.\n├── archive_format\n├── byteorder\n├── .data\n│   ├── serialization_id\n│   └── version\n├── data\n│   ├── aotinductor\n│   │   └── model1\n│   │       ├── aotinductor_pickle_data.json\n│   │       ├── cf5ez6ifexr7i2hezzz4s7xfusj4wtisvu2gddeamh37bw6bghjw.cpp\n│   │       ├── cf5ez6ifexr7i2hezzz4s7xfusj4wtisvu2gddeamh37bw6bghjw.so\n│   │       ├── cg7domx3woam3nnliwud7yvtcencqctxkvvcafuriladwxw4nfiv.cubin\n│   │       └── cubaaxppb6xmuqdm4bej55h2pftbce3bjyyvljxbtdfuolmv45ex.cubin\n│   ├── weights\n│   │  ├── model1_model_param_config.json\n│   │  ├── weight_0\n│   │  ├── weight_1\n│   │  ├── weight_2\n│   └── constants\n│   │  ├── model1_model_constants_config.json\n│   │  ├── tensor_0\n│   │  ├── tensor_1\n│   │  ├── custom_obj_0\n│   │  ├── custom_obj_1\n│   └── sample_inputs\n│       ├── model1.pt\n│       └── model2.pt\n├── extra\n│   └── ....json\n└── models\n    ├── model1.json\n    └── model2.json\n```",
    "1575": "一级标题：PT2 Archive Spec\n二级标题：Contents\n内容：\n### Archive Headers\n\n* `archive_format` declares the format used by this archive. Currently, it can only be “pt2”.\n* `byteorder`. One of “little” or “big”, used by zip file reader\n* `/.data/version` contains the archive version. (Notice that this is neither export serialization’s schema version, nor Aten Opset Version).\n* `/.data/serialization_id` is a hash generated for the current archive, used for verification.\n\n\n### AOTInductor Compiled Artifact\n\nPath: `/data/aotinductor/<model_name>-<backend>/`\n\nAOTInductor compilation artifacts are saved for each model-backend pair. For\nexample, compilation artifacts for the `model1` model on A100 and H100 will be\nsaved in `model1-a100` and `model1-h100` folders separately.\n\nThe folder typically contains\n* `<uuid>.so`: Dynamic library compiled from <uuid>.cpp.\n* `<uuid>.cpp`: AOTInductor generated cpp wrapper file.\n* `*.cubin`: Triton kernels compiled from triton codegen kernels\n* (optional) `<uuid>.json`: External fallback nodes for custom ops to be executed by `ProxyExecutor`, serialized according to `ExternKernelNode` struct. If the model doesn’t use custom ops/ProxyExecutor, this file would be omitted.\n* `<uuid>_metadata.json`: Metadata which was passed in from the `aot_inductor.metadata` inductor config\n\n### Weights\n\nPath: `/data/weights/*`\n\nModel parameters and buffers are saved in the `/data/weights/` folder. Each\ntensor is saved as a separated file. The file only contains the raw data blob,\ntensor metadata are saved separately in the\n`<model_name>_model_param_config.json`.\n\n### Constants\n\nPath: `/data/constants/*`\n\nTensorConstants, non-persistent buffers and TorchBind objects are saved in the\n`/data/constants/` folder. Metadata is saved separately in the\n`<model_name>_model_constants_config.json`\n\n### Sample Inputs\n\nPath: `/data/sample_inputs/<model_name>.pt`\n\nThe `sample_input` used by `torch.export` could be included in the archive for\ndownstream use. Typically, it’s a flattened list of Tensors, combining both args\nand kwargs of the forward() function.\n\nThe .pt file is produced by `torch.save(sample_input)`, and can be loaded by\n`torch.load()` in python and `torch::pickle_load()` in c++.\n\nWhen the model has multiple copies of sample input, it would be packaged as\n`<model_name>_<index>.pt`.\n\n### Models Definitions\n\nPath: `/models/<model_name>.json`\n\nModel definition is the serialized json of the ExportedProgram from\n`torch.export.save`, and other model-level metadata.",
    "1576": "一级标题：PT2 Archive Spec\n二级标题：Multiple Models\n内容：\nThis archive spec supports multiple model definitions coexisting in the same\nfile, with `<model_name>` serving as a unique identifier for the models, and\nwill be used as reference in other folders of the archive.\n\nLower level APIs like {func}`torch.export.pt2_archive._package.package_pt2` and\n{func}`torch.export.pt2_archive._package.load_pt2` allow you to have\nfiner-grained control over the packaging and loading process.",
    "1577": "一级标题：torch.fft\n二级标题：无\n内容：\nDiscrete Fourier transforms and related functions.\n\n```{eval-rst}\n.. automodule:: torch.fft\n```\n\n```{eval-rst}\n.. currentmodule:: torch.fft\n```",
    "1578": "一级标题：torch.fft\n二级标题：Fast Fourier Transforms\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    fft\n    ifft\n    fft2\n    ifft2\n    fftn\n    ifftn\n    rfft\n    irfft\n    rfft2\n    irfft2\n    rfftn\n    irfftn\n    hfft\n    ihfft\n    hfft2\n    ihfft2\n    hfftn\n    ihfftn\n```",
    "1579": "一级标题：torch.fft\n二级标题：Helper Functions\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    fftfreq\n    rfftfreq\n    fftshift\n    ifftshift\n```",
    "1580": "一级标题：FullyShardedDataParallel\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.distributed.fsdp\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.fsdp.FullyShardedDataParallel\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.fsdp.BackwardPrefetch\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.fsdp.ShardingStrategy\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.fsdp.MixedPrecision\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.fsdp.CPUOffload\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.fsdp.StateDictConfig\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.fsdp.FullStateDictConfig\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.fsdp.ShardedStateDictConfig\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.fsdp.LocalStateDictConfig\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.fsdp.OptimStateDictConfig\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.fsdp.FullOptimStateDictConfig\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.fsdp.ShardedOptimStateDictConfig\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.fsdp.LocalOptimStateDictConfig\n  :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.distributed.fsdp.StateDictSettings\n  :members:\n```",
    "1581": "一级标题：torch.func API Reference\n二级标题：无\n内容：\n```{eval-rst}\n.. currentmodule:: torch.func\n```\n\n```{eval-rst}\n.. automodule:: torch.func\n```",
    "1582": "一级标题：torch.func API Reference\n二级标题：Function Transforms\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n     vmap\n     grad\n     grad_and_value\n     vjp\n     jvp\n     linearize\n     jacrev\n     jacfwd\n     hessian\n     functionalize\n```",
    "1583": "一级标题：torch.func API Reference\n二级标题：Utilities for working with torch.nn.Modules\n内容：\nIn general, you can transform over a function that calls a ``torch.nn.Module``.\nFor example, the following is an example of computing a jacobian of a function\nthat takes three values and returns three values:\n\n```python\nmodel = torch.nn.Linear(3, 3)\n\ndef f(x):\n    return model(x)\n\nx = torch.randn(3)\njacobian = jacrev(f)(x)\nassert jacobian.shape == (3, 3)\n```\n\nHowever, if you want to do something like compute a jacobian over the parameters of the model, then there needs to be a way to construct a function where the parameters are the inputs to the function. That's what {func}`functional_call` is for: it accepts an nn.Module, the transformed `parameters`, and the inputs to the Module's forward pass. It returns the value of running the Module's forward pass with the replaced parameters.\n\nHere's how we would compute the Jacobian over the parameters\n\n```python\nmodel = torch.nn.Linear(3, 3)\n\ndef f(params, x):\n    return torch.func.functional_call(model, params, x)\n\nx = torch.randn(3)\njacobian = jacrev(f)(dict(model.named_parameters()), x)\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    functional_call\n    stack_module_state\n    replace_all_batch_norm_modules_\n```\n\nIf you're looking for information on fixing Batch Norm modules, please follow the\nguidance here\n\n```{eval-rst}\n.. toctree::\n   :maxdepth: 1\n\n   func.batch_norm\n```",
    "1584": "一级标题：torch.func API Reference\n二级标题：Debug utilities\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n     debug_unwrap\n```",
    "1585": "一级标题：Patching Batch Norm\n二级标题：无\n内容：",
    "1586": "一级标题：Patching Batch Norm\n二级标题：What's happening?\n内容：\nBatch Norm requires in-place updates to running_mean and running_var of the same size as the input.\nFunctorch does not support inplace update to a regular tensor that takes in a batched tensor (i.e.\n`regular.add_(batched)` is not allowed). So when vmapping over a batch of inputs to a single module,\nwe end up with this error",
    "1587": "一级标题：Patching Batch Norm\n二级标题：How to fix\n内容：\nOne of the best supported ways is to switch BatchNorm for GroupNorm. Options 1 and 2 support this\n\nAll of these options assume that you don't need running stats. If you're using a module this means\nthat it's assumed you won't use batch norm in evaluation mode. If you have a use case that involves\nrunning batch norm with vmap in evaluation mode, please file an issue\n\n### Option 1: Change the BatchNorm\nIf you want to change for GroupNorm, anywhere that you have BatchNorm, replace it with:\n\n```python\nBatchNorm2d(C, G, track_running_stats=False)\n```\n\nHere `C` is the same `C` as in the original BatchNorm. `G` is the number of groups to\nbreak `C` into. As such, `C % G == 0` and as a fallback, you can set `C == G`, meaning\neach channel will be treated separately.\n\nIf you must use BatchNorm and you've built the module yourself, you can change the module to\nnot use running stats. In other words, anywhere that there's a BatchNorm module, set the\n`track_running_stats` flag to be False\n\n```python\nBatchNorm2d(64, track_running_stats=False)\n```\n\n### Option 2: torchvision parameter\nSome torchvision models, like resnet and regnet, can take in a `norm_layer` parameter. These are\noften defaulted to be BatchNorm2d if they've been defaulted.\n\nInstead you can set it to be GroupNorm.\n\n```python\nimport torchvision\nfrom functools import partial\ntorchvision.models.resnet18(norm_layer=lambda c: GroupNorm(num_groups=g, c))\n```\n\nHere, once again, `c % g == 0` so as a fallback, set `g = c`.\n\nIf you are attached to BatchNorm, be sure to use a version that doesn't use running stats\n\n```python\nimport torchvision\nfrom functools import partial\ntorchvision.models.resnet18(norm_layer=partial(BatchNorm2d, track_running_stats=False))\n```\n\n### Option 3: functorch's patching\nfunctorch has added some functionality to allow for quick, in-place patching of the module to not\nuse running stats. Changing the norm layer is more fragile, so we have not offered that. If you\nhave a net where you want the BatchNorm to not use running stats, you can run\n`replace_all_batch_norm_modules_` to update the module in-place to not use running stats\n\n```python\nfrom torch.func import replace_all_batch_norm_modules_\nreplace_all_batch_norm_modules_(net)\n```\n\n### Option 4: eval mode\nWhen run under eval mode, the running_mean and running_var will not be updated. Therefore, vmap can support this mode\n\n```python\nmodel.eval()\nvmap(model)(x)\nmodel.train()\n```",
    "1588": "一级标题：torch.func\n二级标题：无\n内容：\n```{eval-rst}\n.. currentmodule:: torch.func\n```\n\ntorch.func, previously known as \"functorch\", is\n[JAX-like](https://github.com/google/jax) composable function transforms for PyTorch.\n\n```{note}\nThis library is currently in [beta](https://pytorch.org/blog/pytorch-feature-classification-changes/#beta).\nWhat this means is that the features generally work (unless otherwise documented)\nand we (the PyTorch team) are committed to bringing this library forward. However, the APIs\nmay change under user feedback and we don't have full coverage over PyTorch operations.\n\nIf you have suggestions on the API or use-cases you'd like to be covered, please\nopen a GitHub issue or reach out. We'd love to hear about how you're using the library.\n```",
    "1589": "一级标题：torch.func\n二级标题：What are composable function transforms?\n内容：\n- A \"function transform\" is a higher-order function that accepts a numerical function\n  and returns a new function that computes a different quantity.\n\n- {mod}`torch.func` has auto-differentiation transforms (`grad(f)` returns a function that\n  computes the gradient of `f`), a vectorization/batching transform (`vmap(f)`\n  returns a function that computes `f` over batches of inputs), and others.\n\n- These function transforms can compose with each other arbitrarily. For example,\n  composing `vmap(grad(f))` computes a quantity called per-sample-gradients that\n  stock PyTorch cannot efficiently compute today.",
    "1590": "一级标题：torch.func\n二级标题：Why composable function transforms?\n内容：\nThere are a number of use cases that are tricky to do in PyTorch today:\n\n- computing per-sample-gradients (or other per-sample quantities)\n- running ensembles of models on a single machine\n- efficiently batching together tasks in the inner-loop of MAML\n- efficiently computing Jacobians and Hessians\n- efficiently computing batched Jacobians and Hessians\n\nComposing {func}`vmap`, {func}`grad`, and {func}`vjp` transforms allows us to express the above without designing a separate subsystem for each.\nThis idea of composable function transforms comes from the [JAX framework](https://github.com/google/jax).",
    "1591": "一级标题：torch.func\n二级标题：Read More\n内容：\n```{eval-rst}\n.. toctree::\n  :maxdepth: 2\n\n  func.whirlwind_tour\n  func.api\n  func.ux_limitations\n  func.migrating\n```",
    "1592": "一级标题：Migrating from functorch to torch.func\n二级标题：无\n内容：\ntorch.func, previously known as \"functorch\", is\n[JAX-like](https://github.com/google/jax) composable function transforms for PyTorch.\n\nfunctorch started as an out-of-tree library over at\nthe [pytorch/functorch](https://github.com/pytorch/functorch) repository.\nOur goal has always been to upstream functorch directly into PyTorch and provide\nit as a core PyTorch library.\n\nAs the final step of the upstream, we've decided to migrate from being a top level package\n(`functorch`) to being a part of PyTorch to reflect how the function transforms are\nintegrated directly into PyTorch core. As of PyTorch 2.0, we are deprecating\n`import functorch` and ask that users migrate to the newest APIs, which we\nwill maintain going forward. `import functorch` will be kept around to maintain\nbackwards compatibility for a couple of releases.",
    "1593": "一级标题：Migrating from functorch to torch.func\n二级标题：function transforms\n内容：\nThe following APIs are a drop-in replacement for the following\n[functorch APIs](https://pytorch.org/functorch/1.13/functorch.html).\nThey are fully backwards compatible.\n\n| functorch API                      | PyTorch API (as of PyTorch 2.0)                |\n| ----------------------------------- | ---------------------------------------------- |\n| functorch.vmap                      | {func}`torch.vmap` or {func}`torch.func.vmap`              |\n| functorch.grad                      | {func}`torch.func.grad`                              |\n| functorch.vjp                       | {func}`torch.func.vjp`                               |\n| functorch.jvp                       | {func}`torch.func.jvp`                               |\n| functorch.jacrev                    | {func}`torch.func.jacrev`                            |\n| functorch.jacfwd                    | {func}`torch.func.jacfwd`                            |\n| functorch.hessian                   | {func}`torch.func.hessian`                           |\n| functorch.functionalize             | {func}`torch.func.functionalize`                     |\n\nFurthermore, if you are using torch.autograd.functional APIs, please try out\nthe {mod}`torch.func` equivalents instead. {mod}`torch.func` function\ntransforms are more composable and more performant in many cases.\n\n| torch.autograd.functional API               | torch.func API (as of PyTorch 2.0)                |\n| ------------------------------------------- | ---------------------------------------------- |\n| {func}`torch.autograd.functional.vjp`             | {func}`torch.func.grad` or {func}`torch.func.vjp`           |\n| {func}`torch.autograd.functional.jvp`             | {func}`torch.func.jvp`                                |\n| {func}`torch.autograd.functional.jacobian`        | {func}`torch.func.jacrev` or {func}`torch.func.jacfwd`      |\n| {func}`torch.autograd.functional.hessian`         | {func}`torch.func.hessian`                            |",
    "1594": "一级标题：Migrating from functorch to torch.func\n二级标题：NN module utilities\n内容：\nWe've changed the APIs to apply function transforms over NN modules to make them\nfit better into the PyTorch design philosophy. The new API is different, so\nplease read this section carefully.\n\n### functorch.make_functional\n\n{func}`torch.func.functional_call` is the replacement for\n[functorch.make_functional](https://pytorch.org/functorch/1.13/generated/functorch.make_functional.html#functorch.make_functional)\nand\n[functorch.make_functional_with_buffers](https://pytorch.org/functorch/1.13/generated/functorch.make_functional_with_buffers.html#functorch.make_functional_with_buffers).\nHowever, it is not a drop-in replacement.\n\nIf you're in a hurry, you can use\n[helper functions in this gist](https://gist.github.com/zou3519/7769506acc899d83ef1464e28f22e6cf)\nthat emulate the behavior of functorch.make_functional and functorch.make_functional_with_buffers.\nWe recommend using {func}`torch.func.functional_call` directly because it is a more explicit\nand flexible API.\n\nConcretely, functorch.make_functional returns a functional module and parameters.\nThe functional module accepts parameters and inputs to the model as arguments.\n{func}`torch.func.functional_call` allows one to call the forward pass of an existing\nmodule using new parameters and buffers and inputs.\n\nHere's an example of how to compute gradients of parameters of a model using functorch\nvs {mod}`torch.func`:\n\n```python\n# ---------------\n# using functorch\n# ---------------\nimport torch\nimport functorch\ninputs = torch.randn(64, 3)\ntargets = torch.randn(64, 3)\nmodel = torch.nn.Linear(3, 3)\n\nfmodel, params = functorch.make_functional(model)\n\ndef compute_loss(params, inputs, targets):\n    prediction = fmodel(params, inputs)\n    return torch.nn.functional.mse_loss(prediction, targets)\n\ngrads = functorch.grad(compute_loss)(params, inputs, targets)\n\n# ------------------------------------\n# using torch.func (as of PyTorch 2.0)\n# ------------------------------------\nimport torch\ninputs = torch.randn(64, 3)\ntargets = torch.randn(64, 3)\nmodel = torch.nn.Linear(3, 3)\n\nparams = dict(model.named_parameters())\n\ndef compute_loss(params, inputs, targets):\n    prediction = torch.func.functional_call(model, params, (inputs,))\n    return torch.nn.functional.mse_loss(prediction, targets)\n\ngrads = torch.func.grad(compute_loss)(params, inputs, targets)\n```\n\nAnd here's an example of how to compute jacobians of model parameters:\n\n```python\n# ---------------\n# using functorch\n# ---------------\nimport torch\nimport functorch\ninputs = torch.randn(64, 3)\nmodel = torch.nn.Linear(3, 3)\n\nfmodel, params = functorch.make_functional(model)\njacobians = functorch.jacrev(fmodel)(params, inputs)\n\n# ------------------------------------\n# using torch.func (as of PyTorch 2.0)\n# ------------------------------------\nimport torch\nfrom torch.func import jacrev, functional_call\ninputs = torch.randn(64, 3)\nmodel = torch.nn.Linear(3, 3)\n\nparams = dict(model.named_parameters())\n# jacrev computes jacobians of argnums=0 by default.\n# We set it to 1 to compute jacobians of params\njacobians = jacrev(functional_call, argnums=1)(model, params, (inputs,))\n```\n\nNote that it is important for memory consumption that you should only carry\naround a single copy of your parameters. `model.named_parameters()` does not copy\nthe parameters. If in your model training you update the parameters of the model\nin-place, then the `nn.Module` that is your model has the single copy of the\nparameters and everything is OK.\n\nHowever, if you want to carry your parameters around in a dictionary and update\nthem out-of-place, then there are two copies of parameters: the one in the\ndictionary and the one in the `model`. In this case, you should change\n`model` to not hold memory by converting it to the meta device via\n`model.to('meta')`.\n\n### functorch.combine_state_for_ensemble\n\nPlease use {func}`torch.func.stack_module_state` instead of\n[functorch.combine_state_for_ensemble](https://pytorch.org/functorch/1.13/generated/functorch.combine_state_for_ensemble.html)\n{func}`torch.func.stack_module_state` returns two dictionaries, one of stacked parameters, and\none of stacked buffers, that can then be used with {func}`torch.vmap` and {func}`torch.func.functional_call`\nfor ensembling.\n\nFor example, here is an example of how to ensemble over a very simple model:\n\n```python\nimport torch\nnum_models = 5\nbatch_size = 64\nin_features, out_features = 3, 3\nmodels = [torch.nn.Linear(in_features, out_features) for i in range(num_models)]\ndata = torch.randn(batch_size, 3)\n\n# ---------------\n# using functorch\n# ---------------\nimport functorch\nfmodel, params, buffers = functorch.combine_state_for_ensemble(models)\noutput = functorch.vmap(fmodel, (0, 0, None))(params, buffers, data)\nassert output.shape == (num_models, batch_size, out_features)\n\n# ------------------------------------\n# using torch.func (as of PyTorch 2.0)\n# ------------------------------------\nimport copy\n\n# Construct a version of the model with no memory by putting the Tensors on\n# the meta device.\nbase_model = copy.deepcopy(models[0])\nbase_model.to('meta')\n\nparams, buffers = torch.func.stack_module_state(models)\n\n# It is possible to vmap directly over torch.func.functional_call,\n# but wrapping it in a function makes it clearer what is going on.\ndef call_single_model(params, buffers, data):\n    return torch.func.functional_call(base_model, (params, buffers), (data,))\n\noutput = torch.vmap(call_single_model, (0, 0, None))(params, buffers, data)\nassert output.shape == (num_models, batch_size, out_features)\n```",
    "1595": "一级标题：Migrating from functorch to torch.func\n二级标题：functorch.compile\n内容：\nWe are no longer supporting functorch.compile (also known as AOTAutograd)\nas a frontend for compilation in PyTorch; we have integrated AOTAutograd\ninto PyTorch's compilation story. If you are a user, please use\n{func}`torch.compile` instead.",
    "1596": "一级标题：UX Limitations\n二级标题：无\n内容：\ntorch.func, like [JAX](https://github.com/google/jax), has restrictions around\nwhat can be transformed. In general, JAX’s limitations are that transforms\nonly work with pure functions: that is, functions where the output is completely\ndetermined by the input and that do not involve side effects (like mutation).\n\nWe have a similar guarantee: our transforms work well with pure functions.\nHowever, we do support certain in-place operations. On one hand, writing code\ncompatible with function transforms may involve changing how you write PyTorch\ncode, on the other hand, you may find that our transforms let you express things\nthat were previously difficult to express in PyTorch.",
    "1597": "一级标题：UX Limitations\n二级标题：General limitations\n内容：\nAll torch.func transforms share a limitation in that a function should not\nassign to global variables. Instead, all outputs to a function must be returned\nfrom the function. This restriction comes from how torch.func is implemented:\neach transform wraps Tensor inputs in special torch.func Tensor subclasses\nthat facilitate the transform.\n\nSo, instead of the following:\n\n```python\nimport torch\nfrom torch.func import grad\n\n# Don't do this\nintermediate = None\n\ndef f(x):\n  global intermediate\n  intermediate = x.sin()\n  z = intermediate.sin()\n  return z\n\nx = torch.randn([])\ngrad_x = grad(f)(x)\n```\n\nPlease rewrite `f` to return `intermediate`:\n\n```python\ndef f(x):\n  intermediate = x.sin()\n  z = intermediate.sin()\n  return z, intermediate\n\ngrad_x, intermediate = grad(f, has_aux=True)(x)\n```",
    "1598": "一级标题：UX Limitations\n二级标题：torch.autograd APIs\n内容：\nIf you are trying to use a `torch.autograd` API like `torch.autograd.grad`\nor `torch.autograd.backward` inside of a function being transformed by\n{func}`vmap` or one of torch.func's AD transforms ({func}`vjp`, {func}`jvp`,\n{func}`jacrev`, {func}`jacfwd`), the transform may not be able to transform over it.\nIf it is unable to do so, you'll receive an error message.\n\nThis is a fundamental design limitation in how PyTorch's AD support is implemented\nand the reason why we designed the torch.func library. Please instead use the torch.func\nequivalents of the `torch.autograd` APIs:\n- `torch.autograd.grad`, `Tensor.backward` -> `torch.func.vjp` or `torch.func.grad`\n- `torch.autograd.functional.jvp` -> `torch.func.jvp`\n- `torch.autograd.functional.jacobian` -> `torch.func.jacrev` or `torch.func.jacfwd`\n- `torch.autograd.functional.hessian` -> `torch.func.hessian`",
    "1599": "一级标题：UX Limitations\n二级标题：vmap limitations\n内容：\n:::{note}\n{func}`vmap` is our most restrictive transform.\nThe grad-related transforms ({func}`grad`, {func}`vjp`, {func}`jvp`) do not\nhave these limitations. {func}`jacfwd` (and {func}`hessian`, which is\nimplemented with {func}`jacfwd`) is a composition of {func}`vmap` and\n{func}`jvp` so it also has these limitations.\n:::\n\n`vmap(func)` is a transform that returns a function that maps `func` over\nsome new dimension of each input Tensor. The mental model for vmap is that it is\nlike running a for-loop: for pure functions (i.e. in the absence of side\neffects), `vmap(f)(x)` is equivalent to:\n\n```python\ntorch.stack([f(x_i) for x_i in x.unbind(0)])\n```\n\n### Mutation: Arbitrary mutation of Python data structures\n\nIn the presence of side effects, {func}`vmap` no longer acts like it is running\na for-loop. For example, the following function:\n\n```python\ndef f(x, list):\n  list.pop()\n  print(\"hello!\")\n  return x.sum(0)\n\nx = torch.randn(3, 1)\nlst = [0, 1, 2, 3]\n\nresult = vmap(f, in_dims=(0, None))(x, lst)\n```\n\nwill print \"hello!\" once and pop only one element from `lst`.\n\n{func}`vmap` executes `f` a single time, so all side effects only happen once.\n\nThis is a consequence of how vmap is implemented. torch.func has a special,\ninternal BatchedTensor class. `vmap(f)(*inputs)` takes all Tensor inputs,\nturns them into BatchedTensors, and calls `f(*batched_tensor_inputs)`.\nBatchedTensor overrides the PyTorch API to produce batched (i.e. vectorized)\nbehavior for each PyTorch operator.\n\n### Mutation: in-place PyTorch Operations\n\nYou might be here due to receiving an error about vmap-incompatible in-place\noperations. {func}`vmap` will raise an error if it encounters an unsupported PyTorch\nin-place operation and it will succeed otherwise. Unsupported operations\nare those that would cause a Tensor with more elements to be written to a\nTensor with fewer elements. Here's an example of how this can occur:\n\n```python\ndef f(x, y):\n  x.add_(y)\n  return x\n\nx = torch.randn(1)\ny = torch.randn(3, 1)  # When vmapped over, looks like it has shape [1]\n\n# Raises an error because `x` has fewer elements than `y`.\nvmap(f, in_dims=(None, 0))(x, y)\n```\n\n`x` is a Tensor with one element, `y` is a Tensor with three elements.\n`x + y` has three elements (due to broadcasting), but attempting to write\nthree elements back into `x`, which only has one element, raises an error\ndue to attempting to write three elements into a Tensor with a single element.\n\nThere is no problem if the Tensor being written to is batched under\n{func}`~torch.vmap` (i.e. it is being vmapped over).\n\n```python\ndef f(x, y):\n  x.add_(y)\n  return x\n\nx = torch.randn(3, 1)\ny = torch.randn(3, 1)\nexpected = x + y\n\n# Does not raise an error because x is being vmapped over.\nvmap(f, in_dims=(0, 0))(x, y)\nassert torch.allclose(x, expected)\n```\n\nOne common fix for this is to replace calls to factory functions with\ntheir \"new\\_\\*\" equivalent. For example:\n\n- Replace {func}`torch.zeros` with {meth}`Tensor.new_zeros`\n- Replace {func}`torch.empty` with {meth}`Tensor.new_empty`\n\nTo see why this helps, consider the following.\n\n```python\ndef diag_embed(vec):\n  assert vec.dim() == 1\n  result = torch.zeros(vec.shape[0], vec.shape[0])\n  result.diagonal().copy_(vec)\n  return result\n\nvecs = torch.tensor([[0., 1, 2], [3., 4, 5]])\n\n# RuntimeError: vmap: inplace arithmetic(self, *extra_args) is not possible ...\nvmap(diag_embed)(vecs)\n```\n\nInside of {func}`~torch.vmap`, `result` is a Tensor of shape [3, 3].\nHowever, although `vec` looks like it has shape [3], `vec` actually has\nunderlying shape [2, 3].\nIt is not possible to copy `vec` into `result.diagonal()`, which has\nshape [3], because it has too many elements.\n\n```python\ndef diag_embed(vec):\n  assert vec.dim() == 1\n  result = vec.new_zeros(vec.shape[0], vec.shape[0])\n  result.diagonal().copy_(vec)\n  return result\n\nvecs = torch.tensor([[0., 1, 2], [3., 4, 5]])\nvmap(diag_embed)(vecs)\n```\n\nReplacing {func}`torch.zeros` with {meth}`Tensor.new_zeros` makes it so that\n`result` has an underlying Tensor of shape [2, 3, 3], so it is now possible\nto copy `vec`, which has underlying shape [2, 3], into `result.diagonal()`.\n\n### Mutation: out= PyTorch Operations\n\n{func}`vmap` doesn't support the `out=` keyword argument in PyTorch operations.\nIt will error out gracefully if it encounters that in your code.\n\nThis is not a fundamental limitation; we could theoretically support this in the\nfuture but we have chosen not to for now.\n\n### Data-dependent Python control flow\n\nWe don't yet support `vmap` over data-dependent control flow. Data-dependent\ncontrol flow is when the condition of an if-statement, while-loop, or\nfor-loop is a Tensor that is being `vmap`'ed over. For example, the\nfollowing will raise an error message:\n\n```python\ndef relu(x):\n  if x > 0:\n    return x\n  return 0\n\nx = torch.randn(3)\nvmap(relu)(x)\n```\n\nHowever, any control flow that is not dependent on the values in `vmap`'ed\ntensors will work:\n\n```python\ndef custom_dot(x):\n  if x.dim() == 1:\n    return torch.dot(x, x)\n  return (x * x).sum()\n\nx = torch.randn(3)\nvmap(custom_dot)(x)\n```\n\nJAX supports transforming over\n[data-dependent control flow](https://jax.readthedocs.io/en/latest/jax.lax.html#control-flow-operators)\nusing special control flow operators (e.g. `jax.lax.cond`, `jax.lax.while_loop`).\nWe're investigating adding equivalents of those to PyTorch.\n\n### Data-dependent operations (.item())\n\nWe do not (and will not) support vmap over a user-defined function that calls\n`.item()` on a Tensor. For example, the following will raise an error message:\n\n```python\ndef f(x):\n  return x.item()\n\nx = torch.randn(3)\nvmap(f)(x)\n```\n\nPlease try to rewrite your code to not use `.item()` calls.\n\nYou may also encounter an error message about using `.item()` but you might\nnot have used it. In those cases, it is possible that PyTorch internally is\ncalling `.item()` -- please file an issue on GitHub and we'll fix\nPyTorch internals.\n\n### Dynamic shape operations (nonzero and friends)\n\n`vmap(f)` requires that `f` applied to every \"example\" in your input\nreturns a Tensor with the same shape. Operations such as `torch.nonzero`,\n`torch.is_nonzero` are not supported and will error as a result.\n\nTo see why, consider the following example:\n\n```python\nxs = torch.tensor([[0, 1, 2], [0, 0, 3]])\nvmap(torch.nonzero)(xs)\n```\n\n`torch.nonzero(xs[0])` returns a Tensor of shape 2;\nbut `torch.nonzero(xs[1])` returns a Tensor of shape 1.\nWe are unable to construct a single Tensor as an output;\nthe output would need to be a ragged Tensor (and PyTorch does not yet have\nthe concept of a ragged Tensor).",
    "1600": "一级标题：UX Limitations\n二级标题：Randomness\n内容：\nThe user's intention when calling a random operation can be unclear. Specifically, some users may want\nthe random behavior to be the same across batches while others may want it to differ across batches.\nTo address this, `vmap` takes a randomness flag.\n\nThe flag can only be passed to vmap and can take on 3 values, \"error,\" \"different,\" or \"same,\" defaulting\nto error. Under \"error\" mode, any call to a random function will produce an error asking the user to use\none of the other two flags based on their use case.\n\nUnder \"different\" randomness, elements in a batch produce different random values. For instance,\n\n```python\ndef add_noise(x):\n  y = torch.randn(())  # y will be different across the batch\n  return x + y\n\nx = torch.ones(3)\nresult = vmap(add_noise, randomness=\"different\")(x)  # we get 3 different values\n```\n\nUnder \"same\" randomness, elements in a batch produce same random values. For instance,\n\n```python\ndef add_noise(x):\n  y = torch.randn(())  # y will be the same across the batch\n  return x + y\n\nx = torch.ones(3)\nresult = vmap(add_noise, randomness=\"same\")(x)  # we get the same value, repeated 3 times\n```\n\n:::{warning}\nOur system only determine the randomness behavior of PyTorch operators and cannot control the\nbehavior of other libraries, like numpy. This is similar to JAX's limitations with their solutions\n:::\n\n:::{note}\nMultiple vmap calls using either type of supported randomness will not produce\nthe same results. Like with standard PyTorch, a user can get randomness reproducibility through\neither using `torch.manual_seed()` outside of vmap or by using generators.\n:::\n\n:::{note}\nFinally, our randomness differs from JAX because we aren't using a stateless PRNG, in part because PyTorch\ndoesn't have full support for a stateless PRNG. Instead, we've introduced a flag system to allow for the\nmost common forms of randomness that we see. If your use case does not fit these forms of randomness, please\nfile an issue.\n:::",
    "1601": "一级标题：torch.func Whirlwind Tour\n二级标题：无\n内容：",
    "1602": "一级标题：torch.func Whirlwind Tour\n二级标题：What is torch.func?\n内容：\n```{eval-rst}\n.. currentmodule:: torch.func\n```\n\ntorch.func, previously known as functorch, is a library for\n[JAX](https://github.com/google/jax)-like composable function transforms in\nPyTorch.\n\n- A \"function transform\" is a higher-order function that accepts a numerical\n  function and returns a new function that computes a different quantity.\n- torch.func has auto-differentiation transforms (`grad(f)` returns a function\n  that computes the gradient of `f`), a vectorization/batching transform\n  (`vmap(f)` returns a function that computes `f` over batches of inputs),\n  and others.\n- These function transforms can compose with each other arbitrarily. For\n  example, composing `vmap(grad(f))` computes a quantity called\n  per-sample-gradients that stock PyTorch cannot efficiently compute today.",
    "1603": "一级标题：torch.func Whirlwind Tour\n二级标题：Why composable function transforms?\n内容：\nThere are a number of use cases that are tricky to do in PyTorch today:\n\n- computing per-sample-gradients (or other per-sample quantities)\n- running ensembles of models on a single machine\n- efficiently batching together tasks in the inner-loop of MAML\n- efficiently computing Jacobians and Hessians\n- efficiently computing batched Jacobians and Hessians\n\nComposing {func}`vmap`, {func}`grad`, {func}`vjp`, and {func}`jvp` transforms\nallows us to express the above without designing a separate subsystem for each.",
    "1604": "一级标题：torch.func Whirlwind Tour\n二级标题：What are the transforms?\n内容：\n### {func}`grad` (gradient computation)\n\n`grad(func)` is our gradient computation transform. It returns a new function\nthat computes the gradients of `func`. It assumes `func` returns a single-element\nTensor and by default it computes the gradients of the output of `func` w.r.t.\nto the first input.\n\n```python\nimport torch\nfrom torch.func import grad\nx = torch.randn([])\ncos_x = grad(lambda x: torch.sin(x))(x)\nassert torch.allclose(cos_x, x.cos())\n\n# Second-order gradients\nneg_sin_x = grad(grad(lambda x: torch.sin(x)))(x)\nassert torch.allclose(neg_sin_x, -x.sin())\n```\n\n### {func}`vmap` (auto-vectorization)\n\nNote: {func}`vmap` imposes restrictions on the code that it can be used on. For more\ndetails, please see {ref}`ux-limitations`.\n\n`vmap(func)(*inputs)` is a transform that adds a dimension to all Tensor\noperations in `func`. `vmap(func)` returns a new function that maps `func`\nover some dimension (default: 0) of each Tensor in inputs.\n\nvmap is useful for hiding batch dimensions: one can write a function func that\nruns on examples and then lift it to a function that can take batches of\nexamples with `vmap(func)`, leading to a simpler modeling experience:\n\n```python\nimport torch\nfrom torch.func import vmap\nbatch_size, feature_size = 3, 5\nweights = torch.randn(feature_size, requires_grad=True)\n\ndef model(feature_vec):\n    # Very simple linear model with activation\n    assert feature_vec.dim() == 1\n    return feature_vec.dot(weights).relu()\n\nexamples = torch.randn(batch_size, feature_size)\nresult = vmap(model)(examples)\n```\n\nWhen composed with {func}`grad`, {func}`vmap` can be used to compute per-sample-gradients:\n\n```python\nfrom torch.func import vmap\nbatch_size, feature_size = 3, 5\n\ndef model(weights,feature_vec):\n    # Very simple linear model with activation\n    assert feature_vec.dim() == 1\n    return feature_vec.dot(weights).relu()\n\ndef compute_loss(weights, example, target):\n    y = model(weights, example)\n    return ((y - target) ** 2).mean()  # MSELoss\n\nweights = torch.randn(feature_size, requires_grad=True)\nexamples = torch.randn(batch_size, feature_size)\ntargets = torch.randn(batch_size)\ninputs = (weights,examples, targets)\ngrad_weight_per_example = vmap(grad(compute_loss), in_dims=(None, 0, 0))(*inputs)\n```\n\n### {func}`vjp` (vector-Jacobian product)\n\nThe {func}`vjp` transform applies `func` to `inputs` and returns a new function\nthat computes the vector-Jacobian product (vjp) given some `cotangents` Tensors.\n\n```python\nfrom torch.func import vjp\n\ninputs = torch.randn(3)\nfunc = torch.sin\ncotangents = (torch.randn(3),)\n\noutputs, vjp_fn = vjp(func, inputs); vjps = vjp_fn(*cotangents)\n```\n\n### {func}`jvp` (Jacobian-vector product)\n\nThe {func}`jvp` transforms computes Jacobian-vector-products and is also known as\n\"forward-mode AD\". It is not a higher-order function unlike most other transforms,\nbut it returns the outputs of `func(inputs)` as well as the jvps.\n\n```python\nfrom torch.func import jvp\nx = torch.randn(5)\ny = torch.randn(5)\nf = lambda x, y: (x * y)\n_, out_tangent = jvp(f, (x, y), (torch.ones(5), torch.ones(5)))\nassert torch.allclose(out_tangent, x + y)\n```\n\n### {func}`jacrev`, {func}`jacfwd`, and {func}`hessian`\n\nThe {func}`jacrev` transform returns a new function that takes in `x` and returns\nthe Jacobian of the function with respect to `x` using reverse-mode AD.\n\n```python\nfrom torch.func import jacrev\nx = torch.randn(5)\njacobian = jacrev(torch.sin)(x)\nexpected = torch.diag(torch.cos(x))\nassert torch.allclose(jacobian, expected)\n```\n\n{func}`jacrev` can be composed with {func}`vmap` to produce batched jacobians:\n\n```python\nx = torch.randn(64, 5)\njacobian = vmap(jacrev(torch.sin))(x)\nassert jacobian.shape == (64, 5, 5)\n```\n\n{func}`jacfwd` is a drop-in replacement for jacrev that computes Jacobians using\nforward-mode AD:\n\n```python\nfrom torch.func import jacfwd\nx = torch.randn(5)\njacobian = jacfwd(torch.sin)(x)\nexpected = torch.diag(torch.cos(x))\nassert torch.allclose(jacobian, expected)\n```\n\nComposing {func}`jacrev` with itself or {func}`jacfwd` can produce hessians:\n\n```python\ndef f(x):\n    return x.sin().sum()\n\nx = torch.randn(5)\nhessian0 = jacrev(jacrev(f))(x)\nhessian1 = jacfwd(jacrev(f))(x)\n```\n\n{func}`hessian` is a convenience function that combines jacfwd and jacrev:\n\n```python\nfrom torch.func import hessian\n\ndef f(x):\n    return x.sin().sum()\n\nx = torch.randn(5)\nhess = hessian(f)(x)\n```",
    "1605": "一级标题：torch.\\_\\_future\\_\\_\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.__future__\n```\n\n```{eval-rst}\n.. currentmodule:: torch.__future__\n```\n\n```{eval-rst}\n.. autofunction:: set_overwrite_module_params_on_conversion\n```\n\n```{eval-rst}\n.. autofunction:: get_overwrite_module_params_on_conversion\n```\n\n```{eval-rst}\n.. autofunction:: set_swap_module_params_on_conversion\n```\n\n```{eval-rst}\n.. autofunction:: get_swap_module_params_on_conversion\n```",
    "1606": "一级标题：torch.futures\n二级标题：无\n内容：\nThis package provides a {class}`~torch.futures.Future` type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non {class}`~torch.futures.Future` objects. Currently, the\n{class}`~torch.futures.Future` type is primarily used by the\n{ref}`distributed-rpc-framework`.\n\n```{eval-rst}\n.. automodule:: torch.futures\n```\n\n```{eval-rst}\n.. autoclass:: Future\n    :inherited-members:\n```\n\n```{eval-rst}\n.. autofunction:: collect_all\n```\n\n```{eval-rst}\n.. autofunction:: wait_all\n```",
    "1607": "一级标题：torch.fx.experimental\n二级标题：无\n内容：\n:::{warning}\nThese APIs are experimental and subject to change without notice.\n:::",
    "1608": "一级标题：torch.fx.experimental\n二级标题：torch.fx.experimental.symbolic_shapes\n内容：\n```{eval-rst}\n.. currentmodule:: torch.fx.experimental.symbolic_shapes\n```\n\n```{eval-rst}\n.. automodule:: torch.fx.experimental.symbolic_shapes\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    ShapeEnv\n    DimDynamic\n    StrictMinMaxConstraint\n    RelaxedUnspecConstraint\n    EqualityConstraint\n    SymbolicContext\n    StatelessSymbolicContext\n    StatefulSymbolicContext\n    SubclassSymbolicContext\n    DimConstraints\n    ShapeEnvSettings\n    ConvertIntKey\n    CallMethodKey\n    PropagateUnbackedSymInts\n    DivideByKey\n    InnerTensorKey\n    Specialization\n\n    hint_int\n    is_concrete_int\n    is_concrete_bool\n    is_concrete_float\n    has_free_symbols\n    has_free_unbacked_symbols\n    guard_or_true\n    guard_or_false\n    guard_size_oblivious\n    sym_and\n    sym_eq\n    sym_or\n    constrain_range\n    constrain_unify\n    canonicalize_bool_expr\n    statically_known_true\n    statically_known_false\n    has_static_value\n    lru_cache\n    check_consistent\n    compute_unbacked_bindings\n    rebind_unbacked\n    resolve_unbacked_bindings\n    is_accessor_node\n```",
    "1609": "一级标题：torch.fx.experimental\n二级标题：torch.fx.experimental.proxy_tensor\n内容：\n```{eval-rst}\n.. currentmodule:: torch.fx.experimental.proxy_tensor\n```\n\n```{eval-rst}\n.. automodule:: torch.fx.experimental.proxy_tensor\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    make_fx\n    handle_sym_dispatch\n    get_proxy_mode\n    maybe_enable_thunkify\n    maybe_disable_thunkify\n```",
    "1610": "一级标题：torch.fx\n二级标题：无\n内容：",
    "1611": "一级标题：torch.fx\n二级标题：Overview\n内容：\n```{eval-rst}\n.. automodule:: torch.fx\n```\n\n\n(Writing Transformations)=",
    "1612": "一级标题：torch.fx\n二级标题：Writing Transformations\n内容：\nWhat is an FX transform? Essentially, it's a function that looks like this.\n\n```python\n\nimport torch\nimport torch.fx\n\ndef transform(m: nn.Module,\n                tracer_class : type = torch.fx.Tracer) -> torch.nn.Module:\n    # Step 1: Acquire a Graph representing the code in `m`\n\n    # NOTE: torch.fx.symbolic_trace is a wrapper around a call to\n    # fx.Tracer.trace and constructing a GraphModule. We'll\n    # split that out in our transform to allow the caller to\n    # customize tracing behavior.\n    graph : torch.fx.Graph = tracer_class().trace(m)\n\n    # Step 2: Modify this Graph or create a new one\n    graph = ...\n\n    # Step 3: Construct a Module to return\n    return torch.fx.GraphModule(m, graph)\n```\n\nYour transform will take in a {class}`torch.nn.Module`, acquire a {class}`Graph`\nfrom it, do some modifications, and return a new\n{class}`torch.nn.Module`. You should think of the {class}`torch.nn.Module` that your FX\ntransform returns as identical to a regular {class}`torch.nn.Module` -- you can pass it to another\nFX transform, or you can run it. Ensuring that the inputs and outputs of your FX transform are a\n{class}`torch.nn.Module` will allow for composability.\n\n```{note}\n\nIt is also possible to modify an existing {class}`GraphModule` instead of\ncreating a new one, like so:\n\n```python\nimport torch\nimport torch.fx\n\ndef transform(m : nn.Module) -> nn.Module:\n    gm : torch.fx.GraphModule = torch.fx.symbolic_trace(m)\n\n    # Modify gm.graph\n    # <...>\n\n    # Recompile the forward() method of `gm` from its Graph\n    gm.recompile()\n\n    return gm\n```\n\nNote that you MUST call {meth}`GraphModule.recompile` to bring the generated\n`forward()` method on the `GraphModule` in sync with the modified {class}`Graph`.\n\nGiven that you’ve passed in a {class}`torch.nn.Module` that has been traced into a\n{class}`Graph`, there are now two primary approaches you can take to building a new\n{class}`Graph`.\n\n### A Quick Primer on Graphs\n\nFull treatment of the semantics of graphs can be found in the {class}`Graph`\ndocumentation, but we are going to cover the basics here. A {class}`Graph` is\na data structure that represents a method on a {class}`GraphModule`. The\ninformation that this requires is:\n\n- What are the inputs to the method?\n- What are the operations that run inside the method?\n- What is the output (i.e. return) value from the method?\n\nAll three of these concepts are represented with {class}`Node` instances.\nLet's see what we mean by that with a short example:\n\n```python\n\nimport torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return torch.topk(torch.sum(\n            self.linear(x + self.linear.weight).relu(), dim=-1), 3)\n\nm = MyModule()\ngm = torch.fx.symbolic_trace(m)\n\ngm.graph.print_tabular()\n```\n\nHere we define a module `MyModule` for demonstration purposes, instantiate it,\nsymbolically trace it, then call the {meth}`Graph.print_tabular` method to print\nout a table showing the nodes of this {class}`Graph`:\n\n| opcode | name | target | args | kwargs |\n|--------|------|--------|------|--------|\n| placeholder | x | x | () | {} |\n| get_attr | linear_weight | linear.weight | () | {} |\n| call_function | add_1 | | (x, linear_weight) | {} |\n| call_module | linear_1 | linear | (add_1,) | {} |\n| call_method | relu_1 | relu | (linear_1,) | {} |\n| call_function | sum_1 | <built-in method sum ...> | (relu_1,) | {'dim': -1} |\n| call_function | topk_1 | <built-in method topk ...> | (sum_1, 3) | {} |\n| output | output | output | (topk_1,) | {} |\n\nWe can use this information to answer the questions we posed above.\n\n- What are the inputs to the method? In FX, method inputs are specified\n  via special `placeholder` nodes. In this case, we have a single\n  `placeholder` node with a `target` of `x`, meaning we have\n  a single (non-self) argument named x.\n- What are the operations within the method? The `get_attr`,\n  `call_function`, `call_module`, and `call_method` nodes\n  represent the operations in the method. A full treatment of\n  the semantics of all of these can be found in the {class}`Node`\n  documentation.\n- What is the return value of the method? The return value in a\n  {class}`Graph` is specified by a special `output` node.\n\nGiven that we now know the basics of how code is represented in\nFX, we can now explore how we would edit a {class}`Graph`.\n\n### Graph Manipulation\n\n#### Direct Graph Manipulation\n\nOne approach to building this new {class}`Graph` is to directly manipulate your old\none. To aid in this, we can simply take the {class}`Graph` we obtain from symbolic\ntracing and modify it. For example, let’s say we desire to replace\n{func}`torch.add` calls with {func}`torch.mul` calls.\n\n```python\n\nimport torch\nimport torch.fx\n\n# Sample module\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return torch.add(x, y)\n\ndef transform(m: torch.nn.Module,\n                tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph : fx.Graph = tracer_class().trace(m)\n    # FX represents its Graph as an ordered list of\n    # nodes, so we can iterate through them.\n    for node in graph.nodes:\n        # Checks if we're calling a function (i.e:\n        # torch.add)\n        if node.op == 'call_function':\n            # The target attribute is the function\n            # that call_function calls.\n            if node.target == torch.add:\n                node.target = torch.mul\n\n    graph.lint() # Does some checks to make sure the\n                    # Graph is well-formed.\n\n    return fx.GraphModule(m, graph)\n```\n\nWe can also do more involved {class}`Graph` rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the {class}`Graph` documentation. An\nexample of using these APIs to append a {func}`torch.relu` call\ncan be found below.\n\n```python\n\n# Specifies the insertion point. Any nodes added to the\n# Graph within this scope will be inserted after `node`\nwith traced.graph.inserting_after(node):\n    # Insert a new `call_function` node calling `torch.relu`\n    new_node = traced.graph.call_function(\n        torch.relu, args=(node,))\n\n    # We want all places that used the value of `node` to\n    # now use that value after the `relu` call we've added.\n    # We use the `replace_all_uses_with` API to do this.\n    node.replace_all_uses_with(new_node)\n```\n\nFor simple transformations that only consist of substitutions, you can also\nmake use of the [subgraph rewriter.](https://github.com/pytorch/pytorch/blob/main/torch/fx/subgraph_rewriter.py)\n\n#### Subgraph Rewriting With replace_pattern()\n\nFX also provides another level of automation on top of direct graph manipulation.\nThe {func}`replace_pattern` API is essentially a \"find/replace\" tool for editing\n{class}`Graph`\\s. It allows you to specify a `pattern` and `replacement` function\nand it will trace through those functions, find instances of the group of operations\nin the `pattern` graph, and replace those instances with copies of the `replacement`\ngraph. This can help to greatly automate tedious graph manipulation code, which can\nget unwieldy as the transformations get more complex.\n\n#### Graph Manipulation Examples\n\n-  [Replace one\n   op](https://github.com/pytorch/examples/blob/master/fx/replace_op.py)\n-  [Conv/Batch Norm\n   fusion](https://github.com/pytorch/pytorch/blob/40cbf342d3c000712da92cfafeaca651b3e0bd3e/torch/fx/experimental/optimization.py#L50)\n-  [replace_pattern: Basic usage](https://github.com/pytorch/examples/blob/master/fx/subgraph_rewriter_basic_use.py)\n-  [Quantization](https://pytorch.org/docs/main/quantization.html#prototype-fx-graph-mode-quantization)\n-  [Invert Transformation](https://github.com/pytorch/examples/blob/master/fx/invert.py)\n\n### Proxy/Retracing\n\nAnother way of manipulating {class}`Graph`\\s is by reusing the {class}`Proxy`\nmachinery used in symbolic tracing. For example, let’s\nimagine that we wanted to write a transformation that decomposed\nPyTorch functions into smaller operations. It would transform every\n`F.relu(x)` call into `(x > 0) * x`. One possibility would be to\nperform the requisite graph rewriting to insert the comparison and\nmultiplication after the `F.relu`, and then clean up the original\n`F.relu`. However, we can automate this process by using {class}`Proxy`\nobjects to automatically record operations into the {class}`Graph`.\n\nTo use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with {class}`Proxy` objects as arguments.\nThese {class}`Proxy` objects will capture the operations that are performed\non them and append them to the {class}`Graph`.\n\n```python\n\n# Note that this decomposition rule can be read as regular Python\ndef relu_decomposition(x):\n    return (x > 0) * x\n\ndecomposition_rules = {}\ndecomposition_rules[F.relu] = relu_decomposition\n\ndef decompose(model: torch.nn.Module,\n                tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    \"\"\"\n    Decompose `model` into smaller constituent operations.\n    Currently,this only supports decomposing ReLU into its\n    mathematical definition: (x > 0) * x\n    \"\"\"\n    graph : fx.Graph = tracer_class().trace(model)\n    new_graph = fx.Graph()\n    env = {}\n    tracer = torch.fx.proxy.GraphAppendingTracer(new_graph)\n    for node in graph.nodes:\n        if node.op == 'call_function' and node.target in decomposition_rules:\n            # By wrapping the arguments with proxies,\n            # we can dispatch to the appropriate\n            # decomposition rule and implicitly add it\n            # to the Graph by symbolically tracing it.\n            proxy_args = [\n                fx.Proxy(env[x.name], tracer) if isinstance(x, fx.Node) else x for x in node.args]\n            output_proxy = decomposition_rules[node.target](*proxy_args)\n\n            # Operations on `Proxy` always yield new `Proxy`s, and the\n            # return value of our decomposition rule is no exception.\n            # We need to extract the underlying `Node` from the `Proxy`\n            # to use it in subsequent iterations of this transform.\n            new_node = output_proxy.node\n            env[node.name] = new_node\n        else:\n            # Default case: we don't have a decomposition rule for this\n            # node, so just copy the node over into the new graph.\n            new_node = new_graph.node_copy(node, lambda x: env[x.name])\n            env[node.name] = new_node\n    return fx.GraphModule(model, new_graph)\n```\n\nIn addition to avoiding explicit graph manipulation, using {class}`Proxy`\\s\nalso allows you to specify your rewrite rules as native Python code.\nFor transformations that require a large amount of rewrite rules\n(such as vmap or grad), this can often improve readability and\nmaintainability of the rules. Note that while calling {class}`Proxy` we also\npassed a tracer pointing to the underlying variable `graph`. This is done so\nif in case the operations in graph are n-ary (e.g. add is a binary operator)\nthe call to {class}`Proxy` does not create multiple instances of a graph\ntracer which can lead to unexpected runtime errors. We recommend this method\nof using {class}`Proxy` especially when the underlying operators can not be\nsafely assumed to be unary.\n\nA worked example of using {class}`Proxy`\\s for {class}`Graph` manipulation\ncan be found\n[here](https://github.com/pytorch/examples/blob/master/fx/proxy_based_graph_creation.py).\n\n### The Interpreter Pattern\n\nA useful code organizational pattern in FX is to loop over all the {class}`Node`\\s\nin a {class}`Graph` and execute them. This can be used for several things including\nruntime analysis of values flowing through the graph or transformation of the code\nvia retracing with {class}`Proxy`\\s. For example, suppose we want to run a\n{class}`GraphModule` and record the {class}`torch.Tensor` shape and dtype\nproperties on the nodes as we see them at runtime. That might look like:\n\n```python\n\nimport torch\nimport torch.fx\nfrom torch.fx.node import Node\n\nfrom typing import Dict\n\nclass ShapeProp:\n    \"\"\"\n    Shape propagation. This class takes a `GraphModule`.\n    Then, its `propagate` method executes the `GraphModule`\n    node-by-node with the given arguments. As each operation\n    executes, the ShapeProp class stores away the shape and\n    element type for the output values of each operation on\n    the `shape` and `dtype` attributes of the operation's\n    `Node`.\n    \"\"\"\n    def __init__(self, mod):\n        self.mod = mod\n        self.graph = mod.graph\n        self.modules = dict(self.mod.named_modules())\n\n    def propagate(self, *args):\n        args_iter = iter(args)\n        env : Dict[str, Node] = {}\n\n        def load_arg(a):\n            return torch.fx.graph.map_arg(a, lambda n: env[n.name])\n\n        def fetch_attr(target : str):\n            target_atoms = target.split('.')\n            attr_itr = self.mod\n            for i, atom in enumerate(target_atoms):\n                if not hasattr(attr_itr, atom):\n                    raise RuntimeError(f\"Node referenced nonexistent target {'.'.join(target_atoms[:i])}\")\n                attr_itr = getattr(attr_itr, atom)\n            return attr_itr\n\n        for node in self.graph.nodes:\n            if node.op == 'placeholder':\n                result = next(args_iter)\n            elif node.op == 'get_attr':\n\n                result = fetch_attr(node.target)\n            elif node.op == 'call_function':\n                result = node.target(*load_arg(node.args), **load_arg(node.kwargs))\n            elif node.op == 'call_method':\n                self_obj, *args = load_arg(node.args)\n                kwargs = load_arg(node.kwargs)\n                result = getattr(self_obj, node.target)(*args, **kwargs)\n            elif node.op == 'call_module':\n                result = self.modules[node.target](*load_arg(node.args), **load_arg(node.kwargs))\n\n            # This is the only code specific to shape propagation.\n            # you can delete this `if` branch and this becomes\n            # a generic GraphModule interpreter.\n            if isinstance(result, torch.Tensor):\n                node.shape = result.shape\n                node.dtype = result.dtype\n\n            env[node.name] = result\n\n        return load_arg(self.graph.result)\n```\n\nAs you can see, a full interpreter for FX is not that complicated\nbut it can be very useful. To ease using this pattern, we provide\nthe {class}`Interpreter` class, which encompasses the above logic\nin a way that certain aspects of the interpreter's execution can\nbe overridden via method overrides.\n\nIn addition to executing operations, we can also generate a new\n`Graph` by feeding {class}`Proxy` values through an interpreter.\nSimilarly, we provide the {class}`Transformer` class to encompass\nthis pattern. {class}`Transformer` behaves similarly to\n{class}`Interpreter`, but instead of calling the `run` method to\nget a concrete output value from the Module, you would call the\n{meth}`Transformer.transform` method to return a new\n{class}`GraphModule` which was subject to any transformation rules\nyou installed as overridden methods.\n\n#### Examples of the Interpreter Pattern\n\n-  [ShapePropagation](https://github.com/pytorch/pytorch/blob/master/torch/fx/passes/shape_prop.py)\n-  [Performance Profiler](https://github.com/pytorch/tutorials/pull/1319)",
    "1613": "一级标题：torch.fx\n二级标题：Debugging\n内容：\n### Introduction\n\nOften in the course of authoring transformations, our code will not be quite right.\nIn this case, we may need to do some debugging. The key is to work\nbackwards: first, check the results of invoking the generated module to prove or\ndisprove correctness. Then, inspect and debug the generated code. Then, debug the\nprocess of transformations that led to the generated code.\n\nIf you’re not familiar with debuggers, please see the auxiliary section\n{ref}`Available-Debuggers`.\n\n\n### Common Pitfalls in Transform Authoring\n\n* Nondeterministic `set` iteration order. In Python, the `set` datatype is\n  unordered. Using `set` to contain collections of objects like `Node`\\ s,\n  for example, can cause unexpected nondeterminism. An example is iterating\n  over a set of `Node` s to insert them into a `Graph`. Because the\n  `set` data type is unordered, the ordering of the operations in the output\n  program will be nondeterministic and can change across program invocations.\n  The recommended alternative is to use a `dict` data type, which is\n  [insertion ordered](https://mail.python.org/pipermail/python-dev/2017-December/151283.html)\n  as of Python 3.7 (and as of cPython 3.6). A `dict` can be used equivalently\n  to a set by storing values to be deduplicated in the keys of the `dict`.\n\n### Checking Correctness of Modules\n\nBecause the output of most deep learning modules consists of floating\npoint {class}`torch.Tensor` instances, checking for equivalence between\nthe results of two {class}`torch.nn.Module` is not as straightforward\nas doing a simple equality check. To motivate this, let's use an\nexample:\n\n```python\n\nimport torch\nimport torch.fx\nimport torchvision.models as models\n\ndef transform(m : torch.nn.Module) -> torch.nn.Module:\n    gm = torch.fx.symbolic_trace(m)\n\n    # Imagine we're doing some transforms here\n    # <...>\n\n    gm.recompile()\n\n    return gm\n\nresnet18 = models.resnet18()\ntransformed_resnet18 = transform(resnet18)\n\ninput_image = torch.randn(5, 3, 224, 224)\n\nassert resnet18(input_image) == transformed_resnet18(input_image)\n\"\"\"\nRuntimeError: Boolean value of Tensor with more than one value is ambiguous\n\"\"\"\n```\n\nHere, we've tried to check equality of the values of two deep learning\nmodels with the `==` equality operator. However, this is not well-\\\ndefined both due to the issue of that operator returning a tensor\nand not a bool, but also because comparison of floating point values\nshould use a margin of error (or epsilon) to account for the\nnon-commutativity of floating point operations (see\n[here](https://floating-point-gui.de/errors/comparison/) for more\ndetails). We can use {func}`torch.allclose` instead, which will give\nus an approximate comparison taking into account a relative and\nabsolute tolerance threshold:\n\n```python\nassert torch.allclose(resnet18(input_image), transformed_resnet18(input_image))\n```\nThis is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation.\n\n### Debugging the Generated Code\n\nBecause FX generates the `forward()` function on {class}`GraphModule`\\s, using\ntraditional debugging techniques like `print` statements or `pdb` is\nnot as straightforward. Luckily, we have several techniques we can use\nfor debugging the generated code.\n\n#### Use `pdb`\nInvoke `pdb` to step into the running program. Although the code that\nrepresents the {class}`Graph` is not in any source file, we can still step\ninto it manually using `pdb` when the forward pass is invoked.\n\n```python\n\nimport torch\nimport torch.fx\nimport torchvision.models as models\n\ndef my_pass(inp: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph = tracer_class().trace(inp)\n    # Transformation logic here\n    # <...>\n\n    # Return new Module\n    return fx.GraphModule(inp, graph)\n\nmy_module = models.resnet18()\nmy_module_transformed = my_pass(my_module)\n\ninput_value = torch.randn(5, 3, 224, 224)\n\n# When this line is executed at runtime, we will be dropped into an\n# interactive `pdb` prompt. We can use the `step` or `s` command to\n# step into the execution of the next line\nimport pdb; pdb.set_trace()\n\nmy_module_transformed(input_value)\n```\n(Print the Generated Code)=\n\n#### Print the Generated Code\nIf you’d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with `pdb`. In that case, one\napproach is to simply copy-paste the generated `forward` pass into\nyour code and examine it from there.\n\n```python\n\n# Assume that `traced` is a GraphModule that has undergone some\n# number of transforms\n\n# Copy this code for later\nprint(traced)\n# Print the code generated from symbolic tracing. This outputs:\n\"\"\"\ndef forward(self, y):\n    x = self.x\n    add_1 = x + y;  x = y = None\n    return add_1\n\"\"\"\n\n# Subclass the original Module\nclass SubclassM(M):\n    def __init__(self):\n        super().__init__()\n\n    # Paste the generated `forward` function (the one we printed and\n    # copied above) here\n    def forward(self, y):\n        x = self.x\n        add_1 = x + y;  x = y = None\n        return add_1\n\n# Create an instance of the original, untraced Module. Then, create an\n# instance of the Module with the copied `forward` function. We can\n# now compare the output of both the original and the traced version.\npre_trace = M()\npost_trace = SubclassM()\n```\n#### Use the `to_folder` Function From `GraphModule`\n{meth}`GraphModule.to_folder` is a method in `GraphModule` that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in {ref}`Print the Generated Code`,\nit may be easier to examine modules and parameters using `to_folder`.\n\n```python\n\nm = symbolic_trace(M())\nm.to_folder(\"foo\", \"Bar\")\nfrom foo import Bar\ny = Bar()\n```\nAfter running the above example, we can then look at the code within\n`foo/module.py` and modify it as desired (e.g. adding `print`\nstatements or using `pdb`) to debug the generated code.\n\n### Debugging the Transformation\n\nNow that we've identified that a transformation is creating incorrect\ncode, it's time to debug the transformation itself. First, we'll check\nthe {ref}`Limitations of Symbolic Tracing` section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our `GraphModule`\ntransformation. There may be a quick answer in\n{ref}`Writing Transformations`, but, if not, there are several ways to\nexamine our traced module:\n\n```python\n\n# Sample Module\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return x + y\n\n# Create an instance of `M`\nm = M()\n\n# Symbolically trace an instance of `M` (returns a GraphModule). In\n# this example, we'll only be discussing how to inspect a\n# GraphModule, so we aren't showing any sample transforms for the\n# sake of brevity.\ntraced = symbolic_trace(m)\n\n# Print the code produced by tracing the module.\nprint(traced)\n# The generated `forward` function is:\n\"\"\"\ndef forward(self, x, y):\n    add = x + y;  x = y = None\n    return add\n\"\"\"\n\n# Print the internal Graph.\nprint(traced.graph)\n# This print-out returns:\n\"\"\"\ngraph():\n    %x : [num_users=1] = placeholder[target=x]\n    %y : [num_users=1] = placeholder[target=y]\n    %add : [num_users=1] = call_function[target=operator.add](args = (%x, %y), kwargs = {})\n    return add\n\"\"\"\n\n# Print a tabular representation of the internal Graph.\ntraced.graph.print_tabular()\n# This gives us:\n\"\"\"\nopcode         name    target                   args    kwargs\n-------------  ------  -----------------------  ------  --------\nplaceholder    x       x                        ()      {}\nplaceholder    y       y                        ()      {}\ncall_function  add     <built-in function add>  (x, y)  {}\noutput         output  output                   (add,)  {}\n\"\"\"\n```\nUsing the utility functions above, we can compare our traced Module\nbefore and after we've applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it's still\nnot clear what's going wrong, a debugger like `pdb` can be a good\nnext step.\n\nGoing off of the example above, consider the following code:\n\n```python\n\n# Sample user-defined function\ndef transform_graph(module: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    # Get the Graph from our traced Module\n    g = tracer_class().trace(module)\n\n    \"\"\"\n    Transformations on `g` go here\n    \"\"\"\n\n    return fx.GraphModule(module, g)\n\n# Transform the Graph\ntransformed = transform_graph(traced)\n\n# Print the new code after our transforms. Check to see if it was\n# what we expected\nprint(transformed)\n```\nUsing the above example, let’s say that the call to `print(traced)`\nshowed us that there was an error in our transforms. We want to find\nwhat goes wrong using a debugger. We start a `pdb` session. We can see\nwhat’s happening during the transform by breaking on\n`transform_graph(traced)`, then pressing `s` to “step into” the call\nto `transform_graph(traced)`.\n\nWe may also have good luck by editing the `print_tabular` method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might\nwant to see the Node’s `input_nodes` and `users`.)\n\n(Available-Debuggers)=\n\n### Available Debuggers\n\nThe most common Python debugger is\n[pdb](https://docs.python.org/3/library/pdb.html). You can start\nyour program in “debug mode” with `pdb` by typing\n`python -m pdb FILENAME.py` into the command line, where `FILENAME`\nis the name of the file you want to debug. After that, you can use the\n`pdb` [debugger commands](https://docs.python.org/3/library/pdb.html#debugger-commands)\nto move through your running program stepwise. It’s common to set a\nbreakpoint (`b LINE-NUMBER`) when you start `pdb`, then call `c` to\nrun the program until that point. This prevents you from having to step\nthrough each line of execution (using `s` or `n`) to get to the part\nof the code you want to examine. Alternatively, you can write\n`import pdb; pdb.set_trace()` before the line you want to break at.\nIf you add `pdb.set_trace()`, your program will automatically start\nin debug mode when you run it. (In other words, you can just type\n`python FILENAME.py` into the command line instead of\n`python -m pdb FILENAME.py`.) Once you're running your file in\ndebug mode, you can step through the code and examine your program's\ninternal state using certain commands. There are many excellent\ntutorials on `pdb` online, including RealPython’s\n[“Python Debugging With Pdb”](https://realpython.com/python-debugging-pdb/).\n\nIDEs like PyCharm or VSCode usually have a debugger built in. In your\nIDE, you can choose to either a) use `pdb` by pulling up a terminal\nwindow in your IDE (e.g. View → Terminal in VSCode), or b) use the\nbuilt-in debugger (usually a graphical wrapper around `pdb`).\n\n(Limitations of Symbolic Tracing)=",
    "1614": "一级标题：torch.fx\n二级标题：Limitations of Symbolic Tracing\n内容：\nFX uses a system of **symbolic tracing** (a.k.a [symbolic\nexecution](https://en.wikipedia.org/wiki/Symbolic_execution))\nto capture the semantics of programs in a transformable/analyzable form.\nThe system is **tracing** in that it executes the program (really a\n{class}`torch.nn.Module` or function) to record operations. It is\n**symbolic** in that the data flowing through the program during this\nexecution is not real data, but rather symbols ({class}`Proxy` in FX parlance).\n\nAlthough symbolic tracing works for most neural net code, it has some\nlimitations.\n\n### Dynamic Control Flow\n\nThe main limitation of symbolic tracing is it does not currently support\n*dynamic control flow*. That is, loops or `if` statements where the\ncondition may depend on the input values of the program.\n\nFor example, let’s examine the following program:\n\n```python\n\ndef func_to_trace(x):\n    if x.sum() > 0:\n        return torch.relu(x)\n    else:\n\n        return torch.neg(x)\n\ntraced = torch.fx.symbolic_trace(func_to_trace)\n\"\"\"\n    <...>\n    File \"dyn.py\", line 6, in func_to_trace\n    if x.sum() > 0:\n    File \"pytorch/torch/fx/proxy.py\", line 155, in __bool__\n    return self.tracer.to_bool(self)\n    File \"pytorch/torch/fx/proxy.py\", line 85, in to_bool\n    raise TraceError('symbolically traced variables cannot be used as inputs to control flow')\ntorch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow\n\"\"\"\n```\nThe condition to the `if` statement relies on the value of `x.sum()`,\nwhich relies on the value of `x`, a function input. Since\n`x` can change (i.e. if you pass a new input tensor to the traced\nfunction), this is *dynamic control flow*. The traceback walks back up\nthrough your code to show you where this situation happens.\n\n### Static Control Flow\n\nOn the other hand, so-called *static control flow* is supported. Static\ncontrol flow is loops or `if` statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model’s architecture based on\nhyper-parameters. As a concrete example:\n\n```python\n\nimport torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self, do_activation : bool = False):\n        super().__init__()\n        self.do_activation = do_activation\n        self.linear = torch.nn.Linear(512, 512)\n\n    def forward(self, x):\n        x = self.linear(x)\n        # This if-statement is so-called static control flow.\n        # Its condition does not depend on any input values\n        if self.do_activation:\n            x = torch.relu(x)\n        return x\n\nwithout_activation = MyModule(do_activation=False)\nwith_activation = MyModule(do_activation=True)\n\ntraced_without_activation = torch.fx.symbolic_trace(without_activation)\nprint(traced_without_activation.code)\n\"\"\"\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    return linear_1\n\"\"\"\n\ntraced_with_activation = torch.fx.symbolic_trace(with_activation)\nprint(traced_with_activation.code)\n\"\"\"\nimport torch\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    relu_1 = torch.relu(linear_1);  linear_1 = None\n    return relu_1\n\"\"\"\n```\nThe if-statement `if self.do_activation` does not depend on any\nfunction inputs, thus it is static. `do_activation` can be considered\nto be a hyper-parameter, and the traces of different instances of\n`MyModule` with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing.\n\nMany instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to `Module` attributes or by binding concrete values to arguments\nduring symbolic tracing:\n\n```python\n\ndef f(x, flag):\n    if flag: return x\n    else: return x*2\n\nfx.symbolic_trace(f) # Fails!\n\nfx.symbolic_trace(f, concrete_args={'flag': True})\n```\nIn the case of truly dynamic control flow, the sections of the program\nthat contain this code can be traced as calls to the Method (see\n{ref}`Customizing Tracing`) or function (see\n{func}`wrap`) rather than tracing through them.\n\n### Non- `torch` Functions\n\nFX uses `__torch_function__` as the mechanism by which it intercepts\ncalls (see the [technical\noverview](https://github.com/pytorch/pytorch/blob/main/torch/fx/README.md#technical-details)\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the `math` module, are not covered by\n`__torch_function__`, but we would still like to capture them in\nsymbolic tracing. For example:\n\n```python\n\nimport torch\nimport torch.fx\nfrom math import sqrt\n\ndef normalize(x):\n    \"\"\"\n    Normalize `x` by the size of the batch dimension\n    \"\"\"\n    return x / sqrt(len(x))\n\n# It's valid Python code\nnormalize(torch.rand(3, 4))\n\ntraced = torch.fx.symbolic_trace(normalize)\n\"\"\"\n    <...>\n    File \"sqrt.py\", line 9, in normalize\n    return x / sqrt(len(x))\n    File \"pytorch/torch/fx/proxy.py\", line 161, in __len__\n    raise RuntimeError(\"'len' is not supported in symbolic tracing by default. If you want \"\nRuntimeError: 'len' is not supported in symbolic tracing by default. If you want this call to be recorded, please call torch.fx.wrap('len') at module scope\n\"\"\"\n```\nThe error tells us that the built-in function `len` is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the {func}`wrap` API:\n\n```python\n\ntorch.fx.wrap('len')\ntorch.fx.wrap('sqrt')\n\ntraced = torch.fx.symbolic_trace(normalize)\n\nprint(traced.code)\n\"\"\"\nimport math\ndef forward(self, x):\n    len_1 = len(x)\n    sqrt_1 = math.sqrt(len_1);  len_1 = None\n    truediv = x / sqrt_1;  x = sqrt_1 = None\n    return truediv\n\"\"\"\n```\n(Customizing Tracing)=\n\n### Customizing Tracing with the `Tracer` class\n\nThe {class}`Tracer` class is the class that underlies the\nimplementation of `symbolic_trace`. The behavior of tracing can be\ncustomized by subclassing Tracer, like so:\n\n```python\n\nclass MyCustomTracer(torch.fx.Tracer):\n    # Inside here you can override various methods\n    # to customize tracing. See the `Tracer` API\n    # reference\n    pass\n\n\n# Let's use this custom tracer to trace through this module\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return torch.relu(x) + torch.ones(3, 4)\n\nmod = MyModule()\n\ntraced_graph = MyCustomTracer().trace(mod)\n# trace() returns a Graph. Let's wrap it up in a\n# GraphModule to make it runnable\ntraced = torch.fx.GraphModule(mod, traced_graph)\n```",
    "1615": "一级标题：torch.fx\n二级标题：Leaf Modules\n内容：\nLeaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard `torch.nn` module instances. For example:\n\n```python\n\nclass MySpecialSubmodule(torch.nn.Module):\n    def forward(self, x):\n        return torch.neg(x)\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n        self.submod = MySpecialSubmodule()\n\n    def forward(self, x):\n        return self.submod(self.linear(x))\n\ntraced = torch.fx.symbolic_trace(MyModule())\nprint(traced.code)\n# `linear` is preserved as a call, yet `submod` is traced though.\n# This is because the default set of \"Leaf Modules\" includes all\n# standard `torch.nn` modules.\n\"\"\"\nimport torch\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    neg_1 = torch.neg(linear_1);  linear_1 = None\n    return neg_1\n\"\"\"\n```\nThe set of leaf modules can be customized by overriding\n{meth}`Tracer.is_leaf_module`.\n\n### Miscellanea\n\n-  Tensor constructors (e.g. `torch.zeros`, `torch.ones`,\n   `torch.rand`, `torch.randn`, `torch.sparse_coo_tensor`)\n   are currently not traceable.\n\n   -  The deterministic constructors (`zeros`, `ones`) can be used\n      and the value they produce will be embedded in the trace as a\n      constant. This is only problematic if the arguments to these\n      constructors refers to dynamic input sizes. In this case,\n      `ones_like` or `zeros_like` may be a viable substitute.\n   -  Nondeterministic constructors (`rand`, `randn`) will have a\n      single random value embedded in the trace. This is likely not the\n      intended behavior. One workaround is to wrap `torch.randn` in a `torch.fx.wrap` function and call that instead.\n\n    ```python\n\n    @torch.fx.wrap\n    def torch_randn(x, shape):\n        return torch.randn(shape)\n\n    def f(x):\n        return x + torch_randn(x, 5)\n    fx.symbolic_trace(f)\n    ```\n   -  This behavior may be fixed in a future release.\n\n-  Type annotations\n\n   -  Python 3-style type annotations (e.g.\n      `func(x : torch.Tensor, y : int) -> torch.Tensor`) are supported\n      and will be preserved by symbolic tracing.\n   -  Python 2-style comment type annotations\n      `# type: (torch.Tensor, int) -> torch.Tensor` are not currently\n      supported.\n   -  Annotations on local names within a function are not currently\n      supported.\n\n\n-  Gotcha around `training` flag and submodules\n\n   -  When using functionals like `torch.nn.functional.dropout`, it will be common for the training argument to be passed in as `self.training`. During FX tracing, this will likely be baked in as a constant value.\n\n    ```python\n\n    import torch\n    import torch.fx\n\n    class DropoutRepro(torch.nn.Module):\n        def forward(self, x):\n        return torch.nn.functional.dropout(x, training=self.training)\n\n\n    traced = torch.fx.symbolic_trace(DropoutRepro())\n    print(traced.code)\n    \"\"\"\n    def forward(self, x):\n        dropout = torch.nn.functional.dropout(x, p = 0.5, training = True, inplace = False);  x = None\n        return dropout\n    \"\"\"\n\n    traced.eval()\n\n    x = torch.randn(5, 3)\n    torch.testing.assert_close(traced(x), x)\n    \"\"\"\n    AssertionError: Tensor-likes are not close!\n\n    Mismatched elements: 15 / 15 (100.0%)\n    Greatest absolute difference: 1.6207983493804932 at index (0, 2) (up to 1e-05 allowed)\n    Greatest relative difference: 1.0 at index (0, 0) (up to 0.0001 allowed)\n    \"\"\"\n    ```\n   - However, when the standard `nn.Dropout()` submodule is used, the training flag is encapsulated and--because of the preservation of the `nn.Module` object model--can be changed.\n\n    ```python\n\n    class DropoutRepro2(torch.nn.Module):\n        def __init__(self):\n        super().__init__()\n        self.drop = torch.nn.Dropout()\n\n        def forward(self, x):\n        return self.drop(x)\n\n    traced = torch.fx.symbolic_trace(DropoutRepro2())\n    print(traced.code)\n    \"\"\"\n    def forward(self, x):\n        drop = self.drop(x);  x = None\n        return drop\n    \"\"\"\n\n    traced.eval()\n\n    x = torch.randn(5, 3)\n    torch.testing.assert_close(traced(x), x)\n    ```\n  - Because of this difference, consider marking modules that interact with the `training` flag dynamically as leaf modules.",
    "1616": "一级标题：torch.fx\n二级标题：API Reference\n内容：\n```{eval-rst}\n.. autofunction:: torch.fx.symbolic_trace\n```\n```{eval-rst}\n.. autofunction:: torch.fx.wrap\n```\n```{eval-rst}\n.. autoclass:: torch.fx.GraphModule\n  :members:\n\n  .. automethod:: __init__\n```\n```{eval-rst}\n.. autoclass:: torch.fx.Graph\n  :members:\n\n  .. automethod:: __init__\n```\n```{eval-rst}\n.. autoclass:: torch.fx.Node\n  :members:\n```\n```{eval-rst}\n.. autoclass:: torch.fx.Tracer\n  :members:\n  :inherited-members:\n```\n```{eval-rst}\n.. autoclass:: torch.fx.Proxy\n```\n```{eval-rst}\n.. autoclass:: torch.fx.Interpreter\n  :members:\n```\n```{eval-rst}\n.. autoclass:: torch.fx.Transformer\n  :members:\n```\n```{eval-rst}\n.. autofunction:: torch.fx.replace_pattern\n```\n\n<!-- The experimental and passes submodules are missing docs. -->\n<!-- Adding it here for coverage but this doesn't add anything to the -->\n<!-- rendered doc. -->\n```{eval-rst}\n.. py:module:: torch.fx.passes\n.. py:module:: torch.fx.passes.infra\n.. py:module:: torch.fx.passes.backends\n.. py:module:: torch.fx.passes.utils\n.. py:module:: torch.fx.passes.tests\n.. py:module:: torch.fx.experimental\n.. py:module:: torch.fx.experimental.unification\n.. py:module:: torch.fx.experimental.unification.multipledispatch\n.. py:module:: torch.fx.experimental.migrate_gradual_types\n.. py:module:: torch.fx.passes.dialect\n.. py:module:: torch.fx.passes.dialect.common\n.. py:module:: torch.fx.annotate\n.. py:module:: torch.fx.config\n.. py:module:: torch.fx.experimental.accelerator_partitioner\n.. py:module:: torch.fx.experimental.const_fold\n.. py:module:: torch.fx.experimental.debug\n.. py:module:: torch.fx.experimental.graph_gradual_typechecker\n.. py:module:: torch.fx.experimental.merge_matmul\n.. py:module:: torch.fx.experimental.meta_tracer\n.. py:module:: torch.fx.experimental.migrate_gradual_types.constraint\n.. py:module:: torch.fx.experimental.migrate_gradual_types.constraint_generator\n.. py:module:: torch.fx.experimental.migrate_gradual_types.constraint_transformation\n.. py:module:: torch.fx.experimental.migrate_gradual_types.operation\n.. py:module:: torch.fx.experimental.migrate_gradual_types.transform_to_z3\n.. py:module:: torch.fx.experimental.migrate_gradual_types.util\n.. py:module:: torch.fx.experimental.migrate_gradual_types.z3_types\n.. py:module:: torch.fx.experimental.normalize\n.. py:module:: torch.fx.experimental.optimization\n.. py:module:: torch.fx.experimental.partitioner_utils\n.. py:module:: torch.fx.experimental.recording\n.. py:module:: torch.fx.experimental.refinement_types\n.. py:module:: torch.fx.experimental.rewriter\n.. py:module:: torch.fx.experimental.schema_type_annotation\n.. py:module:: torch.fx.experimental.sym_node\n.. py:module:: torch.fx.experimental.unification.core\n.. py:module:: torch.fx.experimental.unification.dispatch\n.. py:module:: torch.fx.experimental.unification.match\n.. py:module:: torch.fx.experimental.unification.more\n.. py:module:: torch.fx.experimental.unification.multipledispatch.conflict\n.. py:module:: torch.fx.experimental.unification.multipledispatch.core\n.. py:module:: torch.fx.experimental.unification.multipledispatch.dispatcher\n.. py:module:: torch.fx.experimental.unification.multipledispatch.utils\n.. py:module:: torch.fx.experimental.unification.multipledispatch.variadic\n.. py:module:: torch.fx.experimental.unification.unification_tools\n.. py:module:: torch.fx.experimental.unification.utils\n.. py:module:: torch.fx.experimental.unification.variable\n.. py:module:: torch.fx.experimental.unify_refinements\n.. py:module:: torch.fx.experimental.validator\n.. py:module:: torch.fx.graph\n.. py:module:: torch.fx.graph_module\n.. py:module:: torch.fx.immutable_collections\n.. py:module:: torch.fx.interpreter\n.. py:module:: torch.fx.node\n.. py:module:: torch.fx.operator_schemas\n.. py:module:: torch.fx.passes.annotate_getitem_nodes\n.. py:module:: torch.fx.passes.backends.cudagraphs\n.. py:module:: torch.fx.passes.dialect.common.cse_pass\n.. py:module:: torch.fx.passes.fake_tensor_prop\n.. py:module:: torch.fx.passes.graph_drawer\n.. py:module:: torch.fx.passes.graph_manipulation\n.. py:module:: torch.fx.passes.graph_transform_observer\n.. py:module:: torch.fx.passes.infra.partitioner\n.. py:module:: torch.fx.passes.infra.pass_base\n.. py:module:: torch.fx.passes.infra.pass_manager\n.. py:module:: torch.fx.passes.net_min_base\n.. py:module:: torch.fx.passes.operator_support\n.. py:module:: torch.fx.passes.param_fetch\n.. py:module:: torch.fx.passes.pass_manager\n.. py:module:: torch.fx.passes.reinplace\n.. py:module:: torch.fx.passes.runtime_assert\n.. py:module:: torch.fx.passes.shape_prop\n.. py:module:: torch.fx.passes.split_module\n.. py:module:: torch.fx.passes.split_utils\n.. py:module:: torch.fx.passes.splitter_base\n.. py:module:: torch.fx.passes.tests.test_pass_manager\n.. py:module:: torch.fx.passes.tools_common\n.. py:module:: torch.fx.passes.utils.common\n.. py:module:: torch.fx.passes.utils.fuser_utils\n.. py:module:: torch.fx.passes.utils.matcher_utils\n.. py:module:: torch.fx.passes.utils.matcher_with_name_node_map_utils\n.. py:module:: torch.fx.passes.utils.source_matcher_utils\n.. py:module:: torch.fx.proxy\n.. py:module:: torch.fx.subgraph_rewriter\n.. py:module:: torch.fx.tensor_type\n.. py:module:: torch.fx.traceback\n```",
    "1617": "一级标题：torch.hub\n二级标题：无\n内容：\nPytorch Hub is a pre-trained model repository designed to facilitate research reproducibility.",
    "1618": "一级标题：torch.hub\n二级标题：Publishing models\n内容：\nPytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a GitHub repository by adding a simple `hubconf.py` file;\n\n`hubconf.py` can have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish).\n\n```python\n  def entrypoint_name(*args, **kwargs):\n      # args & kwargs are optional, for models which take positional/keyword arguments.\n      ...\n```\n\n### How to implement an entrypoint?\n\nHere is a code snippet specifies an entrypoint for `resnet18` model if we expand\nthe implementation in `pytorch/vision/hubconf.py`.\nIn most case importing the right function in `hubconf.py` is sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script in\n[pytorch/vision repo](https://github.com/pytorch/vision/blob/master/hubconf.py)\n\n```python\n  dependencies = ['torch']\n  from torchvision.models.resnet import resnet18 as _resnet18\n\n  # resnet18 is the name of entrypoint\n  def resnet18(pretrained=False, **kwargs):\n      \"\"\" # This docstring shows up in hub.help()\n      Resnet18 model\n      pretrained (bool): kwargs, load pretrained weights into the model\n      \"\"\"\n      # Call the model, load pretrained weights\n      model = _resnet18(pretrained=pretrained, **kwargs)\n      return model\n```\n\n- `dependencies` variable is a **list** of package names required to **load** the model. Note this might\n  be slightly different from dependencies required for training a model.\n- `args` and `kwargs` are passed along to the real callable function.\n- Docstring of the function works as a help message. It explains what does the model do and what\n  are the allowed positional/keyword arguments. It's highly recommended to add a few examples here.\n- Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers.\n- Callables prefixed with underscore are considered as helper functions which won't show up in {func}`torch.hub.list()`.\n- Pretrained weights can either be stored locally in the GitHub repo, or loadable by\n  {func}`torch.hub.load_state_dict_from_url()`. If less than 2GB, it's recommended to attach it to a [project release](https://help.github.com/en/articles/distributing-large-binaries)\n  and use the url from the release.\n  In the example above `torchvision.models.resnet.resnet18` handles `pretrained`, alternatively you can put the following logic in the entrypoint definition.\n\n```python\n  if pretrained:\n      # For checkpoint saved in local GitHub repo, e.g. <RELATIVE_PATH_TO_CHECKPOINT>=weights/save.pth\n      dirname = os.path.dirname(__file__)\n      checkpoint = os.path.join(dirname, <RELATIVE_PATH_TO_CHECKPOINT>)\n      state_dict = torch.load(checkpoint)\n      model.load_state_dict(state_dict)\n\n      # For checkpoint saved elsewhere\n      checkpoint = 'https://download.pytorch.org/models/resnet18-5c106cde.pth'\n      model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=False))\n```\n\n### Important Notice\n\n- The published models should be at least in a branch/tag. It can't be a random commit.",
    "1619": "一级标题：torch.hub\n二级标题：Loading models from Hub\n内容：\nPytorch Hub provides convenient APIs to explore all available models in hub\nthrough {func}`torch.hub.list()`, show docstring and examples through\n{func}`torch.hub.help()` and load the pre-trained models using\n{func}`torch.hub.load()`.\n\n```{eval-rst}\n.. automodule:: torch.hub\n```\n\n```{eval-rst}\n.. autofunction:: list\n```\n\n```{eval-rst}\n.. autofunction:: help\n```\n\n```{eval-rst}\n.. autofunction:: load\n```\n\n```{eval-rst}\n.. autofunction:: download_url_to_file\n```\n\n```{eval-rst}\n.. autofunction:: load_state_dict_from_url\n```\n\n### Running a loaded model:\n\nNote that `*args` and `**kwargs` in {func}`torch.hub.load()` are used to\n**instantiate** a model. After you have loaded a model, how can you find out\nwhat you can do with the model?\nA suggested workflow is\n\n- `dir(model)` to see all available methods of the model.\n- `help(model.foo)` to check what arguments `model.foo` takes to run\n\nTo help users explore without referring to documentation back and forth, we strongly\nrecommend repo owners make function help messages clear and succinct. It's also helpful\nto include a minimal working example.\n\n### Where are my downloaded models saved?\n\nThe locations are used in the order of\n\n- Calling `hub.set_dir(<PATH_TO_HUB_DIR>)`\n- `$TORCH_HOME/hub`, if environment variable `TORCH_HOME` is set.\n- `$XDG_CACHE_HOME/torch/hub`, if environment variable `XDG_CACHE_HOME` is set.\n- `~/.cache/torch/hub`\n\n```{eval-rst}\n.. autofunction:: get_dir\n```\n\n```{eval-rst}\n.. autofunction:: set_dir\n```\n\n### Caching logic\n\nBy default, we don't clean up files after loading it. Hub uses the cache by default if it already exists in the\ndirectory returned by {func}`~torch.hub.get_dir()`.\n\nUsers can force a reload by calling `hub.load(..., force_reload=True)`. This will delete\nthe existing GitHub folder and downloaded weights, reinitialize a fresh download. This is useful\nwhen updates are published to the same branch, users can keep up with the latest release.\n\n### Known limitations:\n\nTorch hub works by importing the package as if it was installed. There are some side effects\nintroduced by importing in Python. For example, you can see new items in Python caches\n`sys.modules` and `sys.path_importer_cache` which is normal Python behavior.\nThis also means that you may have import errors when importing different models\nfrom different repos, if the repos have the same sub-package names (typically, a\n`model` subpackage). A workaround for these kinds of import errors is to\nremove the offending sub-package from the `sys.modules` dict; more details can\nbe found in [this GitHub issue](https://github.com/pytorch/hub/issues/243#issuecomment-942403391).\n\nA known limitation that is worth mentioning here: users **CANNOT** load two different branches of\nthe same repo in the **same python process**. It's just like installing two packages with the\nsame name in Python, which is not good. Cache might join the party and give you surprises if you\nactually try that. Of course it's totally fine to load them in separate processes.",
    "1620": "一级标题：TorchScript Language Reference\n二级标题：无\n内容：\n:::{warning}\nTorchScript is deprecated, please use\n[torch.export](https://docs.pytorch.org/docs/stable/export.html) instead.\n:::",
    "1621": "一级标题：TorchScript Language Reference\n二级标题：无\n内容：\n:::{warning}\nTorchScript is deprecated, please use\n[torch.export](https://docs.pytorch.org/docs/stable/export.html) instead.\n:::",
    "1622": "一级标题：Python Language Reference Coverage\n二级标题：无\n内容：\n:::{warning}\nTorchScript is deprecated, please use\n[torch.export](https://docs.pytorch.org/docs/stable/export.html) instead.\n:::",
    "1623": "一级标题：TorchScript Unsupported PyTorch Constructs\n二级标题：无\n内容：\n:::{warning}\nTorchScript is deprecated, please use\n[torch.export](https://docs.pytorch.org/docs/stable/export.html) instead.\n:::",
    "1624": "一级标题：JIT Utils - torch.utils.jit\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.utils.jit\n```",
    "1625": "一级标题：torch.library\n二级标题：无\n内容：\n```{eval-rst}\n.. py:module:: torch.library\n.. currentmodule:: torch.library\n```\n\ntorch.library is a collection of APIs for extending PyTorch's core library\nof operators. It contains utilities for testing custom operators, creating new\ncustom operators, and extending operators defined with PyTorch's C++ operator\nregistration APIs (e.g. aten operators).\n\nFor a detailed guide on effectively using these APIs, please see\n[PyTorch Custom Operators Landing Page](https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html)\nfor more details on how to effectively use these APIs.",
    "1626": "一级标题：torch.library\n二级标题：Testing custom ops\n内容：\nUse {func}`torch.library.opcheck` to test custom ops for incorrect usage of the\nPython torch.library and/or C++ TORCH_LIBRARY APIs. Also, if your operator supports\ntraining, use {func}`torch.autograd.gradcheck` to test that the gradients are\nmathematically correct.\n\n```{eval-rst}\n.. autofunction:: opcheck\n```",
    "1627": "一级标题：torch.library\n二级标题：Creating new custom ops in Python\n内容：\nUse {func}`torch.library.custom_op` to create new custom ops.\n\n```{eval-rst}\n.. autofunction:: custom_op\n.. autofunction:: triton_op\n.. autofunction:: wrap_triton\n```",
    "1628": "一级标题：torch.library\n二级标题：Extending custom ops (created from Python or C++)\n内容：\nUse the `register.*` methods, such as {func}`torch.library.register_kernel` and\n{func}`torch.library.register_fake`, to add implementations\nfor any operators (they may have been created using {func}`torch.library.custom_op` or\nvia PyTorch's C++ operator registration APIs).\n\n```{eval-rst}\n.. autofunction:: register_kernel\n.. autofunction:: register_autocast\n.. autofunction:: register_autograd\n.. autofunction:: register_fake\n.. autofunction:: register_vmap\n.. autofunction:: impl_abstract\n.. autofunction:: get_ctx\n.. autofunction:: register_torch_dispatch\n.. autofunction:: infer_schema\n.. autoclass:: torch._library.custom_ops.CustomOpDef\n   :members: set_kernel_enabled\n.. autofunction:: get_kernel\n```",
    "1629": "一级标题：torch.library\n二级标题：Low-level APIs\n内容：\nThe following APIs are direct bindings to PyTorch's C++ low-level\noperator registration APIs.\n\n```{eval-rst}\n.. warning:: The low-level operator registration APIs and the PyTorch Dispatcher are a complicated PyTorch concept. We recommend you use the higher level APIs above (that do not require a torch.library.Library object) when possible. `This blog post <http://blog.ezyang.com/2020/09/lets-talk-about-the-pytorch-dispatcher/>`_ is a good starting point to learn about the PyTorch Dispatcher.\n```\n\nA tutorial that walks you through some examples on how to use this API is available on [Google Colab](https://colab.research.google.com/drive/1RRhSfk7So3Cn02itzLWE9K4Fam-8U011?usp=sharing).\n\n```{eval-rst}\n.. autoclass:: torch.library.Library\n  :members:\n\n.. autofunction:: fallthrough_kernel\n\n.. autofunction:: define\n\n.. autofunction:: impl\n```",
    "1630": "一级标题：torch.linalg\n二级标题：无\n内容：\nCommon linear algebra operations.\n\nSee {ref}`Linear Algebra Stability` for some common numerical edge-cases.\n\n```{eval-rst}\n.. automodule:: torch.linalg\n.. currentmodule:: torch.linalg\n```",
    "1631": "一级标题：torch.linalg\n二级标题：Matrix Properties\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    norm\n    vector_norm\n    matrix_norm\n    diagonal\n    det\n    slogdet\n    cond\n    matrix_rank\n```",
    "1632": "一级标题：torch.linalg\n二级标题：Decompositions\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    cholesky\n    qr\n    lu\n    lu_factor\n    eig\n    eigvals\n    eigh\n    eigvalsh\n    svd\n    svdvals\n```\n\n(linalg solvers)=",
    "1633": "一级标题：torch.linalg\n二级标题：Solvers\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    solve\n    solve_triangular\n    lu_solve\n    lstsq\n```\n\n(linalg inverses)=",
    "1634": "一级标题：torch.linalg\n二级标题：Inverses\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    inv\n    pinv\n```",
    "1635": "一级标题：torch.linalg\n二级标题：Matrix Functions\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    matrix_exp\n    matrix_power\n```",
    "1636": "一级标题：torch.linalg\n二级标题：Matrix Products\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    cross\n    matmul\n    vecdot\n    multi_dot\n    householder_product\n```",
    "1637": "一级标题：torch.linalg\n二级标题：Tensor Operations\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    tensorinv\n    tensorsolve\n```",
    "1638": "一级标题：torch.linalg\n二级标题：Misc\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    vander\n```",
    "1639": "一级标题：torch.linalg\n二级标题：Experimental Functions\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    cholesky_ex\n    inv_ex\n    solve_ex\n    lu_factor_ex\n    ldl_factor\n    ldl_factor_ex\n    ldl_solve\n```",
    "1640": "一级标题：torch._logging\n二级标题：无\n内容：\nPyTorch has a configurable logging system, where different components can be\ngiven different log level settings. For instance, one component's log messages\ncan be completely disabled, while another component's log messages can be\nset to maximum verbosity.\n\n:::{warning}\nThis feature is in beta and may have compatibility breaking\nchanges in the future.\n:::\n\n:::{warning}\nThis feature has not been expanded to control the log messages of\nall components in PyTorch yet.\n:::\n\nThere are two ways to configure the logging system: through the environment variable `TORCH_LOGS`\nor the python API torch._logging.set_logs.\n\n```{eval-rst}\n.. automodule:: torch._logging\n.. currentmodule:: torch._logging\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    set_logs\n```\n\nThe environment variable `TORCH_LOGS` is a comma-separated list of\n`[+-]<component>` pairs, where `<component>` is a component specified below. The `+` prefix\nwill decrease the log level of the component, displaying more log messages while the `-` prefix\nwill increase the log level of the component and display fewer log messages. The default setting\nis the behavior when a component is not specified in `TORCH_LOGS`. In addition to components, there are\nalso artifacts. Artifacts are specific pieces of debug information associated with a component that are either displayed or not displayed,\nso prefixing an artifact with `+` or `-` will be a no-op. Since they are associated with a component, enabling that component will typically also enable that artifact,\nunless that artifact was specified to be `off_by_default`. This option is specified in _registrations.py for artifacts that are so spammy they should only be displayed when explicitly enabled.\nThe following components and artifacts are configurable through the `TORCH_LOGS` environment\nvariable (see torch._logging.set_logs for the python API):\n\n```{eval-rst}\nComponents:\n        ``all``\n            Special component which configures the default log level of all components. Default: ``logging.WARN``\n\n        ``dynamo``\n            The log level for the TorchDynamo component. Default: ``logging.WARN``\n\n        ``aot``\n            The log level for the AOTAutograd component. Default: ``logging.WARN``\n\n        ``inductor``\n            The log level for the TorchInductor component. Default: ``logging.WARN``\n\n        ``your.custom.module``\n            The log level for an arbitrary unregistered module. Provide the fully qualified name and the module will be enabled. Default: ``logging.WARN``\n```\n\n```{eval-rst}\nArtifacts:\n        ``bytecode``\n            Whether to emit the original and generated bytecode from TorchDynamo.\n            Default: ``False``\n\n        ``aot_graphs``\n            Whether to emit the graphs generated by AOTAutograd. Default: ``False``\n\n        ``aot_joint_graph``\n            Whether to emit the joint forward-backward graph generated by AOTAutograd. Default: ``False``\n\n        ``compiled_autograd``\n            Whether to emit logs from compiled_autograd. Defaults: ``False``\n\n        ``ddp_graphs``\n            Whether to emit graphs generated by DDPOptimizer. Default: ``False``\n\n        ``graph``\n            Whether to emit the graph captured by TorchDynamo in tabular format.\n            Default: ``False``\n\n        ``graph_code``\n            Whether to emit the python source of the graph captured by TorchDynamo.\n            Default: ``False``\n\n        ``graph_breaks``\n            Whether to emit a message when a unique graph break is encountered during\n            TorchDynamo tracing. Default: ``False``\n\n        ``guards``\n            Whether to emit the guards generated by TorchDynamo for each compiled\n            function. Default: ``False``\n\n        ``recompiles``\n            Whether to emit a guard failure reason and message every time\n            TorchDynamo recompiles a function. Default: ``False``\n\n        ``output_code``\n            Whether to emit the TorchInductor output code. Default: ``False``\n\n        ``schedule``\n            Whether to emit the TorchInductor schedule. Default: ``False``\n```\n\n```{eval-rst}\nExamples:\n    ``TORCH_LOGS=\"+dynamo,aot\"`` will set the log level of TorchDynamo to ``logging.DEBUG`` and  AOT to ``logging.INFO``\n\n    ``TORCH_LOGS=\"-dynamo,+inductor\"`` will set the log level of TorchDynamo to ``logging.ERROR`` and  TorchInductor to ``logging.DEBUG``\n\n    ``TORCH_LOGS=\"aot_graphs\"`` will enable the ``aot_graphs`` artifact\n\n    ``TORCH_LOGS=\"+dynamo,schedule\"`` will enable set the log level of TorchDynamo to ``logging.DEBUG`` and enable the ``schedule`` artifact\n\n    ``TORCH_LOGS=\"+some.random.module,schedule\"`` will set the log level of some.random.module to ``logging.DEBUG`` and enable the ``schedule`` artifact\n```",
    "1641": "一级标题：torch.masked\n二级标题：无\n内容：",
    "1642": "一级标题：torch.masked\n二级标题：Introduction\n内容：\n### Motivation\n\n:::{warning}\nThe PyTorch API of masked tensors is in the prototype stage and may or may not change in the future.\n:::\n\nMaskedTensor serves as an extension to {class}`torch.Tensor` that provides the user with the ability to:\n\n* use any masked semantics (e.g. variable length tensors, nan* operators, etc.)\n* differentiate between 0 and NaN gradients\n* various sparse applications (see tutorial below)\n\n\"Specified\" and \"unspecified\" have a long history in PyTorch without formal semantics and certainly without\nconsistency; indeed, MaskedTensor was born out of a build up of issues that the vanilla {class}`torch.Tensor`\nclass could not properly address. Thus, a primary goal of MaskedTensor is to become the source of truth for\nsaid \"specified\" and \"unspecified\" values in PyTorch where they are a first class citizen instead of an afterthought.\nIn turn, this should further unlock [sparsity's](https://pytorch.org/docs/stable/sparse.html) potential,\nenable safer and more consistent operators, and provide a smoother and more intuitive experience\nfor users and developers alike.\n\n### What is a MaskedTensor?\n\nA MaskedTensor is a tensor subclass that consists of 1) an input (data), and 2) a mask. The mask tells us\nwhich entries from the input should be included or ignored.\n\nBy way of example, suppose that we wanted to mask out all values that are equal to 0 (represented by the gray)\nand take the max:\n\n```{eval-rst}\n.. image:: _static/img/masked/tensor_comparison.jpg\n      :scale: 50%\n```\n\nOn top is the vanilla tensor example while the bottom is MaskedTensor where all the 0's are masked out.\nThis clearly yields a different result depending on whether we have the mask, but this flexible structure\nallows the user to systematically ignore any elements they'd like during computation.\n\nThere are already a number of existing tutorials that we've written to help users onboard, such as:\n\n- [Overview – the place to start for new users, discusses how to use MaskedTensors and why they're useful](https://pytorch.org/tutorials/prototype/maskedtensor_overview)\n- [Sparsity – MaskedTensor supports sparse COO and CSR data and mask Tensors](https://pytorch.org/tutorials/prototype/maskedtensor_sparsity)\n- [Adagrad sparse semantics – a practical example of how MaskedTensor can simplify sparse semantics and implementations](https://pytorch.org/tutorials/prototype/maskedtensor_adagrad)\n- [Advanced semantics – discussion on why certain decisions were made (e.g. requiring masks to match for binary/reduction operations), differences with NumPy's MaskedArray, and reduction semantics](https://pytorch.org/tutorials/prototype/maskedtensor_advanced_semantics)",
    "1643": "一级标题：torch.masked\n二级标题：Supported Operators\n内容：\n### Unary Operators\n\nUnary operators are operators that only contain only a single input.\nApplying them to MaskedTensors is relatively straightforward: if the data is masked out at a given index,\nwe apply the operator, otherwise we'll continue to mask out the data.\n\nThe available unary operators are:\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    abs\n    absolute\n    acos\n    arccos\n    acosh\n    arccosh\n    angle\n    asin\n    arcsin\n    asinh\n    arcsinh\n    atan\n    arctan\n    atanh\n    arctanh\n    bitwise_not\n    ceil\n    clamp\n    clip\n    conj_physical\n    cos\n    cosh\n    deg2rad\n    digamma\n    erf\n    erfc\n    erfinv\n    exp\n    exp2\n    expm1\n    fix\n    floor\n    frac\n    lgamma\n    log\n    log10\n    log1p\n    log2\n    logit\n    i0\n    isnan\n    nan_to_num\n    neg\n    negative\n    positive\n    pow\n    rad2deg\n    reciprocal\n    round\n    rsqrt\n    sigmoid\n    sign\n    sgn\n    signbit\n    sin\n    sinc\n    sinh\n    sqrt\n    square\n    tan\n    tanh\n    trunc\n```\n\nThe available inplace unary operators are all of the above **except**:\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    angle\n    positive\n    signbit\n    isnan\n```\n\n### Binary Operators\n\nAs you may have seen in the tutorial, {class}`MaskedTensor` also has binary operations implemented with the caveat\nthat the masks in the two MaskedTensors must match or else an error will be raised. As noted in the error, if you\nneed support for a particular operator or have proposed semantics for how they should behave instead, please open\nan issue on GitHub. For now, we have decided to go with the most conservative implementation to ensure that users\nknow exactly what is going on and are being intentional about their decisions with masked semantics.\n\nThe available binary operators are:\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    add\n    atan2\n    arctan2\n    bitwise_and\n    bitwise_or\n    bitwise_xor\n    bitwise_left_shift\n    bitwise_right_shift\n    div\n    divide\n    floor_divide\n    fmod\n    logaddexp\n    logaddexp2\n    mul\n    multiply\n    nextafter\n    remainder\n    sub\n    subtract\n    true_divide\n    eq\n    ne\n    le\n    ge\n    greater\n    greater_equal\n    gt\n    less_equal\n    lt\n    less\n    maximum\n    minimum\n    fmax\n    fmin\n    not_equal\n```\n\nThe available inplace binary operators are all of the above **except**:\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    logaddexp\n    logaddexp2\n    equal\n    fmin\n    minimum\n    fmax\n```\n\n### Reductions\n\nThe following reductions are available (with autograd support). For more information, the\n[Overview](https://pytorch.org/tutorials/prototype/maskedtensor_overview.html) tutorial\ndetails some examples of reductions, while the\n[Advanced semantics](https://pytorch.org/tutorials/prototype/maskedtensor_advanced_semantics.html) tutorial\nhas some further in-depth discussions about how we decided on certain reduction semantics.\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    sum\n    mean\n    amin\n    amax\n    argmin\n    argmax\n    prod\n    all\n    norm\n    var\n    std\n```\n\n### View and select functions\n\nWe've included a number of view and select functions as well; intuitively, these operators will apply to\nboth the data and the mask and then wrap the result in a {class}`MaskedTensor`. For a quick example,\nconsider {func}`select`:\n\n```python\n    >>> data = torch.arange(12, dtype=torch.float).reshape(3, 4)\n    >>> data\n    tensor([[ 0.,  1.,  2.,  3.],\n            [ 4.,  5.,  6.,  7.],\n            [ 8.,  9., 10., 11.]])\n    >>> mask = torch.tensor([[True, False, False, True], [False, True, False, False], [True, True, True, True]])\n    >>> mt = masked_tensor(data, mask)\n    >>> data.select(0, 1)\n    tensor([4., 5., 6., 7.])\n    >>> mask.select(0, 1)\n    tensor([False,  True, False, False])\n    >>> mt.select(0, 1)\n    MaskedTensor(\n      [      --,   5.0000,       --,       --]\n    )\n```\n\nThe following ops are currently supported:\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    atleast_1d\n    broadcast_tensors\n    broadcast_to\n    cat\n    chunk\n    column_stack\n    dsplit\n    flatten\n    hsplit\n    hstack\n    kron\n    meshgrid\n    narrow\n    nn.functional.unfold\n    ravel\n    select\n    split\n    stack\n    t\n    transpose\n    vsplit\n    vstack\n    Tensor.expand\n    Tensor.expand_as\n    Tensor.reshape\n    Tensor.reshape_as\n    Tensor.unfold\n    Tensor.view\n```\n\n```{eval-rst}\n.. This module needs to be documented. Adding here in the meantime\n.. for tracking purposes\n.. py:module:: torch.masked.maskedtensor.binary\n.. py:module:: torch.masked.maskedtensor.core\n.. py:module:: torch.masked.maskedtensor.creation\n.. py:module:: torch.masked.maskedtensor.passthrough\n.. py:module:: torch.masked.maskedtensor.reductions\n.. py:module:: torch.masked.maskedtensor.unary\n```",
    "1644": "一级标题：Meta device\n二级标题：无\n内容：\nThe \"meta\" device is an abstract device which denotes a tensor which records\nonly metadata, but no actual data.  Meta tensors have two primary use cases:\n\n* Models can be loaded on the meta device, allowing you to load a\n  representation of the model without actually loading the actual parameters\n  into memory.  This can be helpful if you need to make transformations on\n  the model before you load the actual data.\n\n* Most operations can be performed on meta tensors, producing new meta\n  tensors that describe what the result would have been if you performed\n  the operation on a real tensor.  You can use this to perform abstract\n  analysis without needing to spend time on compute or space to represent\n  the actual tensors.  Because meta tensors do not have real data, you cannot\n  perform data-dependent operations like {func}`torch.nonzero` or\n  {meth}`~torch.Tensor.item`.  In some cases, not all device types (e.g., CPU\n  and CUDA) have exactly the same output metadata for an operation; we\n  typically prefer representing the CUDA behavior faithfully in this\n  situation.\n\n```{warning}\nAlthough in principle meta tensor computation should always be faster than\nan equivalent CPU/CUDA computation, many meta tensor implementations are\nimplemented in Python and have not been ported to C++ for speed, so you\nmay find that you get lower absolute framework latency with small CPU tensors.\n```",
    "1645": "一级标题：Meta device\n二级标题：Idioms for working with meta tensors\n内容：\nAn object can be loaded with {func}`torch.load` onto meta device by specifying\n`map_location='meta'`:\n\n```python\n>>> torch.save(torch.randn(2), 'foo.pt')\n>>> torch.load('foo.pt', map_location='meta')\ntensor(..., device='meta', size=(2,))\n```\n\nIf you have some arbitrary code which performs some tensor construction without\nexplicitly specifying a device, you can override it to instead construct on meta device by using\nthe {func}`torch.device` context manager:\n\n```python\n>>> with torch.device('meta'):\n...     print(torch.randn(30, 30))\n...\ntensor(..., device='meta', size=(30, 30))\n```\n\nThis is especially helpful NN module construction, where you often are not\nable to explicitly pass in a device for initialization:\n\n```python\n>>> from torch.nn.modules import Linear\n>>> with torch.device('meta'):\n...     print(Linear(20, 30))\n...\nLinear(in_features=20, out_features=30, bias=True)\n```\n\nYou cannot convert a meta tensor directly to a CPU/CUDA tensor, because the\nmeta tensor stores no data and we do not know what the correct data values for\nyour new tensor are:\n\n```python\n>>> torch.ones(5, device='meta').to(\"cpu\")\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNotImplementedError: Cannot copy out of meta tensor; no data!\n```\n\nUse a factory function like {func}`torch.empty_like` to explicitly specify how\nyou would like the missing data to be filled in.\n\nNN modules have a convenience method {meth}`torch.nn.Module.to_empty` that\nallows you to move the module to another device, leaving all parameters\nuninitialized.  You are expected to explicitly reinitialize the parameters\nmanually:\n\n```python\n>>> from torch.nn.modules import Linear\n>>> with torch.device('meta'):\n...     m = Linear(20, 30)\n>>> m.to_empty(device=\"cpu\")\nLinear(in_features=20, out_features=30, bias=True)\n```\n\n{mod}`torch._subclasses.meta_utils` contains undocumented utilities for taking\nan arbitrary Tensor and constructing an equivalent meta Tensor with high\nfidelity.  These APIs are experimental and may be changed in a BC breaking way\nat any time.",
    "1646": "一级标题：Miscellaneous Environment Variables\n二级标题：无\n内容：\n| Variable                              | Description |\n|---------------------------------------|-------------|\n| `TORCH_FORCE_WEIGHTS_ONLY_LOAD`       | If set to [`1`, `y`, `yes`, `true`], the `torch.load` will use `weights_only=True`. This will happen even if `weights_only=False` was passed at the callsite. For more documentation on this, see [`torch.load`](https://pytorch.org/docs/stable/generated/torch.load.html). |\n| `TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD`    | If set to [`1`, `y`, `yes`, `true`], the `torch.load` will use `weights_only=False` if the `weights_only` variable was not passed at the callsite. For more documentation on this, see [`torch.load`](https://pytorch.org/docs/stable/generated/torch.load.html). |\n| `TORCH_AUTOGRAD_SHUTDOWN_WAIT_LIMIT`  | Under some conditions, autograd threads can hang on shutdown, therefore we do not wait for them to shutdown indefinitely but rely on a timeout that is by default set to `10` seconds. This environment variable can be used to set the timeout in seconds. |\n| `TORCH_DEVICE_BACKEND_AUTOLOAD`       | If set to `1`, out-of-tree backend extensions will be automatically imported when running `import torch`. |",
    "1647": "一级标题：torch.utils.mobile_optimizer\n二级标题：无\n内容：\nPyTorch Mobile is no longer actively supported. Redirecting to [ExecuTorch documentation](https://docs.pytorch.org/executorch).\n\n```{raw} html\n<meta http-equiv=\"Refresh\" content=\"0; url='https://docs.pytorch.org/executorch'\" />\n```\n\n```{warning}\nPyTorch Mobile is no longer actively supported. Please check out\n[ExecuTorch](https://pytorch.org/executorch-overview), PyTorch's\nall-new on-device inference library. You can also review\ndocumentation on [XNNPACK](https://pytorch.org/executorch/stable/native-delegates-executorch-xnnpack-delegate.html)\nand [Vulkan](https://pytorch.org/executorch/stable/native-delegates-executorch-vulkan-delegate.html) delegates.\n```\n```{eval-rst}\n.. currentmodule:: torch.utils.mobile_optimizer\n```\n```{eval-rst}\n.. autofunction:: optimize_for_mobile\n```",
    "1648": "一级标题：torch.utils.model_zoo\n二级标题：无\n内容：\nMoved to `torch.hub`.\n\n```{eval-rst}\n.. automodule:: torch.utils.model_zoo\n```\n```{eval-rst}\n.. autofunction:: load_url\n```",
    "1649": "一级标题：torch.utils.module_tracker\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.utils.module_tracker\n```\n\nThis utility can be used to track the current position inside an {class}`torch.nn.Module` hierarchy.\nIt can be used within other tracking tools to be able to easily associate measured quantities to user-friendly names. This is used in particular in the FlopCounterMode today.\n\n```{eval-rst}\n.. autoclass:: torch.utils.module_tracker.ModuleTracker\n```",
    "1650": "一级标题：torch.monitor\n二级标题：无\n内容：\n```{warning}\nThis module is a prototype release, and its interfaces and functionality may\nchange without warning in future PyTorch releases.\n```\n\n``torch.monitor`` provides an interface for logging events and counters from\nPyTorch.\n\nThe stat interfaces are designed to be used for tracking high level metrics that\nare periodically logged out to be used for monitoring system performance. Since\nthe stats aggregate with a specific window size you can log to them from\ncritical loops with minimal performance impact.\n\nFor more infrequent events or values such as loss, accuracy, usage tracking the\nevent interface can be directly used.\n\nEvent handlers can be registered to handle the events and pass them to an\nexternal event sink.",
    "1651": "一级标题：torch.monitor\n二级标题：API Reference\n内容：\n```{eval-rst}\n.. automodule:: torch.monitor\n```\n\n```{eval-rst}\n.. autoclass:: torch.monitor.Aggregation\n    :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.monitor.Stat\n    :members:\n    :special-members: __init__\n```\n\n```{eval-rst}\n.. autoclass:: torch.monitor.data_value_t\n    :members:\n```\n\n```{eval-rst}\n.. autoclass:: torch.monitor.Event\n    :members:\n    :special-members: __init__\n```\n\n```{eval-rst}\n.. autoclass:: torch.monitor.EventHandlerHandle\n    :members:\n```\n\n```{eval-rst}\n.. autofunction:: torch.monitor.log_event\n```\n\n```{eval-rst}\n.. autofunction:: torch.monitor.register_event_handler\n```\n\n```{eval-rst}\n.. autofunction:: torch.monitor.unregister_event_handler\n```\n\n```{eval-rst}\n.. autoclass:: torch.monitor.TensorboardEventHandler\n    :members:\n    :special-members: __init__\n```",
    "1652": "一级标题：torch.mps\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.mps\n```\n\n```{eval-rst}\n.. currentmodule:: torch.mps\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    device_count\n    synchronize\n    get_rng_state\n    set_rng_state\n    manual_seed\n    seed\n    empty_cache\n    set_per_process_memory_fraction\n    current_allocated_memory\n    driver_allocated_memory\n    recommended_max_memory\n    compile_shader\n```",
    "1653": "一级标题：torch.mps\n二级标题：MPS Profiler\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    profiler.start\n    profiler.stop\n    profiler.profile\n\n    profiler.is_capturing_metal\n    profiler.is_metal_capture_enabled\n    profiler.metal_capture\n```",
    "1654": "一级标题：torch.mps\n二级标题：MPS Event\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    event.Event\n\n```\n\n% This module needs to be documented. Adding here in the meantime\n\n% for tracking purposes\n\n```{eval-rst}\n.. py:module:: torch.mps.event\n```\n\n```{eval-rst}\n.. py:module:: torch.mps.profiler\n```",
    "1655": "一级标题：MPS Environment Variables\n二级标题：无\n内容：\n**PyTorch Environment Variables**\n\n\n| Variable                         | Description |\n|----------------------------------|-------------|\n| `PYTORCH_DEBUG_MPS_ALLOCATOR`   | If set to `1`, set allocator logging level to verbose. |\n| `PYTORCH_MPS_LOG_PROFILE_INFO`  | Set log options bitmask to `MPSProfiler`. See `LogOptions` enum in `aten/src/ATen/mps/MPSProfiler.h`. |\n| `PYTORCH_MPS_TRACE_SIGNPOSTS`   | Set profile and signpost bitmasks to `MPSProfiler`. See `ProfileOptions` and `SignpostTypes`. |\n| `PYTORCH_MPS_HIGH_WATERMARK_RATIO` | High watermark ratio for MPS allocator. Default is 1.7. |\n| `PYTORCH_MPS_LOW_WATERMARK_RATIO` | Low watermark ratio for MPS allocator. Default is 1.4 (unified) or 1.0 (discrete). |\n| `PYTORCH_MPS_FAST_MATH`         | If `1`, enables fast math for MPS kernels. See section 1.6.3 in the [Metal Shading Language Spec](https://developer.apple.com/metal/Metal-Shading-Language-Specification.pdf). |\n| `PYTORCH_MPS_PREFER_METAL`      | If `1`, uses metal kernels instead of MPS Graph APIs. Used for matmul. |\n| `PYTORCH_ENABLE_MPS_FALLBACK`   | If `1`, falls back to CPU when MPS ops aren't supported. |\n\n```{note}\n**high watermark ratio** is a hard limit for the total allowed allocations\n\n- `0.0` : disables high watermark limit (may cause system failure if system-wide OOM occurs)\n- `1.0` : recommended maximum allocation size (i.e., device.recommendedMaxWorkingSetSize)\n- `>1.0`: allows limits beyond the device.recommendedMaxWorkingSetSize\n\ne.g., value 0.95 means we allocate up to 95% of recommended maximum\nallocation size; beyond that, the allocations would fail with OOM error.\n\n**low watermark ratio** is a soft limit to attempt limiting memory allocations up to the lower watermark\nlevel by garbage collection or committing command buffers more frequently (a.k.a, adaptive commit).\nValue between 0 to m_high_watermark_ratio (setting 0.0 disables adaptive commit and garbage collection)\ne.g., value 0.9 means we 'attempt' to limit allocations up to 90% of recommended maximum\nallocation size.\n```",
    "1656": "一级标题：torch.mtia\n二级标题：无\n内容：\nThe MTIA backend is implemented out of the tree, only interfaces are be defined here.\n\n```{eval-rst}\n.. automodule:: torch.mtia\n```\n\n```{eval-rst}\n.. currentmodule:: torch.mtia\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    StreamContext\n    current_device\n    current_stream\n    default_stream\n    device_count\n    init\n    is_available\n    is_initialized\n    memory_stats\n    get_device_capability\n    empty_cache\n    record_memory_history\n    snapshot\n    attach_out_of_memory_observer\n    set_device\n    set_stream\n    stream\n    synchronize\n    device\n    set_rng_state\n    get_rng_state\n    DeferredMtiaCallError\n```",
    "1657": "一级标题：torch.mtia\n二级标题：Streams and events\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    Event\n    Stream\n```",
    "1658": "一级标题：torch.mtia.memory\n二级标题：无\n内容：\nThe MTIA backend is implemented out of the tree, only interfaces are be defined here.\n\n```{eval-rst}\n.. automodule:: torch.mtia.memory\n```\n\n```{eval-rst}\n.. currentmodule:: torch.mtia.memory\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    memory_stats\n    memory_allocated\n```",
    "1659": "一级标题：Multiprocessing package - torch.multiprocessing\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.multiprocessing\n```\n\n```{eval-rst}\n.. currentmodule:: torch.multiprocessing\n```\n\n:::{warning}\nIf the main process exits abruptly (e.g. because of an incoming signal),\nPython's `multiprocessing` sometimes fails to clean up its children.\nIt's a known caveat, so if you're seeing any resource leaks after\ninterrupting the interpreter, it probably means that this has just happened\nto you.\n:::",
    "1660": "一级标题：Multiprocessing package - torch.multiprocessing\n二级标题：Strategy management\n内容：\n```{eval-rst}\n.. autofunction:: get_all_sharing_strategies\n```\n\n```{eval-rst}\n.. autofunction:: get_sharing_strategy\n```\n\n```{eval-rst}\n.. autofunction:: set_sharing_strategy\n\n```\n\n(multiprocessing-cuda-sharing-details)=",
    "1661": "一级标题：Multiprocessing package - torch.multiprocessing\n二级标题：Sharing CUDA tensors\n内容：\nSharing CUDA tensors between processes is supported only in Python 3, using\na `spawn` or `forkserver` start methods.\n\nUnlike CPU tensors, the sending process is required to keep the original tensor\nas long as the receiving process retains a copy of the tensor. The refcounting is\nimplemented under the hood but requires users to follow the next best practices.\n\n:::{warning}\nIf the consumer process dies abnormally to a fatal signal, the shared tensor\ncould be forever kept in memory as long as the sending process is running.\n:::\n\n1. Release memory ASAP in the consumer.\n\n```",
    "1662": "一级标题：Multiprocessing package - torch.multiprocessing\n二级标题：Good\n内容：\nx = queue.get()\n# do somethings with x\ndel x\n```\n\n```",
    "1663": "一级标题：Multiprocessing package - torch.multiprocessing\n二级标题：Bad\n内容：\nx = queue.get()\n# do somethings with x\n# do everything else (producer have to keep x in memory)\n```\n\n2. Keep producer process running until all consumers exits. This will prevent\nthe situation when the producer process releasing memory which is still in use\nby the consumer.\n\n```",
    "1664": "一级标题：Multiprocessing package - torch.multiprocessing\n二级标题：producer\n内容：\n# send tensors, do something\nevent.wait()\n```\n\n```",
    "1665": "一级标题：Multiprocessing package - torch.multiprocessing\n二级标题：consumer\n内容：\n# receive tensors and use them\nevent.set()\n```\n\n3. Don't pass received tensors.\n\n```\n# not going to work\nx = queue.get()\nqueue_2.put(x)\n```\n\n```\n# you need to create a process-local copy\nx = queue.get()\nx_clone = x.clone()\nqueue_2.put(x_clone)\n```\n\n```\n# putting and getting from the same queue in the same process will likely end up with segfault\nqueue.put(tensor)\nx = queue.get()\n```",
    "1666": "一级标题：Multiprocessing package - torch.multiprocessing\n二级标题：Sharing strategies\n内容：\nThis section provides a brief overview into how different sharing strategies\nwork. Note that it applies only to CPU tensor - CUDA tensors will always use\nthe CUDA API, as that's the only way they can be shared.\n\n### File descriptor - `file_descriptor`\n\n:::{note}\nThis is the default strategy (except for macOS and OS X where it's not\nsupported).\n:::\n\nThis strategy will use file descriptors as shared memory handles. Whenever a\nstorage is moved to shared memory, a file descriptor obtained from `shm_open`\nis cached with the object, and when it's going to be sent to other processes,\nthe file descriptor will be transferred (e.g. via UNIX sockets) to it. The\nreceiver will also cache the file descriptor and `mmap` it, to obtain a shared\nview onto the storage data.\n\nNote that if there will be a lot of tensors shared, this strategy will keep a\nlarge number of file descriptors open most of the time. If your system has low\nlimits for the number of open file descriptors, and you can't raise them, you\nshould use the `file_system` strategy.\n\n### File system - `file_system`\n\nThis strategy will use file names given to `shm_open` to identify the shared\nmemory regions. This has a benefit of not requiring the implementation to cache\nthe file descriptors obtained from it, but at the same time is prone to shared\nmemory leaks. The file can't be deleted right after its creation, because other\nprocesses need to access it to open their views. If the processes fatally\ncrash, or are killed, and don't call the storage destructors, the files will\nremain in the system. This is very serious, because they keep using up the\nmemory until the system is restarted, or they're freed manually.\n\nTo counter the problem of shared memory file leaks, {mod}`torch.multiprocessing`\nwill spawn a daemon named `torch_shm_manager` that will isolate itself from\nthe current process group, and will keep track of all shared memory allocations.\nOnce all processes connected to it exit, it will wait a moment to ensure there\nwill be no new connections, and will iterate over all shared memory files\nallocated by the group. If it finds that any of them still exist, they will be\ndeallocated. We've tested this method and it proved to be robust to various\nfailures. Still, if your system has high enough limits, and `file_descriptor`\nis a supported strategy, we do not recommend switching to this one.",
    "1667": "一级标题：Multiprocessing package - torch.multiprocessing\n二级标题：Spawning subprocesses\n内容：\n:::{note}\nAvailable for Python >= 3.4.\n\nThis depends on the `spawn` start method in Python's\n`multiprocessing` package.\n:::\n\nSpawning a number of subprocesses to perform some function can be done\nby creating `Process` instances and calling `join` to wait for\ntheir completion. This approach works fine when dealing with a single\nsubprocess but presents potential issues when dealing with multiple\nprocesses.\n\nNamely, joining processes sequentially implies they will terminate\nsequentially. If they don't, and the first process does not terminate,\nthe process termination will go unnoticed. Also, there are no native\nfacilities for error propagation.\n\nThe `spawn` function below addresses these concerns and takes care\nof error propagation, out of order termination, and will actively\nterminate processes upon detecting an error in one of them.\n\n```{eval-rst}\n.. automodule:: torch.multiprocessing.spawn\n```\n\n```{eval-rst}\n.. currentmodule:: torch.multiprocessing.spawn\n```\n\n```{eval-rst}\n.. autofunction:: spawn\n```\n\n```{eval-rst}\n.. currentmodule:: torch.multiprocessing\n\n```\n\n```{eval-rst}\n.. class:: SpawnContext\n\n   Returned by :func:`~spawn` when called with ``join=False``.\n\n   .. automethod:: join\n\n```\n\n% This module needs to be documented. Adding here in the meantime\n\n% for tracking purposes\n\n```{eval-rst}\n.. py:module:: torch.multiprocessing.pool\n```\n\n```{eval-rst}\n.. py:module:: torch.multiprocessing.queue\n```\n\n```{eval-rst}\n.. py:module:: torch.multiprocessing.reductions\n```",
    "1668": "一级标题：Named Tensors operator coverage\n二级标题：无\n内容：\nPlease read {ref}`named_tensors-doc` first for an introduction to named tensors.\n\nThis document is a reference for *name inference*, a process that defines how\nnamed tensors:\n\n1. use names to provide additional automatic runtime correctness checks\n2. propagate names from input tensors to output tensors\n\nBelow is a list of all operations that are supported with named tensors\nand their associated name inference rules.\n\nIf you don't see an operation listed here, but it would help your use case, please\n[search if an issue has already been filed](https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3A%22module%3A+named+tensor%22) and if not, [file one](https://github.com/pytorch/pytorch/issues/new/choose).\n\n:::{warning}\nThe named tensor API is experimental and subject to change.\n:::\n\n```{eval-rst}\n.. csv-table:: Supported Operations\n   :header: API, Name inference rule\n   :widths: 20, 20\n\n   \":meth:`Tensor.abs`, :func:`torch.abs`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.abs_`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.acos`, :func:`torch.acos`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.acos_`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.add`, :func:`torch.add`\",:ref:`unifies_names_from_inputs-doc`\n   :meth:`Tensor.add_`,:ref:`unifies_names_from_inputs-doc`\n   \":meth:`Tensor.addmm`, :func:`torch.addmm`\",:ref:`contracts_away_dims-doc`\n   :meth:`Tensor.addmm_`,:ref:`contracts_away_dims-doc`\n   \":meth:`Tensor.addmv`, :func:`torch.addmv`\",:ref:`contracts_away_dims-doc`\n   :meth:`Tensor.addmv_`,:ref:`contracts_away_dims-doc`\n   :meth:`Tensor.align_as`,See documentation\n   :meth:`Tensor.align_to`,See documentation\n   \":meth:`Tensor.all`, :func:`torch.all`\",None\n   \":meth:`Tensor.any`, :func:`torch.any`\",None\n   \":meth:`Tensor.asin`, :func:`torch.asin`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.asin_`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.atan`, :func:`torch.atan`\",:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.atan2`, :func:`torch.atan2`\",:ref:`unifies_names_from_inputs-doc`\n   :meth:`Tensor.atan2_`,:ref:`unifies_names_from_inputs-doc`\n   :meth:`Tensor.atan_`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.bernoulli`, :func:`torch.bernoulli`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.bernoulli_`,None\n   :meth:`Tensor.bfloat16`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.bitwise_not`, :func:`torch.bitwise_not`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.bitwise_not_`,None\n   \":meth:`Tensor.bmm`, :func:`torch.bmm`\",:ref:`contracts_away_dims-doc`\n   :meth:`Tensor.bool`,:ref:`keeps_input_names-doc`\n   :meth:`Tensor.byte`,:ref:`keeps_input_names-doc`\n   :func:`torch.cat`,:ref:`unifies_names_from_inputs-doc`\n   :meth:`Tensor.cauchy_`,None\n   \":meth:`Tensor.ceil`, :func:`torch.ceil`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.ceil_`,None\n   :meth:`Tensor.char`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.chunk`, :func:`torch.chunk`\",:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.clamp`, :func:`torch.clamp`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.clamp_`,None\n   :meth:`Tensor.copy_`,:ref:`out_function_semantics-doc`\n   \":meth:`Tensor.cos`, :func:`torch.cos`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.cos_`,None\n   \":meth:`Tensor.cosh`, :func:`torch.cosh`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.cosh_`,None\n   \":meth:`Tensor.acosh`, :func:`torch.acosh`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.acosh_`,None\n   :meth:`Tensor.cpu`,:ref:`keeps_input_names-doc`\n   :meth:`Tensor.cuda`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.cumprod`, :func:`torch.cumprod`\",:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.cumsum`, :func:`torch.cumsum`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.data_ptr`,None\n   \":meth:`Tensor.deg2rad`, :func:`torch.deg2rad`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.deg2rad_`,None\n   \":meth:`Tensor.detach`, :func:`torch.detach`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.detach_`,None\n   \":attr:`Tensor.device`, :func:`torch.device`\",None\n   \":meth:`Tensor.digamma`, :func:`torch.digamma`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.digamma_`,None\n   :meth:`Tensor.dim`,None\n   \":meth:`Tensor.div`, :func:`torch.div`\",:ref:`unifies_names_from_inputs-doc`\n   :meth:`Tensor.div_`,:ref:`unifies_names_from_inputs-doc`\n   \":meth:`Tensor.dot`, :func:`torch.dot`\",None\n   :meth:`Tensor.double`,:ref:`keeps_input_names-doc`\n   :meth:`Tensor.element_size`,None\n   :func:`torch.empty`,:ref:`factory-doc`\n   :func:`torch.empty_like`,:ref:`factory-doc`\n   \":meth:`Tensor.eq`, :func:`torch.eq`\",:ref:`unifies_names_from_inputs-doc`\n   \":meth:`Tensor.erf`, :func:`torch.erf`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.erf_`,None\n   \":meth:`Tensor.erfc`, :func:`torch.erfc`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.erfc_`,None\n   \":meth:`Tensor.erfinv`, :func:`torch.erfinv`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.erfinv_`,None\n   \":meth:`Tensor.exp`, :func:`torch.exp`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.exp_`,None\n   :meth:`Tensor.expand`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.expm1`, :func:`torch.expm1`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.expm1_`,None\n   :meth:`Tensor.exponential_`,None\n   :meth:`Tensor.fill_`,None\n   \":meth:`Tensor.flatten`, :func:`torch.flatten`\",See documentation\n   :meth:`Tensor.float`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.floor`, :func:`torch.floor`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.floor_`,None\n   \":meth:`Tensor.frac`, :func:`torch.frac`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.frac_`,None\n   \":meth:`Tensor.ge`, :func:`torch.ge`\",:ref:`unifies_names_from_inputs-doc`\n   \":meth:`Tensor.get_device`, :func:`torch.get_device`\",None\n   :attr:`Tensor.grad`,None\n   \":meth:`Tensor.gt`, :func:`torch.gt`\",:ref:`unifies_names_from_inputs-doc`\n   :meth:`Tensor.half`,:ref:`keeps_input_names-doc`\n   :meth:`Tensor.has_names`,See documentation\n   \":meth:`Tensor.index_fill`, :func:`torch.index_fill`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.index_fill_`,None\n   :meth:`Tensor.int`,:ref:`keeps_input_names-doc`\n   :meth:`Tensor.is_contiguous`,None\n   :attr:`Tensor.is_cuda`,None\n   \":meth:`Tensor.is_floating_point`, :func:`torch.is_floating_point`\",None\n   :attr:`Tensor.is_leaf`,None\n   :meth:`Tensor.is_pinned`,None\n   :meth:`Tensor.is_shared`,None\n   \":meth:`Tensor.is_signed`, :func:`torch.is_signed`\",None\n   :attr:`Tensor.is_sparse`,None\n   :attr:`Tensor.is_sparse_csr`,None\n   :func:`torch.is_tensor`,None\n   :meth:`Tensor.item`,None\n   :attr:`Tensor.itemsize`,None\n   \":meth:`Tensor.kthvalue`, :func:`torch.kthvalue`\",:ref:`removes_dimensions-doc`\n   \":meth:`Tensor.le`, :func:`torch.le`\",:ref:`unifies_names_from_inputs-doc`\n   \":meth:`Tensor.log`, :func:`torch.log`\",:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.log10`, :func:`torch.log10`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.log10_`,None\n   \":meth:`Tensor.log1p`, :func:`torch.log1p`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.log1p_`,None\n   \":meth:`Tensor.log2`, :func:`torch.log2`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.log2_`,None\n   :meth:`Tensor.log_`,None\n   :meth:`Tensor.log_normal_`,None\n   \":meth:`Tensor.logical_not`, :func:`torch.logical_not`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.logical_not_`,None\n   \":meth:`Tensor.logsumexp`, :func:`torch.logsumexp`\",:ref:`removes_dimensions-doc`\n   :meth:`Tensor.long`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.lt`, :func:`torch.lt`\",:ref:`unifies_names_from_inputs-doc`\n   :func:`torch.manual_seed`,None\n   \":meth:`Tensor.masked_fill`, :func:`torch.masked_fill`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.masked_fill_`,None\n   \":meth:`Tensor.masked_select`, :func:`torch.masked_select`\",Aligns mask up to input and then unifies_names_from_input_tensors\n   \":meth:`Tensor.matmul`, :func:`torch.matmul`\",:ref:`contracts_away_dims-doc`\n   \":meth:`Tensor.mean`, :func:`torch.mean`\",:ref:`removes_dimensions-doc`\n   \":meth:`Tensor.median`, :func:`torch.median`\",:ref:`removes_dimensions-doc`\n   \":meth:`Tensor.nanmedian`, :func:`torch.nanmedian`\",:ref:`removes_dimensions-doc`\n   \":meth:`Tensor.mm`, :func:`torch.mm`\",:ref:`contracts_away_dims-doc`\n   \":meth:`Tensor.mode`, :func:`torch.mode`\",:ref:`removes_dimensions-doc`\n   \":meth:`Tensor.mul`, :func:`torch.mul`\",:ref:`unifies_names_from_inputs-doc`\n   :meth:`Tensor.mul_`,:ref:`unifies_names_from_inputs-doc`\n   \":meth:`Tensor.mv`, :func:`torch.mv`\",:ref:`contracts_away_dims-doc`\n   :attr:`Tensor.names`,See documentation\n   \":meth:`Tensor.narrow`, :func:`torch.narrow`\",:ref:`keeps_input_names-doc`\n   :attr:`Tensor.nbytes`,None\n   :attr:`Tensor.ndim`,None\n   :meth:`Tensor.ndimension`,None\n   \":meth:`Tensor.ne`, :func:`torch.ne`\",:ref:`unifies_names_from_inputs-doc`\n   \":meth:`Tensor.neg`, :func:`torch.neg`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.neg_`,None\n   :func:`torch.normal`,:ref:`keeps_input_names-doc`\n   :meth:`Tensor.normal_`,None\n   \":meth:`Tensor.numel`, :func:`torch.numel`\",None\n   :func:`torch.ones`,:ref:`factory-doc`\n   \":meth:`Tensor.pow`, :func:`torch.pow`\",:ref:`unifies_names_from_inputs-doc`\n   :meth:`Tensor.pow_`,None\n   \":meth:`Tensor.prod`, :func:`torch.prod`\",:ref:`removes_dimensions-doc`\n   \":meth:`Tensor.rad2deg`, :func:`torch.rad2deg`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.rad2deg_`,None\n   :func:`torch.rand`,:ref:`factory-doc`\n   :func:`torch.rand`,:ref:`factory-doc`\n   :func:`torch.randn`,:ref:`factory-doc`\n   :func:`torch.randn`,:ref:`factory-doc`\n   :meth:`Tensor.random_`,None\n   \":meth:`Tensor.reciprocal`, :func:`torch.reciprocal`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.reciprocal_`,None\n   :meth:`Tensor.refine_names`,See documentation\n   :meth:`Tensor.register_hook`,None\n   :meth:`Tensor.register_post_accumulate_grad_hook`,None\n   :meth:`Tensor.rename`,See documentation\n   :meth:`Tensor.rename_`,See documentation\n   :attr:`Tensor.requires_grad`,None\n   :meth:`Tensor.requires_grad_`,None\n   :meth:`Tensor.resize_`,Only allow resizes that do not change shape\n   :meth:`Tensor.resize_as_`,Only allow resizes that do not change shape\n   \":meth:`Tensor.round`, :func:`torch.round`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.round_`,None\n   \":meth:`Tensor.rsqrt`, :func:`torch.rsqrt`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.rsqrt_`,None\n   \":meth:`Tensor.select`, :func:`torch.select`\",:ref:`removes_dimensions-doc`\n   :meth:`Tensor.short`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.sigmoid`, :func:`torch.sigmoid`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.sigmoid_`,None\n   \":meth:`Tensor.sign`, :func:`torch.sign`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.sign_`,None\n   \":meth:`Tensor.sgn`, :func:`torch.sgn`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.sgn_`,None\n   \":meth:`Tensor.sin`, :func:`torch.sin`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.sin_`,None\n   \":meth:`Tensor.sinh`, :func:`torch.sinh`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.sinh_`,None\n   \":meth:`Tensor.asinh`, :func:`torch.asinh`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.asinh_`,None\n   :meth:`Tensor.size`,None\n   \":meth:`Tensor.softmax`, :func:`torch.softmax`\",:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.split`, :func:`torch.split`\",:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.sqrt`, :func:`torch.sqrt`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.sqrt_`,None\n   \":meth:`Tensor.squeeze`, :func:`torch.squeeze`\",:ref:`removes_dimensions-doc`\n   \":meth:`Tensor.std`, :func:`torch.std`\",:ref:`removes_dimensions-doc`\n   :func:`torch.std_mean`,:ref:`removes_dimensions-doc`\n   :meth:`Tensor.stride`,None\n   \":meth:`Tensor.sub`, :func:`torch.sub`\",:ref:`unifies_names_from_inputs-doc`\n   :meth:`Tensor.sub_`,:ref:`unifies_names_from_inputs-doc`\n   \":meth:`Tensor.sum`, :func:`torch.sum`\",:ref:`removes_dimensions-doc`\n   \":meth:`Tensor.tan`, :func:`torch.tan`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.tan_`,None\n   \":meth:`Tensor.tanh`, :func:`torch.tanh`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.tanh_`,None\n   \":meth:`Tensor.atanh`, :func:`torch.atanh`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.atanh_`,None\n   :func:`torch.tensor`,:ref:`factory-doc`\n   :meth:`Tensor.to`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.topk`, :func:`torch.topk`\",:ref:`removes_dimensions-doc`\n   \":meth:`Tensor.transpose`, :func:`torch.transpose`\",:ref:`permutes_dimensions-doc`\n   \":meth:`Tensor.trunc`, :func:`torch.trunc`\",:ref:`keeps_input_names-doc`\n   :meth:`Tensor.trunc_`,None\n   :meth:`Tensor.type`,None\n   :meth:`Tensor.type_as`,:ref:`keeps_input_names-doc`\n   \":meth:`Tensor.unbind`, :func:`torch.unbind`\",:ref:`removes_dimensions-doc`\n   :meth:`Tensor.unflatten`,See documentation\n   :meth:`Tensor.uniform_`,None\n   \":meth:`Tensor.var`, :func:`torch.var`\",:ref:`removes_dimensions-doc`\n   :func:`torch.var_mean`,:ref:`removes_dimensions-doc`\n   :meth:`Tensor.zero_`,None\n   :func:`torch.zeros`,:ref:`factory-doc`\n\n```\n\n(keeps_input_names-doc)=",
    "1669": "一级标题：Named Tensors operator coverage\n二级标题：Keeps input names\n内容：\nAll pointwise unary functions follow this rule as well as some other unary functions.\n\n- Check names: None\n- Propagate names: input tensor's names are propagated to the output.\n\n```\n>>> x = torch.randn(3, 3, names=('N', 'C'))\n>>> x.abs().names\n('N', 'C')\n```\n\n(removes_dimensions-doc)=",
    "1670": "一级标题：Named Tensors operator coverage\n二级标题：Removes dimensions\n内容：\nAll reduction ops like {meth}`~Tensor.sum` remove dimensions by reducing\nover the desired dimensions. Other operations like {meth}`~Tensor.select` and\n{meth}`~Tensor.squeeze` remove dimensions.\n\nWherever one can pass an integer dimension index to an operator, one can also pass\na dimension name. Functions that take lists of dimension indices can also take in a\nlist of dimension names.\n\n- Check names: If {attr}`dim` or {attr}`dims` is passed in as a list of names,\n  check that those names exist in {attr}`self`.\n- Propagate names: If the dimensions of the input tensor specified by {attr}`dim`\n  or {attr}`dims` are not present in the output tensor, then the corresponding names\n  of those dimensions do not appear in `output.names`.\n\n```\n>>> x = torch.randn(1, 3, 3, 3, names=('N', 'C', 'H', 'W'))\n>>> x.squeeze('N').names\n('C', 'H', 'W')\n\n>>> x = torch.randn(3, 3, 3, 3, names=('N', 'C', 'H', 'W'))\n>>> x.sum(['N', 'C']).names\n('H', 'W')\n\n# Reduction ops with keepdim=True don't actually remove dimensions.\n>>> x = torch.randn(3, 3, 3, 3, names=('N', 'C', 'H', 'W'))\n>>> x.sum(['N', 'C'], keepdim=True).names\n('N', 'C', 'H', 'W')\n```\n\n(unifies_names_from_inputs-doc)=",
    "1671": "一级标题：Named Tensors operator coverage\n二级标题：Unifies names from inputs\n内容：\nAll binary arithmetic ops follow this rule. Operations that broadcast still\nbroadcast positionally from the right to preserve compatibility with unnamed\ntensors. To perform explicit broadcasting by names, use {meth}`Tensor.align_as`.\n\n- Check names: All names must match positionally from the right. i.e., in\n  `tensor + other`, `match(tensor.names[i], other.names[i])` must be true for all\n  `i` in `(-min(tensor.dim(), other.dim()) + 1, -1]`.\n- Check names: Furthermore, all named dimensions must be aligned from the right.\n  During matching, if we match a named dimension `A` with an unnamed dimension\n  `None`, then `A` must not appear in the tensor with the unnamed dimension.\n- Propagate names: unify pairs of names from the right from both tensors to\n  produce output names.\n\nFor example,\n\n```\n# tensor: Tensor[   N, None]\n# other:  Tensor[None,    C]\n>>> tensor = torch.randn(3, 3, names=('N', None))\n>>> other = torch.randn(3, 3, names=(None, 'C'))\n>>> (tensor + other).names\n('N', 'C')\n```\n\nCheck names:\n\n- `match(tensor.names[-1], other.names[-1])` is `True`\n- `match(tensor.names[-2], tensor.names[-2])` is `True`\n- Because we matched `None` in {attr}`tensor` with `'C'`,\n  check to make sure `'C'` doesn't exist in {attr}`tensor` (it does not).\n- Check to make sure `'N'` doesn't exists in {attr}`other` (it does not).\n\nFinally, the output names are computed with\n`[unify('N', None), unify(None, 'C')] = ['N', 'C']`\n\nMore examples:\n\n```\n# Dimensions don't match from the right:\n# tensor: Tensor[N, C]\n# other:  Tensor[   N]\n>>> tensor = torch.randn(3, 3, names=('N', 'C'))\n>>> other = torch.randn(3, names=('N',))\n>>> (tensor + other).names\nRuntimeError: Error when attempting to broadcast dims ['N', 'C'] and dims\n['N']: dim 'C' and dim 'N' are at the same position from the right but do\nnot match.\n\n# Dimensions aren't aligned when matching tensor.names[-1] and other.names[-1]:\n# tensor: Tensor[N, None]\n# other:  Tensor[      N]\n>>> tensor = torch.randn(3, 3, names=('N', None))\n>>> other = torch.randn(3, names=('N',))\n>>> (tensor + other).names\nRuntimeError: Misaligned dims when attempting to broadcast dims ['N'] and\ndims ['N', None]: dim 'N' appears in a different position from the right\nacross both lists.\n```\n\n:::{note}\nIn both of the last examples, it is possible to align the tensors by names\nand then perform the addition. Use {meth}`Tensor.align_as` to align\ntensors by name or {meth}`Tensor.align_to` to align tensors to a custom\ndimension ordering.\n:::\n\n(permutes_dimensions-doc)=",
    "1672": "一级标题：Named Tensors operator coverage\n二级标题：Permutes dimensions\n内容：\nSome operations, like {meth}`Tensor.t()`, permute the order of dimensions. Dimension names\nare attached to individual dimensions so they get permuted as well.\n\nIf the operator takes in positional index {attr}`dim`, it is also able to take a dimension\nname as {attr}`dim`.\n\n- Check names: If {attr}`dim` is passed as a name, check that it exists in the tensor.\n- Propagate names: Permute dimension names in the same way as the dimensions that are\n  being permuted.\n\n```\n>>> x = torch.randn(3, 3, names=('N', 'C'))\n>>> x.transpose('N', 'C').names\n('C', 'N')\n```\n\n(contracts_away_dims-doc)=",
    "1673": "一级标题：Named Tensors operator coverage\n二级标题：Contracts away dims\n内容：\nMatrix multiply functions follow some variant of this. Let's go through\n{func}`torch.mm` first and then generalize the rule for batch matrix multiplication.\n\nFor `torch.mm(tensor, other)`:\n\n- Check names: None\n- Propagate names: result names are `(tensor.names[-2], other.names[-1])`.\n\n```\n>>> x = torch.randn(3, 3, names=('N', 'D'))\n>>> y = torch.randn(3, 3, names=('in', 'out'))\n>>> x.mm(y).names\n('N', 'out')\n```\n\nInherently, a matrix multiplication performs a dot product over two dimensions,\ncollapsing them. When two tensors are matrix-multiplied, the contracted dimensions\ndisappear and do not show up in the output tensor.\n\n{func}`torch.mv`, {func}`torch.dot` work in a similar way: name inference does not\ncheck input names and removes the dimensions that are involved in the dot product:\n\n```\n>>> x = torch.randn(3, 3, names=('N', 'D'))\n>>> y = torch.randn(3, names=('something',))\n>>> x.mv(y).names\n('N',)\n```\n\nNow, let's take a look at `torch.matmul(tensor, other)`. Assume that `tensor.dim() >= 2`\nand `other.dim() >= 2`.\n\n- Check names: Check that the batch dimensions of the inputs are aligned and broadcastable.\n  See {ref}`unifies_names_from_inputs-doc` for what it means for the inputs to be aligned.\n- Propagate names: result names are obtained by unifying the batch dimensions and removing\n  the contracted dimensions:\n  `unify(tensor.names[:-2], other.names[:-2]) + (tensor.names[-2], other.names[-1])`.\n\nExamples:\n\n```\n# Batch matrix multiply of matrices Tensor['C', 'D'] and Tensor['E', 'F'].\n# 'A', 'B' are batch dimensions.\n>>> x = torch.randn(3, 3, 3, 3, names=('A', 'B', 'C', 'D'))\n>>> y = torch.randn(3, 3, 3, names=('B', 'E', 'F'))\n>>> torch.matmul(x, y).names\n('A', 'B', 'C', 'F')\n```\n\nFinally, there are fused `add` versions of many matmul functions. i.e., {func}`addmm`\nand {func}`addmv`. These are treated as composing name inference for i.e. {func}`mm` and\nname inference for {func}`add`.\n\n(factory-doc)=",
    "1674": "一级标题：Named Tensors operator coverage\n二级标题：Factory functions\n内容：\nFactory functions now take a new {attr}`names` argument that associates a name\nwith each dimension.\n\n```\n>>> torch.zeros(2, 3, names=('N', 'C'))\ntensor([[0., 0., 0.],\n        [0., 0., 0.]], names=('N', 'C'))\n```\n\n(out_function_semantics-doc)=",
    "1675": "一级标题：Named Tensors operator coverage\n二级标题：out function and in-place variants\n内容：\nA tensor specified as an `out=` tensor has the following behavior:\n\n- If it has no named dimensions, then the names computed from the operation\n  get propagated to it.\n- If it has any named dimensions, then the names computed from the operation\n  must be exactly equal to the existing names. Otherwise, the operation errors.\n\nAll in-place methods modify inputs to have names equal to the computed names\nfrom name inference. For example:\n\n```\n>>> x = torch.randn(3, 3)\n>>> y = torch.randn(3, 3, names=('N', 'C'))\n>>> x.names\n(None, None)\n\n>>> x += y\n>>> x.names\n('N', 'C')\n```",
    "1676": "一级标题：Named Tensors\n二级标题：无\n内容：\nNamed Tensors allow users to give explicit names to tensor dimensions.\nIn most cases, operations that take dimension parameters will accept\ndimension names, avoiding the need to track dimensions by position.\nIn addition, named tensors use names to automatically check that APIs\nare being used correctly at runtime, providing extra safety. Names can\nalso be used to rearrange dimensions, for example, to support\n\"broadcasting by name\" rather than \"broadcasting by position\".\n\n\n```{warning}\n    The named tensor API is a prototype feature and subject to change.\n```",
    "1677": "一级标题：Named Tensors\n二级标题：Creating named tensors\n内容：\nFactory functions now take a new {attr}`names` argument that associates a name\nwith each dimension.\n\n```\n    >>> torch.zeros(2, 3, names=('N', 'C'))\n    tensor([[0., 0., 0.],\n            [0., 0., 0.]], names=('N', 'C'))\n```\n\nNamed dimensions, like regular Tensor dimensions, are ordered.\n``tensor.names[i]`` is the name of dimension ``i`` of ``tensor``.\n\nThe following factory functions support named tensors:\n\n- {func}`torch.empty`\n- {func}`torch.rand`\n- {func}`torch.randn`\n- {func}`torch.ones`\n- {func}`torch.tensor`\n- {func}`torch.zeros`",
    "1678": "一级标题：Named Tensors\n二级标题：Named dimensions\n内容：\nSee {attr}`~Tensor.names` for restrictions on tensor names.\n\nUse {attr}`~Tensor.names` to access the dimension names of a tensor and\n{meth}`~Tensor.rename` to rename named dimensions.\n\n```\n    >>> imgs = torch.randn(1, 2, 2, 3 , names=('N', 'C', 'H', 'W'))\n    >>> imgs.names\n    ('N', 'C', 'H', 'W')\n\n    >>> renamed_imgs = imgs.rename(H='height', W='width')\n    >>> renamed_imgs.names\n    ('N', 'C', 'height', 'width)\n```\n\nNamed tensors can coexist with unnamed tensors; named tensors are instances of\n{class}`torch.Tensor`. Unnamed tensors have ``None``-named dimensions. Named\ntensors do not require all dimensions to be named.\n\n```\n    >>> imgs = torch.randn(1, 2, 2, 3 , names=(None, 'C', 'H', 'W'))\n    >>> imgs.names\n    (None, 'C', 'H', 'W')\n```",
    "1679": "一级标题：Named Tensors\n二级标题：Name propagation semantics\n内容：\nNamed tensors use names to automatically check that APIs are being called\ncorrectly at runtime. This occurs in a process called *name inference*.\nMore formally, name inference consists of the following two steps:\n\n- **Check names**: an operator may perform automatic checks at runtime that\n  check that certain dimension names must match.\n- **Propagate names**: name inference propagates names to output tensors.\n\nAll operations that support named tensors propagate names.\n\n```\n    >>> x = torch.randn(3, 3, names=('N', 'C'))\n    >>> x.abs().names\n    ('N', 'C')\n```\n\n\n(match_semantics-doc)=\n### match semantics\n\n\nTwo names *match* if they are equal (string equality) or if at least one is ``None``.\nNones are essentially a special \"wildcard\" name.\n\n``unify(A, B)`` determines which of the names ``A`` and ``B`` to propagate to the outputs.\nIt returns the more *specific* of the two names, if they match. If the names do not match,\nthen it errors.\n\n```{note}\nIn practice, when working with named tensors, one should avoid having unnamed\ndimensions because their handling can be complicated. It is recommended to lift\nall unnamed dimensions to be named dimensions by using {meth}`~Tensor.refine_names`.\n```\n\n### Basic name inference rules\n\nLet's see how ``match`` and ``unify`` are used in name inference in the case of\nadding two one-dim tensors with no broadcasting.\n\n```\n    x = torch.randn(3, names=('X',))\n    y = torch.randn(3)\n    z = torch.randn(3, names=('Z',))\n```\n\n**Check names**: check that the names of the two tensors *match*.\n\nFor the following examples:\n\n```\n    >>> # x + y  # match('X', None) is True\n    >>> # x + z  # match('X', 'Z') is False\n    >>> # x + x  # match('X', 'X') is True\n\n    >>> x + z\n    Error when attempting to broadcast dims ['X'] and dims ['Z']: dim 'X' and dim 'Z' are at the same position from the right but do not match.\n```\n\n**Propagate names**: *unify* the names to select which one to propagate.\nIn the case of ``x + y``, ``unify('X', None) = 'X'`` because ``'X'`` is more\nspecific than ``None``.\n\n```\n    >>> (x + y).names\n    ('X',)\n    >>> (x + x).names\n    ('X',)\n```\n\nFor a comprehensive list of name inference rules, see {ref}`name_inference_reference-doc`.\nHere are two common operations that may be useful to go over:\n\n- Binary arithmetic ops: {ref}`unifies_names_from_inputs-doc`\n- Matrix multiplication ops: {ref}`contracts_away_dims-doc`",
    "1680": "一级标题：Named Tensors\n二级标题：Explicit alignment by names\n内容：\nUse {meth}`~Tensor.align_as` or {meth}`~Tensor.align_to` to align tensor dimensions\nby name to a specified ordering. This is useful for performing \"broadcasting by names\".\n\n```\n    # This function is agnostic to the dimension ordering of `input`,\n    # as long as it has a `C` dimension somewhere.\n    def scale_channels(input, scale):\n        scale = scale.refine_names('C')\n        return input * scale.align_as(input)\n\n    >>> num_channels = 3\n    >>> scale = torch.randn(num_channels, names=('C',))\n    >>> imgs = torch.rand(3, 3, 3, num_channels, names=('N', 'H', 'W', 'C'))\n    >>> more_imgs = torch.rand(3, num_channels, 3, 3, names=('N', 'C', 'H', 'W'))\n    >>> videos = torch.randn(3, num_channels, 3, 3, 3, names=('N', 'C', 'H', 'W', 'D')\n\n    >>> scale_channels(imgs, scale)\n    >>> scale_channels(more_imgs, scale)\n    >>> scale_channels(videos, scale)\n```",
    "1681": "一级标题：Named Tensors\n二级标题：Manipulating dimensions\n内容：\nUse {meth}`~Tensor.align_to` to permute large amounts of dimensions without\nmentioning all of them as in required by {meth}`~Tensor.permute`.\n\n```\n    >>> tensor = torch.randn(2, 2, 2, 2, 2, 2)\n    >>> named_tensor = tensor.refine_names('A', 'B', 'C', 'D', 'E', 'F')\n\n    # Move the F (dim 5) and E dimension (dim 4) to the front while keeping\n    # the rest in the same order\n    >>> tensor.permute(5, 4, 0, 1, 2, 3)\n    >>> named_tensor.align_to('F', 'E', ...)\n```\n\nUse {meth}`~Tensor.flatten` and {meth}`~Tensor.unflatten` to flatten and unflatten\ndimensions, respectively. These methods are more verbose than {meth}`~Tensor.view`\nand {meth}`~Tensor.reshape`, but have more semantic meaning to someone reading the code.\n\n\n```\n    >>> imgs = torch.randn(32, 3, 128, 128)\n    >>> named_imgs = imgs.refine_names('N', 'C', 'H', 'W')\n\n    >>> flat_imgs = imgs.view(32, -1)\n    >>> named_flat_imgs = named_imgs.flatten(['C', 'H', 'W'], 'features')\n    >>> named_flat_imgs.names\n    ('N', 'features')\n\n    >>> unflattened_named_imgs = named_flat_imgs.unflatten('features', [('C', 3), ('H', 128), ('W', 128)])\n    >>> unflattened_named_imgs.names\n    ('N', 'C', 'H', 'W')\n```\n\n(named_tensors_autograd-doc)=",
    "1682": "一级标题：Named Tensors\n二级标题：Autograd support\n内容：\nAutograd currently supports named tensors in a limited manner: autograd ignores\nnames on all tensors. Gradient computation is still correct but we lose the\nsafety that names give us.\n\n```\n    >>> x = torch.randn(3, names=('D',))\n    >>> weight = torch.randn(3, names=('D',), requires_grad=True)\n    >>> loss = (x - weight).abs()\n    >>> grad_loss = torch.randn(3)\n    >>> loss.backward(grad_loss)\n    >>> weight.grad  # Unnamed for now. Will be named in the future\n    tensor([-1.8107, -0.6357,  0.0783])\n\n    >>> weight.grad.zero_()\n    >>> grad_loss = grad_loss.refine_names('C')\n    >>> loss = (x - weight).abs()\n    # Ideally we'd check that the names of loss and grad_loss match but we don't yet.\n    >>> loss.backward(grad_loss)\n    >>> weight.grad\n    tensor([-1.8107, -0.6357,  0.0783])\n```",
    "1683": "一级标题：Named Tensors\n二级标题：Currently supported operations and subsystems\n内容：\n### Operators\n\nSee {ref}`name_inference_reference-doc` for a full list of the supported torch and\ntensor operations. We do not yet support the following that is not covered by the link:\n\n- indexing, advanced indexing.\n\nFor ``torch.nn.functional`` operators, we support the following:\n\n- {func}`torch.nn.functional.relu`\n- {func}`torch.nn.functional.softmax`\n- {func}`torch.nn.functional.log_softmax`\n- {func}`torch.nn.functional.tanh`\n- {func}`torch.nn.functional.sigmoid`\n- {func}`torch.nn.functional.dropout`\n\n### Subsystems\n\n\nAutograd is supported, see {ref}`named_tensors_autograd-doc`.\nBecause gradients are currently unnamed, optimizers may work but are untested.\n\nNN modules are currently unsupported. This can lead to the following when calling\nmodules with named tensor inputs:\n\n- NN module parameters are unnamed, so outputs may be partially named.\n- NN module forward passes have code that don't support named tensors and will\n  error out appropriately.\n\nWe also do not support the following subsystems, though some may work out\nof the box:\n\n- distributions\n- serialization ({func}`torch.load`, {func}`torch.save`)\n- multiprocessing\n- JIT\n- distributed\n- ONNX\n\nIf any of these would help your use case, please\n[search if an issue has already been filed](https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3A%22module%3A+named+tensor%22)\nand if not, [file one](https://github.com/pytorch/pytorch/issues/new/choose).",
    "1684": "一级标题：Named Tensors\n二级标题：Named tensor API reference\n内容：\nIn this section please find the documentation for named tensor specific APIs.\nFor a comprehensive reference for how names are propagated through other PyTorch\noperators, see {ref}`name_inference_reference-doc`.\n\n```{eval-rst}\n.. class:: Tensor()\n   :noindex:\n\n   .. autoattribute:: names\n\n   .. automethod:: rename\n\n   .. automethod:: rename_\n\n   .. automethod:: refine_names\n\n   .. automethod:: align_as\n\n   .. automethod:: align_to\n\n   .. py:method:: flatten(dims, out_dim) -> Tensor\n      :noindex:\n\n      Flattens :attr:`dims` into a single dimension with name :attr:`out_dim`.\n\n      All of `dims` must be consecutive in order in the :attr:`self` tensor,\n      but not necessary contiguous in memory.\n\n      Examples::\n\n          >>> imgs = torch.randn(32, 3, 128, 128, names=('N', 'C', 'H', 'W'))\n          >>> flat_imgs = imgs.flatten(['C', 'H', 'W'], 'features')\n          >>> flat_imgs.names, flat_imgs.shape\n          (('N', 'features'), torch.Size([32, 49152]))\n\n      .. warning::\n          The named tensor API is experimental and subject to change.\n```",
    "1685": "一级标题：torch.nested\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.nested\n```",
    "1686": "一级标题：torch.nested\n二级标题：Introduction\n内容：\n```{warning}\n  The PyTorch API of nested tensors is in prototype stage and will change in the near future.\n```\n\nNested tensors allow for ragged-shaped data to be contained within and operated upon as a\nsingle tensor. Such data is stored underneath in an efficient packed representation, while exposing\na standard PyTorch tensor interface for applying operations.\n\nA common application of nested tensors is for expressing batches of variable-length sequential data\npresent in various domains, such as varying sentence lengths, image sizes, and audio / video clip\nlengths. Traditionally, such data has been handled by padding sequences to that of the max length\nwithin a batch, performing computation on the padded form, and subsequently masking to remove\npadding. This is inefficient and error-prone, and nested tensors exist to address these problems.\n\nThe API for calling operations on a nested tensor is no different from that of a regular\n``torch.Tensor``, allowing for seamless integration with existing models, with the main\ndifference being {ref}`construction of the inputs <construction>`.\n\nAs this is a prototype feature, the set of {ref}`operations supported <supported operations>` is\nlimited, but growing. We welcome issues, feature requests, and contributions.\nMore information on contributing can be found\n[in this Readme](https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/nested/README.md).\n\n(construction)=",
    "1687": "一级标题：torch.nested\n二级标题：Construction\n内容：\n```{note}\n\n  There are two forms of nested tensors present within PyTorch, distinguished by layout as\n  specified during construction. Layout can be one of ``torch.strided`` or ``torch.jagged``.\n  We recommend utilizing the ``torch.jagged`` layout whenever possible. While it currently only\n  supports a single ragged dimension, it has better op coverage, receives active development, and\n  integrates well with ``torch.compile``. These docs adhere to this recommendation and refer to\n  nested tensors with the ``torch.jagged`` layout as \"NJTs\" for brevity throughout.\n```\n\nConstruction is straightforward and involves passing a list of tensors to the\n``torch.nested.nested_tensor`` constructor. A nested tensor with the ``torch.jagged`` layout\n(AKA an \"NJT\") supports a single ragged dimension. This constructor will copy the input tensors\ninto a packed, contiguous block of memory according to the layout described in the `data_layout`_\nsection below.\n\n```\n>>> a, b = torch.arange(3), torch.arange(5) + 3\n>>> a\ntensor([0, 1, 2])\n>>> b\ntensor([3, 4, 5, 6, 7])\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n>>> print([component for component in nt])\n[tensor([0, 1, 2]), tensor([3, 4, 5, 6, 7])]\n```\n\nEach tensor in the list must have the same number of dimensions, but the shapes can otherwise vary\nalong a single dimension. If the dimensionalities of the input components don't match, the\nconstructor throws an error.\n```\n>>> a = torch.randn(50, 128) # 2D tensor\n>>> b = torch.randn(2, 50, 128) # 3D tensor\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n...\nRuntimeError: When constructing a nested tensor, all tensors in list must have the same dim\n```\n\nDuring construction, dtype, device, and whether gradients are required can be chosen via the\nusual keyword arguments.\n\n```\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32, device=\"cuda\", requires_grad=True)\n>>> print([component for component in nt])\n[tensor([0., 1., 2.], device='cuda:0',\n       grad_fn=<UnbindBackwardAutogradNestedTensor0>), tensor([3., 4., 5., 6., 7.], device='cuda:0',\n       grad_fn=<UnbindBackwardAutogradNestedTensor0>)]\n```\n\n``torch.nested.as_nested_tensor`` can be used to preserve autograd history from the tensors passed\nto the constructor. When this constructor is utilized, gradients will flow through the nested tensor\nback into the original components. Note that this constructor still copies the input components into\na packed, contiguous block of memory.\n\n```\n>>> a = torch.randn(12, 512, requires_grad=True)\n>>> b = torch.randn(23, 512, requires_grad=True)\n>>> nt = torch.nested.as_nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)\n>>> nt.sum().backward()\n>>> a.grad\ntensor([[1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        ...,\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.]])\n>>> b.grad\ntensor([[1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        ...,\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.],\n        [1., 1., 1.,  ..., 1., 1., 1.]])\n```\n\nThe above functions all create contiguous NJTs, where a chunk of memory is allocated to store\na packed form of the underlying components (see the `data_layout`_ section below for more\ndetails).\n\nIt is also possible to create a non-contiguous NJT view over a pre-existing dense tensor\nwith padding, avoiding the memory allocation and copying. ``torch.nested.narrow()`` is the tool\nfor accomplishing this.\n\n```\n>>> padded = torch.randn(3, 5, 4)\n>>> seq_lens = torch.tensor([3, 2, 5], dtype=torch.int64)\n>>> nt = torch.nested.narrow(padded, dim=1, start=0, length=seq_lens, layout=torch.jagged)\n>>> nt.shape\ntorch.Size([3, j1, 4])\n>>> nt.is_contiguous()\nFalse\n```\n\nNote that the nested tensor acts as a view over the original padded dense tensor, referencing the\nsame memory without copying / allocation. Operation support for non-contiguous NJTs is somewhat more\nlimited, so if you run into support gaps, it's always possible to convert to a contiguous NJT\nusing ``contiguous()``.\n\n(data_layout)=",
    "1688": "一级标题：torch.nested\n二级标题：Data Layout and Shape\n内容：\nFor efficiency, nested tensors generally pack their tensor components into a contiguous chunk of\nmemory and maintain additional metadata to specify batch item boundaries. For the ``torch.jagged``\nlayout, the contiguous chunk of memory is stored in the ``values`` component, with the ``offsets``\ncomponent delineating batch item boundaries for the ragged dimension.\n\n![image](_static/img/nested/njt_visual.png)\n\nIt's possible to directly access the underlying NJT components when necessary.\n\n```\n>>> a = torch.randn(50, 128) # text 1\n>>> b = torch.randn(32, 128) # text 2\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)\n>>> nt.values().shape  # note the \"packing\" of the ragged dimension; no padding needed\ntorch.Size([82, 128])\n>>> nt.offsets()\ntensor([ 0, 50, 82])\n```\n\nIt can also be useful to construct an NJT from the jagged ``values`` and ``offsets``\nconstituents directly; the ``torch.nested.nested_tensor_from_jagged()`` constructor serves\nthis purpose.\n\n```\n>>> values = torch.randn(82, 128)\n>>> offsets = torch.tensor([0, 50, 82], dtype=torch.int64)\n>>> nt = torch.nested.nested_tensor_from_jagged(values=values, offsets=offsets)\n```\n\nAn NJT has a well-defined shape with dimensionality 1 greater than that of its components. The\nunderlying structure of the ragged dimension is represented by a symbolic value (``j1`` in the\nexample below).\n\n```\n>>> a = torch.randn(50, 128)\n>>> b = torch.randn(32, 128)\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)\n>>> nt.dim()\n3\n>>> nt.shape\ntorch.Size([2, j1, 128])\n```\n\nNJTs must have the same ragged structure to be compatible with each other. For example, to run a\nbinary operation involving two NJTs, the ragged structures must match (i.e. they must have the\nsame ragged shape symbol in their shapes). In the details, each symbol corresponds with an exact\n``offsets`` tensor, so both NJTs must have the same ``offsets`` tensor to be compatible with\neach other.\n\n```\n>>> a = torch.randn(50, 128)\n>>> b = torch.randn(32, 128)\n>>> nt1 = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)\n>>> nt2 = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)\n>>> nt1.offsets() is nt2.offsets()\nFalse\n>>> nt3 = nt1 + nt2\nRuntimeError: cannot call binary pointwise function add.Tensor with inputs of shapes (2, j2, 128) and (2, j3, 128)\n```\n\nIn the above example, even though the conceptual shapes of the two NJTs are the same, they don't\nshare a reference to the same ``offsets`` tensor, so their shapes differ, and they are not\ncompatible. We recognize that this behavior is unintuitive and are working hard to relax this\nrestriction for the beta release of nested tensors. For a workaround, see the\n{ref}`Troubleshooting <ragged_structure_incompatibility>` section of this document.\n\nIn addition to the ``offsets`` metadata, NJTs can also compute and cache the minimum and maximum\nsequence lengths for its components, which can be useful for invoking particular kernels (e.g. SDPA).\nThere are currently no public APIs for accessing these, but this will change for the beta release.\n\n(supported operations)=",
    "1689": "一级标题：torch.nested\n二级标题：Supported Operations\n内容：\nThis section contains a list of common operations over nested tensors that you may find useful.\nIt is not comprehensive, as there are on the order of a couple thousand ops within PyTorch. While\na sizeable subset of these are supported for nested tensors today, full support is a large task.\nThe ideal state for nested tensors is full support of all PyTorch operations that are available\nfor non-nested tensors. To help us accomplish this, please consider:\n\n* Requesting particular ops needed for your use case\n  [here](https://github.com/pytorch/pytorch/issues/118107) to help us prioritize.\n* Contributing! It's not too hard to add nested tensor support for a given PyTorch op; see\n  the [Contributions](contributions) section below for details.\n\n### Viewing nested tensor constituents\n\n``unbind()`` allows you to retrieve a view of the nested tensor's constituents.\n\n```\n>>> import torch\n>>> a = torch.randn(2, 3)\n>>> b = torch.randn(3, 3)\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n>>> nt.unbind()\n(tensor([[-0.9916, -0.3363, -0.2799],\n        [-2.3520, -0.5896, -0.4374]]), tensor([[-2.0969, -1.0104,  1.4841],\n        [ 2.0952,  0.2973,  0.2516],\n        [ 0.9035,  1.3623,  0.2026]]))\n>>> nt.unbind()[0] is not a\nTrue\n>>> nt.unbind()[0].mul_(3)\ntensor([[ 3.6858, -3.7030, -4.4525],\n        [-2.3481,  2.0236,  0.1975]])\n>>> nt.unbind()\n(tensor([[-2.9747, -1.0089, -0.8396],\n        [-7.0561, -1.7688, -1.3122]]), tensor([[-2.0969, -1.0104,  1.4841],\n        [ 2.0952,  0.2973,  0.2516],\n        [ 0.9035,  1.3623,  0.2026]]))\n```\n\nNote that ``nt.unbind()[0]`` is not a copy, but rather a slice of the underlying memory, which\nrepresents the first entry or constituent of the nested tensor.\n\n#### Conversions to / from padded\n\n``torch.nested.to_padded_tensor()`` converts an NJT to a padded dense tensor with the specified\npadding value. The ragged dimension will be padded out to the size of the maximum sequence length.\n\n```\n>>> import torch\n>>> a = torch.randn(2, 3)\n>>> b = torch.randn(6, 3)\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n>>> padded = torch.nested.to_padded_tensor(nt, padding=4.2)\n>>> padded\ntensor([[[ 1.6107,  0.5723,  0.3913],\n         [ 0.0700, -0.4954,  1.8663],\n         [ 4.2000,  4.2000,  4.2000],\n         [ 4.2000,  4.2000,  4.2000],\n         [ 4.2000,  4.2000,  4.2000],\n         [ 4.2000,  4.2000,  4.2000]],\n        [[-0.0479, -0.7610, -0.3484],\n         [ 1.1345,  1.0556,  0.3634],\n         [-1.7122, -0.5921,  0.0540],\n         [-0.5506,  0.7608,  2.0606],\n         [ 1.5658, -1.1934,  0.3041],\n         [ 0.1483, -1.1284,  0.6957]]])\n```\n\nThis can be useful as an escape hatch to work around NJT support gaps, but ideally such\nconversions should be avoided when possible for optimal memory usage and performance, as the\nmore efficient nested tensor layout does not materialize padding.\n\nThe reverse conversion can be accomplished using ``torch.nested.narrow()``, which applies\nragged structure to a given dense tensor to produce an NJT. Note that by default, this operation\ndoes not copy the underlying data, and thus the output NJT is generally non-contiguous. It may be\nuseful to explicitly call ``contiguous()`` here if a contiguous NJT is desired.\n\n```\n>>> padded = torch.randn(3, 5, 4)\n>>> seq_lens = torch.tensor([3, 2, 5], dtype=torch.int64)\n>>> nt = torch.nested.narrow(padded, dim=1, length=seq_lens, layout=torch.jagged)\n>>> nt.shape\ntorch.Size([3, j1, 4])\n>>> nt = nt.contiguous()\n>>> nt.shape\ntorch.Size([3, j2, 4])\n```\n\n### Shape manipulations\n\nNested tensors support a wide array of operations for shape manipulation, including views.\n\n```\n>>> a = torch.randn(2, 6)\n>>> b = torch.randn(4, 6)\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n>>> nt.shape\ntorch.Size([2, j1, 6])\n>>> nt.unsqueeze(-1).shape\ntorch.Size([2, j1, 6, 1])\n>>> nt.unflatten(-1, [2, 3]).shape\ntorch.Size([2, j1, 2, 3])\n>>> torch.cat([nt, nt], dim=2).shape\ntorch.Size([2, j1, 12])\n>>> torch.stack([nt, nt], dim=2).shape\ntorch.Size([2, j1, 2, 6])\n>>> nt.transpose(-1, -2).shape\ntorch.Size([2, 6, j1])\n```\n\n### Attention mechanisms\n\nAs variable-length sequences are common inputs to attention mechanisms, nested tensors support\nimportant attention operators\n[Scaled Dot Product Attention (SDPA)](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) and\n[FlexAttention](https://pytorch.org/docs/stable/nn.attention.flex_attention.html#module-torch.nn.attention.flex_attention).\nSee\n[here](https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html#multiheadattention)\nfor usage examples of NJT with SDPA and\n[here](https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html#flexattention-njt)\nfor usage examples of NJT with FlexAttention.\n\n(usage_with_torch_compile)=",
    "1690": "一级标题：torch.nested\n二级标题：Usage with torch.compile\n内容：\nNJTs are designed to be used with ``torch.compile()`` for optimal performance, and we always\nrecommend utilizing ``torch.compile()`` with NJTs when possible. NJTs work out-of-the-box and\ngraph-break-free both when passed as inputs to a compiled function or module OR when\ninstantiated in-line within the function.\n\n```{note}\n    If you're not able to utilize ``torch.compile()`` for your use case, performance and memory\n    usage may still benefit from the use of NJTs, but it's not as clear-cut whether this will be\n    the case. It is important that the tensors being operated on are large enough so the\n    performance gains are not outweighed by the overhead of python tensor subclasses.\n```\n\n```\n>>> import torch\n>>> a = torch.randn(2, 3)\n>>> b = torch.randn(4, 3)\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n>>> def f(x): return x.sin() + 1\n...\n>>> compiled_f = torch.compile(f, fullgraph=True)\n>>> output = compiled_f(nt)\n>>> output.shape\ntorch.Size([2, j1, 3])\n>>> def g(values, offsets): return torch.nested.nested_tensor_from_jagged(values, offsets) * 2.\n...\n>>> compiled_g = torch.compile(g, fullgraph=True)\n>>> output2 = compiled_g(nt.values(), nt.offsets())\n>>> output2.shape\ntorch.Size([2, j1, 3])\n```\n\nNote that NJTs support\n[Dynamic Shapes](https://pytorch.org/docs/stable/torch.compiler_dynamic_shapes.html)\nto avoid unnecessary recompiles with changing ragged structure.\n\n```\n>>> a = torch.randn(2, 3)\n>>> b = torch.randn(4, 3)\n>>> c = torch.randn(5, 3)\n>>> d = torch.randn(6, 3)\n>>> nt1 = torch.nested.nested_tensor([a, b], layout=torch.jagged)\n>>> nt2 = torch.nested.nested_tensor([c, d], layout=torch.jagged)\n>>> def f(x): return x.sin() + 1\n...\n>>> compiled_f = torch.compile(f, fullgraph=True)\n>>> output1 = compiled_f(nt1)\n>>> output2 = compiled_f(nt2)  # NB: No recompile needed even though ragged structure differs\n```\n\nIf you run into problems or arcane errors when utilizing NJT + ``torch.compile``, please file a\nPyTorch issue. Full subclass support within ``torch.compile`` is a long-term effort and there may\nbe some rough edges at this time.\n\n(troubleshooting)=",
    "1691": "一级标题：torch.nested\n二级标题：Troubleshooting\n内容：\nThis section contains common errors that you may run into when utilizing nested tensors, alongside\nthe reason for these errors and suggestions for how to address them.\n\n(unimplemented_op)=\n### Unimplemented ops\n\nThis error is becoming rarer as nested tensor op support grows, but it's still possible to hit it\ntoday given that there are a couple thousand ops within PyTorch.\n\n```\n    NotImplementedError: aten.view_as_real.default\n```\n\nThe error is straightforward; we haven't gotten around to adding op support for this particular op\nyet. If you'd like, you can [contribute](contributions) an implementation yourself OR simply\n[request](https://github.com/pytorch/pytorch/issues/118107) that we add support for this op\nin a future PyTorch release.\n\n(ragged_structure_incompatibility)=\n### Ragged structure incompatibility\n\n```\n    RuntimeError: cannot call binary pointwise function add.Tensor with inputs of shapes (2, j2, 128) and (2, j3, 128)\n```\n\nThis error occurs when calling an op that operates over multiple NJTs with incompatible ragged\nstructures. Currently, it is required that input NJTs have the exact same ``offsets`` constituent\nin order to have the same symbolic ragged structure symbol (e.g. ``j1``).\n\nAs a workaround for this situation, it is possible to construct NJTs from the ``values`` and\n``offsets`` components directly. With both NJTs referencing the same ``offsets`` components, they\nare considered to have the same ragged structure and are thus compatible.\n\n```\n>>> a = torch.randn(50, 128)\n>>> b = torch.randn(32, 128)\n>>> nt1 = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)\n>>> nt2 = torch.nested.nested_tensor_from_jagged(values=torch.randn(82, 128), offsets=nt1.offsets())\n>>> nt3 = nt1 + nt2\n>>> nt3.shape\ntorch.Size([2, j1, 128])\n```\n\n### Data dependent operation within torch.compile\n\n```\n    torch._dynamo.exc.Unsupported: data dependent operator: aten._local_scalar_dense.default; to enable, set torch._dynamo.config.capture_scalar_outputs = True\n```\n\nThis error occurs when calling an op that does data-dependent operation within torch.compile; this\ncommonly occurs for ops that need to examine the values of the NJT's ``offsets`` to determine the\noutput shape. For example:\n\n```\n>>> a = torch.randn(50, 128)\n>>> b = torch.randn(32, 128)\n>>> nt = torch.nested.nested_tensor([a, b], layout=torch.jagged, dtype=torch.float32)\n>>> def f(nt): return nt.chunk(2, dim=0)[0]\n...\n>>> compiled_f = torch.compile(f, fullgraph=True)\n>>> output = compiled_f(nt)\n```\n\nIn this example, calling ``chunk()`` on the batch dimension of the NJT requires examination of the\nNJT's ``offsets`` data to delineate batch item boundaries within the packed ragged dimension. As a\nworkaround, there are a couple torch.compile flags that can be set:\n\n```\n>>> torch._dynamo.config.capture_dynamic_output_shape_ops = True\n>>> torch._dynamo.config.capture_scalar_outputs = True\n```\n\nIf, after setting these, you still see data-dependent operator errors, please file an issue with\nPyTorch. This area of ``torch.compile()`` is still in heavy development and certain aspects of\nNJT support may be incomplete.\n\n(contributions)=",
    "1692": "一级标题：torch.nested\n二级标题：Contributions\n内容：\nIf you'd like to contribute to nested tensor development, one of the most impactful ways to do\nso is to add nested tensor support for a currently-unsupported PyTorch op. This process generally\nconsists of a couple simple steps:\n\n1. Determine the name of the op to add; this should be something like ``aten.view_as_real.default``.\n   The signature for this op can be found in ``aten/src/ATen/native/native_functions.yaml``.\n2. Register an op implementation in ``torch/nested/_internal/ops.py``, following the pattern\n   established there for other ops. Use the signature from ``native_functions.yaml`` for schema\n   validation.\n\nThe most common way to implement an op is to unwrap the NJT into its constituents, redispatch the\nop on the underlying ``values`` buffer, and propagate the relevant NJT metadata (including\n``offsets``) to a new output NJT. If the output of the op is expected to have a different shape\nfrom the input, new ``offsets``, etc. metadata must be computed.\n\nWhen an op is applied over the batch or ragged dimension, these tricks can help quickly get a\nworking implementation:\n\n* For *non-batchwise* operation, an ``unbind()``-based fallback should work.\n* For operation on the ragged dimension, consider converting to padded dense with a properly-selected\n  padding value that won't negatively bias the output, running the op, and converting back to NJT.\n  Within ``torch.compile``, these conversions can be fused to avoid materializing the padded\n  intermediate.\n\n(construction_and_conversion)=",
    "1693": "一级标题：torch.nested\n二级标题：Detailed Docs for Construction and Conversion Functions\n内容：\n```{eval-rst}\n.. currentmodule:: torch.nested\n```\n```{eval-rst}\n.. autofunction:: nested_tensor\n```\n```{eval-rst}\n.. autofunction:: nested_tensor_from_jagged\n```\n```{eval-rst}\n.. autofunction:: as_nested_tensor\n```\n```{eval-rst}\n.. autofunction:: to_padded_tensor\n```\n```{eval-rst}\n.. autofunction:: masked_select\n```\n```{eval-rst}\n.. autofunction:: narrow\n```\n```{eval-rst}\n.. seealso::\n\n   `Accelerating PyTorch Transformers by replacing nn.Transformer with Nested Tensors and torch.compile <https://docs.pytorch.org/tutorials/intermediate/transformer_building_blocks.html>`_\n```",
    "1694": "一级标题：Aliases in torch.nn\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.nn.modules\n```\n\n\nThe following are aliases to their counterparts in ``torch.nn`` in nested namespaces.",
    "1695": "一级标题：Aliases in torch.nn\n二级标题：torch.nn.modules\n内容：\nThe following are aliases to their counterparts in ``torch.nn`` in the ``torch.nn.modules`` namespace.\n\n### Containers (Aliases)\n```{eval-rst}\n.. currentmodule:: torch.nn.modules\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    container.Sequential\n    container.ModuleList\n    container.ModuleDict\n    container.ParameterList\n    container.ParameterDict\n\n```\n\n### Convolution Layers (Aliases)\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    conv.Conv1d\n    conv.Conv2d\n    conv.Conv3d\n    conv.ConvTranspose1d\n    conv.ConvTranspose2d\n    conv.ConvTranspose3d\n    conv.LazyConv1d\n    conv.LazyConv2d\n    conv.LazyConv3d\n    conv.LazyConvTranspose1d\n    conv.LazyConvTranspose2d\n    conv.LazyConvTranspose3d\n    fold.Unfold\n    fold.Fold\n\n```\n\n### Pooling layers (Aliases)\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    pooling.MaxPool1d\n    pooling.MaxPool2d\n    pooling.MaxPool3d\n    pooling.MaxUnpool1d\n    pooling.MaxUnpool2d\n    pooling.MaxUnpool3d\n    pooling.AvgPool1d\n    pooling.AvgPool2d\n    pooling.AvgPool3d\n    pooling.FractionalMaxPool2d\n    pooling.FractionalMaxPool3d\n    pooling.LPPool1d\n    pooling.LPPool2d\n    pooling.LPPool3d\n    pooling.AdaptiveMaxPool1d\n    pooling.AdaptiveMaxPool2d\n    pooling.AdaptiveMaxPool3d\n    pooling.AdaptiveAvgPool1d\n    pooling.AdaptiveAvgPool2d\n    pooling.AdaptiveAvgPool3d\n\n```\n\n### Padding Layers (Aliases)\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    padding.ReflectionPad1d\n    padding.ReflectionPad2d\n    padding.ReflectionPad3d\n    padding.ReplicationPad1d\n    padding.ReplicationPad2d\n    padding.ReplicationPad3d\n    padding.ZeroPad1d\n    padding.ZeroPad2d\n    padding.ZeroPad3d\n    padding.ConstantPad1d\n    padding.ConstantPad2d\n    padding.ConstantPad3d\n    padding.CircularPad1d\n    padding.CircularPad2d\n    padding.CircularPad3d\n\n```\n\n### Non-linear Activations (weighted sum, nonlinearity) (Aliases)\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    activation.ELU\n    activation.Hardshrink\n    activation.Hardsigmoid\n    activation.Hardtanh\n    activation.Hardswish\n    activation.LeakyReLU\n    activation.LogSigmoid\n    activation.MultiheadAttention\n    activation.PReLU\n    activation.ReLU\n    activation.ReLU6\n    activation.RReLU\n    activation.SELU\n    activation.CELU\n    activation.GELU\n    activation.Sigmoid\n    activation.SiLU\n    activation.Mish\n    activation.Softplus\n    activation.Softshrink\n    activation.Softsign\n    activation.Tanh\n    activation.Tanhshrink\n    activation.Threshold\n    activation.GLU\n\n```\n\n### Non-linear Activations (other) (Aliases)\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    activation.Softmin\n    activation.Softmax\n    activation.Softmax2d\n    activation.LogSoftmax\n    adaptive.AdaptiveLogSoftmaxWithLoss\n\n```\n\n### Normalization Layers (Aliases)\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    batchnorm.BatchNorm1d\n    batchnorm.BatchNorm2d\n    batchnorm.BatchNorm3d\n    batchnorm.LazyBatchNorm1d\n    batchnorm.LazyBatchNorm2d\n    batchnorm.LazyBatchNorm3d\n    normalization.GroupNorm\n    batchnorm.SyncBatchNorm\n    instancenorm.InstanceNorm1d\n    instancenorm.InstanceNorm2d\n    instancenorm.InstanceNorm3d\n    instancenorm.LazyInstanceNorm1d\n    instancenorm.LazyInstanceNorm2d\n    instancenorm.LazyInstanceNorm3d\n    normalization.LayerNorm\n    normalization.LocalResponseNorm\n    normalization.RMSNorm\n\n```\n\n### Recurrent Layers (Aliases)\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    rnn.RNNBase\n    rnn.RNN\n    rnn.LSTM\n    rnn.GRU\n    rnn.RNNCell\n    rnn.LSTMCell\n    rnn.GRUCell\n\n```\n\n### Transformer Layers (Aliases)\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    transformer.Transformer\n    transformer.TransformerEncoder\n    transformer.TransformerDecoder\n    transformer.TransformerEncoderLayer\n    transformer.TransformerDecoderLayer\n\n```\n\n### Linear Layers (Aliases)\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    linear.Identity\n    linear.Linear\n    linear.Bilinear\n    linear.LazyLinear\n\n```\n\n### Dropout Layers (Aliases)\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    dropout.Dropout\n    dropout.Dropout1d\n    dropout.Dropout2d\n    dropout.Dropout3d\n    dropout.AlphaDropout\n    dropout.FeatureAlphaDropout\n\n```\n\n### Sparse Layers (Aliases)\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    sparse.Embedding\n    sparse.EmbeddingBag\n\n```\n\n### Distance Functions (Aliases)\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    distance.CosineSimilarity\n    distance.PairwiseDistance\n\n```\n\n### Loss Functions (Aliases)\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    loss.L1Loss\n    loss.MSELoss\n    loss.CrossEntropyLoss\n    loss.CTCLoss\n    loss.NLLLoss\n    loss.PoissonNLLLoss\n    loss.GaussianNLLLoss\n    loss.KLDivLoss\n    loss.BCELoss\n    loss.BCEWithLogitsLoss\n    loss.MarginRankingLoss\n    loss.HingeEmbeddingLoss\n    loss.MultiLabelMarginLoss\n    loss.HuberLoss\n    loss.SmoothL1Loss\n    loss.SoftMarginLoss\n    loss.MultiLabelSoftMarginLoss\n    loss.CosineEmbeddingLoss\n    loss.MultiMarginLoss\n    loss.TripletMarginLoss\n    loss.TripletMarginWithDistanceLoss\n\n```\n\n### Vision Layers (Aliases)\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    pixelshuffle.PixelShuffle\n    pixelshuffle.PixelUnshuffle\n    upsampling.Upsample\n    upsampling.UpsamplingNearest2d\n    upsampling.UpsamplingBilinear2d\n\n```\n\n### Shuffle Layers (Aliases)\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    channelshuffle.ChannelShuffle\n\n```",
    "1696": "一级标题：Aliases in torch.nn\n二级标题：torch.nn.utils\n内容：\nThe following are aliases to their counterparts in ``torch.nn.utils`` in nested namespaces.\n\nUtility functions to clip parameter gradients.\n\n```{eval-rst}\n.. currentmodule:: torch.nn.utils\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    clip_grad.clip_grad_norm_\n    clip_grad.clip_grad_norm\n    clip_grad.clip_grad_value_\n\n\n```\n\nUtility functions to flatten and unflatten Module parameters to and from a single vector.\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    convert_parameters.parameters_to_vector\n    convert_parameters.vector_to_parameters\n\n```\n\nUtility functions to fuse Modules with BatchNorm modules.\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    fusion.fuse_conv_bn_eval\n    fusion.fuse_conv_bn_weights\n    fusion.fuse_linear_bn_eval\n    fusion.fuse_linear_bn_weights\n\n```\n\nUtility functions to convert Module parameter memory formats.\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    memory_format.convert_conv2d_weight_memory_format\n    memory_format.convert_conv3d_weight_memory_format\n\n```\n\nUtility functions to apply and remove weight normalization from Module parameters.\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    weight_norm.weight_norm\n    weight_norm.remove_weight_norm\n    spectral_norm.spectral_norm\n    spectral_norm.remove_spectral_norm\n\n```\n\nUtility functions for initializing Module parameters.\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    init.skip_init\n```",
    "1697": "一级标题：torch.nn.attention.bias\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.nn.attention.bias\n.. currentmodule:: torch.nn.attention.bias\n```",
    "1698": "一级标题：torch.nn.attention.bias\n二级标题：CausalBias\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classnoinheritance.rst\n\n    CausalBias\n```\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    causal_lower_right\n    causal_upper_left\n    CausalVariant\n```",
    "1699": "一级标题：torch.nn.attention.experimental\n二级标题：无\n内容：\n```{eval-rst}\n.. currentmodule:: torch.nn.attention.experimental\n```\n```{eval-rst}\n.. py:module:: torch.nn.attention.experimental\n```\n\n```{warning}\n   These APIs are experimental and subject to change without notice.\n```",
    "1700": "一级标题：torch.nn.attention.flex_attention\n二级标题：无\n内容：\n```{eval-rst}\n.. currentmodule:: torch.nn.attention.flex_attention\n```\n```{eval-rst}\n.. py:module:: torch.nn.attention.flex_attention\n```\n```{eval-rst}\n.. autofunction:: flex_attention\n```",
    "1701": "一级标题：torch.nn.attention.flex_attention\n二级标题：BlockMask Utilities\n内容：\n```{eval-rst}\n.. autofunction:: create_block_mask\n```\n```{eval-rst}\n.. autofunction:: create_mask\n```\n```{eval-rst}\n.. autofunction:: create_nested_block_mask\n```\n```{eval-rst}\n.. autofunction:: and_masks\n```\n```{eval-rst}\n.. autofunction:: or_masks\n```\n```{eval-rst}\n.. autofunction:: noop_mask\n```",
    "1702": "一级标题：torch.nn.attention.flex_attention\n二级标题：FlexKernelOptions\n内容：\n```{eval-rst}\n.. autoclass:: FlexKernelOptions\n    :members:\n    :undoc-members:\n```",
    "1703": "一级标题：torch.nn.attention.flex_attention\n二级标题：BlockMask\n内容：\n```{eval-rst}\n.. autoclass:: BlockMask\n    :members:\n    :undoc-members:\n```",
    "1704": "一级标题：Developer Notes\n二级标题：无\n内容：\n```{toctree}\n:glob:\n:maxdepth: 1\n\nnotes/*\n```",
    "1705": "一级标题：LibTorch Stable ABI\n二级标题：无\n内容：\nThis note will eventually contain more details on how to use the APIs in torch/csrc/stable. For the moment, it contains a table of internal representations:\n1. type in custom extension: type used within the end user custom library.\n2. StableIValue representation: a stable conversion of the type to liaison between the user model vs libtorch.so in an ABI-stable manner.\n3. type in libtorch: type used within libtorch.so (or any code binary locked with libtorch).\n4. Schema Type: type as described by the schema, which we hail as the source of truth for both ATen ops in native_functions.yaml and for user defined custom operators registered to the dispatcher via TORCH_LIBRARY or torch.library.\n\n|  type in custom extension    |   StableIValue representation   |   type in libtorch  |   Schema Type  |\n| -------- | ------- | ------- | ------- |\n| std::optional\\<S> | if there is a value, raw bitwise copy into leading bytes of uint64_t of pointer to a new StableIValue representing S. if there is no value, nullptr. | std::optional\\<T> | Type? |\n| RAIIATH | raw bitwise copy of underlying AtenTensorHandle into leading bytes of uint64_t | at::Tensor |  Tensor |\n| int32_t | raw bitwise copy into leading bytes of uint64_t | at::ScalarType | ScalarType |\n| int32_t | raw bitwise copy into leading bytes of uint64_t | at::Layout | Layout |\n| int32_t | raw bitwise copy into leading bytes of uint64_t | at::MemoryFormat | MemoryFormat |\n| bool | raw bitwise copy into leading bytes of uint64_t | bool | bool |\n| int64_t | raw bitwise copy into leading bytes of uint64_t | int64_t | int |\n| double | raw bitwise copy into leading bytes of uint64_t | double | float |\n| ? | ? | c10::Device | Device |\n| ? | ? | c10::Stream | Stream |\n| ? | ? | c10::complex<double> | complex |\n| ? | ? | at::Scalar | Scalar |\n| ? | ? | std::string/const char*/ivalue::ConstantString | str |\n| ? | ? | at::Storage | Storage |\n| ? | ? | at::Generator | Generator |\n| ? | ? | c10::List\\<T> | Type[] |\n| ? | ? | ivalue::Tuple\\<T> | (Type, ...) |\n| ? | ? | c10::SymInt | SymInt |\n| ? | ? | c10::SymFloat | SymFloat |\n| ? | ? | c10::SymBool | SymBool |\n| ? | ? | at::QScheme | QScheme |\n\nOur confidently supported types are the ones in the table that have completed rows. You can rely on this subset for proper ABI stability.\n\nFor a limited set of use cases, we also implicitly support any literal type that is representable within 64 bits as StableIValues, as the default reinterpret_cast will succeed. (For example: c10::Device.) These types are currently ABI-stable on best effort but might break in the future and thus should be used for short term testing only.\n\nYou can always work with StableIValue abstractions in your custom kernel for types such as c10::Device even if there is no standard defined representation of device in custom extensions by not introspecting into the StableIValue. For example, a custom operator can take as argument a StableIValue device and directly pass it through to an aten operator with `aoti_torch_call_dispatcher`.",
    "1706": "一级标题：LibTorch Stable ABI\n二级标题：How to use stack-based APIs\n内容：\n`aoti_torch_call_dispatcher` is what we consider a stack-based API because it takes as input a stack of StableIValues, which correlates with a `torch::jit::stack` of IValues. Working with the dispatcher will likely bring you into proximity with stack-based APIs, so we are documenting some invariants:\n\n1. The stack is populated left to right.\n    a. For example, a stack representing arguments `arg0`, `arg1`, and `arg2` will have `arg0` at index 0, `arg1` at index 1, and `arg2` at index 2.\n    b. Returns are also populated left to right, e.g., `ret0` will be at index 0 and `ret1` will be at index 1, and so on.\n\n2. The stack always has ownership of the objects it holds.\n    a. When calling a stack-based API, you must give owning references to the calling stack and steal references from the returned stack.\n    b. When registering your function to be called with a stack, you must steal references from your argument stack and push onto the stack new references.",
    "1707": "一级标题：torch.onnx\n二级标题：无\n内容：",
    "1708": "一级标题：torch.onnx\n二级标题：Overview\n内容：\n[Open Neural Network eXchange (ONNX)](https://onnx.ai/) is an open standard\nformat for representing machine learning models. The `torch.onnx` module captures the computation graph from a\nnative PyTorch {class}`torch.nn.Module` model and converts it into an\n[ONNX graph](https://github.com/onnx/onnx/blob/main/docs/IR.md).\n\nThe exported model can be consumed by any of the many\n[runtimes that support ONNX](https://onnx.ai/supported-tools.html#deployModel), including\nMicrosoft's [ONNX Runtime](https://www.onnxruntime.ai).\n\nNext example shows how to export a simple model.\n\n```python\nimport torch\n\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.conv1 = torch.nn.Conv2d(1, 128, 5)\n\n    def forward(self, x):\n        return torch.relu(self.conv1(x))\n\ninput_tensor = torch.rand((1, 1, 128, 128), dtype=torch.float32)\n\nmodel = MyModel()\n\ntorch.onnx.export(\n    model,                  # model to export\n    (input_tensor,),        # inputs of the model,\n    \"my_model.onnx\",        # filename of the ONNX model\n    input_names=[\"input\"],  # Rename inputs for the ONNX model\n    dynamo=True             # True or False to select the exporter to use\n)\n```",
    "1709": "一级标题：torch.onnx\n二级标题：torch.export-based ONNX Exporter\n内容：\n*The torch.export-based ONNX exporter is the newest exporter for PyTorch 2.6 and newer*\n\n{ref}`torch.export <torch.export>` engine is leveraged to produce a traced graph representing only the Tensor computation of the function in an\nAhead-of-Time (AOT) fashion. The resulting traced graph (1) produces normalized operators in the functional\nATen operator set (as well as any user-specified custom operators), (2) has eliminated all Python control\nflow and data structures (with certain exceptions), and (3) records the set of shape constraints needed to\nshow that this normalization and control-flow elimination is sound for future inputs, before it is finally\ntranslated into an ONNX graph.\n\n{doc}`Learn more about the torch.export-based ONNX Exporter <onnx_export>`",
    "1710": "一级标题：torch.onnx\n二级标题：Frequently Asked Questions\n内容：\nQ: I have exported my LLM model, but its input size seems to be fixed?\n\n  The tracer records the shapes of the example inputs. If the model should accept\n  inputs of dynamic shapes, set ``dynamic_shapes`` when calling {func}`torch.onnx.export`.\n\nQ: How to export models containing loops?\n\n  See {ref}`torch.cond <cond>`.",
    "1711": "一级标题：torch.onnx\n二级标题：Contributing / Developing\n内容：\nThe ONNX exporter is a community project and we welcome contributions. We follow the\n[PyTorch guidelines for contributions](https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md), but you might\nalso be interested in reading our [development wiki](https://github.com/pytorch/pytorch/wiki/PyTorch-ONNX-exporter).",
    "1712": "一级标题：torch.onnx\n二级标题：torch.onnx APIs\n内容：\n```{eval-rst}\n.. automodule:: torch.onnx\n```\n\n### Functions\n\n```{eval-rst}\n.. autofunction:: export\n    :noindex:\n.. autofunction:: is_in_onnx_export\n    :noindex:\n.. autofunction:: enable_fake_mode\n    :noindex:\n```\n\n### Classes\n\n```{eval-rst}\n.. autoclass:: ONNXProgram\n    :noindex:\n.. autoclass:: OnnxExporterError\n    :noindex:\n```\n\n```{eval-rst}\n.. toctree::\n    :hidden:\n\n    onnx_export\n    onnx_ops\n    onnx_verification\n```\n\n### Deprecated APIs\n\n```{eval-rst}\n.. deprecated:: 2.6\n    These functions are deprecated and will be removed in a future version.\n\n.. autofunction:: register_custom_op_symbolic\n.. autofunction:: unregister_custom_op_symbolic\n.. autofunction:: select_model_mode_for_export\n.. autoclass:: JitScalarType\n```\n\n```{eval-rst}\n.. py:module:: torch.onnx.errors\n.. py:module:: torch.onnx.operators\n.. py:module:: torch.onnx.symbolic_helper\n.. py:module:: torch.onnx.symbolic_opset10\n.. py:module:: torch.onnx.symbolic_opset11\n.. py:module:: torch.onnx.symbolic_opset12\n.. py:module:: torch.onnx.symbolic_opset13\n.. py:module:: torch.onnx.symbolic_opset14\n.. py:module:: torch.onnx.symbolic_opset15\n.. py:module:: torch.onnx.symbolic_opset16\n.. py:module:: torch.onnx.symbolic_opset17\n.. py:module:: torch.onnx.symbolic_opset18\n.. py:module:: torch.onnx.symbolic_opset19\n.. py:module:: torch.onnx.symbolic_opset20\n.. py:module:: torch.onnx.symbolic_opset7\n.. py:module:: torch.onnx.symbolic_opset8\n.. py:module:: torch.onnx.symbolic_opset9\n.. py:module:: torch.onnx.utils\n```",
    "1713": "一级标题：torch.export-based ONNX Exporter\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.onnx\n  :noindex:\n```\n\n```{contents}\n:local:\n:depth: 1\n```",
    "1714": "一级标题：torch.export-based ONNX Exporter\n二级标题：Overview\n内容：\n{ref}`torch.export <torch.export>` engine is leveraged to produce a traced graph representing only the Tensor computation of the function in an\nAhead-of-Time (AOT) fashion. The resulting traced graph (1) produces normalized operators in the functional\nATen operator set (as well as any user-specified custom operators), (2) has eliminated all Python control\nflow and data structures (with certain exceptions), and (3) records the set of shape constraints needed to\nshow that this normalization and control-flow elimination is sound for future inputs, before it is finally\ntranslated into an ONNX graph.\n\nIn addition, during the export process, memory usage is significantly reduced.",
    "1715": "一级标题：torch.export-based ONNX Exporter\n二级标题：Dependencies\n内容：\nThe ONNX exporter depends on extra Python packages:\n\n  - [ONNX](https://onnx.ai)\n  - [ONNX Script](https://microsoft.github.io/onnxscript)\n\nThey can be installed through [pip](https://pypi.org/project/pip/):\n\n```{code-block} bash\n\n  pip install --upgrade onnx onnxscript\n```\n\n[onnxruntime](https://onnxruntime.ai) can then be used to execute the model\non a large variety of processors.",
    "1716": "一级标题：torch.export-based ONNX Exporter\n二级标题：A simple example\n内容：\nSee below a demonstration of exporter API in action with a simple Multilayer Perceptron (MLP) as example:\n\n```{code-block} python\nimport torch\nimport torch.nn as nn\n\nclass MLPModel(nn.Module):\n  def __init__(self):\n      super().__init__()\n      self.fc0 = nn.Linear(8, 8, bias=True)\n      self.fc1 = nn.Linear(8, 4, bias=True)\n      self.fc2 = nn.Linear(4, 2, bias=True)\n      self.fc3 = nn.Linear(2, 2, bias=True)\n      self.fc_combined = nn.Linear(8 + 8 + 8, 8, bias=True)  # Combine all inputs\n\n  def forward(self, tensor_x: torch.Tensor, input_dict: dict, input_list: list):\n      \"\"\"\n      Forward method that requires all inputs:\n      - tensor_x: A direct tensor input.\n      - input_dict: A dictionary containing the tensor under the key 'tensor_x'.\n      - input_list: A list where the first element is the tensor.\n      \"\"\"\n      # Extract tensors from inputs\n      dict_tensor = input_dict['tensor_x']\n      list_tensor = input_list[0]\n\n      # Combine all inputs into a single tensor\n      combined_tensor = torch.cat([tensor_x, dict_tensor, list_tensor], dim=1)\n\n      # Process the combined tensor through the layers\n      combined_tensor = self.fc_combined(combined_tensor)\n      combined_tensor = torch.sigmoid(combined_tensor)\n      combined_tensor = self.fc0(combined_tensor)\n      combined_tensor = torch.sigmoid(combined_tensor)\n      combined_tensor = self.fc1(combined_tensor)\n      combined_tensor = torch.sigmoid(combined_tensor)\n      combined_tensor = self.fc2(combined_tensor)\n      combined_tensor = torch.sigmoid(combined_tensor)\n      output = self.fc3(combined_tensor)\n      return output\n\nmodel = MLPModel()\n\n# Example inputs\ntensor_input = torch.rand((97, 8), dtype=torch.float32)\ndict_input = {'tensor_x': torch.rand((97, 8), dtype=torch.float32)}\nlist_input = [torch.rand((97, 8), dtype=torch.float32)]\n\n# The input_names and output_names are used to identify the inputs and outputs of the ONNX model\ninput_names = ['tensor_input', 'tensor_x', 'list_input_index_0']\noutput_names = ['output']\n\n# Exporting the model with all required inputs\nonnx_program = torch.onnx.export(model,(tensor_input, dict_input, list_input), dynamic_shapes=({0: \"batch_size\"},{\"tensor_x\": {0: \"batch_size\"}},[{0: \"batch_size\"}]), input_names=input_names, output_names=output_names, dynamo=True,)\n\n# Check the exported ONNX model is dynamic\nassert onnx_program.model.graph.inputs[0].shape == (\"batch_size\", 8)\nassert onnx_program.model.graph.inputs[1].shape == (\"batch_size\", 8)\nassert onnx_program.model.graph.inputs[2].shape == (\"batch_size\", 8)\n```\n\nAs the code above shows, all you need is to provide {func}`torch.onnx.export` with an instance of the model and its input.\nThe exporter will then return an instance of {class}`torch.onnx.ONNXProgram` that contains the exported ONNX graph along with extra information.\n\nThe in-memory model available through ``onnx_program.model_proto`` is an ``onnx.ModelProto`` object in compliance with the [ONNX IR spec](https://github.com/onnx/onnx/blob/main/docs/IR.md).\nThe ONNX model may then be serialized into a [Protobuf file](https://protobuf.dev/) using the {meth}`torch.onnx.ONNXProgram.save` API.\n\n```{code-block} python\n  onnx_program.save(\"mlp.onnx\")\n```",
    "1717": "一级标题：torch.export-based ONNX Exporter\n二级标题：Inspecting the ONNX model using GUI\n内容：\nYou can view the exported model using [Netron](https://netron.app/).\n\n```{image} _static/img/onnx/onnx_dynamo_mlp_model.png\n:alt: MLP model as viewed using Netron\n:width: 30%\n:align: center\n```",
    "1718": "一级标题：torch.export-based ONNX Exporter\n二级标题：When the conversion fails\n内容：\nFunction {func}`torch.onnx.export` should be called a second time with\nparameter ``report=True``. A markdown report is generated to help the user\nto resolve the issue.",
    "1719": "一级标题：torch.export-based ONNX Exporter\n二级标题：Metadata\n内容：\nDuring ONNX export, each ONNX node is annotated with metadata that helps trace its origin and context from the original PyTorch model. This metadata is useful for debugging, model inspection, and understanding the mapping between PyTorch and ONNX graphs.\n\nThe following metadata fields are added to each ONNX node:\n\n- **namespace**\n\n  A string representing the hierarchical namespace of the node, consisting of a stack trace of modules/methods.\n\n  *Example:*\n  `__main__.SimpleAddModel/add: aten.add.Tensor`\n\n- **pkg.torch.onnx.class_hierarchy**\n\n  A list of class names representing the hierarchy of modules leading to this node.\n\n  *Example:*\n  `['__main__.SimpleAddModel', 'aten.add.Tensor']`\n\n- **pkg.torch.onnx.fx_node**\n\n  The string representation of the original FX node, including its name, number of consumers, the targeted torch op, arguments, and keyword arguments.\n\n  *Example:*\n  `%cat : [num_users=1] = call_function[target=torch.ops.aten.cat.default](args = ([%tensor_x, %input_dict_tensor_x, %input_list_0], 1), kwargs = {})`\n\n- **pkg.torch.onnx.name_scopes**\n\n  A list of name scopes (methods) representing the path to this node in the PyTorch model.\n\n  *Example:*\n  `['', 'add']`\n\n- **pkg.torch.onnx.stack_trace**\n\n  The stack trace from the original code where this node was created, if available.\n\n  *Example:*\n  ```\n  File \"simpleadd.py\", line 7, in forward\n      return torch.add(x, y)\n  ```\n\nThese metadata fields are stored in the metadata_props attribute of each ONNX node and can be inspected using Netron or programmatically.\n\nThe overall ONNX graph has the following `metadata_props`:\n\n- **pkg.torch.export.ExportedProgram.graph_signature**\n\n  This property contains a string representation of the graph_signature from the original PyTorch ExportedProgram. The graph signature describes the structure of the model's inputs and outputs and how they map to the ONNX graph. The inputs are defined as `InputSpec` objects, which include the kind of input (e.g., `InputKind.PARAMETER` for parameters, `InputKind.USER_INPUT` for user-defined inputs), the argument name, the target (which can be a specific node in the model), and whether the input is persistent. The outputs are defined as `OutputSpec` objects, which specify the kind of output (e.g., `OutputKind.USER_OUTPUT`) and the argument name.\n\n  To read more about the graph signature, please see the {doc}`torch.export <export>` for more information.\n\n- **pkg.torch.export.ExportedProgram.range_constraints**\n\n  This property contains a string representation of any range constraints that were present in the original PyTorch ExportedProgram. Range constraints specify valid ranges for symbolic shapes or values in the model, which can be important for models that use dynamic shapes or symbolic dimensions.\n\n  *Example:*\n  `s0: VR[2, int_oo]`, which indicates that the size of the input tensor must be at least 2.\n\n  To read more about range constraints, please see the {doc}`torch.export <export>` for more information.\n\nEach input value in the ONNX graph may have the following metadata property:\n\n- **pkg.torch.export.graph_signature.InputSpec.kind**\n\n  The kind of input, as defined by PyTorch's InputKind enum.\n\n  *Example values:*\n  - \"USER_INPUT\": A user-provided input to the model.\n  - \"PARAMETER\": A model parameter (e.g., weight).\n  - \"BUFFER\": A model buffer (e.g., running mean in BatchNorm).\n  - \"CONSTANT_TENSOR\": A constant tensor argument.\n  - \"CUSTOM_OBJ\": A custom object input.\n  - \"TOKEN\": A token input.\n\n- **pkg.torch.export.graph_signature.InputSpec.persistent**\n\n  Indicates whether the input is persistent (i.e., should be saved as part of the model's state).\n\n  *Example values:*\n  - \"True\"\n  - \"False\"\n\nEach output value in the ONNX graph may have the following metadata property:\n\n- **pkg.torch.export.graph_signature.OutputSpec.kind**\n\n  The kind of input, as defined by PyTorch's OutputKind enum.\n\n  *Example values:*\n  - \"USER_OUTPUT\": A user-visible output.\n  - \"LOSS_OUTPUT\": A loss value output.\n  - \"BUFFER_MUTATION\": Indicates a buffer was mutated.\n  - \"GRADIENT_TO_PARAMETER\": Gradient output for a parameter.\n  - \"GRADIENT_TO_USER_INPUT\": Gradient output for a user input.\n  - \"USER_INPUT_MUTATION\": Indicates a user input was mutated.\n  - \"TOKEN\": A token output.\n\nEach initialized value, input, output has the following metadata:\n\n- **pkg.torch.onnx.original_node_name**\n\n  The original name of the node in the PyTorch FX graph that produced this value in the case where the value was renamed. This helps trace initializers back to their source in the original model.\n\n  *Example:*\n  `fc1.weight`",
    "1720": "一级标题：torch.export-based ONNX Exporter\n二级标题：API Reference\n内容：\n```{eval-rst}\n.. autofunction:: torch.onnx.export\n.. autoclass:: torch.onnx.ONNXProgram\n    :members:\n.. autofunction:: torch.onnx.is_in_onnx_export\n.. autoclass:: torch.onnx.OnnxExporterError\n    :members:\n.. autofunction:: torch.onnx.enable_fake_mode\n```",
    "1721": "一级标题：torch.onnx.ops\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.onnx.ops\n```",
    "1722": "一级标题：torch.onnx.ops\n二级标题：Symbolic Operators\n内容：\nOperators that can be used to create any ONNX ops in the FX graph symbolically.\nThese operators do not do actual computation. It's recommended that you used them\ninside an ``if torch.onnx.is_in_onnx_export`` block.\n\n```{eval-rst}\n.. autofunction:: torch.onnx.ops.symbolic\n.. autofunction:: torch.onnx.ops.symbolic_multi_out\n```",
    "1723": "一级标题：torch.onnx.ops\n二级标题：ONNX Operators\n内容：\nThe following operators are implemented as native PyTorch ops and can be exported as\nONNX operators. They can be used natively in an ``nn.Module``.\n\nFor example, you can define a module:\n\n```py\nclass Model(torch.nn.Module):\n    def forward(\n        self, input_data, cos_cache_data, sin_cache_data, position_ids_data\n    ):\n        return torch.onnx.ops.rotary_embedding(\n            input_data,\n            cos_cache_data,\n            sin_cache_data,\n            position_ids_data,\n        )\n```\n\nand export it to ONNX using:\n\n```py\ninput_data = torch.rand(2, 3, 4, 8)\nposition_ids_data = torch.randint(0, 50, (2, 3)).long()\nsin_cache_data = torch.rand(50, 4)\ncos_cache_data = torch.rand(50, 4)\ndynamic_shapes = {\n    \"input_data\": {0: torch.export.Dim.DYNAMIC},\n    \"cos_cache_data\": None,\n    \"sin_cache_data\": None,\n    \"position_ids_data\": {0: torch.export.Dim.DYNAMIC},\n}\nonnx_program = torch.onnx.export(\n    model,\n    (input_data, cos_cache_data, sin_cache_data, position_ids_data),\n    dynamic_shapes=dynamic_shapes,\n    dynamo=True,\n    opset_version=23,\n)\n```\n\nPrinting the ONNX program will show the ONNX operators used in the graph:\n\n```\n<...>\n\ngraph(\n    name=main_graph,\n    inputs=(\n        %\"input_data\"<FLOAT,[s0,3,4,8]>,\n        %\"cos_cache_data\"<FLOAT,[50,4]>,\n        %\"sin_cache_data\"<FLOAT,[50,4]>,\n        %\"position_ids_data\"<INT64,[s0,3]>\n    ),\n    outputs=(\n        %\"rotary_embedding\"<FLOAT,[s0,3,4,8]>\n    ),\n) {\n    0 |  # rotary_embedding\n        %\"rotary_embedding\"<FLOAT,[s0,3,4,8]> ⬅️ ::RotaryEmbedding(%\"input_data\", %\"cos_cache_data\", %\"sin_cache_data\", %\"position_ids_data\")\n    return %\"rotary_embedding\"<FLOAT,[s0,3,4,8]>\n}\n```\n\nwith the corresponding ``ExportedProgram``:\n\nExportedProgram:\n\n```py\nclass GraphModule(torch.nn.Module):\n    def forward(self, input_data: \"f32[s0, 3, 4, 8]\", cos_cache_data: \"f32[50, 4]\", sin_cache_data: \"f32[50, 4]\", position_ids_data: \"i64[s0, 3]\"):\n        rotary_embedding: \"f32[s0, 3, 4, 8]\" = torch.ops.onnx.RotaryEmbedding.opset23(input_data, cos_cache_data, sin_cache_data, position_ids_data);  input_data = cos_cache_data = sin_cache_data = position_ids_data = None\n        return (rotary_embedding,)\n```\n\n```{eval-rst}\n.. autofunction:: torch.onnx.ops.rotary_embedding\n.. autofunction:: torch.onnx.ops.attention\n```",
    "1724": "一级标题：torch.onnx.ops\n二级标题：ONNX to ATen Decomposition Table\n内容：\nYou can use {func}`torch.onnx.ops.aten_decompositions` to obtain a decomposition table\nto decompose ONNX operators defined above to ATen operators.\n\n```py\nclass Model(torch.nn.Module):\n    def forward(\n        self, input_data, cos_cache_data, sin_cache_data, position_ids_data\n    ):\n        return torch.onnx.ops.rotary_embedding(\n            input_data,\n            cos_cache_data,\n            sin_cache_data,\n            position_ids_data,\n        )\n\nmodel = Model()\n\nep = torch.export.export(\n    model,\n    (input_data, cos_cache_data, sin_cache_data, position_ids_data),\n)\n# The program can be decomposed into aten ops\nep_decomposed = ep.run_decompositions(torch.onnx.ops.aten_decompositions())\n```\n\n```{eval-rst}\n.. autofunction:: torch.onnx.ops.aten_decompositions\n```",
    "1725": "一级标题：torch.onnx.verification\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.onnx.verification\n```\n\n```{eval-rst}\n.. autofunction:: verify_onnx_program\n```\n\n```{eval-rst}\n.. autoclass:: VerificationInfo\n    :members:\n```\n\n```{eval-rst}\n.. autofunction:: verify\n```",
    "1726": "一级标题：torch.onnx.verification\n二级标题：Deprecated\n内容：\nThe following classes and functions are deprecated.\n\n<!-- Some deprecated members are not publicly shown -->\n```{eval-rst}\n.. py:class:: check_export_model_diff\n.. py:class:: GraphInfo\n.. py:class:: GraphInfoPrettyPrinter\n.. py:class:: OnnxBackend\n.. py:class:: OnnxTestCaseRepro\n.. py:class:: VerificationOptions\n.. py:function:: find_mismatch\n.. py:function:: verify_aten_graph\n```",
    "1727": "一级标题：Aliases in torch.optim\n二级标题：无\n内容：\nThe following are aliases to their counterparts in ``torch.optim`` in the nested namespaces in which they are defined. For any of these APIs, feel free to use the top-level version in ``torch.optim`` like ``torch.optim.Adam`` or the nested version ``torch.optim.adam.Adam``.\n\n```{eval-rst}\n.. automodule:: torch.optim.adadelta\n.. currentmodule:: torch.optim.adadelta\n.. autosummary::\n   :toctree: generated\n   :nosignatures:\n\n    Adadelta\n    adadelta\n```\n\n```{eval-rst}\n.. automodule:: torch.optim.adagrad\n.. currentmodule:: torch.optim.adagrad\n.. autosummary::\n   :toctree: generated\n   :nosignatures:\n\n    Adagrad\n    adagrad\n```\n\n```{eval-rst}\n.. automodule:: torch.optim.adam\n.. currentmodule:: torch.optim.adam\n.. autosummary::\n   :toctree: generated\n   :nosignatures:\n\n    Adam\n    adam\n```\n\n```{eval-rst}\n.. automodule:: torch.optim.adamax\n.. currentmodule:: torch.optim.adamax\n.. autosummary::\n   :toctree: generated\n   :nosignatures:\n\n    Adamax\n    adamax\n```\n\n```{eval-rst}\n.. automodule:: torch.optim.adamw\n.. currentmodule:: torch.optim.adamw\n.. autosummary::\n   :toctree: generated\n   :nosignatures:\n\n    AdamW\n    adamw\n```\n\n```{eval-rst}\n.. automodule:: torch.optim.asgd\n.. currentmodule:: torch.optim.asgd\n.. autosummary::\n   :toctree: generated\n   :nosignatures:\n\n    ASGD\n    asgd\n```\n\n```{eval-rst}\n.. automodule:: torch.optim.lbfgs\n.. currentmodule:: torch.optim.lbfgs\n.. autosummary::\n   :toctree: generated\n   :nosignatures:\n\n    LBFGS\n```\n\n```{eval-rst}\n.. automodule:: torch.optim.nadam\n.. currentmodule:: torch.optim.nadam\n.. autosummary::\n   :toctree: generated\n   :nosignatures:\n\n    NAdam\n    nadam\n```\n\n```{eval-rst}\n.. automodule:: torch.optim.radam\n.. currentmodule:: torch.optim.radam\n.. autosummary::\n   :toctree: generated\n   :nosignatures:\n\n    RAdam\n    radam\n```\n\n```{eval-rst}\n.. automodule:: torch.optim.rmsprop\n.. currentmodule:: torch.optim.rmsprop\n.. autosummary::\n   :toctree: generated\n   :nosignatures:\n\n    RMSprop\n    rmsprop\n```\n\n```{eval-rst}\n.. automodule:: torch.optim.rprop\n.. currentmodule:: torch.optim.rprop\n.. autosummary::\n   :toctree: generated\n   :nosignatures:\n\n    Rprop\n    rprop\n```\n\n```{eval-rst}\n.. automodule:: torch.optim.sgd\n.. currentmodule:: torch.optim.sgd\n.. autosummary::\n   :toctree: generated\n   :nosignatures:\n\n    SGD\n    sgd\n```\n\n```{eval-rst}\n.. automodule:: torch.optim.sparse_adam\n.. currentmodule:: torch.optim.sparse_adam\n.. autosummary::\n   :toctree: generated\n   :nosignatures:\n\n    SparseAdam\n```",
    "1728": "一级标题：torch.optim\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.optim\n```",
    "1729": "一级标题：torch.optim\n二级标题：How to use an optimizer\n内容：\nTo use {mod}`torch.optim` you have to construct an optimizer object that will hold\nthe current state and will update the parameters based on the computed gradients.\n\n### Constructing it\n\nTo construct an {class}`Optimizer` you have to give it an iterable containing the\nparameters (all should be {class}`~torch.nn.Parameter` s) or named parameters\n(tuples of (str, {class}`~torch.nn.Parameter`)) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc.\n\nExample:\n```python\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\noptimizer = optim.Adam([var1, var2], lr=0.0001)\n```\n\nNamed parameters example:\n\n```python\noptimizer = optim.SGD(model.named_parameters(), lr=0.01, momentum=0.9)\noptimizer = optim.Adam([('layer0', var1), ('layer1', var2)], lr=0.0001)\n```\n\n### Per-parameter options\n\n{class}`Optimizer` s also support specifying per-parameter options. To do this, instead\nof passing an iterable of {class}`~torch.autograd.Variable` s, pass in an iterable of\n{class}`dict` s. Each of them will define a separate parameter group, and should contain\na `params` key, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group.\n\nFor example, this is very useful when one wants to specify per-layer learning rates:\n\n```python\noptim.SGD([\n    {'params': model.base.parameters(), 'lr': 1e-2},\n    {'params': model.classifier.parameters()}\n], lr=1e-3, momentum=0.9)\n\noptim.SGD([\n    {'params': model.base.named_parameters(), 'lr': 1e-2},\n    {'params': model.classifier.named_parameters()}\n], lr=1e-3, momentum=0.9)\n```\n\nThis means that `model.base`'s parameters will use a learning rate of `1e-2`, whereas\n`model.classifier`'s parameters will stick to the default learning rate of `1e-3`.\nFinally a momentum of `0.9` will be used for all parameters.\n\n```{note}\nYou can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn't override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups.\n```\n\nAlso consider the following example related to the distinct penalization of parameters.\nRemember that {func}`~torch.nn.Module.parameters` returns an iterable that\ncontains all learnable parameters, including biases and other\nparameters that may prefer distinct penalization. To address this, one can specify\nindividual penalization weights for each parameter group:\n\n```python\nbias_params = [p for name, p in self.named_parameters() if 'bias' in name]\nothers = [p for name, p in self.named_parameters() if 'bias' not in name]\n\noptim.SGD([\n    {'params': others},\n    {'params': bias_params, 'weight_decay': 0}\n], weight_decay=1e-2, lr=1e-2)\n```\n\nIn this manner, bias terms are isolated from non-bias terms, and a `weight_decay`\nof `0` is set specifically for the bias terms, as to avoid any penalization for\nthis group.\n\n\n### Taking an optimization step\n\nAll optimizers implement a {func}`~Optimizer.step` method, that updates the\nparameters. It can be used in two ways:\n\n#### `optimizer.step()`\n\nThis is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.\n{func}`~torch.autograd.Variable.backward`.\n\nExample:\n\n```python\nfor input, target in dataset:\n    optimizer.zero_grad()\n    output = model(input)\n    loss = loss_fn(output, target)\n    loss.backward()\n    optimizer.step()\n```\n\n#### `optimizer.step(closure)`\n\nSome optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it.\n\nExample:\n```python\nfor input, target in dataset:\n    def closure():\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        return loss\n    optimizer.step(closure)\n```\n\n(optimizer-algorithms)=",
    "1730": "一级标题：torch.optim\n二级标题：Base class\n内容：\n```{eval-rst}\n.. autoclass:: Optimizer\n\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    Optimizer.add_param_group\n    Optimizer.load_state_dict\n    Optimizer.register_load_state_dict_pre_hook\n    Optimizer.register_load_state_dict_post_hook\n    Optimizer.state_dict\n    Optimizer.register_state_dict_pre_hook\n    Optimizer.register_state_dict_post_hook\n    Optimizer.step\n    Optimizer.register_step_pre_hook\n    Optimizer.register_step_post_hook\n    Optimizer.zero_grad\n```",
    "1731": "一级标题：torch.optim\n二级标题：Algorithms\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    Adadelta\n    Adafactor\n    Adagrad\n    Adam\n    AdamW\n    SparseAdam\n    Adamax\n    ASGD\n    LBFGS\n    NAdam\n    RAdam\n    RMSprop\n    Rprop\n    SGD\n```\nMany of our algorithms have various implementations optimized for performance,\nreadability and/or generality, so we attempt to default to the generally fastest\nimplementation for the current device if no particular implementation has been\nspecified by the user.\n\nWe have 3 major categories of implementations: for-loop, foreach (multi-tensor), and\nfused. The most straightforward implementations are for-loops over the parameters with\nbig chunks of computation. For-looping is usually slower than our foreach\nimplementations, which combine parameters into a multi-tensor and run the big chunks\nof computation all at once, thereby saving many sequential kernel calls. A few of our\noptimizers have even faster fused implementations, which fuse the big chunks of\ncomputation into one kernel. We can think of foreach implementations as fusing\nhorizontally and fused implementations as fusing vertically on top of that.\n\nIn general, the performance ordering of the 3 implementations is fused > foreach > for-loop.\nSo when applicable, we default to foreach over for-loop. Applicable means the foreach\nimplementation is available, the user has not specified any implementation-specific kwargs\n(e.g., fused, foreach, differentiable), and all tensors are native. Note that while fused\nshould be even faster than foreach, the implementations are newer and we would like to give\nthem more bake-in time before flipping the switch everywhere. We summarize the stability status\nfor each implementation on the second table below, you are welcome to try them out though!\n\nBelow is a table showing the available and default implementations of each algorithm:\n\n```{eval-rst}\n.. csv-table::\n    :header: \"Algorithm\", \"Default\", \"Has foreach?\", \"Has fused?\"\n    :widths: 25, 25, 25, 25\n    :delim: ;\n\n    :class:`Adadelta`;foreach;yes;no\n    :class:`Adafactor`;for-loop;no;no\n    :class:`Adagrad`;foreach;yes;yes (cpu only)\n    :class:`Adam`;foreach;yes;yes\n    :class:`AdamW`;foreach;yes;yes\n    :class:`SparseAdam`;for-loop;no;no\n    :class:`Adamax`;foreach;yes;no\n    :class:`ASGD`;foreach;yes;no\n    :class:`LBFGS`;for-loop;no;no\n    :class:`NAdam`;foreach;yes;no\n    :class:`RAdam`;foreach;yes;no\n    :class:`RMSprop`;foreach;yes;no\n    :class:`Rprop`;foreach;yes;no\n    :class:`SGD`;foreach;yes;yes\n```\nBelow table is showing the stability status for fused implementations:\n\n```{eval-rst}\n.. csv-table::\n    :header: \"Algorithm\", \"CPU\", \"CUDA\", \"MPS\"\n    :widths: 25, 25, 25, 25\n    :delim: ;\n\n    :class:`Adadelta`;unsupported;unsupported;unsupported\n    :class:`Adafactor`;unsupported;unsupported;unsupported\n    :class:`Adagrad`;beta;unsupported;unsupported\n    :class:`Adam`;beta;stable;beta\n    :class:`AdamW`;beta;stable;beta\n    :class:`SparseAdam`;unsupported;unsupported;unsupported\n    :class:`Adamax`;unsupported;unsupported;unsupported\n    :class:`ASGD`;unsupported;unsupported;unsupported\n    :class:`LBFGS`;unsupported;unsupported;unsupported\n    :class:`NAdam`;unsupported;unsupported;unsupported\n    :class:`RAdam`;unsupported;unsupported;unsupported\n    :class:`RMSprop`;unsupported;unsupported;unsupported\n    :class:`Rprop`;unsupported;unsupported;unsupported\n    :class:`SGD`;beta;beta;beta\n```",
    "1732": "一级标题：torch.optim\n二级标题：How to adjust learning rate\n内容：\n{class}`torch.optim.lr_scheduler.LRScheduler` provides several methods to adjust the learning\nrate based on the number of epochs. {class}`torch.optim.lr_scheduler.ReduceLROnPlateau`\nallows dynamic learning rate reducing based on some validation measurements.\n\nLearning rate scheduling should be applied after optimizer's update; e.g., you\nshould write your code this way:\n\nExample:\n```python\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nscheduler = ExponentialLR(optimizer, gamma=0.9)\n\nfor epoch in range(20):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler.step()\n```\n\nMost learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it.\n\nExample:\n```python\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nscheduler1 = ExponentialLR(optimizer, gamma=0.9)\nscheduler2 = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n\nfor epoch in range(20):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler1.step()\n    scheduler2.step()\n```\n\nIn many places in the documentation, we will use the following template to refer to schedulers\nalgorithms.\n\n```python\n>>> scheduler = ...\n>>> for epoch in range(100):\n>>>     train(...)\n>>>     validate(...)\n>>>     scheduler.step()\n```\n\n```{warning}\nPrior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer's update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (calling `scheduler.step()`) before the optimizer's update\n(calling `optimizer.step()`), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are calling `scheduler.step()` at the wrong time.\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    lr_scheduler.LRScheduler\n    lr_scheduler.LambdaLR\n    lr_scheduler.MultiplicativeLR\n    lr_scheduler.StepLR\n    lr_scheduler.MultiStepLR\n    lr_scheduler.ConstantLR\n    lr_scheduler.LinearLR\n    lr_scheduler.ExponentialLR\n    lr_scheduler.PolynomialLR\n    lr_scheduler.CosineAnnealingLR\n    lr_scheduler.ChainedScheduler\n    lr_scheduler.SequentialLR\n    lr_scheduler.ReduceLROnPlateau\n    lr_scheduler.CyclicLR\n    lr_scheduler.OneCycleLR\n    lr_scheduler.CosineAnnealingWarmRestarts\n```",
    "1733": "一级标题：torch.optim\n二级标题：How to utilize named parameters to load optimizer state dict\n内容：\nThe function {func}`~Optimizer.load_state_dict` stores the optional `param_names` content from the\nloaded state dict if present. However, the process of loading the optimizer state is not affected,\nas the order of the parameters matters to maintain compatibility (in case of different ordering).\nTo utilize the loaded parameters names from the loaded state dict, a custom `register_load_state_dict_pre_hook`\nneeds to be implemented according to the desired behavior.\n\nThis can be useful, for instance, when the model architecture changes, but the weights and optimizer states need to\nremain unchanged. The following example demonstrates how to implement this customization.\n\nExample:\n```python\nclass OneLayerModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Linear(3, 4)\n\n    def forward(self, x):\n        return self.fc(x)\n\nmodel = OneLayerModel()\noptimizer = optim.SGD(model.named_parameters(), lr=0.01, momentum=0.9)\n# training..\ntorch.save(optimizer.state_dict(), PATH)\n```\n\nLet's say that `model` implements an expert (MoE), and we want to duplicate it and resume training\nfor two experts, both initialized the same way as the `fc` layer. For the following `model2` we create two layers identical to `fc` and resume training by loading the model weights and optimizer states from `model` into both `fc1` and `fc2` of `model2` (and adjust them accordingly):\n\n```python\nclass TwoLayerModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(3, 4)\n        self.fc2 = nn.Linear(3, 4)\n\n    def forward(self, x):\n        return (self.fc1(x) + self.fc2(x)) / 2\n\nmodel2 = TwoLayerModel()\n# adapt and load model weights..\noptimizer2 = optim.SGD(model2.named_parameters(), lr=0.01, momentum=0.9)\n```\n\nTo load the state dict for `optimizer2` with the state dict of the previous optimizer such that both\n`fc1` and `fc2` will be initialized with a copy of `fc` optimizer states\n(to resume training for each layer from `fc`), we can use the following hook:\n\n```python\ndef adapt_state_dict_ids(optimizer, state_dict):\n    adapted_state_dict = deepcopy(optimizer.state_dict())\n    # Copy setup parameters (lr, weight_decay, etc.), in case they differ in the loaded state dict.\n    for k, v in state_dict['param_groups'][0].items():\n        if k not in ['params', 'param_names']:\n            adapted_state_dict['param_groups'][0][k] = v\n\n    lookup_dict = {\n        'fc1.weight': 'fc.weight',\n        'fc1.bias': 'fc.bias',\n        'fc2.weight': 'fc.weight',\n        'fc2.bias': 'fc.bias'\n    }\n    clone_deepcopy = lambda d: {k: (v.clone() if isinstance(v, torch.Tensor) else deepcopy(v)) for k, v in d.items()}\n    for param_id, param_name in zip(\n            optimizer.state_dict()['param_groups'][0]['params'],\n            optimizer.state_dict()['param_groups'][0]['param_names']):\n        name_in_loaded = lookup_dict[param_name]\n        index_in_loaded_list = state_dict['param_groups'][0]['param_names'].index(name_in_loaded)\n        id_in_loaded = state_dict['param_groups'][0]['params'][index_in_loaded_list]\n        # Copy the state of the corresponding parameter\n        if id_in_loaded in state_dict['state']:\n            adapted_state_dict['state'][param_id] = clone_deepcopy(state_dict['state'][id_in_loaded])\n\n    return adapted_state_dict\n\noptimizer2.register_load_state_dict_pre_hook(adapt_state_dict_ids)\noptimizer2.load_state_dict(torch.load(PATH)) # The previous optimizer saved state_dict\n```\n\nThis ensures that the adapted state_dict with the correct states for the layers of `model2` will be used\nduring model loading.\nNote that this code is designed specifically for this example (e.g., assuming a single parameter group),\nand other cases might require different adaptations.\n\nThe following example shows how to handle missing parameters in a loaded\n`state dict` when the model structure changes.\nThe `Model_bypass` adds a new `bypass` layer, which is not present in the original `Model1`.\nTo resume training, a custom `adapt_state_dict_missing_param` hook is used to adapt the optimizer's `state_dict`,\nensuring existing parameters are mapped correctly, while missing ones (like the bypass layer) remain unchanged\n(as initialized in this example).\nThis approach enables smooth loading and resuming of the optimizer state despite model changes.\nThe new bypass layer will be trained from scratch:\n\n```python\nclass Model1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Linear(5, 5)\n\n    def forward(self, x):\n        return self.fc(x) + x\n\n\nmodel = Model1()\noptimizer = optim.SGD(model.named_parameters(), lr=0.01, momentum=0.9)\n# training..\ntorch.save(optimizer.state_dict(), PATH)\n\nclass Model_bypass(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Linear(5, 5)\n        self.bypass = nn.Linear(5, 5, bias=False)\n        torch.nn.init.eye_(self.bypass.weight)\n\n    def forward(self, x):\n        return self.fc(x) + self.bypass(x)\n\nmodel2 = Model_bypass()\noptimizer2 = optim.SGD(model2.named_parameters(), lr=0.01, momentum=0.9)\n\ndef adapt_state_dict_missing_param(optimizer, state_dict):\n    adapted_state_dict = deepcopy(optimizer.state_dict())\n    # Copy setup parameters (lr, weight_decay, etc.), in case they differ in the loaded state dict.\n    for k, v in state_dict['param_groups'][0].items():\n        if k not in ['params', 'param_names']:\n            adapted_state_dict['param_groups'][0][k] = v\n\n    lookup_dict = {\n        'fc.weight': 'fc.weight',\n        'fc.bias': 'fc.bias',\n        'bypass.weight': None,\n    }\n\n    clone_deepcopy = lambda d: {k: (v.clone() if isinstance(v, torch.Tensor) else deepcopy(v)) for k, v in d.items()}\n    for param_id, param_name in zip(\n            optimizer.state_dict()['param_groups'][0]['params'],\n            optimizer.state_dict()['param_groups'][0]['param_names']):\n        name_in_loaded = lookup_dict[param_name]\n        if name_in_loaded in state_dict['param_groups'][0]['param_names']:\n            index_in_loaded_list = state_dict['param_groups'][0]['param_names'].index(name_in_loaded)\n            id_in_loaded = state_dict['param_groups'][0]['params'][index_in_loaded_list]\n            # Copy the state of the corresponding parameter\n            if id_in_loaded in state_dict['state']:\n                adapted_state_dict['state'][param_id] = clone_deepcopy(state_dict['state'][id_in_loaded])\n\n    return adapted_state_dict\n\noptimizer2.register_load_state_dict_pre_hook(adapt_state_dict_ids)\noptimizer2.load_state_dict(torch.load(PATH)) # The previous optimizer saved state_dict\n```\n\n\nAs a third example, instead of loading a state according to the order of parameters (the default approach),\nthis hook can be used to load according to the parameters' names:\n\n```python\ndef names_matching(optimizer, state_dict):\n    assert len(state_dict['param_groups']) == len(optimizer.state_dict()['param_groups'])\n    adapted_state_dict = deepcopy(optimizer.state_dict())\n    for g_ind in range(len(state_dict['param_groups'])):\n        assert len(state_dict['param_groups'][g_ind]['params']) == len(\n            optimizer.state_dict()['param_groups'][g_ind]['params'])\n\n        for k, v in state_dict['param_groups'][g_ind].items():\n            if k not in ['params', 'param_names']:\n                adapted_state_dict['param_groups'][g_ind][k] = v\n\n        for param_id, param_name in zip(\n                optimizer.state_dict()['param_groups'][g_ind]['params'],\n                optimizer.state_dict()['param_groups'][g_ind]['param_names']):\n            index_in_loaded_list = state_dict['param_groups'][g_ind]['param_names'].index(param_name)\n            id_in_loaded = state_dict['param_groups'][g_ind]['params'][index_in_loaded_list]\n            # Copy the state of the corresponding parameter\n            if id_in_loaded in state_dict['state']:\n                adapted_state_dict['state'][param_id] = deepcopy(state_dict['state'][id_in_loaded])\n\n    return adapted_state_dict\n```",
    "1734": "一级标题：torch.optim\n二级标题：Weight Averaging (SWA and EMA)\n内容：\n{class}`torch.optim.swa_utils.AveragedModel` implements Stochastic Weight Averaging (SWA) and Exponential Moving Average (EMA),\n{class}`torch.optim.swa_utils.SWALR` implements the SWA learning rate scheduler and\n{func}`torch.optim.swa_utils.update_bn` is a utility function used to update SWA/EMA batch\nnormalization statistics at the end of training.\n\nSWA has been proposed in [Averaging Weights Leads to Wider Optima and Better Generalization](https://arxiv.org/abs/1803.05407).\n\nEMA is a widely known technique to reduce the training time by reducing the number of weight updates needed.\nIt is a variation of [Polyak averaging](https://paperswithcode.com/method/polyak-averaging), but using exponential weights instead of equal weights across iterations.\n\n### Constructing averaged models\n\nThe `AveragedModel` class serves to compute the weights of the SWA or EMA model.\n\nYou can create an SWA averaged model by running:\n\n```python\n>>> averaged_model = AveragedModel(model)\n```\n\nEMA models are constructed by specifying the `multi_avg_fn` argument as follows:\n\n```python\n>>> decay = 0.999\n>>> averaged_model = AveragedModel(model, multi_avg_fn=get_ema_multi_avg_fn(decay))\n```\n\nDecay is a parameter between 0 and 1 that controls how fast the averaged parameters are decayed. If not provided to {func}`torch.optim.swa_utils.get_ema_multi_avg_fn`, the default is 0.999. Decay value should be close to 1.0, as smaller values can cause optimization convergence issues.\n\n{func}`torch.optim.swa_utils.get_ema_multi_avg_fn` returns a function that applies the following EMA equation to the weights:\n\n```{math}\nW^\\textrm{EMA}_{t+1} = \\alpha W^\\textrm{EMA}_{t} + (1 - \\alpha) W^\\textrm{model}_t\n```\n\nwhere alpha is the EMA decay.\n\nHere the model `model` can be an arbitrary {class}`torch.nn.Module` object. `averaged_model`\nwill keep track of the running averages of the parameters of the `model`. To update these\naverages, you should use the {func}`update_parameters` function after the `optimizer.step()`:\n\n```python\n>>> averaged_model.update_parameters(model)\n```\n\nFor SWA and EMA, this call is usually done right after the optimizer `step()`. In the case of SWA, this is usually skipped for some numbers of steps at the beginning of the training.\n\n### Custom averaging strategies\n\nBy default, {class}`torch.optim.swa_utils.AveragedModel` computes a running equal average of\nthe parameters that you provide, but you can also use custom averaging functions with the\n`avg_fn` or `multi_avg_fn` parameters:\n\n- `avg_fn` allows defining a function operating on each parameter tuple (averaged parameter, model parameter) and should return the new averaged parameter.\n- `multi_avg_fn` allows defining more efficient operations acting on a tuple of parameter lists, (averaged parameter list, model parameter list), at the same time, for example using the `torch._foreach*` functions. This function must update the averaged parameters in-place.\n\nIn the following example `ema_model` computes an exponential moving average using the `avg_fn` parameter:\n\n```python\n>>> ema_avg = lambda averaged_model_parameter, model_parameter, num_averaged:\\\n>>>         0.9 * averaged_model_parameter + 0.1 * model_parameter\n>>> ema_model = torch.optim.swa_utils.AveragedModel(model, avg_fn=ema_avg)\n```\n\nIn the following example `ema_model` computes an exponential moving average using the more efficient `multi_avg_fn` parameter:\n\n```python\n>>> ema_model = AveragedModel(model, multi_avg_fn=get_ema_multi_avg_fn(0.9))\n```\n\n### SWA learning rate schedules\n\nTypically, in SWA the learning rate is set to a high constant value. {class}`SWALR` is a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group:\n\n```python\n>>> swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, \\\n>>>         anneal_strategy=\"linear\", anneal_epochs=5, swa_lr=0.05)\n```\n\nYou can also use cosine annealing to a fixed value instead of linear annealing by setting\n`anneal_strategy=\"cos\"`.\n\n\n### Taking care of batch normalization\n\n{func}`update_bn` is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloader `loader` at the end of training:\n\n```python\n>>> torch.optim.swa_utils.update_bn(loader, swa_model)\n```\n\n{func}`update_bn` applies the `swa_model` to every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model.\n\n```{warning}\n{func}`update_bn` assumes that each batch in the dataloader `loader` is either a tensors or a list of\ntensors where the first element is the tensor that the network `swa_model` should be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of the\n`swa_model` by doing a forward pass with the `swa_model` on each element of the dataset.\n```\n\n\n\n### Putting it all together: SWA\n\nIn the example below, `swa_model` is the SWA model that accumulates the averages of the weights.\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\nand start to collect SWA averages of the parameters at epoch 160:\n\n```python\n>>> loader, optimizer, model, loss_fn = ...\n>>> swa_model = torch.optim.swa_utils.AveragedModel(model)\n>>> scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n>>> swa_start = 160\n>>> swa_scheduler = SWALR(optimizer, swa_lr=0.05)\n>>>\n>>> for epoch in range(300):\n>>>       for input, target in loader:\n>>>           optimizer.zero_grad()\n>>>           loss_fn(model(input), target).backward()\n>>>           optimizer.step()\n>>>       if epoch > swa_start:\n>>>           swa_model.update_parameters(model)\n>>>           swa_scheduler.step()\n>>>       else:\n>>>           scheduler.step()\n>>>\n>>> # Update bn statistics for the swa_model at the end\n>>> torch.optim.swa_utils.update_bn(loader, swa_model)\n>>> # Use swa_model to make predictions on test data\n>>> preds = swa_model(test_input)\n```\n\n### Putting it all together: EMA\n\nIn the example below, `ema_model` is the EMA model that accumulates the exponentially-decayed averages of the weights with a decay rate of 0.999.\nWe train the model for a total of 300 epochs and start to collect EMA averages immediately.\n\n```python\n>>> loader, optimizer, model, loss_fn = ...\n>>> ema_model = torch.optim.swa_utils.AveragedModel(model, \\\n>>>             multi_avg_fn=torch.optim.swa_utils.get_ema_multi_avg_fn(0.999))\n>>>\n>>> for epoch in range(300):\n>>>       for input, target in loader:\n>>>           optimizer.zero_grad()\n>>>           loss_fn(model(input), target).backward()\n>>>           optimizer.step()\n>>>           ema_model.update_parameters(model)\n>>>\n>>> # Update bn statistics for the ema_model at the end\n>>> torch.optim.swa_utils.update_bn(loader, ema_model)\n>>> # Use ema_model to make predictions on test data\n>>> preds = ema_model(test_input)\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    swa_utils.AveragedModel\n    swa_utils.SWALR\n\n\n.. autofunction:: torch.optim.swa_utils.get_ema_multi_avg_fn\n.. autofunction:: torch.optim.swa_utils.update_bn\n```\n\n<!-- This module needs to be documented. Adding here in the meantime\nfor tracking purposes -->\n```{eval-rst}\n.. py:module:: torch.optim.lr_scheduler\n.. py:module:: torch.optim.optimizer\n.. py:module:: torch.optim.swa_utils\n```\n\n```{eval-rst}\n.. toctree::\n    :hidden:\n\n    optim.aliases.md\n```",
    "1735": "一级标题：torch.package\n二级标题：无\n内容：\n`torch.package` adds support for creating packages containing both artifacts and arbitrary\nPyTorch code. These packages can be saved, shared, used to load and execute models\nat a later date or on a different machine, and can even be deployed to production using\n`torch::deploy`.\n\nThis document contains tutorials, how-to guides, explanations, and an API reference that\nwill help you learn more about `torch.package` and how to use it.\n\n```{warning}\nThis module depends on the `pickle` module which is not secure. Only unpackage data you trust.\n\nIt is possible to construct malicious pickle data which will **execute arbitrary code during unpickling**.\nNever unpackage data that could have come from an untrusted source, or that could have been tampered with.\n\nFor more information, review the [documentation](https://docs.python.org/3/library/pickle.html) for the `pickle` module.\n```\n\n```{contents}\n:local:\n:depth: 2\n```",
    "1736": "一级标题：torch.package\n二级标题：Tutorials\n内容：\n### Packaging your first model\nA tutorial that guides you through packaging and unpackaging a simple model is available\n[on Colab](https://colab.research.google.com/drive/1lFZkLyViGfXxB-m3jqlyTQuYToo3XLo-).\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages.",
    "1737": "一级标题：torch.package\n二级标题：How do I...\n内容：\n### See what is inside a package?\n\n#### Treat the package like a ZIP archive\n\nThe container format for a `torch.package` is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files:\n\n* `unzip my_package.pt` will unzip the `torch.package` archive to disk, where you can freely inspect its contents.\n\n```\n$ unzip my_package.pt && tree my_package\nmy_package\n├── .data\n│   ├── 94304870911616.storage\n│   ├── 94304900784016.storage\n│   ├── extern_modules\n│   └── version\n├── models\n│   └── model_1.pkl\n└── torchvision\n    └── models\n        ├── resnet.py\n        └── utils.py\n~ cd my_package && cat torchvision/models/resnet.py\n...\n```\n\n* The Python `zipfile` module provides a standard way to read and write ZIP archive contents.\n\n```python\nfrom zipfile import ZipFile\nwith ZipFile(\"my_package.pt\") as myzip:\n    file_bytes = myzip.read(\"torchvision/models/resnet.py\")\n    # edit file_bytes in some way\n    myzip.writestr(\"torchvision/models/resnet.py\", new_file_bytes)\n```\n\n* vim has the ability to natively read ZIP archives. You can even edit files and :`write` them back into the archive!\n\n```vim\n# add this to your .vimrc to treat `*.pt` files as zip files\nau BufReadCmd *.pt call zip#Browse(expand(\"<amatch>\"))\n\n~ vi my_package.pt\n```\n\n#### Use the `file_structure()` API\n{class}`PackageImporter` provides a `file_structure()` method, which will return a printable\nand queryable {class}`Directory` object. The {class}`Directory` object is a simple directory structure that you can use to explore the\ncurrent contents of a `torch.package`.\n\nThe {class}`Directory` object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-style `include` and `exclude` filtering arguments.\n\n```python\nwith PackageExporter('my_package.pt') as pe:\n    pe.save_pickle('models', 'model_1.pkl', mod)\n\nimporter = PackageImporter('my_package.pt')\n# can limit printed items with include/exclude args\nprint(importer.file_structure(include=[\"**/utils.py\", \"**/*.pkl\"], exclude=\"**/*.storage\"))\nprint(importer.file_structure()) # will print out all files\n```\n\nOutput:\n\n```\n# filtered with glob pattern:\n#    include=[\"**/utils.py\", \"**/*.pkl\"], exclude=\"**/*.storage\"\n─── my_package.pt\n    ├── models\n    │   └── model_1.pkl\n    └── torchvision\n        └── models\n            └── utils.py\n\n# all files\n─── my_package.pt\n    ├── .data\n    │   ├── 94304870911616.storage\n    │   ├── 94304900784016.storage\n    │   ├── extern_modules\n    │   └── version\n    ├── models\n    │   └── model_1.pkl\n    └── torchvision\n        └── models\n            ├── resnet.py\n            └── utils.py\n```\n\nYou can also query {class}`Directory` objects with the `has_file()` method.\n\n```python\nimporter_file_structure = importer.file_structure()\nfound: bool = importer_file_structure.has_file(\"package_a/subpackage.py\")\n```\n\n### See why a given module was included as a dependency?\n\nSay there is a given module `foo`, and you want to know why your {class}`PackageExporter` is pulling in `foo` as a dependency.\n\n{meth}`PackageExporter.get_rdeps` will return all modules that directly depend on `foo`.\n\nIf you would like to see how a given module `src` depends on `foo`, the {meth}`PackageExporter.all_paths` method will\nreturn a DOT-formatted graph showing all the dependency paths between `src` and `foo`.\n\nIf you would just like to see the whole dependency graph of your :class:`PackageExporter`, you can use {meth}`PackageExporter.dependency_graph_string`.\n\n\n### Include arbitrary resources with my package and access them later?\n{class}`PackageExporter` exposes three methods, `save_pickle`, `save_text` and `save_binary` that allow you to save\nPython objects, text, and binary data to a package.\n\n```python\nwith torch.PackageExporter(\"package.pt\") as exporter:\n    # Pickles the object and saves to `my_resources/tensor.pkl` in the archive.\n    exporter.save_pickle(\"my_resources\", \"tensor.pkl\", torch.randn(4))\n    exporter.save_text(\"config_stuff\", \"words.txt\", \"a sample string\")\n    exporter.save_binary(\"raw_data\", \"binary\", my_bytes)\n\n```\n{class}`PackageImporter` exposes complementary methods named `load_pickle`, `load_text` and `load_binary` that allow you to load\nPython objects, text and binary data from a package.\n\n```python\nimporter = torch.PackageImporter(\"package.pt\")\nmy_tensor = importer.load_pickle(\"my_resources\", \"tensor.pkl\")\ntext = importer.load_text(\"config_stuff\", \"words.txt\")\nbinary = importer.load_binary(\"raw_data\", \"binary\")\n```\n\n### Customize how a class is packaged?\n`torch.package` allows for the customization of how classes are packaged. This behavior is accessed through defining the method\n`__reduce_package__` on a class and by defining a corresponding de-packaging function. This is similar to defining `__reduce__` for\nPython’s normal pickling process.\n\nSteps:\n\n1. Define the method `__reduce_package__(self, exporter: PackageExporter)` on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by the `PackageExporter` when it encounters an instance of the target class.\n2. Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature’s first parameter should be a `PackageImporter` instance, and the rest of the parameters are user defined.\n\n\n```python\n# foo.py [Example of customizing how class Foo is packaged]\nfrom torch.package import PackageExporter, PackageImporter\nimport time\n\n\nclass Foo:\n    def __init__(self, my_string: str):\n        super().__init__()\n        self.my_string = my_string\n        self.time_imported = 0\n        self.time_exported = 0\n\n    def __reduce_package__(self, exporter: PackageExporter):\n        \"\"\"\n        Called by ``torch.package.PackageExporter``'s Pickler's ``persistent_id`` when\n        saving an instance of this object. This method should do the work to save this\n        object inside of the ``torch.package`` archive.\n\n        Returns function w/ arguments to load the object from a\n        ``torch.package.PackageImporter``'s Pickler's ``persistent_load`` function.\n        \"\"\"\n\n        # use this pattern to ensure no naming conflicts with normal dependencies,\n        # anything saved under this module name shouldn't conflict with other\n        # items in the package\n        generated_module_name = f\"foo-generated._{exporter.get_unique_id()}\"\n        exporter.save_text(\n            generated_module_name,\n            \"foo.txt\",\n            self.my_string + \", with exporter modification!\",\n        )\n        time_exported = time.clock_gettime(1)\n\n        # returns de-packaging function w/ arguments to invoke with\n        return (unpackage_foo, (generated_module_name, time_exported,))\n\n\ndef unpackage_foo(\n    importer: PackageImporter, generated_module_name: str, time_exported: float\n) -> Foo:\n    \"\"\"\n    Called by ``torch.package.PackageImporter``'s Pickler's ``persistent_load`` function\n    when depickling a Foo object.\n    Performs work of loading and returning a Foo instance from a ``torch.package`` archive.\n    \"\"\"\n    time_imported = time.clock_gettime(1)\n    foo = Foo(importer.load_text(generated_module_name, \"foo.txt\"))\n    foo.time_imported = time_imported\n    foo.time_exported = time_exported\n    return foo\n\n```\n\n\n```python\n# example of saving instances of class Foo\n\nimport torch\nfrom torch.package import PackageImporter, PackageExporter\nimport foo\n\nfoo_1 = foo.Foo(\"foo_1 initial string\")\nfoo_2 = foo.Foo(\"foo_2 initial string\")\nwith PackageExporter('foo_package.pt') as pe:\n    # save as normal, no extra work necessary\n    pe.save_pickle('foo_collection', 'foo1.pkl', foo_1)\n    pe.save_pickle('foo_collection', 'foo2.pkl', foo_2)\n\npi = PackageImporter('foo_package.pt')\nprint(pi.file_structure())\nimported_foo = pi.load_pickle('foo_collection', 'foo1.pkl')\nprint(f\"foo_1 string: '{imported_foo.my_string}'\")\nprint(f\"foo_1 export time: {imported_foo.time_exported}\")\nprint(f\"foo_1 import time: {imported_foo.time_imported}\")\n```\n\n```\n# output of running above script\n─── foo_package\n    ├── foo-generated\n    │   ├── _0\n    │   │   └── foo.txt\n    │   └── _1\n    │       └── foo.txt\n    ├── foo_collection\n    │   ├── foo1.pkl\n    │   └── foo2.pkl\n    └── foo.py\n\nfoo_1 string: 'foo_1 initial string, with reduction modification!'\nfoo_1 export time: 9857706.650140837\nfoo_1 import time: 9857706.652698385\n```\n\n### Test in my source code whether or not it is executing inside a package?\n\nA {class}`PackageImporter` will add the attribute `__torch_package__` to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not.\n\n```python\n# In foo/bar.py:\n\nif \"__torch_package__\" in dir():  # true if the code is being loaded from a package\n    def is_in_package():\n        return True\n\n    UserException = Exception\nelse:\n    def is_in_package():\n        return False\n\n    UserException = UnpackageableException\n```\n\nNow, the code will behave differently depending on whether it’s imported normally through your Python environment or imported from a\n`torch.package`.\n\n```python\nfrom foo.bar import is_in_package\n\nprint(is_in_package())  # False\n\nloaded_module = PackageImporter(my_package).import_module(\"foo.bar\")\nloaded_module.is_in_package()  # True\n```\n\n**Warning**: in general, it’s bad practice to have code that behaves differently depending on whether it’s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded.\n\n\n### Patch code into a package?\n{class}`PackageExporter` offers a `save_source_string()` method that allows one to save arbitrary Python source code to a module of your choosing.\n```python\nwith PackageExporter(f) as exporter:\n    # Save the my_module.foo available in your current Python environment.\n    exporter.save_module(\"my_module.foo\")\n\n    # This saves the provided string to my_module/foo.py in the package archive.\n    # It will override the my_module.foo that was previously saved.\n    exporter.save_source_string(\"my_module.foo\", textwrap.dedent(\n        \"\"\"\\\n        def my_function():\n            print('hello world')\n        \"\"\"\n    ))\n\n    # If you want to treat my_module.bar as a package\n    # (e.g. save to `my_module/bar/__init__.py` instead of `my_module/bar.py)\n    # pass is_package=True,\n    exporter.save_source_string(\"my_module.bar\",\n                                \"def foo(): print('hello')\\n\",\n                                is_package=True)\n\nimporter = PackageImporter(f)\nimporter.import_module(\"my_module.foo\").my_function()  # prints 'hello world'\n```\n\n### Access package contents from packaged code?\n{class}`PackageImporter` implements the\n[`importlib.resources`](https://docs.python.org/3/library/importlib.html#module-importlib.resources)\nAPI for accessing resources from inside a package.\n\n```python\nwith PackageExporter(f) as exporter:\n    # saves text to my_resource/a.txt in the archive\n    exporter.save_text(\"my_resource\", \"a.txt\", \"hello world!\")\n    # saves the tensor to my_pickle/obj.pkl\n    exporter.save_pickle(\"my_pickle\", \"obj.pkl\", torch.ones(2, 2))\n\n    # see below for module contents\n    exporter.save_module(\"foo\")\n    exporter.save_module(\"bar\")\n```\n\nThe `importlib.resources` API allows access to resources from within packaged code.\n\n\n```python\n# foo.py:\nimport importlib.resources\nimport my_resource\n\n# returns \"hello world!\"\ndef get_my_resource():\n    return importlib.resources.read_text(my_resource, \"a.txt\")\n```\n\nUsing `importlib.resources` is the recommended way to access package contents from within packaged code, since it complies\nwith the Python standard. However, it is also possible to access the parent :class:`PackageImporter` instance itself from within\npackaged code.\n\n```python\n# bar.py:\nimport torch_package_importer # this is the PackageImporter that imported this module.\n\n# Prints \"hello world!\", equivalent to importlib.resources.read_text\ndef get_my_resource():\n    return torch_package_importer.load_text(\"my_resource\", \"a.txt\")\n\n# You also do things that the importlib.resources API does not support, like loading\n# a pickled object from the package.\ndef get_my_pickle():\n    return torch_package_importer.load_pickle(\"my_pickle\", \"obj.pkl\")\n```\n\n### Distinguish between packaged code and non-packaged code?\nTo tell if an object’s code is from a `torch.package`, use the `torch.package.is_from_package()` function.\nNote: if an object is from a package but its definition is from a module marked `extern` or from `stdlib`,\nthis check will return `False`.\n\n```python\nimporter = PackageImporter(f)\nmod = importer.import_module('foo')\nobj = importer.load_pickle('model', 'model.pkl')\ntxt = importer.load_text('text', 'my_test.txt')\n\nassert is_from_package(mod)\nassert is_from_package(obj)\nassert not is_from_package(txt) # str is from stdlib, so this will return False\n```\n\n### Re-export an imported object?\nTo re-export an object that was previously imported by a {class}`PackageImporter`, you must make the new {class}`PackageExporter`\naware of the original {class}`PackageImporter` so that it can find source code for your object’s dependencies.\n\n```python\nimporter = PackageImporter(f)\nobj = importer.load_pickle(\"model\", \"model.pkl\")\n\n# re-export obj in a new package\nwith PackageExporter(f2, importer=(importer, sys_importer)) as exporter:\n    exporter.save_pickle(\"model\", \"model.pkl\", obj)\n```",
    "1738": "一级标题：torch.package\n二级标题：Explanation\n内容：\n### `torch.package` Format Overview\nA `torch.package` file is a ZIP archive which conventionally uses the `.pt` extension. Inside the ZIP archive, there are two kinds of files:\n\n* Framework files, which are placed in the `.data/`.\n* User files, which is everything else.\n\nAs an example, this is what a fully packaged ResNet model from `torchvision` looks like:\n\n```\nresnet\n├── .data  # All framework-specific data is stored here.\n│   │      # It's named to avoid conflicts with user-serialized code.\n│   ├── 94286146172688.storage  # tensor data\n│   ├── 94286146172784.storage\n│   ├── extern_modules  # text file with names of extern modules (e.g. 'torch')\n│   ├── version         # version metadata\n│   ├── ...\n├── model  # the pickled model\n│   └── model.pkl\n└── torchvision  # all code dependencies are captured as source files\n    └── models\n        ├── resnet.py\n        └── utils.py\n```\n\n#### Framework files\nThe `.data/` directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThe `torch.package` format makes no guarantees about the contents of `.data/`, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load older `torch.packages`).\n\nCurrently, the `.data/` directory contains the following items:\n\n* `version`: a version number for the serialized format, so that the `torch.package` import infrastructures knows how to load this package.\n* `extern_modules`: a list of modules that are considered `extern`. `extern` modules will be imported using the loading environment’s system importer.\n* `*.storage`: serialized tensor data.\n\n```\n.data\n├── 94286146172688.storage\n├── 94286146172784.storage\n├── extern_modules\n├── version\n├── ...\n```\n\n#### User files\nAll other files in the archive were put there by a user. The layout is identical to a Python\n[regular package](https://docs.python.org/3/reference/import.html#regular-packages). For a deeper dive in how Python packaging works,\nplease consult [this essay](https://www.python.org/doc/essays/packages/) (it’s slightly out of date, so double-check implementation details\nwith the [Python reference documentation](https://docs.python.org/3/library/importlib.html).\n\n```\n<package root>\n├── model  # the pickled model\n│   └── model.pkl\n├── another_package\n│   ├── __init__.py\n│   ├── foo.txt         # a resource file , see importlib.resources\n│   └── ...\n└── torchvision\n    └── models\n        ├── resnet.py   # torchvision.models.resnet\n        └── utils.py    # torchvision.models.utils\n```\n\n### How `torch.package` finds your code's dependencies\n#### Analyzing an object's dependencies\nWhen you issue a `save_pickle(obj, ...)` call, {class}`PackageExporter` will pickle the object normally. Then, it uses the\n`pickletools` standard library module to parse the pickle bytecode.\n\nIn a pickle, an object is saved along with a `GLOBAL` opcode that describes where to find the implementation of the object’s type, like:\n\n```\nGLOBAL 'torchvision.models.resnet Resnet`\n```\n\nThe dependency resolver will gather up all `GLOBAL` ops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consult [the Python docs](https://docs.python.org/3/library/pickle.html).\n\n#### Analyzing a module's dependencies\nWhen a Python module is identified as a dependency, `torch.package` walks the module’s python AST representation and looks for import statements with\nfull support for the standard forms: `from x import y`, `import z`, `from w import v as u`, etc. When one of these import statements are\nencountered, `torch.package` registers the imported modules as dependencies that are then themselves parsed in the same AST walking way.\n\n**Note**: AST parsing has limited support for the `__import__(...)` syntax and does not support `importlib.import_module` calls. In general, you should\nnot expect dynamic imports to be detected by `torch.package`.\n\n\n### Dependency Management\n`torch.package` automatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an *action* to take.\n\nThe allowed actions are:\n\n* `intern`: put this module into the package.\n* `extern`: declare this module as an external dependency of the package.\n* `mock`: stub out this module.\n* `deny`: depending on this module will raise an error during package export.\n\nFinally, there is one more important action that is not technically part of `torch.package`:\n\n* Refactoring: remove or change the dependencies in your code.\n\nNote that actions are only defined on entire Python modules. There is no way to package “just” a function or class from a module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that’s what `torch.package` uses.\n\nActions are applied to modules using patterns. Patterns can either be module names (`\"foo.bar\"`) or globs (like `\"foo.**\"`). You associate a pattern\nwith an action using methods on {class}`PackageExporter`, e.g.\n\n```python\nmy_exporter.intern(\"torchvision.**\")\nmy_exporter.extern(\"numpy\")\n```\n\nIf a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken.\n\n\n#### `intern`\nIf a module is `intern`-ed, it will be placed into the package.\n\nThis action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet from `torchvision`,\nyou will need to `intern` the module torchvision.models.resnet.\n\nOn package import, when your packaged code tries to import an `intern`-ed module, PackageImporter will look inside your package for that module.\nIf it can’t find that module, an error will be raised. This ensures that each {class}`PackageImporter` is isolated from the loading environment—even\nif you have `my_interned_module` available in both your package and the loading environment, {class}`PackageImporter` will only use the version in your\npackage.\n\n**Note**: Only Python source modules can be `intern`-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to `intern` them. These kinds of modules need to be `mock`-ed or `extern`-ed.\n\n\n#### `extern`\nIf a module is `extern`-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on `package_exporter.extern_modules`.\n\nOn package import, when the packaged code tries to import an `extern`-ed module, {class}`PackageImporter` will use the default Python importer to find\nthat module, as if you did `importlib.import_module(\"my_externed_module\")`. If it can’t find that module, an error will be raised.\n\nIn this way, you can depend on third-party libraries like `numpy` and `scipy` from within your package without having to package them too.\n\n**Warning**: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of `extern`.\n\n\n#### `mock`\nIf a module is `mock`-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so that `from my_mocked_module import foo` will not error), but any use of that object will raise a `NotImplementedError`.\n\n`mock` should be used for code that you “know” will not be needed in the loaded package, but you still want available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training.\n\n**Warning**: In general, `mock` should be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies.\n\n\n#### Refactoring\nThe best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!):\n\n**Include only what you use**. Do not leave unused imports in your code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them.\n\n**Qualify your imports**. For example, instead of writing import foo and later using `foo.bar.baz`, prefer to write `from foo.bar import baz`. This more\nprecisely specifies your real dependency (`foo.bar`) and lets the dependency resolver know you don’t need all of `foo`.\n\n**Split up large files with unrelated functionality into smaller ones**. If your `utils` module contains a hodge-podge of unrelated functionality, any module\nthat depends on `utils` will need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another.\n\n\n#### Patterns\nPatterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buck\n[glob()](https://docs.bazel.build/versions/master/be/functions.html#glob).\n\nA module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g. `foo.bar.baz`.\n\nA pattern contains one or more segments. Segments can be:\n\n* A literal string (e.g. `foo`), which matches exactly.\n* A string containing a wildcard (e.g. `torch`, or `foo*baz*`). The wildcard matches any string, including the empty string.\n* A double wildcard (`**`). This matches against zero or more complete segments.\n\nExamples:\n\n* `torch.**`: matches `torch` and all its submodules, e.g. `torch.nn` and `torch.nn.functional`.\n* `torch.*`: matches `torch.nn` or `torch.functional`, but not `torch.nn.functional` or `torch`\n* `torch*.**`: matches `torch`, `torchvision`, and all of their submodules\n\nWhen specifying actions, you can pass multiple patterns, e.g.\n\n```python\nexporter.intern([\"torchvision.models.**\", \"torchvision.utils.**\"])\n```\n\nA module will match against this action if it matches any of the patterns.\n\nYou can also specify patterns to exclude, e.g.\n\n```python\nexporter.mock(\"**\", exclude=[\"torchvision.**\"])\n```\n\n\nA module will not match against this action if it matches any of the exclude patterns. In this example, we are mocking all modules except\n`torchvision` and its submodules.\n\nWhen a module could potentially match against multiple actions, the first action defined will be taken.\n\n\n### `torch.package` sharp edges\n#### Avoid global state in your modules\nPython makes it really easy to bind objects and run code at module-level scope. This is generally fine—after all, functions and classes are bound to\nnames this way. However, things become more complicated when you define an object at module scope with the intention of mutating it, introducing mutable\nglobal state.\n\nMutable global state is quite useful—it can reduce boilerplate, allow for open registration into tables, etc. But unless employed very carefully, it can\ncause complications when used with `torch.package`.\n\nEvery {class}`PackageImporter` creates an independent environment for its contents. This is nice because it means we load multiple packages and ensure\nthey are isolated from each other, but when modules are written in a way that assumes shared mutable global state, this behavior can create hard-to-debug\nerrors.\n\n#### Types are not shared between packages and the loading environment\nAny class that you import from a {class}`PackageImporter` will be a version of the class specific to that importer. For example:\n\n\n```python\nfrom foo import MyClass\n\nmy_class_instance = MyClass()\n\nwith PackageExporter(f) as exporter:\n    exporter.save_module(\"foo\")\n\nimporter = PackageImporter(f)\nimported_MyClass = importer.import_module(\"foo\").MyClass\n\nassert isinstance(my_class_instance, MyClass)  # works\nassert isinstance(my_class_instance, imported_MyClass)  # ERROR!\n```\n\nIn this example, `MyClass` and `imported_MyClass` are *not the same type*. In this specific example, `MyClass` and `imported_MyClass` have exactly the\nsame implementation, so you might think it’s okay to consider them the same class. But consider the situation where `imported_MyClass` is coming from an\nolder package with an entirely different implementation of `MyClass` — in that case, it’s unsafe to consider them the same class.\n\nUnder the hood, each importer has a prefix that allows it to uniquely identify classes:\n\n```python\nprint(MyClass.__name__)  # prints \"foo.MyClass\"\nprint(imported_MyClass.__name__)  # prints <torch_package_0>.foo.MyClass\n```\n\nThat means you should not expect `isinstance` checks to work when one of the arguments is from a package and the other is not. If you need this\nfunctionality, consider the following options:\n\n* Doing duck typing (just using the class instead of explicitly checking that it is of a given type).\n* Make the typing relationship an explicit part of the class contract. For example, you can add an attribute tag `self.handler = \"handle_me_this_way\"` and have client code check for the value of `handler` instead of checking the type directly.\n\n\n### How `torch.package` keeps packages isolated from each other\nEach {class}`PackageImporter` instance creates an independent, isolated environment for its modules and objects. Modules in a package can only import\nother packaged modules, or modules marked `extern`. If you use multiple {class}`PackageImporter` instances to load a single package, you will get\nmultiple independent environments that do not interact.\n\nThis is achieved by extending Python’s import infrastructure with a custom importer. {class}`PackageImporter` provides the same core API as the\n`importlib` importer; namely, it implements the `import_module` and `__import__` methods.\n\nWhen you invoke {meth}`PackageImporter.import_module`, {class}`PackageImporter` will construct and return a new module, much as the system importer does.\nHowever, {class}`PackageImporter` patches the returned module to use `self` (i.e. that {class}`PackageImporter` instance) to fulfill future import\nrequests by looking in the package rather than searching the user’s Python environment.\n\n#### Mangling\nTo avoid confusion (“is this `foo.bar` object the one from my package, or the one from my Python environment?”), {class}`PackageImporter` mangles the\n`__name__` and `__file__` of all imported modules, by adding a *mangle prefix* to them.\n\nFor `__name__`, a name like `torchvision.models.resnet18` becomes `<torch_package_0>.torchvision.models.resnet18`.\n\nFor `__file__`, a name like `torchvision/models/resnet18.py` becomes `<torch_package_0>.torchvision/modules/resnet18.py`.\n\nName mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print\nstatements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consult\n`mangling.md` in `torch/package/`.",
    "1739": "一级标题：torch.package\n二级标题：API Reference\n内容：\n```{eval-rst}\n.. autoclass:: torch.package.PackagingError\n\n.. autoclass:: torch.package.EmptyMatchError\n\n.. autoclass:: torch.package.PackageExporter\n  :members:\n\n  .. automethod:: __init__\n\n.. autoclass:: torch.package.PackageImporter\n  :members:\n\n  .. automethod:: __init__\n\n.. autoclass:: torch.package.Directory\n  :members:\n```\n\n<!-- This module needs to be documented. Adding here in the meantime\nfor tracking purposes -->\n```{eval-rst}\n.. py:module:: torch.package.analyze.find_first_use_of_broken_modules\n.. py:module:: torch.package.analyze.is_from_package\n.. py:module:: torch.package.analyze.trace_dependencies\n.. py:module:: torch.package.file_structure_representation\n.. py:module:: torch.package.find_file_dependencies\n.. py:module:: torch.package.glob_group\n.. py:module:: torch.package.importer\n.. py:module:: torch.package.package_exporter\n.. py:module:: torch.package.package_importer\n```",
    "1740": "一级标题：torch.profiler\n二级标题：无\n内容：",
    "1741": "一级标题：torch.profiler\n二级标题：Overview\n内容：\n```{eval-rst}\n.. automodule:: torch.profiler\n```",
    "1742": "一级标题：torch.profiler\n二级标题：API Reference\n内容：\n```{eval-rst}\n.. autoclass:: torch.profiler._KinetoProfile\n  :members:\n\n.. autoclass:: torch.profiler.profile\n  :members:\n\n.. autoclass:: torch.profiler.ProfilerAction\n  :members:\n\n.. autoclass:: torch.profiler.ProfilerActivity\n  :members:\n\n.. autofunction:: torch.profiler.schedule\n\n.. autofunction:: torch.profiler.tensorboard_trace_handler\n```",
    "1743": "一级标题：torch.profiler\n二级标题：Intel Instrumentation and Tracing Technology APIs\n内容：\n```{eval-rst}\n.. autofunction:: torch.profiler.itt.is_available\n\n.. autofunction:: torch.profiler.itt.mark\n\n.. autofunction:: torch.profiler.itt.range_push\n\n.. autofunction:: torch.profiler.itt.range_pop\n```\n\n<!-- This module needs to be documented. Adding here in the meantime\nfor tracking purposes -->\n```{eval-rst}\n.. py:module:: torch.profiler.itt\n.. py:module:: torch.profiler.profiler\n.. py:module:: torch.profiler.python_tracer\n```",
    "1744": "一级标题：Reference API\n二级标题：无\n内容：\n```{toctree}\n:maxdepth: 1\n\nC++ <https://docs.pytorch.org/cppdocs/>\n```\n\n```{toctree}\n:glob:\n:maxdepth: 1\n:caption: Python API\n\ntorch\nnn\nnn.functional\ntensors\ntensor_attributes\ntensor_view\ntorch.amp <amp>\ntorch.autograd <autograd>\ntorch.library <library>\naccelerator\ncpu\ncuda\ntorch.cuda.memory <torch_cuda_memory>\nmps\nxpu\nmtia\nmtia.memory\nmeta\ntorch.backends <backends>\ntorch.export <export>\ntorch.distributed <distributed>\ntorch.distributed.tensor <distributed.tensor>\ntorch.distributed.algorithms.join <distributed.algorithms.join>\ntorch.distributed.elastic <distributed.elastic>\ntorch.distributed.fsdp <fsdp>\ntorch.distributed.fsdp.fully_shard <distributed.fsdp.fully_shard>\ntorch.distributed.tensor.parallel <distributed.tensor.parallel>\ntorch.distributed.optim <distributed.optim>\ntorch.distributed.pipelining <distributed.pipelining>\ntorch.distributed.checkpoint <distributed.checkpoint>\ntorch.distributions <distributions>\ntorch.compiler <torch.compiler>\ntorch.fft <fft>\ntorch.func <func>\nfutures\nfx\nfx.experimental\ntorch.hub <hub>\ntorch.jit <jit>\ntorch.linalg <linalg>\ntorch.monitor <monitor>\ntorch.signal <signal>\ntorch.special <special>\ntorch.overrides\ntorch.package <package>\nprofiler\nnn.init\nnn.attention\nonnx\noptim\ncomplex_numbers\nddp_comm_hooks\nquantization\nrpc\ntorch.random <random>\nmasked\ntorch.nested <nested>\nsize\nsparse\nstorage\ntorch.testing <testing>\ntorch.utils <utils>\ntorch.utils.benchmark <benchmark_utils>\ntorch.utils.bottleneck <bottleneck>\ntorch.utils.checkpoint <checkpoint>\ntorch.utils.cpp_extension <cpp_extension>\ntorch.utils.data <data>\ntorch.utils.deterministic <deterministic>\ntorch.utils.jit <jit_utils>\ntorch.utils.dlpack <dlpack>\ntorch.utils.mobile_optimizer <mobile_optimizer>\ntorch.utils.model_zoo <model_zoo>\ntorch.utils.tensorboard <tensorboard>\ntorch.utils.module_tracker <module_tracker>\ntype_info\nnamed_tensor\nname_inference\ntorch.__config__ <config_mod>\ntorch.__future__ <future_mod>\nlogging\ntorch_environment_variables\n```",
    "1745": "一级标题：Quantization API Reference\n二级标题：无\n内容：",
    "1746": "一级标题：Quantization API Reference\n二级标题：torch.ao.quantization\n内容：\nThis module contains Eager mode quantization APIs.\n\n```{eval-rst}\n.. currentmodule:: torch.ao.quantization\n```\n\n### Top level APIs\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    quantize\n    quantize_dynamic\n    quantize_qat\n    prepare\n    prepare_qat\n    convert\n```\n\n### Preparing model for quantization\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    fuse_modules.fuse_modules\n    QuantStub\n    DeQuantStub\n    QuantWrapper\n    add_quant_dequant\n```\n\n### Utility functions\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    swap_module\n    propagate_qconfig_\n    default_eval_fn\n```",
    "1747": "一级标题：Quantization API Reference\n二级标题：torch.ao.quantization.quantize_fx\n内容：\nThis module contains FX graph mode quantization APIs (prototype).\n\n```{eval-rst}\n.. currentmodule:: torch.ao.quantization.quantize_fx\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    prepare_fx\n    prepare_qat_fx\n    convert_fx\n    fuse_fx\n```",
    "1748": "一级标题：Quantization API Reference\n二级标题：torch.ao.quantization.qconfig_mapping\n内容：\nThis module contains QConfigMapping for configuring FX graph mode quantization.\n\n```{eval-rst}\n.. currentmodule:: torch.ao.quantization.qconfig_mapping\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    QConfigMapping\n    get_default_qconfig_mapping\n    get_default_qat_qconfig_mapping\n```",
    "1749": "一级标题：Quantization API Reference\n二级标题：torch.ao.quantization.backend_config\n内容：\nThis module contains BackendConfig, a config object that defines how quantization is supported\nin a backend. Currently only used by FX Graph Mode Quantization, but we may extend Eager Mode\nQuantization to work with this as well.\n\n```{eval-rst}\n.. currentmodule:: torch.ao.quantization.backend_config\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    BackendConfig\n    BackendPatternConfig\n    DTypeConfig\n    DTypeWithConstraints\n    ObservationType\n```",
    "1750": "一级标题：Quantization API Reference\n二级标题：torch.ao.quantization.fx.custom_config\n内容：\nThis module contains a few CustomConfig classes that's used in both eager mode and FX graph mode quantization\n\n```{eval-rst}\n.. currentmodule:: torch.ao.quantization.fx.custom_config\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    FuseCustomConfig\n    PrepareCustomConfig\n    ConvertCustomConfig\n    StandaloneModuleConfigEntry\n```",
    "1751": "一级标题：Quantization API Reference\n二级标题：torch.ao.quantization.quantizer\n内容：\n```{eval-rst}\n.. automodule:: torch.ao.quantization.quantizer\n```",
    "1752": "一级标题：Quantization API Reference\n二级标题：torch.ao.quantization.pt2e (quantization in pytorch 2.0 export implementation)\n内容：\n```{eval-rst}\n.. automodule:: torch.ao.quantization.pt2e\n.. automodule:: torch.ao.quantization.pt2e.representation\n```",
    "1753": "一级标题：Quantization API Reference\n二级标题：torch.ao.quantization.pt2e.export_utils\n内容：\n```{eval-rst}\n.. currentmodule:: torch.ao.quantization.pt2e.export_utils\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    model_is_exported\n```\n\n```{eval-rst}\n.. currentmodule:: torch.ao.quantization\n```",
    "1754": "一级标题：Quantization API Reference\n二级标题：torch.ao.quantization.pt2e.lowering\n内容：\n```{eval-rst}\n.. currentmodule:: torch.ao.quantization.pt2e.lowering\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    lower_pt2e_quantized_to_x86\n```\n\n```{eval-rst}\n.. currentmodule:: torch.ao.quantization\n```",
    "1755": "一级标题：Quantization API Reference\n二级标题：PT2 Export (pt2e) Numeric Debugger\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    generate_numeric_debug_handle\n    CUSTOM_KEY\n    NUMERIC_DEBUG_HANDLE_KEY\n    prepare_for_propagation_comparison\n    extract_results_from_loggers\n    compare_results\n```",
    "1756": "一级标题：Quantization API Reference\n二级标题：torch (quantization related functions)\n内容：\nThis describes the quantization related functions of the `torch` namespace.\n\n```{eval-rst}\n.. currentmodule:: torch\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    quantize_per_tensor\n    quantize_per_channel\n    dequantize\n```",
    "1757": "一级标题：Quantization API Reference\n二级标题：torch.Tensor (quantization related methods)\n内容：\nQuantized Tensors support a limited subset of data manipulation methods of the\nregular full-precision tensor.\n\n```{eval-rst}\n.. currentmodule:: torch.Tensor\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    view\n    as_strided\n    expand\n    flatten\n    select\n    ne\n    eq\n    ge\n    le\n    gt\n    lt\n    copy_\n    clone\n    dequantize\n    equal\n    int_repr\n    max\n    mean\n    min\n    q_scale\n    q_zero_point\n    q_per_channel_scales\n    q_per_channel_zero_points\n    q_per_channel_axis\n    resize_\n    sort\n    topk\n```",
    "1758": "一级标题：Quantization API Reference\n二级标题：torch.ao.quantization.observer\n内容：\nThis module contains observers which are used to collect statistics about\nthe values observed during calibration (PTQ) or training (QAT).\n\n```{eval-rst}\n.. currentmodule:: torch.ao.quantization.observer\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    ObserverBase\n    MinMaxObserver\n    MovingAverageMinMaxObserver\n    PerChannelMinMaxObserver\n    MovingAveragePerChannelMinMaxObserver\n    HistogramObserver\n    PlaceholderObserver\n    RecordingObserver\n    NoopObserver\n    get_observer_state_dict\n    load_observer_state_dict\n    default_observer\n    default_placeholder_observer\n    default_debug_observer\n    default_weight_observer\n    default_histogram_observer\n    default_per_channel_weight_observer\n    default_dynamic_quant_observer\n    default_float_qparams_observer\n    AffineQuantizedObserverBase\n    Granularity\n    MappingType\n    PerAxis\n    PerBlock\n    PerGroup\n    PerRow\n    PerTensor\n    PerToken\n    TorchAODType\n    ZeroPointDomain\n    get_block_size\n```",
    "1759": "一级标题：Quantization API Reference\n二级标题：torch.ao.quantization.fake_quantize\n内容：\nThis module implements modules which are used to perform fake quantization\nduring QAT.\n\n```{eval-rst}\n.. currentmodule:: torch.ao.quantization.fake_quantize\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    FakeQuantizeBase\n    FakeQuantize\n    FixedQParamsFakeQuantize\n    FusedMovingAvgObsFakeQuantize\n    default_fake_quant\n    default_weight_fake_quant\n    default_per_channel_weight_fake_quant\n    default_histogram_fake_quant\n    default_fused_act_fake_quant\n    default_fused_wt_fake_quant\n    default_fused_per_channel_wt_fake_quant\n    disable_fake_quant\n    enable_fake_quant\n    disable_observer\n    enable_observer\n```",
    "1760": "一级标题：Quantization API Reference\n二级标题：torch.ao.quantization.qconfig\n内容：\nThis module defines `QConfig` objects which are used\nto configure quantization settings for individual ops.\n\n```{eval-rst}\n.. currentmodule:: torch.ao.quantization.qconfig\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    QConfig\n    default_qconfig\n    default_debug_qconfig\n    default_per_channel_qconfig\n    default_dynamic_qconfig\n    float16_dynamic_qconfig\n    float16_static_qconfig\n    per_channel_dynamic_qconfig\n    float_qparams_weight_only_qconfig\n    default_qat_qconfig\n    default_weight_only_qconfig\n    default_activation_only_qconfig\n    default_qat_qconfig_v2\n```",
    "1761": "一级标题：Quantization API Reference\n二级标题：torch.ao.nn.intrinsic\n内容：\n```{eval-rst}\n.. automodule:: torch.ao.nn.intrinsic\n.. automodule:: torch.ao.nn.intrinsic.modules\n```\n\nThis module implements the combined (fused) modules conv + relu which can\nthen be quantized.\n\n```{eval-rst}\n.. currentmodule:: torch.ao.nn.intrinsic\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    ConvReLU1d\n    ConvReLU2d\n    ConvReLU3d\n    LinearReLU\n    ConvBn1d\n    ConvBn2d\n    ConvBn3d\n    ConvBnReLU1d\n    ConvBnReLU2d\n    ConvBnReLU3d\n    BNReLU2d\n    BNReLU3d\n```",
    "1762": "一级标题：Quantization API Reference\n二级标题：torch.ao.nn.intrinsic.qat\n内容：\n```{eval-rst}\n.. automodule:: torch.ao.nn.intrinsic.qat\n.. automodule:: torch.ao.nn.intrinsic.qat.modules\n```\n\nThis module implements the versions of those fused operations needed for\nquantization aware training.\n\n```{eval-rst}\n.. currentmodule:: torch.ao.nn.intrinsic.qat\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    LinearReLU\n    ConvBn1d\n    ConvBnReLU1d\n    ConvBn2d\n    ConvBnReLU2d\n    ConvReLU2d\n    ConvBn3d\n    ConvBnReLU3d\n    ConvReLU3d\n    update_bn_stats\n    freeze_bn_stats\n```",
    "1763": "一级标题：Quantization API Reference\n二级标题：torch.ao.nn.intrinsic.quantized\n内容：\n```{eval-rst}\n.. automodule:: torch.ao.nn.intrinsic.quantized\n.. automodule:: torch.ao.nn.intrinsic.quantized.modules\n```\n\nThis module implements the quantized implementations of fused operations\nlike conv + relu. No BatchNorm variants as it's usually folded into convolution\nfor inference.\n\n```{eval-rst}\n.. currentmodule:: torch.ao.nn.intrinsic.quantized\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    BNReLU2d\n    BNReLU3d\n    ConvReLU1d\n    ConvReLU2d\n    ConvReLU3d\n    LinearReLU\n```",
    "1764": "一级标题：Quantization API Reference\n二级标题：torch.ao.nn.intrinsic.quantized.dynamic\n内容：\n```{eval-rst}\n.. automodule:: torch.ao.nn.intrinsic.quantized.dynamic\n.. automodule:: torch.ao.nn.intrinsic.quantized.dynamic.modules\n```\n\nThis module implements the quantized dynamic implementations of fused operations\nlike linear + relu.\n\n```{eval-rst}\n.. currentmodule:: torch.ao.nn.intrinsic.quantized.dynamic\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    LinearReLU\n```",
    "1765": "一级标题：Quantization API Reference\n二级标题：torch.ao.nn.qat\n内容：\n```{eval-rst}\n.. automodule:: torch.ao.nn.qat\n.. automodule:: torch.ao.nn.qat.modules\n```\n\nThis module implements versions of the key nn modules **Conv2d()** and\n**Linear()** which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization.\n\n```{eval-rst}\n.. currentmodule:: torch.ao.nn.qat\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    Conv2d\n    Conv3d\n    Linear\n```",
    "1766": "一级标题：Quantization API Reference\n二级标题：torch.ao.nn.qat.dynamic\n内容：\n```{eval-rst}\n.. automodule:: torch.ao.nn.qat.dynamic\n.. automodule:: torch.ao.nn.qat.dynamic.modules\n```\n\nThis module implements versions of the key nn modules such as **Linear()**\nwhich run in FP32 but with rounding applied to simulate the effect of INT8\nquantization and will be dynamically quantized during inference.\n\n```{eval-rst}\n.. currentmodule:: torch.ao.nn.qat.dynamic\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    Linear\n```",
    "1767": "一级标题：Quantization API Reference\n二级标题：torch.ao.nn.quantized\n内容：\n```{eval-rst}\n.. automodule:: torch.ao.nn.quantized\n   :noindex:\n.. automodule:: torch.ao.nn.quantized.modules\n```\n\nThis module implements the quantized versions of the nn layers such as\n`~torch.nn.Conv2d` and `torch.nn.ReLU`.\n\n```{eval-rst}\n.. currentmodule:: torch.ao.nn.quantized\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    ReLU6\n    Hardswish\n    ELU\n    LeakyReLU\n    Sigmoid\n    BatchNorm2d\n    BatchNorm3d\n    Conv1d\n    Conv2d\n    Conv3d\n    ConvTranspose1d\n    ConvTranspose2d\n    ConvTranspose3d\n    Embedding\n    EmbeddingBag\n    FloatFunctional\n    FXFloatFunctional\n    QFunctional\n    Linear\n    LayerNorm\n    GroupNorm\n    InstanceNorm1d\n    InstanceNorm2d\n    InstanceNorm3d\n```",
    "1768": "一级标题：Quantization API Reference\n二级标题：torch.ao.nn.quantized.functional\n内容：\n```{eval-rst}\n.. automodule:: torch.ao.nn.quantized.functional\n```\n\n```{eval-rst}\nThis module implements the quantized versions of the functional layers such as\n`~torch.nn.functional.conv2d` and `torch.nn.functional.relu`. Note:\n:math:`~torch.nn.functional.relu` supports quantized inputs.\n```\n\n```{eval-rst}\n.. currentmodule:: torch.ao.nn.quantized.functional\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    avg_pool2d\n    avg_pool3d\n    adaptive_avg_pool2d\n    adaptive_avg_pool3d\n    conv1d\n    conv2d\n    conv3d\n    interpolate\n    linear\n    max_pool1d\n    max_pool2d\n    celu\n    leaky_relu\n    hardtanh\n    hardswish\n    threshold\n    elu\n    hardsigmoid\n    clamp\n    upsample\n    upsample_bilinear\n    upsample_nearest\n```",
    "1769": "一级标题：Quantization API Reference\n二级标题：torch.ao.nn.quantizable\n内容：\nThis module implements the quantizable versions of some of the nn layers.\nThese modules can be used in conjunction with the custom module mechanism,\nby providing the ``custom_module_config`` argument to both prepare and convert.\n\n```{eval-rst}\n.. currentmodule:: torch.ao.nn.quantizable\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    LSTM\n    MultiheadAttention\n```",
    "1770": "一级标题：Quantization API Reference\n二级标题：torch.ao.nn.quantized.dynamic\n内容：\n```{eval-rst}\n.. automodule:: torch.ao.nn.quantized.dynamic\n.. automodule:: torch.ao.nn.quantized.dynamic.modules\n```\n\nDynamically quantized {class}`~torch.nn.Linear`, {class}`~torch.nn.LSTM`,\n{class}`~torch.nn.LSTMCell`, {class}`~torch.nn.GRUCell`, and\n{class}`~torch.nn.RNNCell`.\n\n```{eval-rst}\n.. currentmodule:: torch.ao.nn.quantized.dynamic\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    Linear\n    LSTM\n    GRU\n    RNNCell\n    LSTMCell\n    GRUCell\n```",
    "1771": "一级标题：Quantization API Reference\n二级标题：Quantized dtypes and quantization schemes\n内容：\nNote that operator implementations currently only\nsupport per channel quantization for weights of the **conv** and **linear**\noperators. Furthermore, the input data is\nmapped linearly to the quantized data and vice versa\nas follows:\n\n```{eval-rst}\n    .. math::\n\n        \\begin{aligned}\n            \\text{Quantization:}&\\\\\n            &Q_\\text{out} = \\text{clamp}(x_\\text{input}/s+z, Q_\\text{min}, Q_\\text{max})\\\\\n            \\text{Dequantization:}&\\\\\n            &x_\\text{out} = (Q_\\text{input}-z)*s\n        \\end{aligned}\n```\n\n```{eval-rst}\nwhere :math:`\\text{clamp}(.)` is the same as :func:`~torch.clamp` while the\nscale :math:`s` and zero point :math:`z` are then computed\nas described in :class:`~torch.ao.quantization.observer.MinMaxObserver`, specifically:\n```\n\n```{eval-rst}\n    .. math::\n\n        \\begin{aligned}\n            \\text{if Symmetric:}&\\\\\n            &s = 2 \\max(|x_\\text{min}|, x_\\text{max}) /\n                \\left( Q_\\text{max} - Q_\\text{min} \\right) \\\\\n            &z = \\begin{cases}\n                0 & \\text{if dtype is qint8} \\\\\n                128 & \\text{otherwise}\n            \\end{cases}\\\\\n            \\text{Otherwise:}&\\\\\n                &s = \\left( x_\\text{max} - x_\\text{min}  \\right ) /\n                    \\left( Q_\\text{max} - Q_\\text{min} \\right ) \\\\\n                &z = Q_\\text{min} - \\text{round}(x_\\text{min} / s)\n        \\end{aligned}\n```\n\nwhere :math:`[x_\\text{min}, x_\\text{max}]` denotes the range of the input data while\n:math:`Q_\\text{min}` and :math:`Q_\\text{max}` are respectively the minimum and maximum values of the quantized dtype.\n\nNote that the choice of :math:`s` and :math:`z` implies that zero is represented with no quantization error whenever zero is within\nthe range of the input data or symmetric quantization is being used.\n\nAdditional data types and quantization schemes can be implemented through\nthe `custom operator mechanism <https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html>`_.\n\n```{eval-rst}\n* :attr:`torch.qscheme` — Type to describe the quantization scheme of a tensor.\n  Supported types:\n\n  * :attr:`torch.per_tensor_affine` — per tensor, asymmetric\n  * :attr:`torch.per_channel_affine` — per channel, asymmetric\n  * :attr:`torch.per_tensor_symmetric` — per tensor, symmetric\n  * :attr:`torch.per_channel_symmetric` — per channel, symmetric\n\n* ``torch.dtype`` — Type to describe the data. Supported types:\n\n  * :attr:`torch.quint8` — 8-bit unsigned integer\n  * :attr:`torch.qint8` — 8-bit signed integer\n  * :attr:`torch.qint32` — 32-bit signed integer\n```\n\n```{eval-rst}\n.. These modules are missing docs. Adding them here only for tracking\n.. automodule:: torch.ao.nn.quantizable.modules\n   :noindex:\n.. automodule:: torch.ao.nn.quantized.reference\n   :noindex:\n.. automodule:: torch.ao.nn.quantized.reference.modules\n   :noindex:\n\n.. automodule:: torch.nn.quantizable\n.. automodule:: torch.nn.qat.dynamic.modules\n.. automodule:: torch.nn.qat.modules\n.. automodule:: torch.nn.qat\n.. automodule:: torch.nn.intrinsic.qat.modules\n.. automodule:: torch.nn.quantized.dynamic\n.. automodule:: torch.nn.intrinsic\n.. automodule:: torch.nn.intrinsic.quantized.modules\n.. automodule:: torch.quantization.fx\n.. automodule:: torch.nn.intrinsic.quantized.dynamic\n.. automodule:: torch.nn.qat.dynamic\n.. automodule:: torch.nn.intrinsic.qat\n.. automodule:: torch.nn.quantized.modules\n.. automodule:: torch.nn.intrinsic.quantized\n.. automodule:: torch.nn.quantizable.modules\n.. automodule:: torch.nn.quantized\n.. automodule:: torch.nn.intrinsic.quantized.dynamic.modules\n.. automodule:: torch.nn.quantized.dynamic.modules\n.. automodule:: torch.quantization\n.. automodule:: torch.nn.intrinsic.modules\n```",
    "1772": "一级标题：torch.random\n二级标题：无\n内容：\n```{eval-rst}\n.. currentmodule:: torch.random\n```\n\n```{eval-rst}\n.. automodule:: torch.random\n   :members:\n```",
    "1773": "一级标题：Distributed RPC Framework\n二级标题：无\n内容：\nThe distributed RPC framework provides mechanisms for multi-machine model\ntraining through a set of primitives to allow for remote communication, and a\nhigher-level API to automatically differentiate models split across several\nmachines.\n\n```{warning}\nAPIs in the RPC package are stable and in maintenance mode.\n```\n\n```{warning}\nCUDA support is a **beta** feature.\nNot all features of the RPC package are yet compatible with CUDA support and\nthus their use is discouraged. These unsupported features include: RRefs,\nJIT compatibility, dist autograd and dist optimizer, and profiling.\n```\n\n```{note}\nPlease refer to `PyTorch Distributed Overview <https://pytorch.org/tutorials/beginner/dist_overview.html>`__\nfor a brief introduction to all features related to distributed training.\n```",
    "1774": "一级标题：Distributed RPC Framework\n二级标题：Basics\n内容：\nThe distributed RPC framework makes it easy to run functions remotely, supports\nreferencing remote objects without copying the real data around, and provides\nautograd and optimizer APIs to transparently run backward and update parameters\nacross RPC boundaries. These features can be categorized into four sets of APIs.\n\n1) **Remote Procedure Call (RPC)** supports running a function on the specified\n   destination worker with the given arguments and getting the return value back\n   or creating a reference to the return value. There are three main RPC APIs:\n   {meth}`~torch.distributed.rpc.rpc_sync` (synchronous),\n   {meth}`~torch.distributed.rpc.rpc_async` (asynchronous), and\n   {meth}`~torch.distributed.rpc.remote` (asynchronous and returns a reference\n   to the remote return value). Use the synchronous API if the user code cannot\n   proceed without the return value. Otherwise, use the asynchronous API to get\n   a future, and wait on the future when the return value is needed on the\n   caller. The {meth}`~torch.distributed.rpc.remote` API is useful when the\n   requirement is to create something remotely but never need to fetch it to\n   the caller. Imagine the case that a driver process is setting up a parameter\n   server and a trainer. The driver can create an embedding table on the\n   parameter server and then share the reference to the embedding table with the\n   trainer, but itself will never use the embedding table locally. In this case,\n   {meth}`~torch.distributed.rpc.rpc_sync` and\n   {meth}`~torch.distributed.rpc.rpc_async` are no longer appropriate, as they\n   always imply that the return value will be returned to the caller\n   immediately or in the future.\n2) **Remote Reference (RRef)** serves as a distributed shared pointer to a local\n   or remote object. It can be shared with other workers and reference counting\n   will be handled transparently. Each RRef only has one owner and the object\n   only lives on that owner. Non-owner workers holding RRefs can get copies of\n   the object from the owner by explicitly requesting it. This is useful when\n   a worker needs to access some data object, but itself is neither the creator\n   (the caller of {meth}`~torch.distributed.rpc.remote`) or the owner of the\n   object. The distributed optimizer, as we will discuss below, is one example\n   of such use cases.\n3) **Distributed Autograd** stitches together local autograd engines on all the\n   workers involved in the forward pass, and automatically reach out to them\n   during the backward pass to compute gradients. This is especially helpful if\n   the forward pass needs to span multiple machines when conducting, e.g.,\n   distributed model parallel training, parameter-server training, etc. With\n   this feature, user code no longer needs to worry about how to send gradients\n   across RPC boundaries and in which order should the local autograd engines\n   be launched, which can become quite complicated where there are nested and\n   inter-dependent RPC calls in the forward pass.\n4) **Distributed Optimizer**'s constructor takes a\n   {meth}`~torch.optim.Optimizer` (e.g., {meth}`~torch.optim.SGD`,\n   {meth}`~torch.optim.Adagrad`, etc.) and a list of parameter RRefs, creates an\n   {meth}`~torch.optim.Optimizer` instance on each distinct RRef owner, and\n   updates parameters accordingly when running ``step()``. When you have\n   distributed forward and backward passes, parameters and gradients will be\n   scattered across multiple workers, and hence it requires an optimizer on each\n   of the involved workers. Distributed Optimizer wraps all those local\n   optimizers into one, and provides a concise constructor and ``step()`` API.\n\n\n(rpc)=",
    "1775": "一级标题：Distributed RPC Framework\n二级标题：RPC\n内容：\nBefore using RPC and distributed autograd primitives, initialization must take\nplace. To initialize the RPC framework we need to use\n{meth}`~torch.distributed.rpc.init_rpc` which would initialize the RPC\nframework, RRef framework and distributed autograd.\n\n```{eval-rst}\n.. automodule:: torch.distributed.rpc\n.. autofunction:: init_rpc\n```\n\nThe following APIs allow users to remotely execute functions as well as create\nreferences (RRefs) to remote data objects. In these APIs, when passing a\n``Tensor`` as an argument or a return value, the destination worker will try to\ncreate a ``Tensor`` with the same meta (i.e., shape, stride, etc.). We\nintentionally disallow transmitting CUDA tensors because it might crash if the\ndevice lists on source and destination workers do not match. In such cases,\napplications can always explicitly move the input tensors to CPU on the caller\nand move it to the desired devices on the callee if necessary.\n\n```{eval-rst}\n.. autofunction:: rpc_sync\n.. autofunction:: rpc_async\n.. autofunction:: remote\n.. autofunction:: get_worker_info\n.. autofunction:: shutdown\n.. autoclass:: WorkerInfo\n    :members:\n```\n\nThe RPC package also provides decorators which allow applications to specify\nhow a given function should be treated on the callee side.\n\n```{eval-rst}\n.. autofunction:: torch.distributed.rpc.functions.async_execution\n```\n\n(rpc-backends)=\n### Backends\n\nThe RPC module can leverage different backends to perform the communication\nbetween the nodes. The backend to be used can be specified in the\n{func}`~torch.distributed.rpc.init_rpc` function, by passing a certain value of\nthe {class}`~torch.distributed.rpc.BackendType` enum. Regardless of what backend\nis used, the rest of the RPC API won't change. Each backend also defines its own\nsubclass of the {class}`~torch.distributed.rpc.RpcBackendOptions` class, an\ninstance of which can also be passed to {func}`~torch.distributed.rpc.init_rpc`\nto configure the backend's behavior.\n\n```{eval-rst}\n.. autoclass:: BackendType\n\n.. autoclass:: RpcBackendOptions\n    :members:\n```\n\n#### TensorPipe Backend\n\nThe TensorPipe agent, which is the default, leverages [the TensorPipe library](https://github.com/pytorch/tensorpipe), which provides a natively\npoint-to-point communication primitive specifically suited for machine learning\nthat fundamentally addresses some of the limitations of Gloo. Compared to Gloo,\nit has the advantage of being asynchronous, which allows a large number of\ntransfers to occur simultaneously, each at their own speed, without blocking\neach other. It will only open pipes between pairs of nodes when needed, on\ndemand, and when one node fails only its incident pipes will be closed, while\nall other ones will keep working as normal. In addition, it is able to support\nmultiple different transports (TCP, of course, but also shared memory, NVLink,\nInfiniBand, ...) and can automatically detect their availability and negotiate\nthe best transport to use for each pipe.\n\nThe TensorPipe backend comes with a TCP-based transport, just like Gloo. It is also able to\nautomatically chunk and multiplex large tensors over multiple sockets and\nthreads in order to achieve very high bandwidths. The agent will be able to pick\nthe best transport on its own, with no intervention required.\n\nExample:\n\n```{code-block} python\nimport os\nfrom torch.distributed import rpc\nos.environ['MASTER_ADDR'] = 'localhost'\nos.environ['MASTER_PORT'] = '29500'\n\nrpc.init_rpc(\n    \"worker1\",\n    rank=0,\n    world_size=2,\n    rpc_backend_options=rpc.TensorPipeRpcBackendOptions(\n        num_worker_threads=8,\n        rpc_timeout=20 # 20 second timeout\n    )\n)\n\n# omitting init_rpc invocation on worker2\n```\n\n```{eval-rst}\n.. autoclass:: TensorPipeRpcBackendOptions\n    :members:\n    :inherited-members:\n```\n\n```{note}\nThe RPC framework does not automatically retry any\n{meth}`~torch.distributed.rpc.rpc_sync`,\n{meth}`~torch.distributed.rpc.rpc_async` and\n{meth}`~torch.distributed.rpc.remote` calls. The reason being that there is\nno way the RPC framework can determine whether an operation is idempotent or\nnot and whether it is safe to retry. As a result, it is the application's\nresponsibility to deal with failures and retry if necessary. RPC communication\nis based on TCP and as a result failures could happen due to network failures\nor intermittent network connectivity issues. In such scenarios, the application\nneeds to retry appropriately with reasonable backoffs to ensure the network\nisn't overwhelmed by aggressive retries.\n```\n(rref)=",
    "1776": "一级标题：Distributed RPC Framework\n二级标题：RRef\n内容：\n```{warning}\nRRefs are not currently supported when using CUDA tensors\n```\n\nAn ``RRef`` (Remote REFerence) is a reference to a value of some type ``T``\n(e.g. ``Tensor``) on a remote worker. This handle keeps the referenced remote\nvalue alive on the owner, but there is no implication that the value will be\ntransferred to the local worker in the future. RRefs can be used in\nmulti-machine training by holding references to [nn.Modules](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) that exist on\nother workers, and calling the appropriate functions to retrieve or modify their\nparameters during training. See {ref}`remote-reference-protocol` for more\ndetails.\n\n```{eval-rst}\n.. autoclass:: PyRRef(RRef)\n    :members:\n    :inherited-members:\n```\n\n```{toctree}\n:caption: More Information about RRef\n\nrpc/rref\n```\n\n(remote_module)=",
    "1777": "一级标题：Distributed RPC Framework\n二级标题：RemoteModule\n内容：\n```{warning}\nRemoteModule is not currently supported when using CUDA tensors\n```\n\n``RemoteModule`` is an easy way to create an nn.Module remotely on a different\nprocess. The actual module resides on a remote host, but the local host has a\nhandle to this module and invoke this module similar to a regular nn.Module.\nThe invocation however incurs RPC calls to the remote end and can be performed\nasynchronously if needed via additional APIs supported by RemoteModule.\n\n```{eval-rst}\n.. autoclass:: torch.distributed.nn.api.remote_module.RemoteModule\n    :members: remote_parameters, get_module_rref\n```",
    "1778": "一级标题：Distributed RPC Framework\n二级标题：Distributed Autograd Framework\n内容：\n```{warning}\nDistributed autograd is not currently supported when using CUDA tensors\n```\n\nThis module provides an RPC-based distributed autograd framework that can be\nused for applications such as model parallel training. In short, applications\nmay send and receive gradient recording tensors over RPC. In the forward pass,\nwe record when gradient recording tensors are sent over RPC and during the\nbackward pass we use this information to perform a distributed backward pass\nusing RPC. For more details see {ref}`distributed-autograd-design`.\n\n```{eval-rst}\n.. automodule:: torch.distributed.autograd\n    :members: context, backward, get_gradients\n```\n\n```{toctree}\n:caption: More Information about RPC Autograd\n\nrpc/distributed_autograd\n```",
    "1779": "一级标题：Distributed RPC Framework\n二级标题：Distributed Optimizer\n内容：\nSee the [torch.distributed.optim](https://pytorch.org/docs/main/distributed.optim.html) page for documentation on distributed optimizers.",
    "1780": "一级标题：Distributed RPC Framework\n二级标题：Design Notes\n内容：\nThe distributed autograd design note covers the design of the RPC-based distributed autograd framework that is useful for applications such as model parallel training.\n\n-  {ref}`distributed-autograd-design`\n\nThe RRef design note covers the design of the {ref}`rref` (Remote REFerence) protocol used to refer to values on remote workers by the framework.\n\n-  {ref}`remote-reference-protocol`",
    "1781": "一级标题：Distributed RPC Framework\n二级标题：Tutorials\n内容：\nThe RPC tutorials introduce users to the RPC framework, provide several example applications\nusing {ref}`torch.distributed.rpc<distributed-rpc-framework>` APIs, and demonstrate how\nto use [the profiler](https://pytorch.org/docs/stable/autograd.html#profiler) to profile RPC-based workloads.\n\n-  [Getting started with Distributed RPC Framework](https://pytorch.org/tutorials/intermediate/rpc_tutorial.html)\n-  [Implementing a Parameter Server using Distributed RPC Framework](https://pytorch.org/tutorials/intermediate/rpc_param_server_tutorial.html)\n-  [Combining Distributed DataParallel with Distributed RPC Framework](https://pytorch.org/tutorials/advanced/rpc_ddp_tutorial.html) (covers **RemoteModule** as well)\n-  [Implementing batch RPC processing](https://pytorch.org/tutorials/intermediate/rpc_async_execution.html)",
    "1782": "一级标题：torch.signal\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.signal\n.. currentmodule:: torch.signal\n```\n\nThe `torch.signal` module, modeled after SciPy's [signal](https://docs.scipy.org/doc/scipy/reference/signal.html)module.",
    "1783": "一级标题：torch.signal\n二级标题：torch.signal.windows\n内容：\n```{eval-rst}\n.. automodule:: torch.signal.windows\n.. currentmodule:: torch.signal.windows\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    bartlett\n    blackman\n    cosine\n    exponential\n    gaussian\n    general_cosine\n    general_hamming\n    hamming\n    hann\n    kaiser\n    nuttall\n```",
    "1784": "一级标题：torch.Size\n二级标题：无\n内容：\n{class}`torch.Size` is the result type of a call to {func}`torch.Tensor.size`. It describes the size of all dimensions\nof the original tensor. As a subclass of {class}`tuple`, it supports common sequence operations like indexing and\nlength.\n\n\nExample:\n\n```{code-block} python\n    >>> x = torch.ones(10, 20, 30)\n    >>> s = x.size()\n    >>> s\n    torch.Size([10, 20, 30])\n    >>> s[1]\n    20\n    >>> len(s)\n    3\n```\n\n```{eval-rst}\n.. autoclass:: torch.Size\n   :members:\n   :undoc-members:\n   :inherited-members:\n```",
    "1785": "一级标题：torch.special\n二级标题：无\n内容：\nThe torch.special module, modeled after SciPy's [special](https://docs.scipy.org/doc/scipy/reference/special.html) module.\n\n```{eval-rst}\n.. automodule:: torch.special\n.. currentmodule:: torch.special\n```",
    "1786": "一级标题：torch.special\n二级标题：Functions\n内容：\n```{eval-rst}\n.. autofunction:: airy_ai\n.. autofunction:: bessel_j0\n.. autofunction:: bessel_j1\n.. autofunction:: bessel_y0\n.. autofunction:: bessel_y1\n.. autofunction:: chebyshev_polynomial_t\n.. autofunction:: chebyshev_polynomial_u\n.. autofunction:: chebyshev_polynomial_v\n.. autofunction:: chebyshev_polynomial_w\n.. autofunction:: digamma\n.. autofunction:: entr\n.. autofunction:: erf\n.. autofunction:: erfc\n.. autofunction:: erfcx\n.. autofunction:: erfinv\n.. autofunction:: exp2\n.. autofunction:: expit\n.. autofunction:: expm1\n.. autofunction:: gammainc\n.. autofunction:: gammaincc\n.. autofunction:: gammaln\n.. autofunction:: hermite_polynomial_h\n.. autofunction:: hermite_polynomial_he\n.. autofunction:: i0\n.. autofunction:: i0e\n.. autofunction:: i1\n.. autofunction:: i1e\n.. autofunction:: laguerre_polynomial_l\n.. autofunction:: legendre_polynomial_p\n.. autofunction:: log1p\n.. autofunction:: log_ndtr\n.. autofunction:: log_softmax\n.. autofunction:: logit\n.. autofunction:: logsumexp\n.. autofunction:: modified_bessel_i0\n.. autofunction:: modified_bessel_i1\n.. autofunction:: modified_bessel_k0\n.. autofunction:: modified_bessel_k1\n.. autofunction:: multigammaln\n.. autofunction:: ndtr\n.. autofunction:: ndtri\n.. autofunction:: polygamma\n.. autofunction:: psi\n.. autofunction:: round\n.. autofunction:: scaled_modified_bessel_k0\n.. autofunction:: scaled_modified_bessel_k1\n.. autofunction:: shifted_chebyshev_polynomial_t\n.. autofunction:: shifted_chebyshev_polynomial_u\n.. autofunction:: shifted_chebyshev_polynomial_v\n.. autofunction:: shifted_chebyshev_polynomial_w\n.. autofunction:: sinc\n.. autofunction:: softmax\n.. autofunction:: spherical_bessel_j0\n.. autofunction:: xlog1py\n.. autofunction:: xlogy\n.. autofunction:: zeta\n```",
    "1787": "一级标题：torch.testing\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.testing\n```\n\n```{eval-rst}\n.. currentmodule:: torch.testing\n```\n\n```{eval-rst}\n.. autofunction:: assert_close\n```\n\n```{eval-rst}\n.. autofunction:: make_tensor\n```\n\n```{eval-rst}\n.. autofunction:: assert_allclose\n```",
    "1788": "一级标题：Threading Environment Variables\n二级标题：无\n内容：\n```{list-table}\n:header-rows: 1\n\n* - Variable\n  - Description\n* - ``OMP_NUM_THREADS``\n  - Sets the maximum number of threads to use for OpenMP parallel regions.\n* - ``MKL_NUM_THREADS``\n  - Sets the maximum number of threads to use for the Intel MKL library. Note that MKL_NUM_THREADS takes precedence over ``OMP_NUM_THREADS``.\n```",
    "1789": "一级标题：Aliases in torch\n二级标题：无\n内容：\nThe following are aliases in ``torch`` to their counterparts in the nested namespaces\nin which they are defined. Feel free to use either the top-level version in ``torch``\n(e.g. ``torch.broadcast_tensors()``) or the nested version ``torch.functional.broadcast_tensors()``.\n\n```{eval-rst}\n.. automodule:: torch.functional\n.. currentmodule:: torch.functional\n.. autosummary::\n   :toctree: generated\n   :nosignatures:\n\n    align_tensors\n    atleast_1d\n    atleast_2d\n    atleast_3d\n    block_diag\n    broadcast_shapes\n    broadcast_tensors\n    cartesian_prod\n    cdist\n    chain_matmul\n    einsum\n    lu\n    meshgrid\n    norm\n    split\n    stft\n    tensordot\n    unique\n    unique_consecutive\n    unravel_index\n```",
    "1790": "一级标题：torch.compiler.config\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.compiler.config\n```\n\n```{eval-rst}\n.. autodata:: torch.compiler.config.job_id\n```",
    "1791": "一级标题：torch.compiler\n二级标题：无\n内容：\n`torch.compiler` is a namespace through which some of the internal compiler\nmethods are surfaced for user consumption. The main function and the feature in\nthis namespace is `torch.compile`.\n\n`torch.compile` is a PyTorch function introduced in PyTorch 2.x that aims to\nsolve the problem of accurate graph capturing in PyTorch and ultimately enable\nsoftware engineers to run their PyTorch programs faster. `torch.compile` is\nwritten in Python and it marks the transition of PyTorch from C++ to Python.\n\n`torch.compile` leverages the following underlying technologies:\n\n- **TorchDynamo (torch._dynamo)** is an internal API that uses a CPython\n  feature called the Frame Evaluation API to safely capture PyTorch graphs.\n  Methods that are available externally for PyTorch users are surfaced\n  through the `torch.compiler` namespace.\n- **TorchInductor** is the default `torch.compile` deep learning compiler\n  that generates fast code for multiple accelerators and backends. You\n  need to use a backend compiler to make speedups through `torch.compile`\n  possible. For NVIDIA, AMD and Intel GPUs, it leverages OpenAI Triton as the key\n  building block.\n- **AOT Autograd** captures not only the user-level code, but also backpropagation,\n  which results in capturing the backwards pass \"ahead-of-time\". This enables\n  acceleration of both forwards and backwards pass using TorchInductor.\n\nTo better understand how `torch.compile` tracing behavior on your code, or to\nlearn more about the internals of `torch.compile`, please refer to the [`torch.compile` programming model](compile/programming_model.md).\n\n:::{note}\nIn some cases, the terms `torch.compile`, TorchDynamo, `torch.compiler`\nmight be used interchangeably in this documentation.\n:::\n\nAs mentioned above, to run your workflows faster, `torch.compile` through\nTorchDynamo requires a backend that converts the captured graphs into a fast\nmachine code. Different backends can result in various optimization gains.\nThe default backend is called TorchInductor, also known as *inductor*,\nTorchDynamo has a list of supported backends developed by our partners,\nwhich can be seen by running `torch.compiler.list_backends()` each of which\nwith its optional dependencies.\n\nSome of the most commonly used backends include:\n\n**Training & inference backends**\n\n```{eval-rst}\n.. list-table::\n   :widths: 50 50\n   :header-rows: 1\n\n   * - Backend\n     - Description\n   * - ``torch.compile(m, backend=\"inductor\")``\n     - Uses the TorchInductor backend. `Read more <https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747>`__\n   * - ``torch.compile(m, backend=\"cudagraphs\")``\n     - CUDA graphs with AOT Autograd. `Read more <https://github.com/pytorch/torchdynamo/pull/757>`__\n   * - ``torch.compile(m, backend=\"ipex\")``\n     - Uses IPEX on CPU. `Read more <https://github.com/intel/intel-extension-for-pytorch>`__\n```\n\n**Inference-only backends**\n\n```{eval-rst}\n.. list-table::\n   :widths: 50 50\n   :header-rows: 1\n\n   * - Backend\n     - Description\n   * - ``torch.compile(m, backend=\"tensorrt\")``\n     - Uses Torch-TensorRT for inference optimizations. Requires ``import torch_tensorrt`` in the calling script to register backend. `Read more <https://github.com/pytorch/TensorRT>`__\n   * - ``torch.compile(m, backend=\"ipex\")``\n     - Uses IPEX for inference on CPU. `Read more <https://github.com/intel/intel-extension-for-pytorch>`__\n   * - ``torch.compile(m, backend=\"tvm\")``\n     - Uses Apache TVM for inference optimizations. `Read more <https://tvm.apache.org/>`__\n   * - ``torch.compile(m, backend=\"openvino\")``\n     - Uses OpenVINO for inference optimizations. `Read more <https://docs.openvino.ai/torchcompile>`__\n```",
    "1792": "一级标题：torch.compiler\n二级标题：Read More\n内容：\n```{eval-rst}\n.. toctree::\n   :caption: Getting Started for PyTorch Users\n   :maxdepth: 1\n\n   torch.compiler_get_started\n   torch.compiler_api\n   torch.compiler.config\n   torch.compiler_fine_grain_apis\n   torch.compiler_backward\n   torch.compiler_aot_inductor\n   torch.compiler_inductor_profiling\n   torch.compiler_profiling_torch_compile\n   torch.compiler_faq\n   torch.compiler_troubleshooting\n   torch.compiler_performance_dashboard\n   torch.compiler_inductor_provenance\n```\n\n```{eval-rst}\n.. toctree::\n   :caption: `torch.compile` Programming Model\n\n   compile/programming_model\n```\n\n% _If you want to contribute a developer-level topic\n%  that provides in-depth overview of a torch._dynamo feature,\n%  add in the below toc.\n\n```{eval-rst}\n.. toctree::\n   :caption: Deep Dive for PyTorch Developers\n   :maxdepth: 1\n\n   torch.compiler_dynamo_overview\n   torch.compiler_dynamo_deepdive\n   torch.compiler_dynamic_shapes\n   torch.compiler_nn_module\n   torch.compiler_cudagraph_trees\n   torch.compiler_fake_tensor\n```\n\n```{eval-rst}\n.. toctree::\n   :caption: HowTo for PyTorch Backend Vendors\n   :maxdepth: 1\n\n   torch.compiler_custom_backends\n   torch.compiler_transformations\n   torch.compiler_ir\n```",
    "1793": "一级标题：AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models\n二级标题：无\n内容：\n```{warning}\nAOTInductor and its related features are in prototype status and are\nsubject to backwards compatibility breaking changes.\n```\n\nAOTInductor is a specialized version of\n[TorchInductor](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747),\ndesigned to process exported PyTorch models, optimize them, and produce shared libraries as well\nas other relevant artifacts.\nThese compiled artifacts are specifically crafted for deployment in non-Python environments,\nwhich are frequently employed for inference deployments on the server side.\n\nIn this tutorial, you will gain insight into the process of taking a PyTorch model, exporting it,\ncompiling it into an artifact, and conducting model predictions using C++.",
    "1794": "一级标题：AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models\n二级标题：Model Compilation\n内容：\nTo compile a model using AOTInductor, we first need to use\n{func}`torch.export.export` to capture a given PyTorch model into a\ncomputational graph. {ref}`torch.export <torch.export>` provides soundness\nguarantees and a strict specification on the IR captured, which AOTInductor\nrelies on.\n\nWe will then use {func}`torch._inductor.aoti_compile_and_package` to compile the\nexported program using TorchInductor, and save the compiled artifacts into one\npackage. The package is in the format of a {ref}`PT2 Archive Spec <export.pt2_archive>`.\n\n```{note}\nIf you have a CUDA-enabled device on your machine and you installed PyTorch with CUDA support,\nthe following code will compile the model into a shared library for CUDA execution.\nOtherwise, the compiled artifact will run on CPU. For better performance during CPU inference,\nit is suggested to enable freezing by setting `export TORCHINDUCTOR_FREEZING=1`\nbefore running the Python script below. The same behavior works in an environment with Intel®\nGPU as well.\n```\n\n```python\nimport os\nimport torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(10, 16)\n        self.relu = torch.nn.ReLU()\n        self.fc2 = torch.nn.Linear(16, 1)\n        self.sigmoid = torch.nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return x\n\nwith torch.no_grad():\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model = Model().to(device=device)\n    example_inputs=(torch.randn(8, 10, device=device),)\n    batch_dim = torch.export.Dim(\"batch\", min=1, max=1024)\n    # [Optional] Specify the first dimension of the input x as dynamic.\n    exported = torch.export.export(model, example_inputs, dynamic_shapes={\"x\": {0: batch_dim}})\n    # [Note] In this example we directly feed the exported module to aoti_compile_and_package.\n    # Depending on your use case, e.g. if your training platform and inference platform\n    # are different, you may choose to save the exported model using torch.export.save and\n    # then load it back using torch.export.load on your inference platform to run AOT compilation.\n    output_path = torch._inductor.aoti_compile_and_package(\n        exported,\n        # [Optional] Specify the generated shared library path. If not specified,\n        # the generated artifact is stored in your system temp directory.\n        package_path=os.path.join(os.getcwd(), \"model.pt2\"),\n    )\n```\n\nIn this illustrative example, the `Dim` parameter is employed to designate the first dimension of\nthe input variable \"x\" as dynamic. Notably, the path and name of the compiled library remain unspecified,\nresulting in the shared library being stored in a temporary directory.\nTo access this path from the C++ side, we save it to a file for later retrieval within the C++ code.",
    "1795": "一级标题：AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models\n二级标题：Inference in Python\n内容：\nThere are multiple ways to deploy the compiled artifact for inference, and one of that is using Python.\nWe have provided a convenient utility API in Python {func}`torch._inductor.aoti_load_package` for loading\nand running the artifact, as shown in the following example:\n\n```python\nimport os\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = torch._inductor.aoti_load_package(os.path.join(os.getcwd(), \"model.pt2\"))\nprint(model(torch.randn(8, 10, device=device)))\n```\n\nThe input at inference time should have the same size, dtype, and stride as the input at export time.",
    "1796": "一级标题：AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models\n二级标题：Inference in C++\n内容：\nNext, we use the following example C++ file `inference.cpp` to load the compiled artifact,\nenabling us to conduct model predictions directly within a C++ environment.\n\n```cpp\n#include <iostream>\n#include <vector>\n\n#include <torch/torch.h>\n#include <torch/csrc/inductor/aoti_package/model_package_loader.h>\n\nint main() {\n    c10::InferenceMode mode;\n\n    torch::inductor::AOTIModelPackageLoader loader(\"model.pt2\");\n    // Assume running on CUDA\n    std::vector<torch::Tensor> inputs = {torch::randn({8, 10}, at::kCUDA)};\n    std::vector<torch::Tensor> outputs = loader.run(inputs);\n    std::cout << \"Result from the first inference:\"<< std::endl;\n    std::cout << outputs[0] << std::endl;\n\n    // The second inference uses a different batch size and it works because we\n    // specified that dimension as dynamic when compiling model.pt2.\n    std::cout << \"Result from the second inference:\"<< std::endl;\n    // Assume running on CUDA\n    std::cout << loader.run({torch::randn({1, 10}, at::kCUDA)})[0] << std::endl;\n\n    return 0;\n}\n```\n\nFor building the C++ file, you can make use of the provided `CMakeLists.txt` file, which\nautomates the process of invoking `python model.py` for AOT compilation of the model and compiling\n`inference.cpp` into an executable binary named `aoti_example`.\n\n```cmake\ncmake_minimum_required(VERSION 3.18 FATAL_ERROR)\nproject(aoti_example)\n\nfind_package(Torch REQUIRED)\n\nadd_executable(aoti_example inference.cpp model.pt2)\n\nadd_custom_command(\n    OUTPUT model.pt2\n    COMMAND python ${CMAKE_CURRENT_SOURCE_DIR}/model.py\n    DEPENDS model.py\n)\n\ntarget_link_libraries(aoti_example \"${TORCH_LIBRARIES}\")\nset_property(TARGET aoti_example PROPERTY CXX_STANDARD 17)\n```\n\nProvided the directory structure resembles the following, you can execute the subsequent commands\nto construct the binary. It is essential to note that the `CMAKE_PREFIX_PATH` variable\nis crucial for CMake to locate the LibTorch library, and it should be set to an absolute path.\nPlease be mindful that your path may vary from the one illustrated in this example.\n\n```\naoti_example/\n    CMakeLists.txt\n    inference.cpp\n    model.py\n```\n\n```bash\n$ mkdir build\n$ cd build\n$ CMAKE_PREFIX_PATH=/path/to/python/install/site-packages/torch/share/cmake cmake ..\n$ cmake --build . --config Release\n```\n\nAfter the `aoti_example` binary has been generated in the `build` directory, executing it will\ndisplay results akin to the following:\n\n```bash\n$ ./aoti_example\nResult from the first inference:\n0.4866\n0.5184\n0.4462\n0.4611\n0.4744\n0.4811\n0.4938\n0.4193\n[ CUDAFloatType{8,1} ]\nResult from the second inference:\n0.4883\n0.4703\n[ CUDAFloatType{2,1} ]\n```",
    "1797": "一级标题：AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models\n二级标题：Troubleshooting\n内容：\nBelow are some useful tools for debugging AOT Inductor.\n\n```{toctree}\n:caption: Debugging Tools\n:maxdepth: 1\n\nlogging\ntorch.compiler_aot_inductor_minifier\ntorch.compiler_aot_inductor_debugging_guide\n```\n\nTo enable runtime checks on inputs, set the environment variable `AOTI_RUNTIME_CHECK_INPUTS` to 1. This will raise a `RuntimeError` if the inputs to the compiled model differ in size, data type, or strides from those used during export.",
    "1798": "一级标题：AOTInductor: Ahead-Of-Time Compilation for Torch.Export-ed Models\n二级标题：API Reference\n内容：\n```{eval-rst}\n.. autofunction:: torch._inductor.aoti_compile_and_package\n.. autofunction:: torch._inductor.aoti_load_package\n```",
    "1799": "一级标题：AOTInductor Debugging Guide\n二级标题：无\n内容：\nIf you encounter CUDA illegal memory access (IMA) errors while using [AOT Inductor](./torch.compiler_aot_inductor.md), this guide provides a systematic approach to debug such errors. AOT Inductor is part of the PT2 stack, similar to torch.compile, but it produces a compilation artifact that can work in a C++ environment. CUDA illegal memory errors can happen non-deterministically and even appear transient at times.\n\nOn a high-level, there are three main steps in debugging CUDA IMA errors:\n\n- **Sanity checks**: Use basic debugging flags to catch common issues before diving deeper.\n- **Pinpoint the CUDA IMA**: Make the error deterministic and identify the problematic kernel.\n- **Identify problematic kernels**: Use intermediate value debugging to inspect kernel inputs and outputs.",
    "1800": "一级标题：AOTInductor Debugging Guide\n二级标题：Step 1: Sanity Checks\n内容：\nBefore diving deep into reliably reproducing the error, try out some existing debugging flags:\n\n```bash\nAOTI_RUNTIME_CHECK_INPUTS=1\nTORCHINDUCTOR_NAN_ASSERTS=1\n```\n\nThese flags take effect at compilation time (more precisely, at codegen time):\n\n- `AOTI_RUNTIME_CHECK_INPUTS=1` checks if the inputs satisfy the same set of guards used during compilation. See {ref}`torch.compiler_troubleshooting` for more details.\n- `TORCHINDUCTOR_NAN_ASSERTS=1` adds codegen before and after each Inductor's kernel to check for NaN.",
    "1801": "一级标题：AOTInductor Debugging Guide\n二级标题：Step 2: Pinpoint the CUDA IMA\n内容：\nOne hard part is CUDA IMA errors can be non-deterministic. They can happen at different locations, and sometimes not happen at all (though that just means the numerics are silently incorrect). With the following two flags, we can trigger the error deterministically:\n\n```bash\nPYTORCH_NO_CUDA_MEMORY_CACHING=1\nCUDA_LAUNCH_BLOCKING=1\n```\n\nThese flags take effect at runtime:\n\n- `PYTORCH_NO_CUDA_MEMORY_CACHING=1` disables PyTorch's Caching Allocator, which allocates a bigger buffer than needed immediately to reduce the number of buffer allocations. This is usually the reason why CUDA illegal memory access errors are non-deterministic.\n![How PyTorch's caching allocator can mask CUDA illegal memory access errors](./_static/img/aoti_debugging_guide/cuda_ima_cca.png)\n*Figure: How PyTorch's caching allocator can mask CUDA illegal memory access errors*\n\n- `CUDA_LAUNCH_BLOCKING=1` forces the kernels to launch one at a time. Without this, we would get the famous \"CUDA kernel errors might be asynchronously reported at some other API call\" warning since kernels are launched asynchronously.",
    "1802": "一级标题：AOTInductor Debugging Guide\n二级标题：Step 3: Identify Problematic Kernels with Intermediate Value Debugger\n内容：\nThe AOTI Intermediate Value Debugger can help pinpoint the problematic kernel and get information about the inputs and outputs of said kernel.\n\nFirst, use:\n\n```bash\nAOT_INDUCTOR_DEBUG_INTERMEDIATE_VALUE_PRINTER=3\n```\n\nThis flag takes effect at compilation time and prints the kernels one by one at runtime. Together with the previous flags, this would let us know which kernel was launched right before the error happened.\n\nHowever, it is important to note that just because the error happened in that kernel, it doesn't mean that kernel is problematic. For example, it can happen that an earlier kernel is problematic and produces some wrong outputs. So the natural next step is to inspect the inputs to the problematic kernel:\n\n```bash\nAOT_INDUCTOR_FILTERED_KERNELS_TO_PRINT=\"triton_poi_fused_add_ge_logical_and_logical_or_lt_231,_add_position_embeddings_kernel_5\" AOT_INDUCTOR_DEBUG_INTERMEDIATE_VALUE_PRINTER=2\n```\n\nThe filtered kernels to print environment variable has the names of the kernels you want to inspect. If the inputs to the kernel are not as expected, you then inspect the kernel that produces the bad input.",
    "1803": "一级标题：AOTInductor Debugging Guide\n二级标题：Additional Debugging Tools\n内容：\n### Logging and Tracing\n\n- **tlparse / TORCH_TRACE**: Provides complete output codes for inspection and records the set of guards used. See {ref}`tlparse / TORCH_TRACE <tlparse-torch-trace>` for more details.\n- **TORCH_LOGS**: Use `TORCH_LOGS=\"+inductor,output_code\"` to see more PT2 internal logs. See {ref}`TORCH_LOGS <torch-logs>` for more details.\n- **TORCH_SHOW_CPP_STACKTRACES**: Set `TORCH_SHOW_CPP_STACKTRACES=1` to potentially see more stack traces.\n\n### Common Sources of Issues\n\n- [**Dynamic shapes**](./torch.compiler_dynamic_shapes.md): Historically a source of many IMAs. Pay special attention when debugging dynamic shape scenarios.\n- **Custom ops**: Especially when implemented in C++ and used with dynamic shapes. There is a need to Symint'ify the meta function.",
    "1804": "一级标题：AOTInductor Minifier\n二级标题：无\n内容：\nIf you encounter an error while using AOT Inductor APIs such as\n`torch._inductor.aoti_compile_and_package`, `torch._indcutor.aoti_load_package`,\nor running the loaded model of `aoti_load_package` on some inputs, you can use the AOTInductor Minifier\nto create a minimal nn.Module that reproduce the error by setting `from torch._inductor import config; config.aot_inductor.dump_aoti_minifier = True`.\n\nOne a high-level, there are two steps in using the minifier:\n\n- Set `from torch._inductor import config; config.aot_inductor.dump_aoti_minifier = True` or set the environment variable `DUMP_AOTI_MINIFIER=1`. Then running the script that errors would produce a `minifier_launcher.py` script. The output directory is configurable by setting `torch._dynamo.config.debug_dir_root` to a valid directory name.\n\n- Run the `minifier_launcher.py` script. If the minifier runs successfully, it generates runnable python code in `repro.py` which reproduces the exact error.",
    "1805": "一级标题：AOTInductor Minifier\n二级标题：Example Code\n内容：\nHere is sample code which will generate an error because we injected an error on relu with\n`torch._inductor.config.triton.inject_relu_bug_TESTING_ONLY = \"compile_error\"`.\n\n\n```\nimport torch\nfrom torch._inductor import config as inductor_config\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = torch.nn.Linear(10, 16)\n        self.relu = torch.nn.ReLU()\n        self.sigmoid = torch.nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.sigmoid(x)\n        return x\n\n\ninductor_config.aot_inductor.dump_aoti_minifier = True\ntorch._inductor.config.triton.inject_relu_bug_TESTING_ONLY = \"compile_error\"\n\nwith torch.no_grad():\n    model = Model().to(\"cuda\")\n    example_inputs = (torch.randn(8, 10).to(\"cuda\"),)\n    ep = torch.export.export(model, example_inputs)\n    package_path = torch._inductor.aoti_compile_and_package(ep)\n    compiled_model = torch._inductor.aoti_load_package(package_path)\n    result = compiled_model(*example_inputs)\n```\n\nThe code above generates the following error:\n\n```text\nRuntimeError: Failed to import /tmp/torchinductor_shangdiy/fr/cfrlf4smkwe4lub4i4cahkrb3qiczhf7hliqqwpewbw3aplj5g3s.py\nSyntaxError: invalid syntax (cfrlf4smkwe4lub4i4cahkrb3qiczhf7hliqqwpewbw3aplj5g3s.py, line 29)\n```\n\n\nThis is because we injected an error on relu, and so the generated triton kernel looks like below. Note that we have `compile error!`\ninstead if `relu`, so we get a `SyntaxError`.\n\n```\n@triton.jit\ndef triton_poi_fused_addmm_relu_sigmoid_0(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 128\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x2 = xindex\n    x0 = xindex % 16\n    tmp0 = tl.load(in_out_ptr0 + (x2), xmask)\n    tmp1 = tl.load(in_ptr0 + (x0), xmask, eviction_policy='evict_last')\n    tmp2 = tmp0 + tmp1\n    tmp3 = compile error!\n    tmp4 = tl.sigmoid(tmp3)\n    tl.store(in_out_ptr0 + (x2), tmp4, xmask)\n```\n\n\nSince we have `torch._inductor.config.aot_inductor.dump_aoti_minifier=True`, we also see an additional line indicating where `minifier_launcher.py` has\nbeen written to. The output directory is configurable by setting\n`torch._dynamo.config.debug_dir_root` to a valid directory name.\n\n```text\nW1031 16:21:08.612000 2861654 pytorch/torch/_dynamo/debug_utils.py:279] Writing minified repro to:\nW1031 16:21:08.612000 2861654 pytorch/torch/_dynamo/debug_utils.py:279] /data/users/shangdiy/pytorch/torch_compile_debug/run_2024_10_31_16_21_08_602433-pid_2861654/minifier/minifier_launcher.py\n```",
    "1806": "一级标题：AOTInductor Minifier\n二级标题：Minifier Launcher\n内容：\nThe `minifier_launcher.py` file has the following code. The `exported_program` contains the inputs to `torch._inductor.aoti_compile_and_package`.\nThe `command='minify'` parameter means the script will run the minifier to create a minimal graph module that reproduce the error. Alternatively, you set\nuse `command='run'` to just compile, load, and run the loaded model (without running the minifier).\n\n\n```\nimport torch\nimport torch._inductor.inductor_prims\n\nimport torch._dynamo.config\nimport torch._inductor.config\nimport torch._functorch.config\nimport torch.fx.experimental._config\n\ntorch._inductor.config.triton.inject_relu_bug_TESTING_ONLY = 'compile_error'\ntorch._inductor.config.aot_inductor.dump_aoti_minifier = True\n\n\n\n\nisolate_fails_code_str = None\n\n\n\n# torch version: 2.6.0a0+gitcd9c6e9\n# torch cuda version: 12.0\n# torch git version: cd9c6e9408dd79175712223895eed36dbdc84f84\n\n\n# CUDA Info:\n# nvcc: NVIDIA (R) Cuda compiler driver\n# Copyright (c) 2005-2023 NVIDIA Corporation\n# Built on Fri_Jan__6_16:45:21_PST_2023\n# Cuda compilation tools, release 12.0, V12.0.140\n# Build cuda_12.0.r12.0/compiler.32267302_0\n\n# GPU Hardware Info:\n# NVIDIA PG509-210 : 8\n\nexported_program = torch.export.load('/data/users/shangdiy/pytorch/torch_compile_debug/run_2024_11_06_13_52_35_711642-pid_3567062/minifier/checkpoints/exported_program.pt2')\n# print(exported_program.graph)\nconfig_patches={}\nif __name__ == '__main__':\n    from torch._dynamo.repro.aoti import run_repro\n    with torch.no_grad():\n        run_repro(exported_program, config_patches=config_patches, accuracy=False, command='minify', save_dir='/data/users/shangdiy/pytorch/torch_compile_debug/run_2024_11_06_13_52_35_711642-pid_3567062/minifier/checkpoints', check_str=None)\n```\n\n\nSuppose we kept the `command='minify'` option, and run the script, we would get the following output:\n\n```text\n...\nW1031 16:48:08.938000 3598491 torch/_dynamo/repro/aoti.py:89] Writing checkpoint with 3 nodes to /data/users/shangdiy/pytorch/torch_compile_debug/run_2024_10_31_16_48_02_720863-pid_3598491/minifier/checkpoints/3.py\nW1031 16:48:08.975000 3598491 torch/_dynamo/repro/aoti.py:101] Copying repro file for convenience to /data/users/shangdiy/pytorch/repro.py\nWrote minimal repro out to repro.py\n```\n\n\nIf you get an `AOTIMinifierError` when running `minifier_launcher.py`, please report a bug [here](https://github.com/pytorch/pytorch/issues/new?assignees=&labels=&projects=&template=bug-report.yml).",
    "1807": "一级标题：AOTInductor Minifier\n二级标题：Minified Result\n内容：\nThe `repro.py` looks like this. Notice that the exported program is printed at the top of the file, and it contains only the relu node. The minifier successfully reduced the graph to the op that raises the error.\n\n\n```\n# from torch.nn import *\n# class Repro(torch.nn.Module):\n#     def __init__(self) -> None:\n#         super().__init__()\n\n\n\n#     def forward(self, linear):\n#         relu = torch.ops.aten.relu.default(linear);  linear = None\n#         return (relu,)\n\nimport torch\nfrom torch import tensor, device\nimport torch.fx as fx\nfrom torch._dynamo.testing import rand_strided\nfrom math import inf\nimport torch._inductor.inductor_prims\n\nimport torch._dynamo.config\nimport torch._inductor.config\nimport torch._functorch.config\nimport torch.fx.experimental._config\n\ntorch._inductor.config.generate_intermediate_hooks = True\ntorch._inductor.config.triton.inject_relu_bug_TESTING_ONLY = 'compile_error'\ntorch._inductor.config.aot_inductor.dump_aoti_minifier = True\n\n\n\n\nisolate_fails_code_str = None\n\n\n\n# torch version: 2.6.0a0+gitcd9c6e9\n# torch cuda version: 12.0\n# torch git version: cd9c6e9408dd79175712223895eed36dbdc84f84\n\n\n# CUDA Info:\n# nvcc: NVIDIA (R) Cuda compiler driver\n# Copyright (c) 2005-2023 NVIDIA Corporation\n# Built on Fri_Jan__6_16:45:21_PST_2023\n# Cuda compilation tools, release 12.0, V12.0.140\n# Build cuda_12.0.r12.0/compiler.32267302_0\n\n# GPU Hardware Info:\n# NVIDIA PG509-210 : 8\n\n\nexported_program = torch.export.load('/data/users/shangdiy/pytorch/torch_compile_debug/run_2024_11_25_13_59_33_102283-pid_3658904/minifier/checkpoints/exported_program.pt2')\n# print(exported_program.graph)\nconfig_patches={'aot_inductor.package': True}\nif __name__ == '__main__':\n    from torch._dynamo.repro.aoti import run_repro\n    with torch.no_grad():\n        run_repro(exported_program, config_patches=config_patches, accuracy=False, command='run', save_dir='/data/users/shangdiy/pytorch/torch_compile_debug/run_2024_11_25_13_59_33_102283-pid_3658904/minifier/checkpoints', check_str=None)\n```",
    "1808": "一级标题：torch.compiler API reference\n二级标题：无\n内容：\nFor a quick overview of `torch.compiler`, see {ref}`torch.compiler_overview`.\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n     compile\n     reset\n     allow_in_graph\n     substitute_in_graph\n     assume_constant_result\n     list_backends\n     disable\n     set_stance\n     set_enable_guard_collectives\n     cudagraph_mark_step_begin\n     is_compiling\n     is_dynamo_compiling\n     is_exporting\n     skip_guard_on_inbuilt_nn_modules_unsafe\n     skip_guard_on_all_nn_modules_unsafe\n     keep_tensor_guards_unsafe\n     skip_guard_on_globals_unsafe\n     nested_compile_region\n```",
    "1809": "一级标题：CUDAGraph Trees\n二级标题：无\n内容：",
    "1810": "一级标题：CUDAGraph Trees\n二级标题：**Background**\n内容：\n### CUDAGraph\n\nFor a longer background on CUDAGraphs, read [accelerating pytorch with CUDAGraphs](https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/).\n\n[CUDA Graphs](https://developer.nvidia.com/blog/cuda-10-features-revealed/), which made its debut in CUDA 10, let a series of CUDA kernels to be defined and encapsulated as a single unit, i.e., a graph of operations, rather than a sequence of individually-launched operations. It provides a mechanism to launch multiple GPU operations through a single CPU operation, and hence reduces the launching overheads.\n\nCUDA Graphs can give large speedups, especially for models with high CPU overhead or small compute. There are a number of limitations from requiring the same kernels to be run with the same arguments and dependencies, and memory addresses.\n\n- Control Flow is not possible\n- Kernels which trigger host to device syncs (such as .item()) errors\n- All input arguments to kernels are fixed to what they were recorded\n- CUDA Memory addresses are fixed, however the values of the memory at those addresses can change\n- No Essential CPU ops or CPU side effects\n\n### PyTorch CUDAGraph Integration\n\nPyTorch provides a [convenience wrapper](https://pytorch.org/docs/stable/generated/torch.cuda.CUDAGraph.html) around CUDAGraphs that handles a couple of tricky interactions with PyTorch’s caching allocator.\n\nThe CachingAllocator uses a separate memory pool for all the new allocations. During CUDAGraph recording, memory is accounted for, allocated, and freed exactly as during eager run. On replay, just the kernels are invoked, and there are no changes to the allocator. Subsequent to initial recording, the allocator does not know which memory is actively being used in user programs.\n\nUsing a separate memory pool between eager allocations and cudagraph allocations may increase the memory of your program if there is substantial memory allocated to both.\n\n### Make Graphed Callables\n\n[Make Graphed Callables](https://pytorch.org/docs/stable/generated/torch.cuda.make_graphed_callables.html) is a PyTorch Abstraction to share a single memory pool over a series of callables. Graphed Callables takes advantage of the fact that on CUDA Graph recording, memory is exactly accounted for by the caching allocator to safely share memory between separate CUDA Graph recordings. In each invocation, outputs are preserved as live memory, preventing one callable from overwriting the live memory of another. Graphed Callables can only be invoked in a single order; memory addresses from the first run are burned into the second, and so forth.\n\n### TorchDynamo Previous CUDA Graphs Integration\n\nRunning with `cudagraph_trees=False` does not reuse memory across separate graph captures, which can lead to large memory regressions. Even for a model that has no graph breaks, this has issues. The forward and backward are separate graph captures, so the memory pools for forward and backward are not shared. In particular, memory for activations that are saved in the forward cannot be reclaimed in the backward.",
    "1811": "一级标题：CUDAGraph Trees\n二级标题：**CUDAGraph Trees Integration**\n内容：\nLike Graph Callables, CUDA Graph Trees use a single memory pool across all graph captures. However, instead of requiring a single sequence of invocations, CUDA Graph Trees create separate trees of CUDA Graph captures. Let’s take a look at an illustrative example:\n\n```python\n@torch.compile(mode=\"reduce-overhead\")\ndef foo(x):\n    # GRAPH 1\n    y = x * x * x\n    # graph break triggered here\n    if y.sum() > 0:\n        # GRAPH 2\n        z = y ** y\n    else:\n        # GRAPH 3\n        z = (y.abs() ** y.abs())\n    torch._dynamo.graph_break()\n    # GRAPH 4\n    return z * torch.rand_like(z)\n\n# the first run warms up each graph, which does things like CuBlas or Triton benchmarking\nfoo(torch.arange(0, 10, device=\"cuda\"))\n# The second run does a CUDA Graph recording, and replays it\nfoo(torch.arange(0, 10, device=\"cuda\"))\n# Finally we hit the optimized, CUDA Graph replay path\nfoo(torch.arange(0, 10, device=\"cuda\"))\n```\n\nIn this example, there are two separate paths that we make through the function: 1 -> 2 -> 4, or 1 -> 3 -> 4.\n\nWe share all of the memory in a single memory pool between separate recordings by building up a tape of CUDA Graph recordings, in this instance, 1 -> 2 -> 4. We add invariants to ensure that memory is always in the same location as it were recorded, and no live tensors exist in user programs that might be overwritten.\n\n- Same constraints from CUDA Graphs apply: same kernels must be invoked with the same arguments (static sizes, addresses, etc)\n- The same pattern of memory must be observed between recording and replay: if a tensor output of one graph dies subsequent to another graph during recording, it must also do so during replay.\n- Live memory in the CUDA pool forces a dependence between two recordings\n- These recordings can only be invoked in a single order 1 - > 2 -> 4\n\nAll of the memory is shared in a single memory pool, so there is no additional memory overhead compared to eager. Now, what happens if we were to hit a new path and run Graph 3?\n\nGraph 1 gets replayed, and then we hit Graph 3, which we have not yet recorded. On graph replays, the private memory pool is not updated, so y is not reflected in the allocator. Without care, we would overwrite it. To support reusing the same memory pool after replaying other graphs, we checkpoint the memory pool back to its state at the end of graph 1. Now that our live tensors are reflected in the caching allocator, we are safe to run a new graph.\n\nFirst, we would hit the optimized, CUDAGraph.replay() path that we have already recorded in graph 1. Then we would hit Graph 3. Just as before, we will need to warm up the graph once before recording. On the warmup run, the memory addresses are not fixed, so graph 4 will also fallback to the inductor, non-cudagraph invocation.\n\nThe second time we hit graph 3 we are warmed up and ready to record. We record graph 3 and then record graph 4 again since the input memory addresses have changed. This creates a tree of CUDA Graph recordings. A CUDA Graph Tree!\n\n```\n  1\n / \\\\\n2   3\n \\\\   \\\\\n  4   4\n```\n\n### Input Mutation Support\n\nInput mutation function refers to a function conducting in-place writes to an input tensor,\nas illustrated below:\n\n```python\ndef foo(x, y):\n    # mutates input x\n    x.add_(1)\n    return x + y\n```\n\nInput mutation functions generally lead to challenges for CUDAGraph Trees. Due to the static\nCUDA memory address requirement from CUDAGraph, for each input tensor x, CUDAGraph Trees may\nallocate a static memory address x'. During execution, CUDAGraph Trees first copy the input\ntensor x to the static memory address x', and then replay the recorded CUDAGraph. For input\nmutation function, x' is in-place updated, which is not reflected on the input tensor x since\nx and x' reside on different CUDA memory addresses.\n\nA closer look at input mutation functions reveals that there are three types of inputs:\n\n- **inputs from eager**: These tensors we assume will vary input tensor addresses from\n  execution to execution. Because cudagraphs freeze memory addresses, we need to copy these\n  inputs to a static address tensor prior to graph recording and execution.\n- **Parameters and buffers**: These tensors we assume (and runtime-check) have the same tensor\n  addresses on every execution. We do not need to copy over their contents because the recorded\n  memory address will be the same as the executed memory address.\n- **Tensors which are prior outputs from CUDAGraph Trees**: Because the output tensor addresses\n  of a cudagraph are fixed, if we run CUDAGraph1, then run CUDAGraph2, the inputs which came from\n  CUDAGraph1 into CUDAGraph2 will have a fixed memory address. These inputs, like parameters and\n  buffers, do not require copying over to a static address tensor. We check to make sure that\n  these inputs are stable at runtime, and if they're not we will re-record.\n\nCUDAGraph Trees support input mutation on parameters and buffers, and tensors which are prior\noutputs from CUDAGraph Trees. For mutation on inputs from eager, CUDAGraph Trees will run the\nfunction without CUDAGraph and emit *skipping due to mutated inputs* log. The following example\nshows CUDAGraph Trees' support for tensors which are prior outputs from CUDAGraph Trees.\n\n```python\nimport torch\n\n@torch.compile(mode=\"reduce-overhead\")\ndef foo(x):\n    return x + 1\n\n@torch.compile(mode=\"reduce-overhead\")\ndef mut(x):\n    return x.add_(2)\n\n# Enable input mutation support\ntorch._inductor.config.triton.cudagraph_support_input_mutation = True\n\nfor i in range(3):\n    torch.compiler.cudagraph_mark_step_begin()\n    inp = torch.rand([4], device=\"cuda\")\n\n    # CUDAGraph is applied since `foo` does not mutate `inp`\n    tmp = foo(inp)\n    # Although `mut` mutates `tmp`, which is an output of a CUDAGraph\n    # managed function. So CUDAGraph is still applied.\n    mut(tmp)\n\n\ntorch.compiler.cudagraph_mark_step_begin()\ninp = torch.rand([4], device=\"cuda\")\n\ntmp = foo(inp)\n# While `tmp` is a CUDAGraph Tree managed function's output, `tmp.clone()`\n# is not. So CUDAGraph is not applied to `mut` and there is a log\n# `skipping cudagraphs due to mutated inputs`\nmut(tmp.clone())\n```\n\nTo enable CUDAGraph Trees for a function mutating inputs from eager, please re-write\nthe function to avoid input mutation.\n\n\n\n> **Note**\\\n> Enable input mutation support by setting\n[torch.\\_inductor.config.cudagraph_support_input_mutation = True](https://github.com/pytorch/pytorch/blob/main/torch/_inductor/config.py#L662) for \"reduce-overhead\" mode.\n\n\n### Dynamic Shape Support\n\n[Dynamic shape](https://pytorch.org/docs/stable/torch.compiler_dynamic_shapes.html)\nmeans that an input tensor has different shapes across function calls. Since CUDAGraph\nrequires fixed tensor addresses, CUDAGraph Trees re-record CUDAGraph for every unique\nshape of an input tensor. This leads to multiple CUDAGraphs for a single inductor graph.\nWhen there are limited shapes (e.g., batch sizes in inference), it is profitable to\nre-record CUDAGraphs. However, if input tensor shapes change frequently or even on\nevery invocation, re-recording CUDAGraph may not be profitable. Nvidia uses 64 KB of\ndevice memory per kernel launch in CUDAGraph, up until CUDA 12.4 and Driver Version 550+.\nThis memory cost can be significant with many CUDAGraph re-recordings.\n\nFor functions with frequently changing input tensor shapes, we suggest padding input\ntensors to a few fixed tensor shapes to still enjoy benefits from CUDAGraph. In addition,\nsetting [torch.\\_inductor.config.triton.cudagraph_skip_dynamic_graphs=True](https://github.com/pytorch/pytorch/blob/main/torch/_inductor/config.py#L653)\nallows to skip cudagraphing functions with dynamic shape inputs and only cudagraphing\nfunctions with static input tensor shapes.\n\n### NCCL Support\n\nCUDAGraph Trees support functions with nccl operators. While CUDAGraph Trees perform per-device\nrecord for CUDAGraph, NCCL support allows cross-device communication.\n\n```python\n@torch.compile(mode=\"reduce-overhead\")\ndef func(x):\n    y = x * x\n    y = torch.distributed.all_reduce(y, op=torch.distributed.ReduceOp.SUM)\n    x = torch.nn.functional.silu(x)\n    return x * y\n```\n\n### Reasons for Skipping CUDAGraph\n\nSince CUDAGraph has requirements such as static input tensor addresses and not supporting\nCPU operators, CUDAGraph Trees check whether a function satisfies these requirements and\nmay skip CUDAGraph when necessary. Here, we list common reasons for skipping CUDAGraph.\n\n- **Input mutation**: CUDAGraph Trees skip functions that in-place mutates eager input.\n  In-place mutating parameters and buffers, or output tensors from CUDAGraph Tree managed\n  functions are still supported. Please see *Input Mutation Support* section for more details.\n- **CPU operators**: Functions containing CPU operator are skipped. Please split the\n  function into multiple functions and apply CUDAGraph Trees on functions with only GPU operators.\n- **Multi-device operators**: A function is skipped if it contains operators on multiple\n  devices. Currently, CUDAGraph is applied on a per-device basis. Please use supported\n  libraries such as NCCL for cross-device communication. Please see *NCCL Support*\n  section for more details.\n- **Free unbacked symbols**: Free unbacked symbols usually happen during\n  [dynamic shapes](https://pytorch.org/docs/stable/torch.compiler_dynamic_shapes.html).\n  CUDAGraph Trees currently record a CUDAGraph for every unique input tensor shapes.\n  Please see *Dynamic Shape Support* for more details.\n- **CUDAGraph-unsafe custom ops**: Some custom ops may include cudagraph unsafe ops, which causes cudagraph to be skipped. Please see *CUDAGraph Unsafe Custom Ops* for more details.\n- **Incompatible operators**: CUDAGraph Trees skip a function if it contain incompatible\n  operators. Please replace these operators in a function with supported operators. We\n  show an exhaustive list of incompatible operators:\n\n```python\naten._fused_moving_avg_obs_fq_helper.default\naten._fused_moving_avg_obs_fq_helper_functional.default\naten.multinomial.default\nfbgemm.dense_to_jagged.default\nfbgemm.jagged_to_padded_dense.default\nrun_and_save_rng_state\nrun_with_rng_state\naten._local_scalar_dense\naten._assert_scalar\n```\n\nThe following operators are incompatible when [torch.are_deterministic_algorithms_enabled()](https://pytorch.org/docs/stable/generated/torch.are_deterministic_algorithms_enabled.html).\n\n```python\naten._fused_moving_avg_obs_fq_helper.default\naten._fused_moving_avg_obs_fq_helper_functional.default\naten.multinomial.default\nfbgemm.dense_to_jagged.default\nfbgemm.jagged_to_padded_dense.default\nrun_and_save_rng_state\nrun_with_rng_state\naten._local_scalar_dense\naten._assert_scalar\n```\n\n### CUDAGraph Unsafe Custom Ops\nCustom ops are assumed to be safe for CUDAGraph by default. However, some custom ops may include unsupported ops such as cpu ops. Since custom op are treated as black boxes by the compiler, users must explicitly mark these ops as unsafe for CUDAGraph by setting the `torch._C.Tag.cudagraph_unsafe` tag, as demonstrated in the example below. When a function contains cudagraph-unsafe custom ops, it will be skipped by CUDAGraph unless *CUDAGraph partition* is enabled.\n\n```python\n@torch.library.custom_op(\n    \"mylib::modify\",\n    mutates_args=(),\n    tags=(torch._C.Tag.cudagraph_unsafe,),\n)\ndef modify(pic: torch.Tensor) -> torch.Tensor:\n    pic1 = pic + 1\n    pic1_cpu = (pic1.cpu() + 1) * 2\n    return pic1_cpu.cuda() + pic\n\n@modify.register_fake\ndef _(pic):\n    return torch.empty_like(pic)\n```\n\n### CUDAGraph Partition\n\nAs we discussed earlier, CUDAGraph does not support some ops (e.g., cpu ops) which may limit its adoption. CUDAGraph partition is a compiler solution that automatically splits off these ops, reorders ops to reduce the number of partitions, and applies CUDAGraph to each partition individually. Please set `torch._inductor.config.graph_partition=True` to enable CUDAGraph partition.\n\nConsider the following example where `x` and `y` are gpu inputs but `y_cpu` is a cpu tensor. Without graph partition, this function must be skipped due to cpu ops. With graph partition, the CPU ops are split off, and the remaining GPU ops are cudagraphified, resulting in two separate separate CUDAGraphs.\n\n```python\ndef f(x, y):\n    x1 = x + 1\n    y1 = y + 1\n    y_cpu = y1.cpu() + 1\n    z = x @ y\n    return x1 + y1 + z + y_cpu.cuda()\n```\n\nCurrently, CUDAGraph partition supports splitting off the following types of ops:\n\n- **Non-GPU Ops**: Popular examples include computation on cpu tensors.\n- **Device Copy Ops**: Data transfers between devices, such as the `y1.cpu()` in the example above.\n- **Control Flow Ops**: [Control flow ops](https://docs.pytorch.org/docs/stable/cond.html) are split off since they are not yet supported by CUDAGraph.\n- **CUDAGraph Unsafe Custom Ops**: Custom ops tagged with `torch._C.Tag.cudagraph_unsafe` are split off. See *CUDAGraph Unsafe Custom Ops* section for details.\n- **Unbacked Symints**: Please refer to *Dynamic Shape Support* section for more information.\n\n\n### Limitations\n\nBecause CUDA Graph fixes memory addresses, CUDA Graphs do not have a great way of handling live tensors from a previous invocation.\n\nLet’s say we are benchmarking running inference with the following code:\n\n```python\nimport torch\n\n@torch.compile(mode=\"reduce-overhead\")\ndef my_model(x):\n    y = torch.matmul(x, x)\n    return y\n\nx = torch.randn(10, 10, device=\"cuda\")\ny1 = my_model(x)\ny2 = my_model(x)\nprint(y1)\n# RuntimeError: Error: accessing tensor output of CUDAGraphs that has been overwritten by a subsequent run.\n```\n\nIn the Separate CUDA Graph implementation, the output from the first invocation will be overwritten by the second invocation. In CUDAGraph\nTrees, we don’t want to add unintended dependencies between iterations that would cause us to not hit the hot path, nor do we want we want\nto prematurely free memory from a prior invocation. Our heuristics are in inference we start a new iteration on each invocation for\ntorch.compile, and in training we do the same so long as there is not a pending backward that has not been invoked. If those heuristics\nare wrong, you can mark the start of a new iteration with\n[torch.compiler.mark_step_begin()](https://pytorch.org/docs/stable/generated/torch.compiler.cudagraph_mark_step_begin.html), or clone\ntensors of a prior iteration (outside of torch.compile) before you begin the next run.\n\n### Comparisons\n\n| Footguns      | Separate CudaGraph                                         | CUDAGraph Trees                                                        |\n|---------------|------------------------------------------------------------|------------------------------------------------------------------------|\n| Memory Can Increase | On each graph compilation (new sizes, etc.)              | If you are also running non-cudagraph memory                           |\n| Recordings    | On any new invocation of a graph                           | Will re-record on any new, unique path you take through your program   |\n| Footguns      | Invocation of one graph will overwrite prior invocation    | Cannot persist memory between separate runs through your model - one training loop training, or one run of inference |",
    "1812": "一级标题：Custom Backends\n二级标题：无\n内容：",
    "1813": "一级标题：Custom Backends\n二级标题：Overview\n内容：\n`torch.compile` provides a straightforward method to enable users\nto define custom backends.\n\nA backend function has the contract\n`(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]) -> Callable`.\n\nBackend functions can be called by TorchDynamo, the graph tracing component of `torch.compile`,\nafter tracing an FX graph and are\nexpected to return a compiled function that is equivalent to the traced FX graph.\nThe returned callable should have the same contract as the `forward` function of the original `torch.fx.GraphModule`\npassed into the backend:\n`(*args: torch.Tensor) -> List[torch.Tensor]`.\n\nIn order for TorchDynamo to call your backend, pass your backend function as the `backend` kwarg in\n`torch.compile`. For example,\n\n```python\nimport torch\n\ndef my_custom_backend(gm, example_inputs):\n    return gm.forward\n\ndef f(...):\n    ...\n\nf_opt = torch.compile(f, backend=my_custom_backend)\n\n@torch.compile(backend=my_custom_backend)\ndef g(...):\n    ...\n```\n\nSee below for more examples.",
    "1814": "一级标题：Custom Backends\n二级标题：Registering Custom Backends\n内容：\nYou can register your backend using the `register_backend` decorator, for example,\n\n```python\nfrom torch._dynamo import register_backend\n\n@register_backend\ndef my_compiler(gm, example_inputs):\n    ...\n```\n\nBesides the `register_backend` decorator, if your backend is in another python package, you could also register your\nbackend through entry points of python package, which provides a way for a package to register a plugin for another one.\n\n:::{hint}\nYou can learn more about `entry_points` in the\n[python packaging documentation](https://setuptools.pypa.io/en/latest/userguide/entry_point.html).\n:::\n\nTo register your backend through `entry_points`, you could add your backend function to the `torch_dynamo_backends` entry point group in the\n`setup.py` file of your package like:\n\n```python\n...\nsetup(\n    ...\n    'torch_dynamo_backends': [\n        'my_compiler = your_module.submodule:my_compiler',\n    ]\n    ...\n)\n```\n\nPlease replace the `my_compiler` before `=` to the name of your backend's name and replace the part after `=` to\nthe module and function name of your backend function.\nThe entry point will be added to your python environment after the installation of the package.\nWhen you call `torch.compile(model, backend=\"my_compiler\")`, PyTorch would first search the backend named `my_compiler`\nthat has been registered with `register_backend`. If not found, it will continue to search in all backends registered\nvia `entry_points`.\n\nRegistration serves two purposes:\n\n- You can pass a string containing your backend function's name to `torch.compile` instead of the function itself,\n  for example, `torch.compile(model, backend=\"my_compiler\")`.\n- It is required for use with the [minifier](https://pytorch.org/docs/main/torch.compiler_troubleshooting_old.html#minifier). Any generated\n  code from the minifier must call your code that registers your backend function, typically through an `import` statement.",
    "1815": "一级标题：Custom Backends\n二级标题：Custom Backends after AOTAutograd\n内容：\nIt is possible to define custom backends that are called by AOTAutograd rather than TorchDynamo.\nThis is useful for 2 main reasons:\n\n- Users can define backends that support model training, as AOTAutograd can generate the backward graph for compilation.\n- AOTAutograd produces FX graphs consisting of [core Aten ops](https://pytorch.org/docs/main/torch.compiler_ir.html#core-aten-ir). As a result,\n  custom backends only need to support the core Aten opset, which is a significantly smaller opset than the entire torch/Aten opset.\n\nWrap your backend with\n`torch._dynamo.backends.common.aot_autograd` and use `torch.compile` with the `backend` kwarg as before.\nBackend functions wrapped by `aot_autograd` should have the same contract as before.\n\nBackend functions are passed to `aot_autograd` through the `fw_compiler` (forward compiler)\nor `bw_compiler` (backward compiler) kwargs. If `bw_compiler` is not specified, the backward compile function\ndefaults to the forward compile function.\n\nOne caveat is that AOTAutograd requires compiled functions returned by backends to be \"boxed\". This can be done by wrapping\nthe compiled function with `functorch.compile.make_boxed_func`.\n\nFor example,\n\n```python\nfrom torch._dynamo.backends.common import aot_autograd\nfrom functorch.compile import make_boxed_func\n\ndef my_compiler(gm, example_inputs):\n    return make_boxed_func(gm.forward)\n\nmy_backend = aot_autograd(fw_compiler=my_compiler)  # bw_compiler=my_compiler\n\nmodel_opt = torch.compile(model, backend=my_backend)\n```",
    "1816": "一级标题：Custom Backends\n二级标题：Examples\n内容：\n### Debugging Backend\n\nIf you want to better understand what is going on during a\ncompilation, you can create a custom compiler, which is referred to as\nbackend in this section, that will print pretty print the fx\n`GraphModule` extracted from Dynamo’s bytecode analysis\nand return a `forward()` callable.\n\nFor example:\n\n```python\nfrom typing import List\nimport torch\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my_compiler() called with FX graph:\")\n    gm.graph.print_tabular()\n    return gm.forward  # return a python callable\n@torch.compile(backend=my_compiler)\ndef fn(x, y):\n    a = torch.cos(x)\n    b = torch.sin(y)\n    return a + b\nfn(torch.randn(10), torch.randn(10))\n```\n\nRunning the above example produces the following output:\n\n```\nmy_compiler() called with FX graph:\nopcode         name    target                                                  args        kwargs\n-------------  ------  ------------------------------------------------------  ----------  --------\nplaceholder    x       x                                                       ()          {}\nplaceholder    y       y                                                       ()          {}\ncall_function  cos     <built-in method cos of type object at 0x7f1a894649a8>  (x,)        {}\ncall_function  sin     <built-in method sin of type object at 0x7f1a894649a8>  (y,)        {}\ncall_function  add     <built-in function add>                                 (cos, sin)  {}\noutput         output  output                                                  ((add,),)   {}\n```\n\nThis works for `torch.nn.Module` as well as shown below:\n\n```python\nfrom typing import List\nimport torch\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my_compiler() called with FX graph:\")\n    gm.graph.print_tabular()\n    return gm.forward  # return a python callable\nclass MockModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.relu = torch.nn.ReLU()\n    def forward(self, x):\n        return self.relu(torch.cos(x))\nmod = MockModule()\noptimized_mod = torch.compile(mod, backend=my_compiler)\noptimized_mod(torch.randn(10))\n```\n\nLet’s take a look at one more example with control flow:\n\n```python\nfrom typing import List\nimport torch\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my_compiler() called with FX graph:\")\n    gm.graph.print_tabular()\n    return gm.forward  # return a python callable\n@torch.compile(backend=my_compiler)\ndef toy_example(a, b):\n    x = a / (torch.abs(a) + 1)\n    if b.sum() < 0:\n        b = b * -1\n    return x * b\nfor _ in range(100):\n    toy_example(torch.randn(10), torch.randn(10))\n```\n\nRunning this example produces the following output:\n\n```\nmy_compiler() called with FX graph:\nopcode         name     target                                                  args              kwargs\n-------------  -------  ------------------------------------------------------  ----------------  --------\nplaceholder    a        a                                                       ()                {}\nplaceholder    b        b                                                       ()                {}\ncall_function  abs_1    <built-in method abs of type object at 0x7f8d259298a0>  (a,)              {}\ncall_function  add      <built-in function add>                                 (abs_1, 1)        {}\ncall_function  truediv  <built-in function truediv>                             (a, add)          {}\ncall_method    sum_1    sum                                                     (b,)              {}\ncall_function  lt       <built-in function lt>                                  (sum_1, 0)        {}\noutput         output   output                                                  ((truediv, lt),)  {}\n\nmy_compiler() called with FX graph:\nopcode         name    target                   args         kwargs\n-------------  ------  -----------------------  -----------  --------\nplaceholder    b       b                        ()           {}\nplaceholder    x       x                        ()           {}\ncall_function  mul     <built-in function mul>  (b, -1)      {}\ncall_function  mul_1   <built-in function mul>  (x, mul)     {}\noutput         output  output                   ((mul_1,),)  {}\n\nmy_compiler() called with FX graph:\nopcode         name    target                   args       kwargs\n-------------  ------  -----------------------  ---------  --------\nplaceholder    b       b                        ()         {}\nplaceholder    x       x                        ()         {}\ncall_function  mul     <built-in function mul>  (x, b)     {}\noutput         output  output                   ((mul,),)  {}\n\nThe order of the last two graphs is nondeterministic depending\non which one is encountered first by the just-in-time compiler.\n```\n\n### Speedy Backend\n\nIntegrating a custom backend that offers superior performance is also\neasy and we’ll integrate a real one\nwith [optimize_for_inference](https://pytorch.org/docs/stable/generated/torch.jit.optimize_for_inference.html):\n\n```python\ndef optimize_for_inference_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    scripted = torch.jit.script(gm)\n    return torch.jit.optimize_for_inference(scripted)\n```\n\nAnd then you should be able to optimize any existing code with:\n\n```python\n@torch.compile(backend=optimize_for_inference_compiler)\ndef code_to_accelerate():\n    ...\n```\n\n### Composable Backends\n\nTorchDynamo includes many backends, which can be listed with\n`torch._dynamo.list_backends()`. You can combine these backends\ntogether with the following code:\n\n```python\nfrom torch._dynamo import lookup_backend\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    try:\n        trt_compiled = lookup_backend(\"tensorrt\")(gm, example_inputs)\n        if trt_compiled is not None:\n            return trt_compiled\n    except Exception:\n        pass\n    # first backend failed, try something else...\n    try:\n        inductor_compiled = lookup_backend(\"inductor\")(gm, example_inputs)\n        if inductor_compiled is not None:\n            return inductor_compiled\n    except Exception:\n        pass\n    return gm.forward\n```",
    "1817": "一级标题：Dynamic Shapes\n二级标题：无\n内容：\nCode: [symbolic_shapes.py](https://github.com/pytorch/pytorch/blob/db4572dbf18f1cf50cf662547e272d3117063747/torch/fx/experimental/symbolic_shapes.py)\n\nSee also: [The dynamic shapes manual](https://docs.google.com/document/d/1GgvOe7C8_NVOMLOCwDaYV1mXXyHMXY7ExoewHqooxrs/edit#heading=h.fh8zzonyw8ng)",
    "1818": "一级标题：Dynamic Shapes\n二级标题：Motivation\n内容：\nDeep learning compilers commonly only work for static shapes, that is to say, they produced compiled programs which only work for a single specific configuration of input shapes, and must recompile if any input shape changes. This assumption works great for the majority of commonly run deep learning models today, but there are a few situations where it is insufficient:\n\n- Some dimensions, such as batch size or sequence length, may vary. For example, an inference service performing adaptive batching will execute inference requests with varying batch sizes depending on how many requests it received within its batching window. We may also want to consider padding out variable size sequences only to the maximum sequence length within a batch, which may vary from batch-to-batch.\n- Some models exhibit data-dependent output shapes, that is to say, the size of their outputs and intermediates may depend on the actual input data which may vary across runs. For example, detection models may first generate a variable number of potential bounding boxes before running a more expensive image recognition model to identify if the subject is in a bounding box. The number of bounding boxes is data dependent.\n- One particularly important case of data-dependent shapes occurs when dealing with sparse representations, such as sparse tensors, jagged tensors, and graph neural networks. In all of these cases, the amount of data to be processed depends on the sparse structure of the problem, which will typically vary in a data-dependent way.\n\nIn supporting dynamic shapes, we chose not to support dynamic rank programs, e.g., programs whose inputs tensors change in dimensionality, as this pattern rarely occurs in real-world deep learning programs, and it avoids the need to reason inductively over symbolic lists of shapes.",
    "1819": "一级标题：Dynamic Shapes\n二级标题：Abridged public API\n内容：\nThe default dynamic behavior in PyTorch 2.1 is:\n\n- PT2 assumes everything is static by default\n- If we recompile because a size changed, we will instead attempt to recompile\n  that size as being dynamic (sizes that have changed are likely to change in\n  the future). This generalization may fail (e.g., because user code does a\n  conditional branch on the size in question or missing dynamic shapes support\n  in PT2). If you are trying to understand why PT2 has overspecialized some\n  code, run with `TORCH_LOGS=dynamic` and look for \"eval\" entries that say\n  when guards are added and why.\n- If you know ahead of time something will be dynamic, you can skip the first\n  recompile with `torch._dynamo.mark_dynamic(tensor, dim)`. If you know ahead of time\n  the `min` and `max` value this dimension can take, you can specify `torch._dynamo.mark_dynamic(tensor, dim, min=min, max=max)`\n- If you say `torch.compile(dynamic=False)`, we will turn off automatic\n  dynamic shapes on recompiles and always recompile for each distinct size.\n  Conversely, if you say `torch.compile(dynamic=True)`, we will try to make\n  everything as dynamic as possible. This is mostly useful for small\n  operators; if you try it on a big model it will (1) probably crash PT2 and (2) run slow for no good reason.\n- You can whitelist specific sources to be marked as dynamic using the\n  `TORCH_COMPILE_DYNAMIC_SOURCES` environment variable or by setting\n  `torch.compiler.config.dynamic_sources`. This is particularly useful for large\n  models with graph breaks, as you can maintain dynamism across graph breaks since\n  source names stay consistent. You can also use this to mark integers as dynamic.\n  The format is a comma-delimited list of source names, e.g., `\"L['x'], L['y']\"`.\n  You can also use regexes, e.g., `\"L\\['x.*'\\], L\\['y.*'\\]\")`.\n  This whitelist takes precedence over other flags like `dynamic=False`,\n  `force_nn_module_property_static_shapes`, and `force_parameter_static_shapes`.\n- Sometimes it can be cumbersome to find the right inputs to mark as dynamic. If\n  you're willing to take a performance hit for the first batch, one other affordable\n  option we have are the eager_then_compile stances which derive dynamism for you.\n  See [torch.compiler.set_stance](https://docs.pytorch.org/docs/stable/generated/torch.compiler.set_stance.html) for more details.",
    "1820": "一级标题：Dynamic Shapes\n二级标题：The Guard Model\n内容：\nWhen considering how to add support for dynamic shapes to TorchDynamo and TorchInductor, we made a major design decision: in order to reuse decompositions and other preexisting code written in Python/C++ targeting the PyTorch API, we must be able to trace through dynamic shapes. Unlike a fully symbolic system which might capture both branches of a conditional, we always pick one branch and specialize our trace under the assumption that we only use this trace when we would have made the same choice for that branch in the future. To do this, we maintain a \"hint\" for every symbolic size saying what its concrete value is at compile time (as TorchDynamo is a just-in-time compiler, it always knows what the actual input sizes are.) When we perform a condition on a tensor, we simply consult the hint to find out which branch to take.\n\nThis greatly simplifies the symbolic shape formulas we produce, but means we have a much more involved system for managing guards. Consider, for example, the following program:\n\n```python\ndef f(x, y):\n    z = torch.cat([x, y])\n    if z.size(0) > 2:\n        return z.mul(2)\n    else:\n        return z.add(2)\n```\n\nThe final IR we will compile with TorchInductor will either be `torch.cat([x, y]).add(2)` or `torch.cat([x, y]).mul(2)` (with the condition flattened away), but to determine which branch we are in, we would need to know the size of `z`, an intermediate. Because TorchDynamo must know upfront if a compiled trace is valid (we do not support bailouts, like some JIT compilers), we must be able to reduce `z.size(0)` as an expression in terms of the inputs, `x.size(0) + y.size(0)`. This is done by writing meta functions for all operators in PyTorch which can propagate size information to the output of a tensor without actually performing computation on the node.",
    "1821": "一级标题：Dynamic Shapes\n二级标题：Overall architecture\n内容：\nSymbolic shapes workflow:\n\n1. When we start compiling a frame in Dynamo, we allocate a ShapeEnv (attached to FakeTensorMode) which keeps track of symbolic shapes state.\n2. We allocate symbolic sizes for tensors on entry (what is static or dynamic is a policy decision, with some knobs).\n3. We propagate the symbolic sizes through operators, maintaining both (1) FX IR so that we can faithfully export symbolic compute, and (2) Sympy expressions representing the size vars, so we can reason about them.\n4. When we condition on symbolic sizes, either in Dynamo tracing or in Inductor optimization, we add guards based on the conditional. These can be induced from both Python and C++.\n5. These guards can induce further simplifications on symbolic variables. For example, if you assert `s0 == 4`, we can now replace all occurrences of `s0` with `4`.\n6. When we're done tracing and optimizing, we install all of these guards with the compiled code; the compiled code is only reusable if all the guards evaluate true.\n\nImportant files:\n\n- C++ SymInt API: `c10/core/SymInt.h`, `SymFloat.h`, `SymBool.h`\n- Python SymInt API: `torch/__init__.py` (look for `SymInt/SymFloat/SymBool`)\n- C++ plumbing: `c10/core/SymNodeImpl.h`, `torch/csrc/utils/python_symnode.h`, `torch/csrc/jit/python/init.cpp`\n- Python infrastructure: `torch/fx/experimental/symbolic_shapes.py`\n- Other important files: `torch/_subclasses/fake_tensor.py`, `torch/_meta_registrations.py`, decomps, PrimTorch refs",
    "1822": "一级标题：Dynamic Shapes\n二级标题：Abridged internal API\n内容：\nUnderstanding the Python class hierarchy:\n\n- SymInt/SymFloat/SymBool: these are user-visible classes that simulate their int/float/bool counterparts. If you add two SymInts, we give you a new SymInt that symbolically tracks that the integer addition had occurred.\n- SymNode: this is the internal structure (accessible via e.g., `symint.node`) which holds the actual symbolic tracking info. SymNode is type erased; this makes it more convenient to represent mixed-type operations. Note that technically you don't have to call into Python SymNode from SymInt; for example, XLA's C++ `SymNodeImpl` would take the place of SymNode.\n- ShapeEnv: per-compile context state which keeps track of all the free symbols and guards we have accumulated so far. Every SymNode records its ShapeEnv (but not vice versa; SymNodes only get used if they participate in a guard).\n\nC++ is fairly similar:\n\n- c10::SymInt/SymFloat/SymBool: user-visible classes that simulate int/float/bool.\n- c10::SymNode/SymNodeImpl: analogous to SymNode\n- There is no ShapeEnv in C++; for ease of debugging, the entire symbolic reasoning apparatus is in Python.\n\nWhen you write code that is traceable with `make_fx`, it must be able to deal with SymInt/SymFloat/SymBool flowing through it. [The dynamic shapes manual](https://docs.google.com/document/d/1GgvOe7C8_NVOMLOCwDaYV1mXXyHMXY7ExoewHqooxrs/edit#heading=h.fh8zzonyw8ng) gives some guidance for how to do this.",
    "1823": "一级标题：Dynamic Shapes\n二级标题：DimDynamic policy\n内容：\nSymbolic reasoning:\n\n- Value ranges\n- Sympy usage notes\n- Constraints\n- DimDynamic/Constraint",
    "1824": "一级标题：Dynamic Shapes\n二级标题：Unbacked SymInts\n内容：\nTo resolve control flow, we check the hint, aka actual value, of a symbolic integer to determine which branch to go. However, in some cases, we may not have a hint: so-called unbacked symbolic integers arise when a size variable emerges from a data-dependent operation like `.nonzero()` or `.item()`. It is illegal to perform control flow on these symbolic integers, so we must graph break on these operations.\n\nNaively implemented, this is too restrictive: most PyTorch programs will immediately fail if you try to do anything with unbacked symbolic integers. Here are the most important enhancements to make this actually work:\n\n- On tensor creation, PyTorch precomputes a lot of data about a tensor; for example, if you use `empty_strided` to create a tensor, we will eagerly sort the strides and determine if the tensor is non-overlapping and dense. Sorts produce a lot of guards. However, it is more common to produce a tensor directly with a higher-level API like `empty`, which is guaranteed to produce a non-overlapping and dense tensor. We modified PyTorch to avoid needlessly recomputing these properties.\n- Even if nontrivial compute is needed, sometimes a property is never actually queried at all. Making these precomputed properties lazy allows us to avoid guarding on an unbacked symbolic integer unless it is actually needed.\n- The data in an integer tensor is generally not known to be non-negative. However, we provide an API `constrain_range` whereby a user can specify that a size is bounded above and below by known limits.\n\nSimilar to the dynamic APIs, there are corresponding unbacked APIs: namely you can use mark_unbacked instead of `mark_dynamic` and `TORCH_COMPILE_UNBACKED_SOURCES` instead of `TORCH_COMPILE_DYNAMIC_SOURCES` to tell the compiler to mark an input as unbacked.\n\nIn future versions of PT2 (beyond PT2.1), we will extend our reasoning system\nto infer that an unbacked symbolic integer is size-like based on usage. For\nexample, if you pass the result of an `.item()` call to a factory function\nlike `torch.empty`, we will automatically infer that the result is a size\n(because if it was not, it would fail.) This assumption would get validated\nat runtime, raising an error if it was not fulfilled.",
    "1825": "一级标题：Dynamo Deep-Dive\n二级标题：无\n内容：\nTorchDynamo (or simply Dynamo) is the tracer within `torch.compile`,\nand it is, more often than not, the one to blame for those insane\nbacktraces. However, we cannot blindly blame Dynamo for these errors. In\norder to provide the user with the flexibility it does, Dynamo is given\nthe arduous task of understanding any Python program. In particular,\nDynamo has to implement a good part of the Python programming language\ninternally!\n\nIn this post, we will go over the internal design of Dynamo from the\nground up. We will discuss the functionality it provides, and how it is\nimplemented. By the end of this post, you will have a better\nunderstanding of what went wrong when you `torch.compiled` a PyTorch\nprogram and the compilation errored out, or succeeded but the speed-up\nwas not what you expected.",
    "1826": "一级标题：Dynamo Deep-Dive\n二级标题：A Gentle Introduction to Dynamo\n内容：\nBefore getting our hands dirty with all the implementation details,\nlet’s start by discussing what it is that Dynamo does.\n\nDynamo is a tracer. This means, given and function and inputs to it, it\nexecutes the function and records a linear sequence of instructions\n(without control flow) into a graph. For example, consider the following\nprogram:\n\n```python\nimport torch\n\n@torch.compile\ndef mse(x, y):\n    z = (x - y) ** 2\n    return z.sum()\n\nx = torch.randn(200)\ny = torch.randn(200)\nmse(x, y)\n```\n\nIf we save this program into the file `example.py` and we run\n\n```bash\nTORCH_LOGS=graph_code python example.py\n```\n\nwe see the output that Dynamo traced\n\n```python\ndef forward(l_x_: torch.Tensor, l_y_: torch.Tensor):\n    # File: example.py:5, code: z = (x - y) ** 2\n    sub = l_x_ - l_y_\n    z = sub ** 2\n    # File: example.py:6, code: return z.sum()\n    sum_1 = z.sum()\n    return (sum_1,)\n```\n\nWe call this a **graph (or trace) of the function for the given\ninputs**. This is represented via an [FX\ngraph](https://pytorch.org/docs/main/fx.html). We will simply think\nof an FX graph as a container that stores a list of function calls.\n\nThe first thing we should notice is that the graph is a linear sequence\nof PyTorch operations. [^1] Dynamo records all the PyTorch operations\nand stores them sequentially. For example, it split `z = (x - y) ** 2`\ninto its two constituting operations, `sub = l_x_ - l_y_` and\n`z = sub ** 2`.\n\nWhen we say that the trace is linear, we mean that there is no branching\nor any control flow. To see this, consider\n\n```python\nimport torch\n\n@torch.compile\ndef fn(x, n):\n    y = x ** 2\n    if n >= 0:\n        return (n + 1) * y\n    else:\n        return y / n\n\nx = torch.randn(200)\nfn(x, 2)\n```\n\nwhich, when executed with `TORCH_LOGS=graph_code`, returns\n\n```python\ndef forward(l_x_: torch.Tensor):\n    # File: example.py:5, code: y = x ** 2\n    y = l_x_ ** 2\n    # File: example.py:7, code: return (n + 1) * y\n    mul = 3 * y\n    return (mul,)\n```\n\nWe see that Dynamo completely removed the `if` statement from the\ntrace and just recorded the operations that were executed with the\ninputs.\n\nAs such, it should be clear that **the trace of a function depends on\nthe inputs**. In particular, this means that the trace is not generated\nwhen we write `@torch.compile`, but when we execute the function\n`fn(x, 2)` with the actual arguments.\n\nThe other interesting thing to note here is that Dynamo removed the\nsecond argument to the function. Instead, it treated it as a constant\nand recorded the result of the operation `n + 1` in the graph. This is\nanother feature of Dynamo: Dynamo will treat as constant any non-tensor\nvalue… other than ints. Let’s see now how are ints special.\n\nThe last defining property of Dynamo is that it knows how to handle\ndynamic shapes. Symbolic shapes refer to Dynamo’s ability of tracing\nshapes, and more generally, integers, rather than leaving them as\nconstants. This allows for avoiding recompilations and deploying generic\nmodels that work for any size in production. The main examples of places\nwhere dynamic shapes appear are the batch size, where we might train a\nmodel with a fixed batch size but then perform inference for an\narbitrary batch size, or the variable sequence length that one\nencounters when processing text or audio.\n\nWe can see this by executing a few more times the example above\n\n```python\nimport torch\n\n@torch.compile\ndef fn(x, n):\n    y = x ** 2\n    if n >= 0:\n        return (n + 1) * y\n    else:\n        return y / n\n\nx = torch.randn(200)\nfn(x, 2)\nfn(x, 3)\nfn(x, -2)\n```\n\nIn this case, `TORCH_LOGS=graph_code` generates two more graphs\n\n```python\n# Graph for n==2 omitted\n\ndef forward(self, l_x_: torch.Tensor, l_n_: torch.SymInt):\n    # File: a.py:5, code: y = x ** 2\n    y = l_x_ ** 2\n\n    # File: a.py:7, code: return (n + 1) * y\n    add = l_n_ + 1\n    mul = add * y\n    return (mul,)\n```\n\n```python\ndef forward(self, l_x_: torch.Tensor, l_n_: torch.SymInt):\n    # File: a.py:5, code: y = x ** 2\n    y = l_x_ ** 2\n\n    # File: a.py:9, code: return y / n\n    truediv = y / l_n_\n    return (truediv,)\n```\n\nDynamo detected that one integer changed its value after the first call\nand started tracing it. We see that these graphs are generic, and trace\nthe variable `n` symbolically via an object of type `SymInt`.\n\nIf after these calls we call `fn(x, 4)`, Dynamo would not recompile,\nbut rather reuse the graph that was already traced.\n\nTo summarize: 1. Dynamo is a Python tracer 2. Given some inputs, it\nreturns an FX graph with the PyTorch functions that were executed 3. It\ncan also trace integers if it detects that they changed between calls 4.\nIt specializes any other value that is not a tensor or a scalar\n\nOf course, Dynamo does many more things, like figuring out when it needs\nto retrace, rewriting the bytecode of the function, implementing graph\nbreaks… To keep the introduction short, we will incrementally discuss\nall these in the sequel.",
    "1827": "一级标题：Dynamo Deep-Dive\n二级标题：PEP 523: Adding a frame evaluation API to CPython\n内容：\nImagine now that we are given the task to implement Dynamo. Where would\nwe even start? Rather conveniently for us, [PEP\n523](https://peps.python.org/pep-0523/) was released with Python 3.6.\nThis PEP [was\ndesigned](https://peps.python.org/pep-0523/#a-jit-for-cpython) to\nallow third parties to create JIT compilers for Python. Let’s see how.\n\n**A note on CPython**: CPython is internally implemented as a [stack\nmachine](https://en.wikipedia.org/wiki/Stack_machine). A Python\nprogram is compiled into\n[bytecodes](https://en.wikipedia.org/wiki/Bytecode) that then are\nexecuted by this interpreter. To learn more about these bytecodes, see\nthe [dis module](https://docs.python.org/3/library/dis.html) from the\nstandard library. See also [the developer\ndocs](https://devguide.python.org/internals/interpreter/) for an\nintroduction to CPython’s interpreter. We will assume that the reader is\nfamiliar with the notion of a stack machine.\n\nPEP 523 exposes an API where a user can add a custom per-function\ninterpreter. Then, CPython will use this interpreter rather than its own\nto execute the function. In order to be able to execute the function, on\nentry, CPython provides the custom interpreter with things like - The\nbytecode of the function - The value of the arguments of the function\n(i.e., the local variables) and their names - The value of the global\nvariables and their names - The builtin functions like `abs` or\n`print`\n\nYou can see all the fields\n[here](https://github.com/pytorch/pytorch/blob/e891a3bba9f05697d72776f6e89347231a141f03/torch/csrc/dynamo/eval_frame.c#L50-L59). [^2]\n\nIn summary, CPython provides the user’s interpreter with all the\ninformation necessary to execute the function. [^3]\n\nWith this API, we can implement a tracer by implementing an interpreter\nthat runs the code and records in a graph all the PyTorch operations\nthat occur during this execution. This is exactly what Dynamo does.\n\nDynamo uses this CPython API to parse all these objects and packs them\ninto [a Python\nstructure](https://github.com/pytorch/pytorch/blob/e891a3bba9f05697d72776f6e89347231a141f03/torch/csrc/dynamo/eval_frame.c#L93-L108).\nAfter it has done so… it goes back from C to python. Other than for this\npiece of code that communicates with CPython, Dynamo is fully\nimplemented in Python.\n\nIt should be clear that it is the decorator `@torch.compile`’s job\nto install the necessary scaffolding that will pass the bytecode, the\nargs, global variables and so on to Dynamo when the function is called.\nAgain, `@torch.compile` does not actually compile anything.",
    "1828": "一级标题：Dynamo Deep-Dive\n二级标题：Implementing CPython in Python\n内容：\nSo, we are back in the Python world. We have the bytecode of a function,\nand all the context necessary to execute it. In particular, we have\nlanded at\n[_convert_frame_assert](https://github.com/pytorch/pytorch/blob/b6df8414601e1e086e830ca9e919e7fdc8874e71/torch/_dynamo/convert_frame.py#L272-L274).\nThis is the function that the decorator `torch.compile` returns! We\nget to this function from\n[_dynamo.optimize](https://github.com/pytorch/pytorch/blob/b6df8414601e1e086e830ca9e919e7fdc8874e71/torch/_dynamo/eval_frame.py#L715-L727).\nThe decorator `torch.compile` is just a nice API around\n`_dynamo.optimize`.\n\nBefore getting into implementing a Python interpreter, we want to define\nan [IR](https://en.wikipedia.org/wiki/Intermediate_representation).\nIn particular, we want to wrap all the local and global variables in our\nown internal classes. This allows us to better track these objects and\ngroup together objects that can be treated in the same way to the eyes\nof Dynamo.\n\nThe parent class of the internal class structure is `VariableTracker`\nand represents the different objects that Dynamo understands. For\nexample, `ListVariable`, represents a `list` object, and keeps\ninternally a [list of VariableTrackers](https://github.com/pytorch/pytorch/blob/e38a3a6079a3861b4bc9f256120ec661f34e726d/torch/_dynamo/variables/lists.py#L48-L56).\nAnother example of `VariableTracker` is\n[ConstantVariable](https://github.com/pytorch/pytorch/blob/83c0763dda1f93c6cf552ba88260a0dc7a3ecb70/torch/_dynamo/variables/constant.py#L30).\nConstantVariable wraps all the [objects considered constant by\nDynamo](https://github.com/pytorch/pytorch/blob/83c0763dda1f93c6cf552ba88260a0dc7a3ecb70/torch/_dynamo/variables/constant.py#L98-L107).\nWe also have special subclasses for objects that require special\nattention, like\n[TensorVariable](https://github.com/pytorch/pytorch/blob/83c0763dda1f93c6cf552ba88260a0dc7a3ecb70/torch/_dynamo/variables/tensor.py#L68-L69).\nAll these internal classes are defined in the\n[torch/_dynamo/variables](https://github.com/pytorch/pytorch/tree/83c0763dda1f93c6cf552ba88260a0dc7a3ecb70/torch/_dynamo/variables)\nfolder.\n\nPython objects are wrapped into their corresponding `VariableTracker`\nclass in\n[VariableBuilder._wrap](https://github.com/pytorch/pytorch/blob/83c0763dda1f93c6cf552ba88260a0dc7a3ecb70/torch/_dynamo/variables/builder.py#L365).\nThis function is just a very long chain of `elif`s that tries to\nrecursively pattern-match the Python inputs into the appropriate type of\n`VariableTracker`.\n\n**Debugging tip**. When we get unexpected results from dynamo, it is\nsometimes caused by the builder. If the logic of the builder is wrong,\nsometimes Dynamo may wrap a variable in the incorrect\n`VariableTracker` type, and this may cause issues later on. It is\nrather useful to have a look at the `VariableTracker` types that\nappear in the errors, and the `VariableTracker` method that throws the\nexception when you encounter a Dynamo error. In particular, sometimes we\nfind that an object is tracked as a `UserDefinedObjectVariable` (this\nis Dynamo’s catch-all class), when it should have been tracked as\nsomething more specific. In these cases, the `VariableBuilder`\nlogic is often to blame.\n\n**Debugging tip**. When running a program with `TORCH_LOGS=dynamo`,\none of the artifacts that are printed out is lines of the form\n\n```\nTRACE LOAD_GLOBAL y [TorchInGraphFunctionVariable(<built-in method any>), TensorVariable()]\n```\n\nThis is the bytecode for the original program and the state of the stack\nat that point. This is very useful to find where an object was not\ntraced into the right `VariableTracker`.\n\nOk, so we have an IR for our tracer, now we *just* need to reimplement\nCPython’s stack machine. This is implemented by\n[InstructorTranslatorBase](https://github.com/pytorch/pytorch/blob/69f112d5867f785a3a090a0c6d6644ae047033ac/torch/_dynamo/symbolic_convert.py#L576-L594)\nin\n[symbolic_convert.py](https://github.com/pytorch/pytorch/blob/69f112d5867f785a3a090a0c6d6644ae047033ac/torch/_dynamo/symbolic_convert.py).\n\n`InstructionTranslatorBase` has about 200 methods, implementing almost\nall of Python bytecodes. As an example, we can see the implementation of\n`BUILD_LIST`\n\n```python\ndef BUILD_LIST(self, inst):\n    items = self.popn(inst.argval)\n    self.push(ListVariable(items, mutation_type=ValueMutationNew()))\n```\n\nThis is the bytecode generated by constructions like `l = [2, 3, 4]`.\nIn this case, since there are three elements, the generated bytecode is\n`BUILD_LIST 3`. This means that we pop the top `3` elements of the\nstack and push a new list object to the top of the stack formed by these\nthree elements.",
    "1829": "一级标题：Dynamo Deep-Dive\n二级标题：Generating the Output Graph\n内容：\nWith a way to symbolically execute Python code, we are set to extract\nthe PyTorch operations that happen during the symbolic execution of a\nprogram given some inputs. This is implemented in Dynamo via the\n[OutputGraph](https://github.com/pytorch/pytorch/blob/69f112d5867f785a3a090a0c6d6644ae047033ac/torch/_dynamo/output_graph.py#L221-L230)\nobject. The `OutputGraph` object is [bound to an\n`InstructionTranslator object](https://github.com/pytorch/pytorch/blob/69f112d5867f785a3a090a0c6d6644ae047033ac/torch/_dynamo/symbolic_convert.py#L2060-L2071)\nand it tracks all the data necessary to create the FX graph which will\nbe returned by Dynamo.\n\nAll the inputs and intermediary elements of the FX graph are\n`fx.Node`s. In Dynamo, `fx.Node`s are wrapped in\n`fx.Proxy`s. `fx.Proxy`s are used to build the FX graph.\nIn particular, they record every PyTorch operation performed on them\ninto the graph. You can create a new operation to be added to\nthe graph by calling [create_proxy](https://github.com/pytorch/pytorch/blob/fb80f05ee2e1cba17892980701bfd5dbce58349f/torch/_dynamo/output_graph.py#L430-L431).\nThen, we can add it to the graph through the function\n[wrap_fx_proxy](https://github.com/pytorch/pytorch/blob/fb80f05ee2e1cba17892980701bfd5dbce58349f/torch/_dynamo/variables/builder.py#L1311).\n\nA graph stores operations on tensors… and operations on symbolic\nintegers. We will discuss symbolic integers later on, but first we will\ndiscuss how Dynamo addresses a rather important correctness issue.\n\n(making-dynamo-sound-guards)=",
    "1830": "一级标题：Dynamo Deep-Dive\n二级标题：Making Dynamo Sound: Guards\n内容：\nAt this point, we have a way to trace programs completely disregarding control flow.\nAnd for that, we have reimplemented all of CPython… If this sounds like a bit of an\noverkill, that is because it is.\n[torch.jit.trace](https://pytorch.org/docs/main/generated/torch.jit.trace.html)\nalready implements this without all this machinery, so what gives?\n\nThe issue with `torch.jit.trace`, as it is warned in its docs, is that\nit just works if the traced program is not data dependent. In other\nwords, it will just work if the program itself is linear. This means\nwriting our program without using if-elses, for-while loops, exceptions.\nEven more, none of the libraries that we use can use any control flow!\nAll in all, not using control flow in a language as dynamic as Python\nis, in fact, a huge constraint.\n\nJAX solves this problem by always retracing and caching the graph after\nretracing. Dynamo, on the other hand, uses guards to avoid retracing the\nwhole program every time.\n\nA **guard** is an assumption (a boolean expression on an input) made in\norder to specialize a frame for one set of example inputs. Reusing the\ngraph is only valid if these assumptions hold on the new inputs.\n\nFor example, any constant input to a function, like a string, installs a\nguard stating that that input should be of type `str` and equal to the\nstring we passed. Running\n\n```python\nimport torch\n\n@torch.compile\ndef fn(a, b):\n    return a * len(b)\n\nfn(torch.arange(10), \"Hello\")\n```\n\nwith `TORCH_LOGS=guards` prints (among other guards)\n\n```python\n___check_type_id(L['b'], 94334122025024)\nL['b'] == 'Hello'\n```\n\nThis reads as “the local variable `b` should have a specific type\n(`str` in this case, represented by the constant `9433...`) and\nits value should be `'Hello'`”. If we then execute the function\nagain passing a different argument\n\n```python\nimport torch\n\n@torch.compile\ndef fn(a, b):\n    return a * len(b)\n\nfn(torch.arange(10), \"Hello\")\nfn(torch.arange(10), \"Hi\")\n```\n\nwe can see the guard that failed by running `TORCH_LOGS=recompiles`\n\n```python\nRecompiling function fn in script.py:3\ntriggered by the following guard failure(s):\n     - L['b'] == 'Hello'\n```\n\nGuards are accumulated while [the inputs to the function are wrapped in\nthe\nbuilder](https://github.com/pytorch/pytorch/blob/69f112d5867f785a3a090a0c6d6644ae047033ac/torch/_dynamo/variables/builder.py#L808-L810)\nand [during the execution of the\nprogram](https://github.com/pytorch/pytorch/blob/69f112d5867f785a3a090a0c6d6644ae047033ac/torch/_dynamo/variables/dicts.py#L763-L769).\nWe will show many more examples of guards in the next section, but first\nlet us discuss sources.\n\nA **source** tracks how to reconstruct a variable from the original\nlocal or global variables present when entering the current frame. In\nparticular, it tracks the original local and global objects and any of\nthe objects they contain. In\n\n```python\ndef foo(x: Tensor, y: List[Tensor]):\n    a = x * y[0]\n    return a * x\n```\n\n`x` and `y` have\n[LocalSource](https://github.com/pytorch/pytorch/blob/40dc0580a69565b06ec5263efe5d87cecc8200f7/torch/_dynamo/source.py#L80-L92)\nas their source, and `y[0]` has\n[GetItemSource](https://github.com/pytorch/pytorch/blob/40dc0580a69565b06ec5263efe5d87cecc8200f7/torch/_dynamo/source.py#L302),\nwhich stores a `LocalSource` inside. On the other hand, `a` will not\nhave a source as it is an intermediate variable that only exists within\nthe fx graph.\n\nAll these are defined in\n[torch/_dynamo/source.py](https://github.com/pytorch/pytorch/blob/main/torch/_dynamo/source.py).\nWe can see the guard generated by `GetItemSource` in the following\nexample:\n\n```python\nimport torch\n\n@torch.compile\ndef fn(x, l):\n    return x * len(l[0])\n\nfn(torch.randn(8), [\"Hi\", \"Hello\"])\n```\n\ngenerates the following guards\n\n```python\n___check_type_id(L['l'], 94439025877664)\nlen(L['l']) == 2\n___check_type_id(L['l'][0], 94439025840192)\nL['l'][0] == 'Hi'\n___check_type_id(L['l'][1], 94439025840192)\nL['l'][1] == 'Hello'\n```\n\nHere, we see the code generated by `GetItemSource` (`[0]` and\n`[1]`) wrapping a `LocalSource` (`L['l']`).\n\nAt this point, with sources and guards, we are able to implement a\ncaching system to avoid recompilation without having to retrace every\ntime. We will discuss a bit more in detail this caching system in the\nsequel.\n\nThe attentive reader will have noticed that this does not explain yet\nwhy we need to have such fine control over the Python interpreter as to\nhaving to reimplement it. The examples of guards that we have shown\ndepend on the input objects, so we could still compute these before\nexecuting the function. In other words, we could implement this guard\nsystem on top of `torch.jit.trace` and get the same functionality with\nmuch less effort… Enter symbolic shapes.",
    "1831": "一级标题：Dynamo Deep-Dive\n二级标题：Symbolic Shapes\n内容：\nAnother point we discussed in the introduction is that Dynamo knows how\nto trace integers. In order to implement this, we use a symbolic class\n[torch.SymInt](https://github.com/pytorch/pytorch/blob/fb80f05ee2e1cba17892980701bfd5dbce58349f/torch/__init__.py#L244-L249)\nthat acts like an `int` but it records all the operations performed on\nit in the output FX graph. [^4] We already saw this class in the introduction\nwhen introducing symbolic integer tracing.\n\nLet us now discuss the three properties that define symbolic shape\ntracing in Dynamo, and how to implement them.\n\n### Static by default\n\nDynamo assumes that every integer, let that be an input or the shape of\na tensor, is static by default. In other words, no integers will be\ntraced on the first execution of a function. Then, only if it detects\nthat an integer or a shape changed value during the execution, it will\ntrace it and generate a graph generic on that variable.\n\nWe already saw this behavior in the introduction using integers. Let us\nnow look at an example using shapes of tensors.\n\n```python\nimport torch\n\n@torch.compile\ndef fn(a, b):\n    return a.shape[0] * a * b\n\nfn(torch.randn(4, 3), torch.randn(4, 3))\nfn(torch.randn(8, 3), torch.randn(8, 3))\n```\n\nRunning this program with `TORCH_LOGS=graph_code` we see that these\ntwo calls are traced as\n\n```python\ndef forward(self, l_a_: torch.Tensor, l_b_: torch.Tensor):\n    mul = 4 * l_a_\n    mul_1 = mul * l_b_\n    return (mul_1,)\n\ndef forward(self, s0: torch.SymInt, l_a_: torch.Tensor, l_b_: torch.Tensor):\n    size = l_a_.size()\n    getitem = size[0]\n    mul = getitem * l_a_\n    mul_1 = mul * l_b_\n    return (mul_1,)\n```\n\nIn the first graph the shape is traced as a constant, but once it\nchanges, it traces it symbolically using a `SymInt`s. In general, a\nsimpler way to see the shapes of the intermediary values is by running\nthe program with `TORCH_LOGS=graph_sizes`\n\n```\nTRACED GRAPH TENSOR SIZES\n===== __compiled_fn_1 =====\nl_a_: (s0, 3)\nl_a_ (concrete): (8, 3)\nl_b_: (s0, 3)\nl_b_ (concrete): (8, 3)\nmul: (s0, 3)\nmul (concrete): (8, 3)\nmul_1: (s0, 3)\nmul_1 (concrete): (8, 3)\n```\n\nwhere we can see that the first dimension of the two tensor args is\ndynamic, given that it is represented by the `s0` variable.\n\nWe can find how Dynamo implements this by running `TORCH_LOGS=guards`\n\n```python\n# Guards first call\ncheck_tensor(L['a'], torch.float32, device=None, requires_grad=False, size=[4, 3], stride=[3, 1])\ncheck_tensor(L['b'], torch.float32, device=None, requires_grad=False, size=[4, 3], stride=[3, 1])\n\n# Guards second call\ncheck_tensor(L['a'], torch.float32, device=None, requires_grad=False, size=[None, 3], stride=[3, 1])\ncheck_tensor(L['b'], torch.float32, device=None, requires_grad=False, size=[None, 3], stride=[3, 1])\n\nL['b'].size()[0] == L['a'].size()[0]\n2 <= L['a'].size()[0]\n```\n\nWe see that on the first call, the guards check that the tensors have\nsome fixed sizes and strides. These guards fail in the second execution,\nso it retraces. Since it was an `int` guard that failed, in this\nsecond iteration it traces this `int` symbolically and it installs\nmore general guards on this more generic kernel.\n\n**Compilation performance tip**. If you know that a dimension will vary\nin size, you can mark it as dynamic by calling\n[torch._dynamo.mark_dynamic](https://github.com/pytorch/pytorch/blob/66a76516bfc341b2b55bb2056d2faa9c2de46d69/torch/_dynamo/decorators.py#L176)\nbefore calling `torch.compile`. This will avoid the first compilation\nwith a static shape. There are other useful utility functions like\n`maybe_mark_dynamic` or `mark_static`. You can also have all\nintegers and shapes traced by calling `torch.compile(dynamic=True)`.\nThis is mostly useful for debugging purposes.\n\n### 0, 1 are always specialized\n\nRegardless of whether we mark a dimension as dynamic, if we pass an input\nwhere that dimension is 0 or 1, Dynamo will trace it as non-dynamic and it\nwill generate a specific graph for it. This is the reason why in the example\nabove we find guards of the form `2 <= L['a'].size()[0]`.\n\nThere are several reasons for this choice. There are two particularly\nimportant - A tensor is empty if and only if any of its dimensions is\nzero - A tensor can only be contiguous if one of the strides is one\n\nThis policy decision does NOT apply to plain Python ints; if we think a Python\nint should be compiled dynamically, we won't specialize them by default;\ninstead, whether or not it gets specialized depends on its usage.\n\n### Duck shaping\n\nDynamo performs what we call “duck shaping”. If two dynamic integers\nhave the same value at trace time, we will assume that they are equal\nand guard on it. Effectively, this means that rather than having two\nsymbols `s0`, `s1` in the example above, we just unified them to\n`s0` and had the guard `L['b'].size()[0] == L['a'].size()[0]`. This\nenables performing fusions within the compiler while being able to\ngenerate kernels that are generic enough.\n\n### Guards on symbolic ints\n\nWe now understand how symbolic shapes are implemented at a high level\nand the properties they have. Now, why is that symbolic shapes forced us\nthrough the tricky route of getting control of the CPython interpreter?\nConsider the following example:\n\n```python\nimport torch\n\n@torch.compile(dynamic=True)\ndef fn(a):\n    if a.shape[0] * 2 < 16:\n        return a\n    else:\n        return a + 1\n\nfn(torch.randn(8))\n```\n\nThis code has a guard of the form `2*L['a'].size()[0] >= 16`. This is\na non-trivial guard in terms of the inputs of the function, but it is\nregistered in the middle of the execution of the program. Even more so,\nwe cannot know this guard is needed until we see the `if` statement\nconditional on a `SymNodeVariable` argument. Such conditions are\ninvisible to `torch.jit.trace` and require deep analysis of the python\ncode.\n\n**Debugging tip** Running this code with `TORCH_LOGS=dynamo` tells us\nwhere this guard was added\n\n```\neval 2*s0 >= 16 [guard added] at script.py:5 in fn (_dynamo/variables/tensor.py:812 in evaluate_expr)\n```\n\nPlacing a breakpoint there and looking at the backtrace is rather useful\nto understand where a guard came from.",
    "1832": "一级标题：Dynamo Deep-Dive\n二级标题：Making Dynamo Complete: Graph Breaks\n内容：\nWith all the tools we have discussed, we have a tracer that can trace\nPyTorch operations on tensors and integers and has a caching system that\nknows when it can reuse a previously traced graph and when it needs to\nretrace. All this executing arbitrary Python code!\n\nThere is just one small issue with this. The statement “executing\narbitrary Python code” is perhaps a bit too general. Dynamo implements a\ngood part of Python, but does it implement the more complex parts, like\ncoroutines or async? Does it implement the whole Python standard\nlibrary? NumPy also has a Python API. Does `torch.compile` also\nunderstand NumPy? and Django? [^5]\n\nPython’s ecosystem is massive, and a good part of it is written in other\nmore performant languages like C++ or Rust, and it just exposes Python\nbindings. There is no hope in Dynamo tracing through Python objects that\nare implemented in C++. What can a tracer do when it finds an operation\nthat it does not understand?\n\nThe usual way machine learning tracers handle this issue is by informing\nthe user that the operation they choked on and giving up tracing\naltogether. This would pose a real usability issue in the case of\nPyTorch, where its users are used to the flexibility it gives them. As a\nreal-world example the `doctr_det_predictor` model uses NumPy and the\n`cv2` library to [postprocess the model’s\nresult](https://github.com/mindee/doctr/blob/f2114758d529ed8d3d0030581638f0520b6b98d8/doctr/models/detection/core.py#L86).\n\nHere is another place where having access to CPython is interesting.\nRather than erroring out, Dynamo can let CPython run that problematic\ncode! To do this, Dynamo generates at trace time one graph with all the\noperations before the problematic code, and one with all the operations\nafter. [^6] Then, at runtime, it will delegate to CPython to execute the\nfirst graph, then the problematic code, and then the second graph. This\nprocess of stopping the tracing and generating multiple graphs is called\na **graph break**.\n\nA small confession: I lied all throughout the introduction and the first\nsections. Dynamo does not generate one graph, but **multiple graphs**!\nFor all practical purposes, starting retracing after a second graph can\nbe thought of as starting tracing a new function. The new graph after\nthe graph break will have its own guards, its new set of local\nvariables, and so on.\n\nTo discuss how to implement graph breaks, we need to first revisit how\nDynamo interacts with CPython. Using PEP 523, CPython allows a user to\nuse their own frame evaluation mechanism. What we had not discussed is\nthat CPython also exposes its own frame evaluation for others to use.\nDynamo leverages this to let the fast CPython interpreter run the\ncompiled code. For a function without graph breaks, the whole tracing /\nexecution process of a program that calls the function 2 times with the\nsame arguments looks like this:\n\n1. In the first call to the function\n\n   1. Dynamo traces the function into an FX graph\n\n      1. The FX graph is compiled by the compiler (Inductor) into\n         efficient low-level code… but that’s a story for another day\n\n   2. It rewrites the bytecode of the function so that it simply calls\n      the compiled function\n\n   3. It gives CPython this new bytecode and asks it to run it\n      [here](https://github.com/pytorch/pytorch/blob/e891a3bba9f05697d72776f6e89347231a141f03/torch/csrc/dynamo/eval_frame.c#L1006)\n\n2. In the second call to the function\n\n   1. It checks the guards from the first call against the new arguments\n      [here](https://github.com/pytorch/pytorch/blob/e891a3bba9f05697d72776f6e89347231a141f03/torch/csrc/dynamo/eval_frame.c#L658).\n      Since they are the same arguments as before, they pass\n   2. It asks CPython to run the bytecode associated to those guards\n      [here](https://github.com/pytorch/pytorch/blob/e891a3bba9f05697d72776f6e89347231a141f03/torch/csrc/dynamo/eval_frame.c#L972-L975)\n\nThis process on its own looks overly complicated. Why generate new\nbytecode and ask CPython to run it rather than simply creating a C++\nbinding to the compiled function and executing it? Well, this pattern\nallows us to implement graph breaks! The bytecode generated by a graph\nbreak has the following structure:\n\n1. Bytecode that executes the first graph\n2. Bytecode that leaves the stack as it would be if CPython would have\n   executed the first graph. It also replays any modifications to local\n   or global variables that would be visible at this point\n3. The bytecode that made Dynamo graph break\n4. Bytecode that executes the second graph\n\nLet us see this in a simple example\n\n```python\nimport torch\n\n@torch.compile\ndef fn(a):\n    b = a + 2\n    print(\"Hi\")\n    return b + a\n\nfn(torch.randn(4))\n```\n\nRunning this with `TORCH_LOGS=bytecode` shows us the initial bytecode\nand the modified bytecode\n\n```python\nMODIFIED BYTECODE fn script.py line 3\n 0 LOAD_GLOBAL              1 (__compiled_fn_0)\n 2 LOAD_FAST                0 (a)\n 4 CALL_FUNCTION            1\n 6 STORE_FAST               3 (graph_out_0)\n 8 LOAD_GLOBAL              0 (print)\n10 LOAD_CONST               2 ('Hi')\n12 LOAD_FAST                3 (graph_out_0)\n14 LOAD_CONST               3 (0)\n16 BINARY_SUBSCR\n18 STORE_FAST               1 (b)\n\n20 CALL_FUNCTION            1\n22 LOAD_GLOBAL              2 (__resume_at_14_1)\n24 ROT_TWO\n26 LOAD_FAST                0 (a)\n28 LOAD_FAST                1 (b)\n30 CALL_FUNCTION            3\n32 RETURN_VALUE\n\nMODIFIED BYTECODE resume_in_fn script.py line 6\n 0 LOAD_GLOBAL              1 (__compiled_fn_2)\n 2 LOAD_FAST                2 (b)\n 4 LOAD_FAST                1 (a)\n 6 CALL_FUNCTION            2\n 8 UNPACK_SEQUENCE          1\n10 RETURN_VALUE\n```\n\nWe can see that the modified bytecode is split into two functions,\n`fn`, the original function, and a function called `resume_in_fn`.\nThis second function is a function created by Dynamo to implement the\nexecution of the program starting at the graph break. This is often\ncalled a [continuation\nfunction](https://en.wikipedia.org/wiki/Continuation). This\ncontinuation function simply calls the second compiled function with the\nright arguments. The code for the initial function is rewritten\nimplementing the strategy that we described before\n\n- L0-4. Call the compiled function (`a + 2`).\n- L6. Store its result in a local variable called `graph_out_0`.\n  `graph_out_0` is a tuple\n- L8-18. Leave the stack as it would be at the point of the graph break\n- L20. Execute the code that caused the graph break\n- L22-32. Call the compiled continuation function (`a + b`)\n\nThe code generation of the stack in Dynamo is delegated to\n`VariableTracker` subclasses. Every `VariableTracker` object in\nDynamo has a [reconstruct](https://github.com/pytorch/pytorch/blob/e891a3bba9f05697d72776f6e89347231a141f03/torch/_dynamo/variables/lists.py#L307-L309)\nmethod that generates the necessary bytecode to create the python object\nit represents on the stack.\n\n**Debugging tip**. Graph breaks hamper performance, and as such, it is\nbest to avoid them. Running a program with `TORCH_LOGS=graph_breaks`\nis a great way to find how many graph breaks did our program hit. The\ninformation it returns is in terms of `VariableTracker` objects, so\nthe debugging tips above are sometimes also helpful to figure out what\ncaused that graph break.",
    "1833": "一级标题：Dynamo Deep-Dive\n二级标题：Conclusion\n内容：\nDynamo is a complex piece of software. Once you sign up to implement a\nCPython interpreter you know you are in for a ride. That being said, we\nhope that this post helps demystify it a bit.\n\nDynamo is (mostly) implemented in Python. We left plenty of links to the\npieces of the code that we discussed. We hope that reading those pieces\nof code and grepping for the places that call them, or putting\nbreakpoints on them and looking at the call stack helps understanding\nthe rest of the code base.\n\nOf course, the best way to learn how a piece of software works is by\nextending it. In this case, the best way is to have a look at the [open\ndynamo issues on\ngithub](https://github.com/pytorch/pytorch/issues?q=is%3Aissue+is%3Aopen+label%3A%22module%3A+dynamo%22+).\nMany of them require very minor changes in the code, once you find where\nyou need to make those changes.",
    "1834": "一级标题：Dynamo Deep-Dive\n二级标题：Footnotes\n内容：\nBelow are additional details and references for concepts mentioned in this document.\n\n[^1]: In the literature, this is called a Directed Acyclical Graph (DAG).\n\n[^2]: All this binding code lives in `torch/csrc/dynamo/eval_frame.c`.\n\n[^3]: In CPython lingo, the set of all these objects are called [a\n    frame](https://github.com/python/cpython/blob/f26bfe4b25f7e5a4f68fcac26207b7175abad208/Include/internal/pycore_frame.h#L57-L71).\n\n[^4]: There are also `SymBool` and `SymFloat` classes. The latter one\n    is not used all that much at the time of this writing.\n\n[^5]: Interestingly enough, it does understand NumPy code! Have a look at\n    [this blogpost](https://pytorch.org/blog/compiling-numpy-code/)\n    and [the docs](https://pytorch.org/docs/main/torch.compiler_faq.html#does-numpy-work-with-torch-compile).\n    Now, this is just possible because we reimplemented NumPy using\n    PyTorch. Good luck implementing Django in PyTorch though…\n\n[^6]: Assuming there is just one piece of problematic code. If there are\n    more, Dynamo can split the code into as many graphs as it needs.",
    "1835": "一级标题：Dynamo Overview\n二级标题：无\n内容：\nBefore you read this section, read {ref}`torch.compiler_overview`.\n\nTorchDynamo (or simply Dynamo) is a Python-level Just-In-Time (JIT) compiler designed to make\nunmodified PyTorch programs faster. Dynamo hooks into the frame evaluation\nAPI in CPython ([PEP 523](https://peps.python.org/pep-0523/)) to\ndynamically modify Python bytecode right before it is executed. It\nrewrites Python bytecode to extract sequences of PyTorch\noperations into an [FX Graph](https://pytorch.org/docs/stable/fx.html)\nwhich is then compiled with a customizable backend.\nIt creates this FX Graph through bytecode analysis and is designed to\nmix Python execution with compiled backends to get the best of both\nworlds — usability and performance.\n\nDynamo makes it easy to experiment with different compiler\nbackends to make PyTorch code faster with a single line decorator\n`torch._dynamo.optimize()` which is wrapped for convenience by `torch.compile()`\n\nThe following diagram demonstrates how PyTorch works with `torch.compile`\nand without it:\n\n```{image} _static/img/dynamo/TorchDynamo.png\n```\n\n`TorchInductor` is one of the backends\nsupported by [Dynamo Graph](https://pytorch.org/docs/stable/fx.html)\ninto [Triton](https://github.com/openai/triton) for GPUs or\n[C++/OpenMP](https://www.openmp.org/) for CPUs. We have a\n[training performance dashboard](https://github.com/pytorch/torchdynamo/issues/681#issuecomment-1233828468)\nthat provides performance comparison for different training backends. You can read\nmore in the [TorchInductor post on PyTorch\ndev-discuss](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747).\n\nFor an in-depth overview, read the sections below, watch the deep-dive video,\nand check out the dev-discuss topics.\n\n- [Dynamo deep-dive video](https://www.youtube.com/watch?v=egZB5Uxki0I)\n- [dev-discuss topics](https://dev-discuss.pytorch.org/search?q=TorchDynamo%20order%3Alatest)",
    "1836": "一级标题：Dynamo Overview\n二级标题：Dynamo Internals\n内容：\n**Author**: [Jason Ansel](https://github.com/jansel) and [Kaichao You](https://github.com/youkaichao)\n\nThis section will go over some of the Dynamo internals and will\ndemonstrate how Dynamo works under the hood.\n\n### What is a guard?\n\nDynamo operates just-in-time and specializes graphs based on\ndynamic properties. Below is a basic example of how to use Dynamo.\nOne can decorate a function or a method using `torchdynamo.optimize` to enable\nDynamo optimization:\n\n```python\nfrom typing import List\nimport torch\nfrom torch import _dynamo as torchdynamo\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my_compiler() called with FX graph:\")\n    gm.graph.print_tabular()\n    return gm.forward  # return a python callable\n\n@torchdynamo.optimize(my_compiler)\ndef toy_example(a, b):\n    x = a / (torch.abs(a) + 1)\n    if b.sum() < 0:\n        b = b * -1\n    return x * b\nfor _ in range(100):\n    toy_example(torch.randn(10), torch.randn(10))\n```\n\nFor example, the first graph above has the following\nguards:\n\n```\nGUARDS:\nhasattr(L['a'], '_dynamo_dynamic_indices') == False\nhasattr(L['b'], '_dynamo_dynamic_indices') == False\nutils_device.CURRENT_DEVICE == None\n___skip_backend_check() or ___current_backend() == ___lookup_backend(140355900538256)\ncheck_tensor(L['a'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[10], stride=[1])\ncheck_tensor(L['b'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[10], stride=[1])\n```\n\nIf any of those guards fail, the graph will be recaptured and\nrecompiled. The interesting guard there is `check_tensor`, which\nchecks the following `torch.Tensor` properties:\n\n- Python class of the tensor (tensor subclassing, etc)\n- dtype\n- device\n- requires_grad\n- dispatch_key (with thread-local includes/excludes applied)\n- ndim\n- sizes\\*\n- strides\\*\n\nThe full specialization mode allows the backend compiler to assume an\nentirely static graph. Unfortunately, most backends require this.\nOperators which return dynamic shapes will trigger a graph break when\nnot in dynamic shape mode.\n\n### What is Dynamo doing?\n\nIf you want to understand better what Dynamo is doing, you can run your code with:\n\n```\nTORCH_LOGS=\"+dynamo,guards,bytecode\"\n```\n\nIf you are not familiar with Python bytecode, you can add a decompiler hook\nto decompile the bytecode into human-readable source code. One available\ntool is [depyf](https://github.com/youkaichao/depyf). If you don't have\n`depyf` already installed, run `pip install depyf`. Then, add the\nfollowing code to install decompilation hooks before you run any code.\n\n```python\nimport depyf\ndepyf.install()\n```\n\nThis code triggers useful (but spammy) printouts.\n\nFor example, the printouts for the first graph in the `toy_example`\nare:\n\n```\n__compiled_fn_0 <eval_with_key>.1\nopcode         name     target                                                  args              kwargs\n-------------  -------  ------------------------------------------------------  ----------------  --------\nplaceholder    a        a                                                       ()                {}\nplaceholder    b        b                                                       ()                {}\ncall_function  abs_1    <built-in method abs of type object at 0x7f9ca082f8a0>  (a,)              {}\ncall_function  add      <built-in function add>                                 (abs_1, 1)        {}\ncall_function  truediv  <built-in function truediv>                             (a, add)          {}\ncall_method    sum_1    sum                                                     (b,)              {}\ncall_function  lt       <built-in function lt>                                  (sum_1, 0)        {}\noutput         output   output                                                  ((truediv, lt),)  {}\nORIGINAL BYTECODE toy_example example.py line 12\n 14           0 LOAD_FAST                0 (a)\n              2 LOAD_GLOBAL              0 (torch)\n              4 LOAD_METHOD              1 (abs)\n              6 LOAD_FAST                0 (a)\n              8 CALL_METHOD              1\n             10 LOAD_CONST               1 (1)\n             12 BINARY_ADD\n             14 BINARY_TRUE_DIVIDE\n             16 STORE_FAST               2 (x)\n 15          18 LOAD_FAST                1 (b)\n             20 LOAD_METHOD              2 (sum)\n             22 CALL_METHOD              0\n             24 LOAD_CONST               2 (0)\n             26 COMPARE_OP               0 (<)\n             28 POP_JUMP_IF_FALSE       19 (to 38)\n 16          30 LOAD_FAST                1 (b)\n             32 LOAD_CONST               3 (-1)\n             34 BINARY_MULTIPLY\n             36 STORE_FAST               1 (b)\n 17     >>   38 LOAD_FAST                2 (x)\n             40 LOAD_FAST                1 (b)\n             42 BINARY_MULTIPLY\n             44 RETURN_VALUE\nMODIFIED BYTECODE toy_example example.py line 12\n 12           0 LOAD_GLOBAL              3 (__compiled_fn_0)\n              2 LOAD_FAST                0 (a)\n              4 LOAD_FAST                1 (b)\n              6 CALL_FUNCTION            2\n              8 UNPACK_SEQUENCE          2\n             10 STORE_FAST               2 (x)\n             12 POP_JUMP_IF_FALSE       12 (to 24)\n             14 LOAD_GLOBAL              4 (__resume_at_30_1)\n             16 LOAD_FAST                1 (b)\n             18 LOAD_FAST                2 (x)\n             20 CALL_FUNCTION            2\n             22 RETURN_VALUE\n        >>   24 LOAD_GLOBAL              5 (__resume_at_38_2)\n             26 LOAD_FAST                1 (b)\n             28 LOAD_FAST                2 (x)\n             30 CALL_FUNCTION            2\n             32 RETURN_VALUE\npossible source code:\ndef toy_example(a, b):\n    __temp_1 = __compiled_fn_0(a, b)\n    x = __temp_1[0]\n    if __temp_1[1]:\n        return __resume_at_30_1(b, x)\n    return __resume_at_38_2(b, x)\nIf you find the decompiled code is wrong,please submit an issue at https://github.com/youkaichao/depyf/issues.\n```\n\nAt the top you can see the FX graph.\nNext, you see the original bytecode of the function, followed by the\nmodified bytecode generated by Dynamo, and the decompiled source\ncode for reference. Finally, you see the guards which we covered above.\n\nIn the modified bytecode, `__compiled_fn_0` is the return value of\n`my_compiler()` (the compiled graph). `__resume_at_30_1` and\n`__resume_at_38_2` are both generated continuation functions that pick\nup execution after a graph break (at bytecode offsets 30 and 38). Each\nof these functions take the form:\n\n```\n__resume_at_<offset>:\n    ... restore stack state if needed ...\n    JUMP_ABSOLUTE <offset> into toy_example\n    ... original bytecode of toy_example ...\n```\n\nBy generating this `resume_at` function, we force the remainder of the\nfunction to be executed in a new Python frame which recursively\ntriggers Dynamo to restart its capture once execution reaches that\npoint for the first time.\n\n### How to inspect artifacts generated by Dynamo?\n\nTo inspect the artifacts generated by Dynamo, there is an API `torch._dynamo.eval_frame._debug_get_cache_entry_list` that retrieves compiled code and guards out of a function's `__code__` object. A compiled function can have several cache entries, and each cache entry consists a generated function to check guards, and a `types.CodeType` object to keep the code to be executed if the guarding conditions are satisfied.\n\n```python\nfrom torch._dynamo.eval_frame import _debug_get_cache_entry_list, innermost_fn\ncache_entries = _debug_get_cache_entry_list(innermost_fn(toy_example))\ncache_entry = cache_entries[0]\nguard, code = cache_entry.check_fn, cache_entry.code\n# the guard takes the local variables of an input frame, and tells whether a re-compilation should be triggered.\nimport dis\ndis.dis(guard)\ndis.dis(code)\n```\n\nIf you know Python bytecode, you can understand the above output.\n\nFor the guard function, there is no need to inspect the bytecode. We can directly access its guarding conditions:\n\n```python\nfor code_part in guard.code_parts:\n    print(code_part)\n```\n\nThe output is:\n\n```\n___guarded_code.valid\n___check_global_state()\nhasattr(L['a'], '_dynamo_dynamic_indices') == False\nhasattr(L['b'], '_dynamo_dynamic_indices') == False\nutils_device.CURRENT_DEVICE == None\n___skip_backend_check() or ___current_backend() == ___lookup_backend(140215810860528)\n___check_tensors(L['a'], L['b'], tensor_check_names=tensor_check_names)\n```\n\nOnly when all the conditions are satisfied, the guard function returns true, and the compiled code is executed.\n\nFor the compiled code, we cannot directly access its source but have to decompile it.\n\n```python\nfrom depyf import decompile\nprint(decompile(code))\n```\n\nThe output is:\n\n```\ndef toy_example(a, b):\n    __temp_1 = __compiled_fn_0(a, b)\n    x = __temp_1[0]\n    if __temp_1[1]:\n        return __resume_at_30_1(b, x)\n    return __resume_at_38_2(b, x)\n```\n\nSome names referenced in the code are:\n\n- Compiled functions, stored in the global namespace of the module containing the original function `toy_example`. These include names like `__compiled_fn_0` / `__resume_at_30_1` / `__resume_at_38_2`.\n- Closure variables used for checking guards. The names can be accessed from `guard.__code__.co_freevars`, and the values are stored in `guard.__closure__`. These include names like `___guarded_code` / `___is_grad_enabled` / `___are_deterministic_algorithms_enabled` / `___is_torch_function_enabled` / `utils_device` / `___check_tensors` / `tensor_check_names`.\n- Argument `L` of the `guard` function. This is a dict mapping the name of arguments of `toy_example` to its values. This is only available when the function is called, where the frame evaluation API comes into play. In short, `L` is a `dict` with structure of `{'a': value_a, 'b': value_b}`. Therefore, you can see the code uses `L['a']` to refer to the input variable `a`.\n\nThe graph break is shown in the code of compiled `toy_example`, where we have to use Python interpreter to select the following graph to execute.\n\nNote that we pass a simple `my_compiler` function as the backend compiler, therefore the subgraph code `__resume_at_38_2`, `__resume_at_30_1`, and `__compiled_fn_0` remain Python code. This can also be inspected (please ignore the function name, and only use the function signature and function body code):\n\n```python\nprint(\"source code of __compiled_fn_0:\")\nprint(innermost_fn(__compiled_fn_0).__self__.code)\nprint(\"=\" * 60)\nprint(\"source code of __resume_at_30_1:\")\nprint(decompile(__resume_at_30_1))\nprint(\"=\" * 60)\nprint(\"source code of __resume_at_38_2:\")\nprint(decompile(__resume_at_38_2))\n```\n\n```\nsource code of __compiled_fn_0:\ndef forward(self, L_a_ : torch.Tensor, L_b_ : torch.Tensor):\n    l_a_ = L_a_\n    l_b_ = L_b_\n    abs_1 = torch.abs(l_a_)\n    add = abs_1 + 1;  abs_1 = None\n    truediv = l_a_ / add;  l_a_ = add = None\n    sum_1 = l_b_.sum();  l_b_ = None\n    lt = sum_1 < 0;  sum_1 = None\n    return (truediv, lt)\n# To see more debug info, please use ``graph_module.print_readable()``\n============================================================\nsource code of __resume_at_30_1:\ndef <resume in toy_example>(b, x):\n    b = b * -1\n    return x * b\n============================================================\nsource code of __resume_at_38_2:\ndef <resume in toy_example>(b, x):\n    return x * b\n```\n\nHowever, if we use other backends like the built-in `inductor`, the subgraph code will be compiled CUDA kernels for GPU or C++ code for CPU.\n\nTo summarize, the compiled code is conceptually equivalent to the code below:\n\n```python\ndef compiled_example(a, b):\n    L = {'a': a, 'b': b}\n    for guard, code in get_cache_entries():\n        if guard(L):\n            return code(a, b)\n    recompile_and_add_another_cache_entry()\n```\n\nThe following diagram demonstrates how `torch.compile` transforms and optimizes user-written code: it first extracts computation graphs from the user-written function, and compiles these graphs into optimized functions, then assembles them into a new function, which is functionally equivalent to the user-written code but optimized to have a good computation speed.\n\n```{image} _static/img/dynamo/flowchart.jpg\n```\n\nTo learn more about how all this is implemented internally, see {ref}`torch.compiler_dynamo_deepdive`.",
    "1837": "一级标题：Fake tensor\n二级标题：无\n内容：\nCode: [fake_tensor.py](https://github.com/pytorch/pytorch/blob/db4572dbf18f1cf50cf662547e272d3117063747/torch/_subclasses/fake_tensor.py)",
    "1838": "一级标题：Fake tensor\n二级标题：Motivation\n内容：\nWhen doing Dynamo symbolic evaluation and compiler passes, we often want to be able to run tensor operations to understand what output sizes/dtypes/devices are, without actually running those operations (or trashing preexisting tensors), which would be slower (if you're doing a lot of compute) and take a lot of memory (it's bad if your compiler needs to use GPU memory while you are compiling the program). A fake tensor is like a real tensor in all respects, except that it doesn't actually have any data. For example, when we do Dynamo tracing, we need to trace through user Tensor code and answer questions about intermediates (e.g., if a user does a conditional on an intermediate tensor). Without fake tensor, we would not have accurate information for these queries.\n\nSimilarly, suppose you want to store metadata for a tensor, e.g., on an FX IR node (meta['val']). You can instead store a fake tensor directly on the node, which will give you all the metadata you need for the tensor, including subtle stuff that you probably wouldn't have handled (e.g., aliasing relationships).",
    "1839": "一级标题：Fake tensor\n二级标题：Related work\n内容：\n- A meta tensor is a tensor with device='meta'. This is actually a lot of what you want for fake tensor, but meta tensors don't model devices, and sometimes stride behavior varies depending on your device, so fake tensors really can get a lot more accurate info this way. Also, meta tensors are \"global\" (they exist on their own, similar to how a CPU/CUDA tensor exist on their own), whereas fake tensors are scoped to a FakeTensorMode.\n- A tensor subclass lets you subclass torch.Tensor and customize their behavior. Fake tensors are implemented as a tensor subclass; that means almost all of its implementation lives in Python! For more simple examples of tensor subclasses check out [subclass_zoo](https://github.com/albanD/subclass_zoo/).\n- Dynamic shapes allow you to create tensors with symbolic sizes rather than only concrete sizes, and propagate these sizes symbolically through operations. Dynamic shapes maintain state in a ShapeEnv, which is always associated with a FakeTensorMode (so fake tensors also are responsible for managing symbolic sizes.) In general, whenever we compile a subgraph with PT2, there is a tracing context associated with this compilation, which contains, among other things, a FakeTensorMode and (possibly) a ShapeEnv.",
    "1840": "一级标题：Fake tensor\n二级标题：Overall architecture\n内容：\nAll fake tensors are associated with a FakeTensorMode. Because fake tensor's primary use case is to do analysis on real tensors, the general workflow is you have a bunch of real tensors, you allocate a FakeTensorMode, and then you use from_real_tensor to convert all those real tensors into fake tensors, and then you do things to the fake tensors. In particular, the FakeTensorMode maintains a memo table persistently mapping tensors (and storages) to the same storages. If you fakeify the same tensor multiple times, you will get the same fake tensor; if you fakeify two tensors which alias each other, you will get two fake tensors which alias the same fake storage. FakeTensors are tensor subclasses, so if you do operations on them, you'll automatically get a fake tensor, but in general you will want to do operations on fake tensors (e.g., if you're running an FX pass) with the FakeTensorMode active; what a tensor operation will do is automatically turn on the fake tensor mode and try again.\n\nA fake tensor is represented as a \\_\\_torch_dispatch\\_\\_ tensor subclass of a meta tensor. This means under the hood, fake tensors are meta device tensors; they then use extra extensibility hooks, specifically dispatch_device, to lie about what the actual device of the tensor is. This was one of the more error-prone parts of fake tensors in the early days: sometimes, fake tensors were too good at lying about being CPU/CUDA whatever, and you'd end up with a CPU kernel getting called with a fake tensor trying to dereference the data pointer, which obviously won't work. If you are segfaulting in fake tensor code, this is the first thing you should check: is the C++ backtrace in a CPU kernel (unexpected!) or a meta kernel (expected!) A meta kernel is like a real kernel, but all it does is allocate the outputs, it doesn't do any data compute.\n\nA tensor subclass has to define how to implement various operations. Here is the general fake tensor recipe:\n\n- Run the meta kernel on the input fake tensors, reinterpreting them as meta tensors. This is done via a magic context manager in_kernel_invocation_manager which instructs all of PyTorch to view fake tensors as their underlying meta tensors, rather than \"unwrapping\" fake tensors into meta tensors (a fake tensor is a meta tensor). Fake tensors are represented this way to avoid having to keep two sets of metadata in sync (the meta tensor's metadata, and the fake tensor's metadata); the \"is a\" relationship ensures there is only one canonical copy of metadata.\n- If you're a factory function, you'll instead call the underlying factory function with device='meta'.\n- Convert the resulting meta tensor into a fake tensor, computing what the output device of the tensor should be (this is usually trivial, but sometimes it is not, e.g., cpu scalar promotion, or device-converting operations.)",
    "1841": "一级标题：Fake tensor\n二级标题：API: the important bits\n内容：\nNon-PT2 usage (check out test/test_fake_tensor.py for more examples):\n\n```python\n# Create a fake mode\nfrom torch._subclasses.fake_tensor import FakeTensorMode\nfake_mode = FakeTensorMode()\nconverter = fake_mode.fake_tensor_converter\n# Fakeify some real tensors\nfake_x = converter.from_real_tensor(fake_mode, x)\nwith fake_mode:\n    # Do some operations on the fake tensors\n    fake_y = fake_x * 2\n    # Factory operations automatically get fakeified in the context manager\n    fake_z = torch.empty(20)\n```\n\nQ: Why do you have real tensors as inputs?\n\nA: In a PT2 context, this is because you typically are compiling just-in-time, so for all the inputs to a graph you're compiling, you already have the \"real\" inputs, because you're compiling while you're executing the program.\n\nPT2 pre-AOTAutograd usage (this is unusual, you probably don't want to do this):\n\n```python\n# Fake mode is not enabled!\nfrom torch._guards import detect_fake_mode\nfake_mode = detect_fake_mode(args)\n# if fake_mode isn't None\nconverter = fake_mode.fake_tensor_converter\nfake_args = [converter.from_real_tensor(fake_mode, arg) for arg in args]\nwith fake_mode:\n    ... # do stuff with the fake args, if needed ...\n```\n\ndetect_fake_mode will search a number of locations to try to find \"the\" fake tensor mode associated with the lifecycle. Typically it will be pulled off of the tracing context.\n\nPT2 post-AOTAutograd usage:\n\n```python\n# Fake mode is enabled! example_inputs is typically fake already\n# TODO: we probably want to change this\n# Still do this to access fake mode\nfake_mode = detect_fake_mode(example_inputs)\n# But in general you don't have to turn it on\n```\n\nOther useful stuff:\n\n```python\nfrom torch._subclasses.fake_tensor import unset_fake_temporarily\nwith unset_fake_temporarily():\n    ... # fake mode is disabled here, you can do real tensor compute\n```\n\nWhen might you want to disable fake tensor mode? Usually you don't want to do this. One niche case where we've found it useful is to implement constant propagation on fake tensors: in this case, we need to do some actual tensor computation even though we're in a fake tensor mode.\n\n```python\nimport FakeTensorProp from torch.fx.passes.fake_tensor_prop\ngm: GraphModule\nreal_inputs: List[Tensor]\nFakeTensorProp(gm).propagate(*real_inputs)\n# This will populate meta['val'] on all the FX nodes with a fake tensor\n# or if you have a preexisting fake mode, you should use it\nFakeTensorProp(gm, mode=fake_mode).propagate(*real_inputs)\n# There is also propagate_dont_convert_inputs if your inputs are already fake\nfake_inputs: List[FakeTensor]\nFakeTensorProp(gm, mode=fake_mode).propagate_dont_convert_inputs(*fake_inputs)\n```",
    "1842": "一级标题：Fake tensor\n二级标题：Details\n内容：\nAuto-convert or not?\nOriginally, FakeTensorMode would not automatically fakeify real tensors if you tried to do compute on them inside a FakeTensorMode region. The motivation behind this was to prevent the following footgun:\n\n```python\nwith FakeTensorMode():\n    real_tensor.t_()\n```\n\nWhat should this code do? It would be surprising if we actually modified the metadata on the real tensor. But at the same time, there isn't any obvious opportunity to create a FakeTensor. So we conservatively decided to make this raise an error: \"Invoking operators with non-Fake Tensor inputs in FakeTensorMode is not yet supported. Please convert all Tensors to FakeTensors first.\"\n\nThis error is pretty annoying in practice. For example, suppose you have a real nn.Module and you want to feed fake tensors through it. You need to somehow fakeify the nn.Module. This motivated FakeCopyMode.\n\nEventually, we gave up and added automatic fakeification. However, this is still not yet enabled by default in many uses of FakeTensorMode.\n\nMetadata mutation on fake tensor\nIf you have a fake tensor, and you t\\_() it, the metadata on the fake tensor changes. This is reasonable on its face, but sometimes you want to also store fake tensors as metadata on FX nodes; mutating a fake tensor is bad because this will invalidate old metadata!\n\nIn fact, there is a fundamental tension here, which is that fake tensors maintain extremely accurate metadata about tensors, up to and including object identity. If object metadata changes over time in an FX graph, there is not actually any way to represent this change over time. Most of the time, our serious FX analyses are done on functionalized graphs, which don't have this, but occasionally you need to do an analysis on a non-functionalized graph. Maybe it was a mistake to put fake tensor in meta['val']",
    "1843": "一级标题：Fake tensor\n二级标题：About the tensor subclass\n内容：\nFake tensor uses both a subclass and a mode tensor subclass pattern, where FakeTensor.\\_\\_torch_dispatch\\_\\_ enables the FakeTensorMode associated with the fake tensor, and then redispatches (relying on FakeTensorMode to do the heavy lifting). If fake tensor operations get a subclass argument it doesn't recognize, it will return NotImplemented, giving the other subclass a chance to run first (hopefully desugaring into plain tensor operations), before it tries again. This can cause infinite loops.",
    "1844": "一级标题：Fake tensor\n二级标题：How is each individual operator implemented?\n内容：\nUnfortunately, there is a pretty complicated set of places where any given operator may be implemented. Some important cases to know about:\n\n- Tensor subclasses support limited constant propagation if the number of elements is very small (this helps deal with some cases where we immediately call item() on such tensors.)\n- We have some fastpath implementations for certain operators, which are done entirely in fake tensor, for performance reasons.\n- If you use @custom_op to generate a custom tensor, these will register impl_abstract directly to fake tensor.\n- Fake tensor itself has some hardcoded special cases for device-converting operations.\n- If there is no meta implementation nor any decomposition, we will generate real zero-filled tensors and attempt to run the operator directly to find out what the results will be. This can cause segfaults if the operator attempts to do indexing with data, so we don't turn this on by default for custom ops.",
    "1845": "一级标题：Fake tensor\n二级标题：How does the converter work?\n内容：\nBecause fake tensors are used in situations that are very sensitive to the exact properties of a tensor, fake tensors do conversion very carefully, preserving leaf-ness, requires_grad'ness, aliasing, and a whole host of other properties. The bulk of the heavy lifting is in MetaConverter.",
    "1846": "一级标题：Fake tensor\n二级标题：Performance characteristics\n内容：\nYou would think fake tensors are fast because they don't do any tensor compute. But at small tensor sizes we are actually entirely overhead bound, and, well, fake tensor is in Python, and we often do a LOT of work to do a single tensor operation (because they are implemented as decompositions). So fake tensors are actually pretty slow in practice, especially when symbolic shapes are involved. There are two important fastpaths we currently have in fake tensor that make a big difference in practice:\n\n- Pointwise ops don't go through PrimTorch decomps, instead we've hand-coded their propagation rule.\n- If possible, we should.",
    "1847": "一级标题：Fake tensor\n二级标题：Fake tensor of fake tensor?\n内容：\nThere is interest in sending fake tensors as user inputs into the PT2 stack, which would imply we would need to be able to create a fake tensor of a fake tensor. This isn't really supported right now, but maybe it would not be too difficult to do.",
    "1848": "一级标题：Fake tensor\n二级标题：Interaction with dynamic shapes\n内容：\nEvery FakeTensorMode contains a ShapeEnv, which tracks all symbolic shapes information. Their lifetimes are typically tied: they live and die together.\n\nBecause FakeTensorMode has a ShapeEnv (but meta implementations do not), meta functions that are data-dependent and require allocating an unbacked SymInt live in fake tensor. Fake tensor also takes care of memoizing unbacked SymInts, so that, e.g., if you call nonzero() on the same fake tensor twice, you get the same symbolic size.",
    "1849": "一级标题：Fake tensor\n二级标题：Other resources\n内容：\n[Colab Tutorial On Using FakeTensor To Determine Max Batch Size](https://colab.research.google.com/drive/1zjAisRrc8R6uixKsrs1DRm3lwz5MWN68)",
    "1850": "一级标题：Frequently Asked Questions\n二级标题：无\n内容：\n**Author**: [Mark Saroufim](https://github.com/msaroufim)",
    "1851": "一级标题：Frequently Asked Questions\n二级标题：Does `torch.compile` support training?\n内容：\n`torch.compile` supports training, using AOTAutograd to capture backwards:\n\n1. The `.forward()` graph and `optimizer.step()` is captured by\n   TorchDynamo’s python `evalframe` frontend.\n2. For each segment of `.forward()` that torchdynamo captures, it uses\n   AOTAutograd to generate a backward graph segment.\n3. Each pair of forward and backward graph are (optionally) min-cut\n   partitioned to save the minimal state between forward and backward.\n4. The forward and backward pairs are wrapped in `autograd.function` modules.\n5. User code calling `.backward()` still triggers eager’s autograd engine,\n   which runs each *compiled backward* graph as if it were one op, also running\n   any non-compiled eager ops’ `.backward()` functions.",
    "1852": "一级标题：Frequently Asked Questions\n二级标题：Do you support Distributed code?\n内容：\n`torch.compile` supports `DistributedDataParallel` (DDP).\nSupport for other distributed training libraries is being considered.\n\nThe main reason why Distributed code is challenging with dynamo is\nbecause AOTAutograd unrolls both the forward and backward pass and\nprovides 2 graphs for backends to optimize. This is a problem for\ndistributed code because we’d like to ideally overlap communication\noperations with computations. Eager pytorch accomplishes this in\ndifferent ways for DDP/FSDP- using autograd hooks, module hooks, and\nmodifications/mutations of module states. In a naive application of\ndynamo, hooks that should run directly after an operation during\nbackwards may be delayed until after the entire compiled region of\nbackwards ops, due to how AOTAutograd compiled functions interact with\ndispatcher hooks.\n\nThe basic strategy for optimizing DDP with Dynamo is outlined in\n[distributed.py](https://github.com/pytorch/pytorch/blob/main/torch/_dynamo/backends/distributed.py)\nwhere the main idea will be to graph break on [DDP bucket\nboundaries](https://pytorch.org/docs/stable/notes/ddp.html#internal-design).\n\nWhen each node in DDP needs to synchronize its weights with the other\nnodes it organizes its gradients and parameters into buckets which\nreduces communication times and allows a node to broadcast a fraction of\nits gradients to other waiting nodes.\n\nGraph breaks in distributed code mean you can expect dynamo and its\nbackends to optimize the compute overhead of a distributed program but\nnot its communication overhead. Graph-breaks may interfere with\ncompilation speedups, if the reduced graph-size robs the compiler of\nfusion opportunities. However, there are diminishing returns with\nincreasing graph size since most of the current compute optimizations\nare local fusions. So in practice this approach may be sufficient.",
    "1853": "一级标题：Frequently Asked Questions\n二级标题：Do I still need to export whole graphs?\n内容：\nFor the vast majority of models you probably don’t and you can use\n`torch.compile()` as is but there are a few situations where\nfull graphs are necessary and you can can ensure a full graph by simply\nrunning `torch.compile(..., fullgraph=True)`. These situations include:\n\n- Large scale training runs, such as $250K+ that require pipeline parallelism\n  and other advanced sharding strategies.\n- Inference optimizers like [TensorRT](https://github.com/pytorch/TensorRT)\n  or [AITemplate](https://github.com/facebookincubator/AITemplate) that\n  rely on fusing much more aggressively than training optimizers.\n- Mobile training or inference.\n\nFuture work will include tracing communication operations into graphs,\ncoordinating these operations with compute optimizations, and optimizing\nthe communication operations.",
    "1854": "一级标题：Frequently Asked Questions\n二级标题：Why is my code crashing?\n内容：\nIf your code ran just fine without `torch.compile` and started to\ncrash with it is enabled, then the most important first step is figuring\nout which part of the stack your failure occurred. To troubleshoot that,\nfollow the steps below and only try the next step if the previous one\nsucceeded.\n\n1. `torch.compile(..., backend=\"eager\")` which only runs TorchDynamo\n   forward graph capture and then runs the captured graph with PyTorch.\n   If this fails then there’s an issue with TorchDynamo.\n2. `torch.compile(..., backend=\"aot_eager\")`\n   which runs TorchDynamo to capture a forward graph, and then AOTAutograd\n   to trace the backward graph without any additional backend compiler\n   steps. PyTorch eager will then be used to run the forward and backward\n   graphs. If this fails then there’s an issue with AOTAutograd.\n3. `torch.compile(..., backend=\"inductor\")` which runs TorchDynamo to capture a\n   forward graph, and then AOTAutograd to trace the backward graph with the\n   TorchInductor compiler. If this fails then there’s an issue with TorchInductor",
    "1855": "一级标题：Frequently Asked Questions\n二级标题：Why is compilation slow?\n内容：\n- **Dynamo Compilation**– TorchDynamo has a builtin stats function for\n  collecting and displaying the time spent in each compilation phase.\n  These stats can be accessed by calling `torch._dynamo.utils.compile_times()`\n  after executing `torch._dynamo`. By default, this returns a string\n  representation of the compile times spent in each TorchDynamo function by name.\n- **Inductor Compilation**– TorchInductor has a builtin stats and trace function\n  for displaying time spent in each compilation phase, output code, output\n  graph visualization and IR dump. `env TORCH_COMPILE_DEBUG=1 python repro.py`.\n  This is a debugging tool designed to make it easier to debug/understand the\n  internals of TorchInductor with an output that will look something like\n  [this](https://gist.github.com/jansel/f4af078791ad681a0d4094adeb844396)\n  Each file in that debug trace can be enabled/disabled via\n  `torch._inductor.config.trace.*`. The profile and the diagram are both\n  disabled by default since they are expensive to generate. See the\n  [example debug directory\n  output](https://gist.github.com/jansel/f4af078791ad681a0d4094adeb844396)\n  for more examples.\n- **Excessive Recompilation**\n  When TorchDynamo compiles a function (or part of one), it makes certain\n  assumptions about locals and globals in order to allow compiler\n  optimizations, and expresses these assumptions as guards that check\n  particular values at runtime. If any of these guards fail, Dynamo will\n  recompile that function (or part) up to\n  `torch._dynamo.config.recompile_limit` times. If your program is\n  hitting the cache limit, you will first need to determine which guard is\n  failing and what part of your program is triggering it. The\n  Use `TORCH_TRACE/tlparse` or `TORCH_LOGS=recompiles` to trace the root of the issue, check {ref}`torch.compiler_troubleshooting` for more details.",
    "1856": "一级标题：Frequently Asked Questions\n二级标题：Why are you recompiling in production?\n内容：\nIn some cases, you may not want unexpected compiles after a program has\nwarmed up. For example, if you are serving production traffic in a\nlatency critical application. For this, TorchDynamo provides an\nalternate mode where prior compiled graphs are used, but no new ones are\ngenerated:\n\n```python\nfrozen_toy_example = dynamo.run(toy_example)\nfrozen_toy_example(torch.randn(10), torch.randn(10))\n```",
    "1857": "一级标题：Frequently Asked Questions\n二级标题：How are you speeding up my code?\n内容：\nThere are 3 major ways to accelerate PyTorch code:\n\n1. Kernel fusion via vertical fusions which fuse sequential operations to avoid\n   excessive read/writes. For example, fuse 2 subsequent cosines means you\n   can can do 1 read 1 write instead 2 reads 2 writes 2. Horizontal fusion:\n   the simplest example being batching where a single matrix is multiplied\n   with a batch of examples but the more general scenario is a grouped GEMM\n   where a group of matrix multiplications are scheduled together\n2. Out of order execution: A general optimization for compilers, by looking ahead\n   at the exact data dependencies within a graph we can decide on the most\n   opportune time to execute a node and which buffers can be reused\n3. Automatic work placement: Similar of the out of order execution point,\n   but by matching nodes of a graph to resources like physical hardware or\n   memory we can design an appropriate schedule\n\nThe above are general principles for accelerating PyTorch code but\ndifferent backends will each make different tradeoffs on what to\noptimize. For example Inductor first takes care of fusing whatever it\ncan and only then generates [Triton](https://openai.com/blog/triton/)\nkernels.\n\nTriton in addition offers speedups because of automatic memory\ncoalescing, memory management and scheduling within each Streaming\nMultiprocessor and has been designed to handle tiled computations.\n\nHowever, regardless of the backend you use it’s best to use a benchmark\nand see approach so try out the PyTorch profiler, visually inspect the\ngenerated kernels and try to see what’s going on for yourself.\n\n\n(torch.compiler_graph_breaks)=",
    "1858": "一级标题：Frequently Asked Questions\n二级标题：Why am I not seeing speedups?\n内容：\n### Graph Breaks\n\nThe main reason you won’t see the speedups you’d like to by using dynamo\nis excessive graph breaks. So what’s a graph break?\n\nGiven a program like:\n\n```python\ndef some_fun(x):\n    ...\n\ntorch.compile(some_fun)(x)\n...\n```\n\nTorchdynamo will attempt to compile all of the torch/tensor operations\nwithin `some_fun()` into a single FX graph, but it may fail to capture\neverything into one graph.\n\nSome graph break reasons are insurmountable to TorchDynamo like calling\ninto a C extension other than PyTorch is invisible to TorchDynamo, and\ncould do arbitrary things without TorchDynamo being able to introduce\nnecessary guards to ensure that the compiled program would be safe to reuse.\n\n> To maximize performance, it’s important to have as few graph breaks\n> as possible.\n### Identifying the cause of a graph break\n\nTo identify all graph breaks in a program and the associated reasons for\nthe breaks, `torch._dynamo.explain` can be used. This tool runs\nTorchDynamo on the supplied function and aggregates the graph breaks\nthat are encountered. Here is an example usage:\n\n```python\nimport torch\nimport torch._dynamo as dynamo\ndef toy_example(a, b):\n    x = a / (torch.abs(a) + 1)\n    print(\"woo\")\n    if b.sum() < 0:\n        b = b * -1\n    return x * b\nexplanation = dynamo.explain(toy_example)(torch.randn(10), torch.randn(10))\nprint(explanation)\n\"\"\"\nGraph Count: 3\nGraph Break Count: 2\nOp Count: 5\nBreak Reasons:\n  Break Reason 1:\n    Reason: builtin: print [<class 'torch._dynamo.variables.constant.ConstantVariable'>] False\n    User Stack:\n      <FrameSummary file foo.py, line 5 in toy_example>\n  Break Reason 2:\n    Reason: generic_jump TensorVariable()\n    User Stack:\n      <FrameSummary file foo.py, line 6 in torch_dynamo_resume_in_toy_example_at_5>\nOps per Graph:\n  ...\nOut Guards:\n  ...\n\"\"\"\n```\n\nTo throw an error on the first graph break encountered you can\ndisable python fallbacks by using `fullgraph=True`, this should be\nfamiliar if you’ve worked with export based compilers.\n\n```python\ndef toy_example(a, b):\n   ...\n\ntorch.compile(toy_example, fullgraph=True, backend=<compiler>)(a, b)\n```\n\n### Why didn’t my code recompile when I changed it?\n\nIf you enabled dynamic shapes by setting\n`env TORCHDYNAMO_DYNAMIC_SHAPES=1 python model.py` then your code\nwon’t recompile on shape changes. We’ve added support for dynamic shapes\nwhich avoids recompilations in the case when shapes vary by less than a\nfactor of 2. This is especially useful in scenarios like varying image\nsizes in CV or variable sequence length in NLP. In inference scenarios\nit’s often not possible to know what a batch size will be beforehand\nbecause you take what you can get from different client apps.\n\nIn general, TorchDynamo tries very hard not to recompile things\nunnecessarily so if for example TorchDynamo finds 3 graphs and your\nchange only modified one graph then only that graph will recompile. So\nanother tip to avoid potentially slow compilation times is to warmup a\nmodel by compiling it once after which subsequent compilations will be\nmuch faster. Cold start compile times is still a metric we track\nvisibly.",
    "1859": "一级标题：Frequently Asked Questions\n二级标题：Why am I getting incorrect results?\n内容：\nAccuracy issues can also be minified if you set the environment variable\n`TORCHDYNAMO_REPRO_LEVEL=4`, it operates with a similar git bisect\nmodel and a full repro might be something like\n`TORCHDYNAMO_REPRO_AFTER=\"aot\" TORCHDYNAMO_REPRO_LEVEL=4` the reason\nwe need this is downstream compilers will codegen code whether it’s\nTriton code or the C++ backend, the numerics from those downstream\ncompilers can be different in subtle ways yet have dramatic impact on\nyour training stability. So the accuracy debugger is very useful for us\nto detect bugs in our codegen or with a backend compiler.\n\nIf you'd like to ensure that random number generation is the same across both torch\nand triton then you can enable `torch._inductor.config.fallback_random = True`",
    "1860": "一级标题：Frequently Asked Questions\n二级标题：Why am I getting OOMs?\n内容：\nDynamo is still an alpha product so there’s a few sources of OOMs and if\nyou’re seeing an OOM try disabling the following configurations in this\norder and then open an issue on GitHub so we can solve the root problem\n1\\. If you’re using dynamic shapes try disabling them, we’ve disabled\nthem by default: `env TORCHDYNAMO_DYNAMIC_SHAPES=0 python model.py` 2.\nCUDA graphs with Triton are enabled by default in inductor but removing\nthem may alleviate some OOM issues: `torch._inductor.config.triton.cudagraphs = False`.",
    "1861": "一级标题：Frequently Asked Questions\n二级标题：Does `torch.func` work with `torch.compile` (for `grad` and `vmap` transforms)?\n内容：\nApplying a `torch.func` transform to a function that uses `torch.compile`\ndoes work:\n\n```python\nimport torch\n\n@torch.compile\ndef f(x):\n    return torch.sin(x)\n\ndef g(x):\n    return torch.grad(f)(x)\n\nx = torch.randn(2, 3)\ng(x)\n```\n\n### Calling `torch.func` transform inside of a function handled with `torch.compile`\n\n### Compiling `torch.func.grad` with `torch.compile`\n\n```python\nimport torch\n\ndef wrapper_fn(x):\n    return torch.func.grad(lambda x: x.sin().sum())(x)\n\nx = torch.randn(3, 3, 3)\ngrad_x = torch.compile(wrapper_fn)(x)\n```\n\n### Compiling `torch.vmap` with `torch.compile`\n\n```python\nimport torch\n\ndef my_fn(x):\n    return torch.vmap(lambda x: x.sum(1))(x)\n\nx = torch.randn(3, 3, 3)\noutput = torch.compile(my_fn)(x)\n```\n\n### Compiling functions besides the ones which are supported (escape hatch)\n\nFor other transforms, as a workaround, use `torch._dynamo.allow_in_graph`\n\n`allow_in_graph` is an escape hatch. If your code does not work with\n`torch.compile`, which introspects Python bytecode, but you believe it\nwill work via a symbolic tracing approach (like `jax.jit`), then use\n`allow_in_graph`.\n\nBy using `allow_in_graph` to annotate a function, you must make sure\nyour code meets the following requirements:\n\n- All outputs in your function only depend on the inputs and\n  do not depend on any captured Tensors.\n- Your function is functional. That is, it does not mutate any state. This may\n  be relaxed; we actually support functions that appear to be functional from\n  the outside: they may have in-place PyTorch operations, but may not mutate\n  global state or inputs to the function.\n- Your function does not raise data-dependent errors.\n\n```python\nimport torch\n\n@torch.compile\ndef f(x):\n    return torch._dynamo.allow_in_graph(torch.vmap(torch.sum))(x)\n\nx = torch.randn(2, 3)\nf(x)\n```\n\nA common pitfall is using `allow_in_graph` to annotate a function that\ninvokes an `nn.Module`. This is because the outputs now depend on the\nparameters of the `nn.Module`. To get this to work, use\n`torch.func.functional_call` to extract the module state.",
    "1862": "一级标题：Frequently Asked Questions\n二级标题：Does NumPy work with `torch.compile`?\n内容：\nStarting in 2.1, `torch.compile` understands native NumPy programs that\nwork on NumPy arrays, and mixed PyTorch-NumPy programs that convert from PyTorch\nto NumPy and back via `x.numpy()`, `torch.from_numpy`, and related functions.\n\n(nonsupported-numpy-feats)=\n\n### Which NumPy features does `torch.compile` support?\n\nNumPy within `torch.compile` follows NumPy 2.0 pre-release.\n\nGenerally, `torch.compile` is able to trace through most NumPy constructions,\nand when it cannot, it falls back to eager and lets NumPy execute that piece of\ncode. Even then, there are a few features where `torch.compile` semantics\nslightly deviate from those of NumPy:\n\n- NumPy scalars: We model them as 0-D arrays. That is, `np.float32(3)` returns\n  a 0-D array under `torch.compile`. To avoid a graph break, it is best to use this 0-D\n  array. If this breaks your code, you can workaround this by casting the NumPy scalar\n  to the relevant Python scalar type `bool/int/float`.\n- Negative strides: `np.flip` and slicing with a negative step return a copy.\n- Type promotion: NumPy's type promotion will change in NumPy 2.0. The new rules\n  are described in [NEP 50](https://numpy.org/neps/nep-0050-scalar-promotion.html).\n  `torch.compile` implements NEP 50 rather than the current soon-to-be deprecated rules.\n- `{tril,triu}_indices_from/{tril,triu}_indices` return arrays rather than a tuple of arrays.\n\nThere are other features for which we do not support tracing and we gracefully\nfallback to NumPy for their execution:\n\n- Non-numeric dtypes like datetimes, strings, chars, void, structured dtypes and recarrays.\n- Long dtypes `np.float128/np.complex256` and some unsigned dtypes `np.uint16/np.uint32/np.uint64`.\n- `ndarray` subclasses.\n- Masked arrays.\n- Esoteric ufunc machinery like `axes=[(n,k),(k,m)->(n,m)]` and ufunc methods (e.g., `np.add.reduce`).\n- Sorting / ordering `complex64/complex128` arrays.\n- NumPy `np.poly1d` and `np.polynomial`.\n- Positional `out1, out2` args in functions with 2 or more returns (`out=tuple` does work).\n- `__array_function__`, `__array_interface__` and `__array_wrap__`.\n- `ndarray.ctypes` attribute.\n\n### Can I compile NumPy code using `torch.compile`?\n\nOf course you do! `torch.compile` understands NumPy code natively, and treats it\nas if it were PyTorch code. To do so, simply wrap NumPy code with the `torch.compile`\ndecorator.\n\n```python\nimport torch\nimport numpy as np\n\n@torch.compile\ndef numpy_fn(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n    return np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1))\n\nX = np.random.randn(1024, 64)\nY = np.random.randn(1024, 64)\nZ = numpy_fn(X, Y)\nassert isinstance(Z, np.ndarray)\n```\n\nExecuting this example with the environment variable `TORCH_LOGS=output_code`, we can see\nthat `torch.compile` was able to fuse the multiplication and the sum into one C++ kernel.\nIt was also able to execute them in parallel using OpenMP (native NumPy is single-threaded).\nThis can easily make your NumPy code `n` times faster, where `n` is the number of cores\nin your processor!\n\nTracing NumPy code this way also supports graph breaks within the compiled code.\n\n### Can I execute NumPy code on CUDA and compute gradients via `torch.compile`?\n\nYes you can! To do so, you may simply execute your code within a `torch.device(\"cuda\")`\ncontext. Consider the example\n\n```python\nimport torch\nimport numpy as np\n\n@torch.compile\ndef numpy_fn(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n    return np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1))\n\nX = np.random.randn(1024, 64)\nY = np.random.randn(1024, 64)\nwith torch.device(\"cuda\"):\n    Z = numpy_fn(X, Y)\nassert isinstance(Z, np.ndarray)\n```\n\nIn this example, `numpy_fn` will be executed in CUDA. For this to be\npossible, `torch.compile` automatically moves `X` and `Y` from CPU\nto CUDA, and then it moves the result `Z` from CUDA to CPU. If we are\nexecuting this function several times in the same program run, we may want\nto avoid all these rather expensive memory copies. To do so, we just need\nto tweak our `numpy_fn` so that it accepts cuda Tensors and returns tensors.\nWe can do so by using `torch.compiler.wrap_numpy`:\n\n```python\n@torch.compile(fullgraph=True)\n@torch.compiler.wrap_numpy\ndef numpy_fn(X, Y):\n    return np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1))\n\nX = torch.randn(1024, 64, device=\"cuda\")\nY = torch.randn(1024, 64, device=\"cuda\")\nZ = numpy_fn(X, Y)\nassert isinstance(Z, torch.Tensor)\nassert Z.device.type == \"cuda\"\n```\n\nHere, we explicitly create the tensors in CUDA memory, and pass them to the\nfunction, which performs all the computations on the CUDA device.\n`wrap_numpy` is in charge of marking any `torch.Tensor` input as an input\nwith `np.ndarray` semantics at a `torch.compile` level. Marking tensors\ninside the compiler is a very cheap operation, so no data copy or data movement\nhappens during runtime.\n\nUsing this decorator, we can also differentiate through NumPy code!\n\n```python\n@torch.compile(fullgraph=True)\n@torch.compiler.wrap_numpy\ndef numpy_fn(X, Y):\n    return np.mean(np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1)))\n\nX = torch.randn(1024, 64, device=\"cuda\", requires_grad=True)\nY = torch.randn(1024, 64, device=\"cuda\")\nZ = numpy_fn(X, Y)\nassert isinstance(Z, torch.Tensor)\nZ.backward()\n# X.grad now holds the gradient of the computation\nprint(X.grad)\n```\n\nWe have been using `fullgraph=True` as graph break are problematic in this context.\nWhen a graph break occurs, we need to materialize the NumPy arrays. Since NumPy arrays\ndo not have a notion of `device` or `requires_grad`, this information is lost during\na graph break.\n\nWe cannot propagate gradients through a graph break, as the graph break code may execute\narbitrary code that don't know how to differentiate. On the other hand, in the case of\nthe CUDA execution, we can work around this problem as we did in the first example, by\nusing the `torch.device(\"cuda\")` context manager:\n\n```python\n@torch.compile\n@torch.compiler.wrap_numpy\ndef numpy_fn(X, Y):\n    prod = X[:, :, None] * Y[:, None, :]\n    print(\"oops, a graph break!\")\n    return np.sum(prod, axis=(-2, -1))\n\nX = torch.randn(1024, 64, device=\"cuda\")\nY = torch.randn(1024, 64, device=\"cuda\")\n\nwith torch.device(\"cuda\"):\n    Z = numpy_fn(X, Y)\nassert isinstance(Z, torch.Tensor)\nassert Z.device.type == \"cuda\"\n```\n\nDuring the graph break, the intermediary tensors still need to be moved to CPU, but when the\ntracing is resumed after the graph break, the rest of the graph is still traced on CUDA.\nGiven this CUDA <> CPU and CPU <> CUDA movement, graph breaks are fairly costly in the NumPy\ncontext and should be avoided, but at least they allow tracing through complex pieces of code.\n\n### How do I debug NumPy code under `torch.compile`?\n\nDebugging JIT compiled code is challenging, given the complexity of modern\ncompilers and the daunting errors that they raise.\n{ref}`The torch.compile troubleshooting doc <torch.compiler_troubleshooting>`\ncontains a few tips and tricks on how to tackle this task.\n\nIf the above is not enough to pinpoint the origin of the issue, there are still\na few other NumPy-specific tools we can use. We can discern whether the bug\nis entirely in the PyTorch code by disabling tracing through NumPy functions:\n\n```python\nfrom torch._dynamo import config\nconfig.trace_numpy = False\n```\n\nIf the bug lies in the traced NumPy code, we can execute the NumPy code eagerly (without `torch.compile`)\nusing PyTorch as a backend by importing `import torch._numpy as np`.\nThis should just be used for **debugging purposes** and is in no way a\nreplacement for the PyTorch API, as it is **much less performant** and, as a\nprivate API, **may change without notice**. At any rate, `torch._numpy` is a\nPython implementation of NumPy in terms of PyTorch and it is used internally by `torch.compile` to\ntransform NumPy code into Pytorch code. It is rather easy to read and modify,\nso if you find any bug in it feel free to submit a PR fixing it or simply open\nan issue.\n\nIf the program does work when importing `torch._numpy as np`, chances are\nthat the bug is in TorchDynamo. If this is the case, please feel free to open an issue\nwith a {ref}`minimal reproducer <torch.compiler_troubleshooting>`.\n\n### I `torch.compile` some NumPy code and I did not see any speed-up.\n\nThe best place to start is the\n[tutorial with general advice for how to debug these sort of torch.compile issues](https://pytorch.org/docs/main/torch.compiler_faq.html#why-am-i-not-seeing-speedups).\n\nSome graph breaks may happen because of the use of unsupported features. See\n{ref}`nonsupported-numpy-feats`. More generally, it is useful to keep in mind\nthat some widely used NumPy features do not play well with compilers. For\nexample, in-place modifications make reasoning difficult within the compiler and\noften yield worse performance than their out-of-place counterparts.As such, it is best to avoid\nthem. Same goes for the use of the `out=` parameter. Instead, prefer\nout-of-place ops and let `torch.compile` optimize the memory use. Same goes\nfor data-dependent ops like masked indexing through boolean masks, or\ndata-dependent control flow like `if` or `while` constructions.",
    "1863": "一级标题：Frequently Asked Questions\n二级标题：Which API to use for fine grain tracing?\n内容：\nIn some cases, you might need to exclude small parts of your code from the\ntorch.compile compilations. This section provides some of the answers and\nyou can find more information in {ref}`torchdynamo_fine_grain_tracing`.\n\n### How do I graph break on a function?\n\nGraph break on a function is not enough to sufficiently express what you want\nPyTorch to do. You need to be more specific about your use case. Some of the\nmost common use cases you might want to consider:\n\n- If you want to disable compilation on this function frame and the recursively\n  invoked frames, use `torch._dynamo.disable`.\n- If you want a particular operator, such as `fbgemm` to use the eager mode,\n  use `torch._dynamo.disallow_in_graph`.\n\nSome of the uncommon use cases include:\n\n- If you want to disable TorchDynamo on the function frame but enable it back\n  on the recursively invoked frames – use `torch._dynamo.disable(recursive=False)`.\n- If you want to prevent inlining of a function frame – use `torch._dynamo.graph_break`\n  at the beginning of the function you want to prevent inlining.\n\n### What's the difference between `torch._dynamo.disable` and `torch._dynamo.disallow_in_graph`\n\nDisallow-in-graph works at the level of operators, or more specifically,\nthe operators that you see in the TorchDynamo extracted graphs.\n\nDisable works at the function frame level and decides if TorchDynamo\nshould look into the function frame or not.\n\n### What's the difference between `torch._dynamo.disable` and `torch._dynamo_skip`\n\n:::{note}\n`torch._dynamo_skip` is deprecated.\n:::\n\nYou most likely need `torch._dynamo.disable`. But in an unlikely scenario, you\nmight need even finer control. Suppose you want to disable the tracing on just\nthe `a_fn` function, but want to continue the tracing back in `aa_fn` and\n`ab_fn`. The image below demonstrates this use case:\n\n:::{figure} _static/img/fine_grained_apis/call_stack_diagram.png\n:alt: diagram of torch.compile + disable(a_fn, recursive=False)\n:::\n\nIn this case, you can use `torch._dynamo.disable(recursive=False)`.\nIn previous versions, this functionality was provided by `torch._dynamo.skip`.\nThis is now supported by the `recursive` flag inside `torch._dynamo.disable`.",
    "1864": "一级标题：TorchDynamo APIs for fine-grained tracing\n二级标题：无\n内容：\n:::{note}\nIn this document `torch.compiler.compile` and `torch.compile` are used interchangeably.\nBoth versions will work in your code.\n:::\n\n`torch.compile` performs TorchDynamo tracing on the whole user model.\nHowever, it is possible that a small part of the model code cannot be\nhandled by `torch.compiler`. In this case, you might want to disable\nthe compiler on that particular portion, while running compilation on\nthe rest of the model. This section describe the existing APIs that\nuse to define parts of your code in which you want to skip compilation\nand the relevant use cases.\n\nThe API that you can use to define portions of the code on which you can\ndisable compilation are listed in the following table:\n\n```{eval-rst}\n.. csv-table:: TorchDynamo APIs to control fine-grained tracing\n   :header: \"API\", \"Description\", \"When to use?\"\n   :widths: auto\n\n   \"``torch.compiler.disable``\", \"Disables Dynamo on the decorated function as well as recursively invoked functions.\", \"Excellent for unblocking a user, if a small portion of the model cannot be handled with ``torch.compile``.\"\n   \"``torch._dynamo.disallow_in_graph``\", \"Disallows the marked op in the TorchDynamo graph. TorchDynamo causes graph break, and runs the op in the eager (no compile) mode.\\n\\nThis is suitable for the ops, while ``torch.compiler.disable`` is suitable for decorating functions.\", \"This API is excellent for both debugging and unblocking if a custom op like ``torch.ops.fbgemm.*`` is causing issues with the ``torch.compile`` function.\"\n   \"``torch.compile.allow_in_graph``\", \"The annotated callable goes as is in the TorchDynamo graph. For example, a black-box for TorchDynamo Dynamo.\\n\\nNote that AOT Autograd will trace through it, so the ``allow_in_graph`` is only a Dynamo-level concept.\", \"This API is useful for portions of the model which have known TorchDynamo hard-to-support features, like hooks or ``autograd.Function``. However, each usage of ``allow_in_graph`` **must be carefully screened** (no graph breaks, no closures).\"\n   \"``torch._dynamo.graph_break``\", \"Adds a graph break. The code before and after the graph break goes through TorchDynamo.\", \"**Rarely useful for deployment** - If you think you need this, most probably you need either ``disable`` or ``disallow_in_graph``.\"\n   \"``torch.compiler.is_compiling``\", \"Indicates whether a graph is executed/traced as part of torch.compile() or torch.export().\"\n   \"``torch.compiler.is_dynamo_compiling``\", \"Indicates whether a graph is traced via TorchDynamo. It's stricter than torch.compiler.is_compiling() flag, as it would only be set to True when TorchDynamo is used.\"\n   \"``torch.compiler.is_exporting``\", \"Indicates whether a graph is traced via export. It's stricter than torch.compiler.is_compiling() flag, as it would only be set to True when torch.export is used.\"\n```",
    "1865": "一级标题：TorchDynamo APIs for fine-grained tracing\n二级标题：`torch.compiler.disable`\n内容：\n`torch.compiler.disable` disables compilation on the decorated function frame and all the function frames recursively invoked from the decorated function frame.\n\nTorchDynamo intercepts the execution of each Python function frame. So, suppose you have a code structure (image below) where the function `fn` calls functions `a_fn` and `b_fn`. And `a_fn` calls `aa_fn` and `ab_fn`. When you use the PyTorch eager mode rather than `torch.compile`, these function frames run as is. With `torch.compile`, TorchDynamo intercepts each of these function frames (indicated by the green color):\n\n:::{figure} _static/img/fine_grained_apis/api_diagram.png\n:alt: Callstack diagram of different apis.\n:::\n\nLet's imagine, that function `a_fn` is causing troubles with `torch.compile`.\nAnd this is a non-critical portion of the model. You can use `compiler.disable`\non function `a_fn`. As shown above, TorchDynamo will stop looking at frames\noriginating from the `a_fn` call (white color indicates original Python behavior).\n\nTo skip compilation, you can decorate the offending function with\n`@torch.compiler.disable`.\n\nYou can also use the non-decorator syntax if you don’t want to change the source\ncode\nHowever, we recommend that you avoid this style if possible. Here, you have to\ntake care that all users of the original function are now using the patched\nversion.",
    "1866": "一级标题：TorchDynamo APIs for fine-grained tracing\n二级标题：`torch._dynamo.disallow_in_graph`\n内容：\n`torch._dynamo.disallow_in_graph` disallows an operator but not the function\nto be present in the TorchDynamo extracted graph. Note that this is suitable\nfor operators and not general functions as in the case of `_dynamo.disable`.\n\nLet's imagine you compile your model with PyTorch. TorchDynamo is able to\nextract a graph, but then you see the downstream compiler failing. For example,\nthe meta kernel is missing, or some Autograd dispatch key is set incorrectly\nfor a particular operator. Then you can mark that operator as\n`disallow_in_graph`, and TorchDynamo will cause a graph break and run that\noperator by using the PyTorch eager mode.\n\nThe catch is that you will have to find the corresponding Dynamo level operator,\nand not the ATen level operator. See more in the Limitations section of the doc.\n\n:::{warning}\n`torch._dynamo.disallow_in_graph` is a global flag. If you are comparing\ndifferent backend compilers, you might have to call `allow_in_graph` for\nthe disallowed operator when switching to the other compiler.\n:::",
    "1867": "一级标题：TorchDynamo APIs for fine-grained tracing\n二级标题：`torch.compiler.allow_in_graph`\n内容：\n`torch.compiler.allow_in_graph` is useful when the relevant function frame\nhas some known hard-to-support TorchDynamo feature, such as hooks and\n`autograd.Function`, and you are confident that downstream PyTorch components\nsuch as AOTAutograd can safely trace through the decorated function. When a\nfunction is decorated with `allow_in_graph`, TorchDynamo treats it as a\nblack-box and puts it as is in the generated graph.\n\n:::{warning}\n`allow_in_graph` skips TorchDynamo completely on the decorated function\nomitting all TorchDynamo safety checks, including graph breaks, handling\nclosures, and others. Use `allow_in_graph` with caution. PyTorch downstream\ncomponents, such as AOTAutograd rely on TorchDynamo to handle complex Python\nfeatures, but `allow_in_graph` bypasses TorchDynamo. Using `allow_in_graph`\ncould lead to soundness and hard-to-debug issues.\n:::",
    "1868": "一级标题：TorchDynamo APIs for fine-grained tracing\n二级标题：Limitations\n内容：\nAll the existing APIs are applied at the TorchDynamo level. Therefore, these\nAPIs have visibility to only what TorchDynamo sees. This can lead to confusing\nscenarios.\n\nFor example, `torch._dynamo.disallow_in_graph` will not work for ATen operators\nbecause they are visible to AOT Autograd. For example,\n`torch._dynamo.disallow_in_graph(torch.ops.aten.add)` will not work in the\nabove example.",
    "1869": "一级标题：Getting Started\n二级标题：无\n内容：\nBefore you read this section, make sure to read the {ref}`torch.compiler_overview`\n\nlet's start by looking at a simple `torch.compile` example that demonstrates\nhow to use `torch.compile` for inference. This example demonstrates the\n`torch.cos()` and `torch.sin()` features which are examples of pointwise\noperators as they operate element by element on a vector. This example might\nnot show significant performance gains but should help you form an intuitive\nunderstanding of how you can use `torch.compile` in your own programs.\n\n:::{note}\nTo run this script, you need to have at least one GPU on your machine.\nIf you do not have a GPU, you can remove the `.to(device=\"cuda:0\")` code\nin the snippet below and it will run on CPU. You can also set device to\n`xpu:0` to run on Intel® GPUs.\n:::\n\n```python\nimport torch\ndef fn(x):\n   a = torch.cos(x)\n   b = torch.sin(a)\n   return b\nnew_fn = torch.compile(fn, backend=\"inductor\")\ninput_tensor = torch.randn(10000).to(device=\"cuda:0\")\na = new_fn(input_tensor)\n```\n\nA more famous pointwise operator you might want to use would\nbe something like `torch.relu()`. Pointwise ops in eager mode are\nsuboptimal because each one would need to read a tensor from the\nmemory, make some changes, and then write back those changes. The single\nmost important optimization that inductor performs is fusion. In the\nexample above we can turn 2 reads (`x`, `a`) and\n2 writes (`a`, `b`) into 1 read (`x`) and 1 write (`b`), which\nis crucial especially for newer GPUs where the bottleneck is memory\nbandwidth (how quickly you can send data to a GPU) rather than compute\n(how quickly your GPU can crunch floating point operations).\n\nAnother major optimization that inductor provides is automatic\nsupport for CUDA graphs.\nCUDA graphs help eliminate the overhead from launching individual\nkernels from a Python program which is especially relevant for newer GPUs.\n\nTorchDynamo supports many different backends, but TorchInductor specifically works\nby generating [Triton](https://github.com/openai/triton) kernels. Let's save\nour example above into a file called `example.py`. We can inspect the code\ngenerated Triton kernels by running `TORCH_COMPILE_DEBUG=1 python example.py`.\nAs the script executes, you should see `DEBUG` messages printed to the\nterminal. Closer to the end of the log, you should see a path to a folder\nthat contains `torchinductor_<your_username>`. In that folder, you can find\nthe `output_code.py` file that contains the generated kernel code similar to\nthe following:\n\n```python\n@pointwise(size_hints=[16384], filename=__file__, triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32'}, 'device': 0, 'constants': {}, 'mutated_arg_names': [], 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]})\n@triton.jit\ndef triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n   xnumel = 10000\n   xoffset = tl.program_id(0) * XBLOCK\n   xindex = xoffset + tl.arange(0, XBLOCK)[:]\n   xmask = xindex < xnumel\n   x0 = xindex\n   tmp0 = tl.load(in_ptr0 + (x0), xmask, other=0.0)\n   tmp1 = tl.cos(tmp0)\n   tmp2 = tl.sin(tmp1)\n   tl.store(out_ptr0 + (x0 + tl.zeros([XBLOCK], tl.int32)), tmp2, xmask)\n```\n\n:::{note}\nThe above code snippet is an example. Depending on your hardware,\nyou might see different code generated.\n:::\n\nAnd you can verify that fusing the `cos` and `sin` did actually occur\nbecause the `cos` and `sin` operations occur within a single Triton kernel\nand the temporary variables are held in registers with very fast access.\n\nRead more on Triton's performance\n[here](https://openai.com/blog/triton/). Because the code is written\nin Python, it's fairly easy to understand even if you have not written all that\nmany CUDA kernels.\n\nNext, let's try a real model like resnet50 from the PyTorch\nhub.\n\n```python\nimport torch\nmodel = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\nopt_model = torch.compile(model, backend=\"inductor\")\nopt_model(torch.randn(1,3,64,64))\n```\n\nAnd that is not the only available backend, you can run in a REPL\n`torch.compiler.list_backends()` to see all the available backends. Try out the\n`cudagraphs` next as inspiration.",
    "1870": "一级标题：Getting Started\n二级标题：Using a pretrained model\n内容：\nPyTorch users frequently leverage pretrained models from\n[transformers](https://github.com/huggingface/transformers) or\n[TIMM](https://github.com/rwightman/pytorch-image-models) and one of\nthe design goals is TorchDynamo and TorchInductor is to work out of the box with\nany model that people would like to author.\n\nLet's download a pretrained model directly from the HuggingFace hub and optimize\nit:\n\n```python\nimport torch\nfrom transformers import BertTokenizer, BertModel\n# Copy pasted from here https://huggingface.co/bert-base-uncased\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-uncased\").to(device=\"cuda:0\")\nmodel = torch.compile(model, backend=\"inductor\") # This is the only line of code that we changed\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt').to(device=\"cuda:0\")\noutput = model(**encoded_input)\n```\n\nIf you remove the `to(device=\"cuda:0\")` from the model and\n`encoded_input`, then Triton will generate C++ kernels that will be\noptimized for running on your CPU. You can inspect both Triton or C++\nkernels for BERT. They are more complex than the trigonometry\nexample we tried above but you can similarly skim through it and see if you\nunderstand how PyTorch works.\n\nSimilarly, let's try out a TIMM example:\n\n```python\nimport timm\nimport torch\nmodel = timm.create_model('resnext101_32x8d', pretrained=True, num_classes=2)\nopt_model = torch.compile(model, backend=\"inductor\")\nopt_model(torch.randn(64,3,7,7))\n```",
    "1871": "一级标题：Getting Started\n二级标题：Next Steps\n内容：\nIn this section, we have reviewed a few inference examples and developed a\nbasic understanding of how torch.compile works. Here is what you check out next:\n\n- [torch.compile tutorial on training](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)\n- {ref}`torch.compiler_api`\n- {ref}`torchdynamo_fine_grain_tracing`",
    "1872": "一级标题：TorchInductor GPU Profiling\n二级标题：无\n内容：\nThis section lists useful commands and workflows that can help\nyou dive into a model’s performance in TorchInductor. When a model is not\nrunning as fast as expected, you may want to check individual kernels of the\nmodel. Usually, those kernels taking the majority of the\nGPU time are the most interesting ones. After that, you\nmay also want to run individual kernels directly and inspect its perf.\nPyTorch provides tools to cover everything mentioned above.",
    "1873": "一级标题：TorchInductor GPU Profiling\n二级标题：Relevant Environment Variables\n内容：\nYou can use the following environment variables in your analysis:\n\n-  ``TORCHINDUCTOR_UNIQUE_KERNEL_NAMES``\n\n   -  By default, TorchInductor names a Triton kernel as ``‘triton\\_’``. When\n      this environmental variable is enabled, inductor generates a more\n      meaningful kernel name in the trace, for example,\n      ``triton_poi_fused_cat_155`` which contains the kernel category\n      (``poi`` for pointwise) and original ATen\n      operator. This config is disabled by default to improve the chance of\n      compilation cache hit.\n\n-  ``TORCHINDUCTOR_BENCHMARK_KERNEL``\n\n   -  Enabling this will make inductor codegen harness to benchmark\n      individual triton kernels.\n\n-  ``TORCHINDUCTOR_MAX_AUTOTUNE``\n\n   -  Inductor autotuner will benchmark more ``triton.Configs`` and pick the\n      one with the best performance results. This will increase compilation\n      time with the hope to improve performance.",
    "1874": "一级标题：TorchInductor GPU Profiling\n二级标题：Breakdown Model GPU Time\n内容：\nBelow are the steps to breakdown execution time of a model into\nindividual kernels. We take ``mixnet_l`` as an example.\n\n1. Run the benchmark script for the model:\n\n   ```bash\n      TORCHINDUCTOR_UNIQUE_KERNEL_NAMES=1 TORCHINDUCTOR_BENCHMARK_KERNEL=1\n      python -u benchmarks/dynamo/timm_models.py –backend inductor –amp\n      –performance –dashboard –only mixnet_l –disable-cudagraphs –training\n   ```\n   ```{note}\n   The tool relies on kernel name to decide its category. Enabling\n      ``TORCHINDUCTOR_UNIQUE_KERNEL_NAMES`` is crucial for that.\n   ```\n2. In the output log, look for lines:\n\n   ```bash\n      **Compiled module path:\n      /tmp/torchinductor_shunting/qz/cqz7hvhood7y3psp7fy6msjxsxyli7qiwiybizdwtjw6ffyq5wwd.py**\n   ```\n\nWe have one line for each compiled module. If there are no extra graph\nbreaks, we would see 2 such lines in the log, one for the forward graph\nand one for the backward graph.\n\nFor our example command, we get the following compiled module for the\nforward and backward graphs respectively:\n\n-  [Forward graph compiled module](https://gist.github.com/shunting314/c2a4d8a28b00fcb5586d0e9d9bf77f9f)\n-  [Backward graph compiled module](https://gist.github.com/shunting314/48efc83b12ec3ead950052e4a0220b10)\n\n3. Now we can dive into the perf for each individual compiled module.\n   Let’s pick the one for the forward graph for illustration purposes.\n   I’ll name it ``fwd.py`` for convenience. Run it directly with the\n   ``-p`` argument:\n\n   ```bash\n      **> python fwd.py -p**\n   ```\n\nSee the full output log in this [example gist](https://gist.github.com/shunting314/8243734a38b5733ea78479209c0ae893)\n\nIn the output, you can notice the following:\n\n* We write a chrome trace file for the profile so we can load the trace and interact with it. In the log, look for lines as follows to find the path of the trace file.\n\n\n  **Chrome trace for the profile is written to /tmp/compiled_module_profile.json**\n\n   Loading the trace into Chrome (visit chrome://tracing in the chrome browser and load the file as the UI suggested) will show UI as follows:\n\n   ```{image} _static/img/inductor_profiling/trace.png\n   ```\n\n   You can zoom in and out to check the profile.\n\n* We report the percent of GPU time regarding to the wall time by log line like:\n\n  **Percent of time when GPU is busy: 102.88%**\n\n  Sometimes you may see a value larger than 100%. The reason is because PyTorch\n  uses the kernel execution time with profiling enabled while using wall time\n  with profiling disabled. Profiling may distort the kernel execution time a\n  bit. But overall it should not be a big deal.\n\n  If we run the model like ``densenet121`` with a small batch size, we would see\n  low percent of time when GPU is busy:\n\n   ```bash\n     (Forward graph) Percent of time when GPU is busy: 32.69%\n   ```\n\n  This means the model has a lot of CPU overhead. This is consistent with\n  the fact that enabling cudagraphs improve densenet121’s perf a lot.\n\n* We can break down the GPU time to different categories of kernels.\n  In the ``mixnet_l`` example, we see\n\n  -  pointwise kernel takes 28.58%\n  -  reduction kernel takes 13.85%\n  -  persistent reduction kernel takes 3.89%\n  -  the rest are cutlass/cudnn kernels for mm/conv which takes 56.57%\n\n  This information can be found in the summary line (last line)\n  of the report for each kernel category.\n\n* We also call zoom into a certain category of kernels. For example,\n  let’s check reduction kernels:\n\n  ```{image} _static/img/inductor_profiling/kernel_breakdown.png\n  ```\n\n  We can see an ordered table of execution time for each individual\n  reduction kernel. We also see how many times a kernel is executed. This\n  is helpful for a few reasons:\n\n  - If a kernel only takes a tiny amount of time, for example, 0.1%,\n    improving it will at most bring 0.1% overall gain. It is not\n    worth spending a lot of effort on it.\n  - Ff a kernel takes 2% of time, improving it by 2x will bring in 1%\n    overall gain which justifies the effort.",
    "1875": "一级标题：TorchInductor GPU Profiling\n二级标题：Benchmark Individual Triton Kernel\n内容：\nLet’s say we want to take a closer look at\n``triton_red_fused\\__native_batch_norm_legit_functional_16`` which is the\nmost expensive reduction kernel and takes 2.19% of overall wall time for\nthe forward graph.\n\nWe can lookup the kernel name in the ``fwd.py``, and find comment like:\n\n**# kernel path:\n/tmp/torchinductor_shunting/jk/cjk2vm3446xrk7rth7hr6pun7xxo3dnzubwcn6ydrpifal4eykrz.py**\n\n```{image} _static/img/inductor_profiling/inductor_code.png\n```\n\nI’ll rename it k.py for convenience. Here is a paste for this [file](https://gist.github.com/shunting314/96a0afef9dce53d6357bf1633094f358).\n\n``k.py`` is a standalone Python module containing the kernel code and its\nbenchmark.\n\nRun ``k.py`` directly will report its execution time and bandwidth:\n\n ```{image} _static/img/inductor_profiling/terminal_printout.png\n ```\n\nWe can check if max-autotune helps this kernel, by running:\n\n```bash\n   **TORCHINDUCTOR_MAX_AUTOTUNE=1 python /tmp/k.py**\n```\nWe may also temporarily add more reduction heuristics and run the script\nagain to check how that helps with the kernel.",
    "1876": "一级标题：IRs\n二级标题：无\n内容：\nPyTorch 2.0 offers two set of IRs for backends to interface with: Core Aten IR and Prims IR.",
    "1877": "一级标题：IRs\n二级标题：Core Aten IR\n内容：\nCore aten ops is the core subset of aten operators that can be used to compose other operators.\nCore aten IR is fully functional, and there is no `inplace` or `_out` variants in this opset.\nIn contrast to Prims IR, core aten ops reuses the existing aten ops in \"native_functions.yaml\",\nand it doesn't further decompose ops into explicit type promotion and broadcasting ops.\nThis opset is designed to serve as the functional IR to interface with backends.\n\n```{warning}\n  This opset is still under active development, more ops will be added in the future.\n```\n\n```{csv-table}\n   :file: ../build/ir/aten_ops.csv\n   :widths: auto\n   :header-rows: 1\n```",
    "1878": "一级标题：IRs\n二级标题：Prims IR\n内容：\nPrims IR is a set of primitive operators that can be used to compose other operators.\nPrims IR is a lower level opset than core aten IR, and it further decomposes ops into explicit\ntype promotion and broadcasting ops: prims.convert_element_type and prims.broadcast_in_dim.\nThis opset is designed to interface with compiler backends.\n\n```{warning}\n  This opset is still under active development, more ops will be added in the future.\n```\n\n```{csv-table}\n   :file: ../build/ir/prims_ops.csv\n   :widths: auto\n   :header-rows: 1\n```",
    "1879": "一级标题：PyTorch 2.0 NNModule Support\n二级标题：无\n内容：\n**Author**: [Will Constable](https://github.com/wconstab)\n\n`torch.compile` has special handling for torch.nn.Module objects, tracing them differently than it traces\narbitrary python classes, with the intent of producing faster code by making assumptions about the structure.\n\nThis doc describes some of the tradeoffs or edge cases that come up due to this specialization.",
    "1880": "一级标题：PyTorch 2.0 NNModule Support\n二级标题：NNModule Hooks Support\n内容：\nPreviously, `torch.compile` had no support for hooks on nn.Modules, and if hooks were registered\nthey would simply be ignored in the compiled program. Indeed many users do not\nuse nn.Module hooks at all, or only use them for debug workflows, but there are valid use cases\nfor composing nn.Module hooks with `torch.compile`.\n\nHooks that are orchestrated via nn.Module.__call__ implementation include `_forward_pre_hooks`,\n`forward_hooks`, `_backward_pre_hooks`, and `_backward_hooks`, and will be referred to as 'call hooks'.\nThese hooks are partially supported by `torch.compile` with limitations described below.\n\nAnother category of hooks includes `_state_dict_hooks` and its `pre` and `load_` variants, and are still\nunsupported by `torch.compile`.",
    "1881": "一级标题：PyTorch 2.0 NNModule Support\n二级标题：`nn.Module.__call__` Hooks Usage and limitations\n内容：\nBy default, `torch.compile` will trace the contents of `nn.Module.__call__` which means it will encounter\nand run forward/pre-forward hooks.  If you install hooks before calling `torch.compile` and then do not remove\nor alter the hooks later, your use case should be supported by default.\n\nBackward/Pre-backward hooks are generally also supported, with similar caveats: currently graph-breaks in dynamo\noccur when accessing backward_hooks dicts, which is probably avoiable with some work.  Graph-breaks also impact the\ntiming of firing backward hooks, since graph-segments are run as autograd-functions which produce all their grads at\nthe same time.  Assuming it were possible for dynamo to not graph-break on the presence of backward-hooks, we would\nstill expect the backward hooks for a series of modules to all fire together after the whole compiled graph's backward\nran.\n\n**hooks on 'allowed modules'**\n`torch.compile` treats common modules such as torch.conv, as well as modules that are difficult to trace, specially\nby allowing them to be called opaquely in the dynamo graph instead of traced into by dynamo.  For such modules, hooks\ncurrently trigger a graph-break so that the affected modules run outside of dynamo.  Depending on the model, this could\nintroduce a significant performance regression, and additional work is required to improve this support.\n\n**skip_nnmodule_hook_guards**\nBy default, `torch._dynamo.config.skip_nnmodule_hook_guards` is set to True, meaning no guards will be installed\non each nn.Module hook dictionary, improving runtime by reducing guard execution time, at the cost of not noticing\nif any hook dict is changed after compilation.\n\nIf you want to be able to remove or modify hooks after compilation and have `torch.compile` react appropriately\n(by recompiling), then you need to set `skip_nnmodule_hook_guards=False` and expect a runtime penalty for the added\nguards.\n\nTODO: confirm if backward/pre_backward hooks are working or not and document accordingly",
    "1882": "一级标题：PyTorch 2.0 NNModule Support\n二级标题：state_dict Hooks\n内容：\nState dict hooks have not yet been supported in `torch.compile`.\n\n\nTODO: warn_once if graph-breaking on hooks.  warn_once to point to this doc if hooks are present.",
    "1883": "一级标题：PyTorch 2.0 Performance Dashboard\n二级标题：无\n内容：\n**Author:** [Bin Bao](https://github.com/desertfire) and [Huy Do](https://github.com/huydhn)\n\nPyTorch 2.0's performance is tracked nightly on this [dashboard](https://hud.pytorch.org/benchmark/compilers).\nThe performance collection runs on 12 GCP A100 nodes every night. Each node contains a 40GB A100 Nvidia GPU and\na 6-core 2.2GHz Intel Xeon CPU. The corresponding CI workflow file can be found\n[here](https://github.com/pytorch/pytorch/blob/main/.github/workflows/inductor-perf-test-nightly.yml).",
    "1884": "一级标题：PyTorch 2.0 Performance Dashboard\n二级标题：How to read the dashboard?\n内容：\nThe landing page shows tables for all three benchmark suites we measure, ``TorchBench``, ``Huggingface``, and ``TIMM``,\nand graphs for one benchmark suite with the default setting. For example, the default graphs currently show the AMP\ntraining performance trend in the past 7 days for ``TorchBench``. Droplists on the top of that page can be\nselected to view tables and graphs with different options. In addition to the pass rate, there are 3 key\nperformance metrics reported there: ``Geometric mean speedup``, ``Mean compilation time``, and\n``Peak memory footprint compression ratio``.\nBoth ``Geometric mean speedup`` and ``Peak memory footprint compression ratio`` are compared against\nthe PyTorch eager performance, and the larger the better. Each individual performance number on those tables can be clicked,\nwhich will bring you to a view with detailed numbers for all the tests in that specific benchmark suite.",
    "1885": "一级标题：PyTorch 2.0 Performance Dashboard\n二级标题：What is measured on the dashboard?\n内容：\nAll the dashboard tests are defined in this\n[function](https://github.com/pytorch/pytorch/blob/3e18d3958be3dfcc36d3ef3c481f064f98ebeaf6/.ci/pytorch/test.sh#L305).\nThe exact test configurations are subject to change, but at the moment, we measure both inference and training\nperformance with AMP precision on the three benchmark suites. We also measure different settings of TorchInductor,\nincluding ``default``, ``with_cudagraphs (default + cudagraphs)``, and ``dynamic (default + dynamic_shapes)``.",
    "1886": "一级标题：PyTorch 2.0 Performance Dashboard\n二级标题：Can I check if my PR affects TorchInductor's performance on the dashboard before merging?\n内容：\nIndividual dashboard runs can be triggered manually by clicking the ``Run workflow`` button\n[here](https://github.com/pytorch/pytorch/actions/workflows/inductor-perf-test-nightly.yml)\nand submitting with your PR's branch selected. This will kick off a whole dashboard run with your PR's changes.\nOnce it is done, you can check the results by selecting the corresponding branch name and commit ID\non the performance dashboard UI. Be aware that this is an expensive CI run. With the limited\nresources, please use this functionality wisely.",
    "1887": "一级标题：PyTorch 2.0 Performance Dashboard\n二级标题：How can I run any performance test locally?\n内容：\nThe exact command lines used during a complete dashboard run can be found in any recent CI run logs.\nThe [workflow page](https://github.com/pytorch/pytorch/actions/workflows/inductor-perf-test-nightly.yml)\nis a good place to look for logs from some of the recent runs.\nIn those logs, you can search for lines like\n`python benchmarks/dynamo/huggingface.py --performance --cold-start-latency --inference --amp --backend inductor --disable-cudagraphs --device cuda`\nand run them locally if you have a GPU working with PyTorch 2.0.\n``python benchmarks/dynamo/huggingface.py -h`` will give you a detailed explanation on options of the benchmarking script.",
    "1888": "一级标题：Profiling to understand torch.compile performance\n二级标题：无\n内容：",
    "1889": "一级标题：Profiling to understand torch.compile performance\n二级标题：What to use torch.profiler for:\n内容：\ntorch.profiler is helpful for understanding the performance of your program at a kernel-level granularity - for example, it can show graph breaks and resources utilization at the level of the program. The data provided by the profiler can often help users understand where to investigate further to understand model performance.\n\nTo understand kernel-level performance, other tools exist, such as [Nvidia Nsight compute tool](https://developer.nvidia.com/nsight-compute), [AMD Omnitrace](https://rocm.docs.amd.com/projects/omnitrace/en/latest/),  Intel® VTune™ Profiler or [inductor's profiling tools](https://docs.pytorch.org/docs/stable/torch.compiler_inductor_profiling.html#torchinductor-gpu-profiling) can be used.\n\nSee also the [general pytorch profiler guide](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html).",
    "1890": "一级标题：Profiling to understand torch.compile performance\n二级标题：Basics of using torch.profiler and viewing traces\n内容：\n**Example program**: We'll use this example of profiling resnet18. Notice the following parts of this example program:\n\n* Include a warm-up run to wait for compilation to complete (this will warm up systems like the CUDA caching allocator)\n* Use `torch.profiler.profile()` context for profiling the section we are interested in\n* Use `prof.export_chrome_trace(\"trace.json\")` to export the profiling artifact.\n\n```python\n\n    import torch\n    from torchvision.models import resnet18\n\n    device = 'cuda'      # or 'cpu', 'xpu', etc.\n    model = resnet18().to(device)\n\n    inputs = [torch.randn((5, 3, 224, 224), device=device) for _ in range(10)]\n\n    model_c = torch.compile(model)\n\n    def fwd_bwd(inp):\n        out = model_c(inp)\n        out.sum().backward()\n\n    # warm up\n    fwd_bwd(inputs[0])\n\n    with torch.profiler.profile() as prof:\n        for i in range(1, 4):\n            fwd_bwd(inputs[i])\n            prof.step()\n\n    prof.export_chrome_trace(\"trace.json\")\n```\n\n**Viewing chrome traces**: In the Chrome browser, open chrome://tracing and load the json file. Use the “w” and “s” keys to zoom in and out, and use “a” and “d” to scroll left and right. “?” will show a “help” screen with a list of shortcuts.\n\n```{figure}  _static/img/profiling_torch_compile/basic_chrome_trace.png\n:alt: Example of a basic chrome trace, visualized in the chrome://tracing viewer\n```\n\nHere, we observe:\n* CompiledFunction and CompiledFunctionBackward events, which correspond to the dynamo-compiled regions.\n* CPU events at the top, and GPU events at the bottom.\n\n**Flows between CPU and accelerator events**\n\nEvery kernel on the accelerator occurs after being launched by code running on the CPU. The profiler can draw connections (i.e. “flows”) between the accelerator and CPU events to show which CPU event launched a accelerator kernel. This is particularly helpful because, with a few exceptions, accelerator kernels are launched asynchronously.\n\nTo view a flow connection, click on a GPU kernel and click “ac2g”:\n\n```{figure}  _static/img/profiling_torch_compile/ac2g.png\n:alt: Visualization in the chrome://trace viewer, showing an async flow between a kernel and its launching location.\n```\n\nAlternatively, turn on *all* flows with the “Flow events” dropdown at the top.",
    "1891": "一级标题：Profiling to understand torch.compile performance\n二级标题：Working around CUDA Graph profiling issues\n内容：\nWhen CUDA graphs are enabled, some CUDA configurations (driver version under 525.85.12 or CUDA < 12)  can encounter issues between the profiling tools and CUDA graphs. To fix these issues, add an empty profiling context at the top of your program:\n\n```python\n\n    import torch\n\n    torch.profiler._utils._init_for_cuda_graphs()\n\n    # ... rest of program\n```",
    "1892": "一级标题：Profiling to understand torch.compile performance\n二级标题：Understanding compilation time\n内容：\nTo understand why compilation is taking a long time, you can profile the first invocation of a torch.compile-ed program. Keep in mind that profile traces of compilations can be distorted more than typical profiling, because compilation workloads can be quite different from typical PyTorch workloads. In some cases, trace files may also be quite large. Traces > 1GB can be difficult to open with the chrome tracing tool.\n\nNote: roughly the same information can also be obtained in non-graphical format with :code:`torch._dynamo.utils.compile_times()`. This utility won’t show when the compilation steps occur, but it will show the amount of time spent on each step - and times will not be affected by any profiling overhead.\n\nSee an example below:\n\n```python\n\n    import torch\n    from torchvision.models import resnet18\n\n    # user can switch between cuda and xpu\n    device = 'cuda'\n    model = resnet18().to(device)\n    inputs = [torch.randn((5, 3, 224, 224), device=device) for _ in range(10)]\n\n    model_c = torch.compile(model)\n\n    def fwd_bwd(inp):\n        out = model_c(inp)\n        out.sum().backward()\n\n    def warmup_compile():\n        def fn(x):\n            return x.sin().relu()\n\n        x = torch.rand((2, 2), device=device, requires_grad=True)\n        fn_c = torch.compile(fn)\n        out = fn_c(x)\n        out.sum().backward()\n\n    with torch.profiler.profile() as prof:\n        with torch.profiler.record_function(\"warmup compile\"):\n            warmup_compile()\n\n        with torch.profiler.record_function(\"resnet18 compile\"):\n            fwd_bwd(inputs[0])\n\n    prof.export_chrome_trace(\"trace_compile.json\")\n```\n\n```{figure} _static/img/profiling_torch_compile/compilation_profiling.png\n:alt: A visualization in the chrome://trace viewer, showing dynamo and inductor compilation steps\n```\n\nNote a few things:\n\n* The first invocation should occur *during* profiling in order to capture compilation\n* Add a warm-up compilation in order to initialize any systems that need to be lazily initialized.\n\n# Finding graph breaks: \"Torch-Compiled Region\" and \"CompiledFunction\"\n\nAlthough there are logging tools for identifying graph breaks, the profiler provides a quick visual method of identifying :ref:`graph breaks <torch.compiler_graph_breaks>`. There are two profiler events to look for: **Torch-Compiled Region** and **CompiledFunction**.\n\n**Torch-Compiled Region** - which was introduced in PyTorch 2.2 - is a profiler event that covers the entire compiled region. Graph breaks almost always look the same: nested “Torch-Compiled Region” events. Starting in PyTorch 2.5, the profiler event will also contain the frame ID and the frame compile ID. The frame ID is a unique identifier for the frame, and the frame compile ID denotes how many times the frame has been compiled.\n\nIf you run two separate functions with torch.compile() applied independently on each of them, you should generally expect to see two adjacent (i.e NOT stacked/nested) Torch-Compiled regions. Meanwhile, if you encounter graph breaks (or disable()'ed/skipped regions), expect nested “Torch-Compiled Region” events.\n\n**CompiledFunction** - introduced in PyTorch 2.0 - is a profiler event that appears when gradients are required for any inputs.  Each graph break will interrupt a CompiledFunction block, splitting it in two. CompiledFunction events only appear when Autograd is involved, i.e. some of the input tensors to the graph have requires_grad=True.\n\nWhen a CompiledFunction appears in a trace, it is typically paired with a CompiledFunctionBackward event in the backward pass. A “fwd-bwd link” should appear in the trace connecting the two, if the backward function is called.\n\nIf your use case includes a graph that doesn't require grad and doesn't include \"Torch-Compiled Region\" events, it can be more difficult to identify whether torch.compile is being applied correctly. One clue can be the existence of Inductor-generated Triton kernels.\n\nSee the synthetic example below for a demonstration:\n\n```python\n\n    import torch\n    import torch._dynamo\n    # user can switch between cuda and xpu\n    device = 'cuda'\n\n    class ModelWithBreaks(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            def create_sequential():\n                return torch.nn.Sequential(\n                    torch.nn.Linear(128, 128),\n                    torch.nn.ReLU(),\n                    torch.nn.Linear(128, 128),\n                    torch.nn.ReLU(),\n                )\n            self.mod1 = create_sequential()\n            self.mod2 = create_sequential()\n            self.mod3 = create_sequential()\n            self.mod4 = create_sequential()\n\n        def forward(self, inp):\n            mod1 = self.mod1(inp)\n            torch._dynamo.graph_break()\n            mod2 = self.mod2(mod1)\n            torch._dynamo.graph_break()\n            mod3 = self.mod3(mod2)\n            torch._dynamo.graph_break()\n            mod4 = self.mod4(mod3)\n            return mod4\n\n    model = ModelWithBreaks().to(device)\n    inputs = [torch.randn((128, 128), device=device) for _ in range(10)]\n\n    model_c = torch.compile(model)\n\n    def fwd_bwd(inp):\n        out = model_c(inp)\n        out.sum().backward()\n\n    # warm up\n    fwd_bwd(inputs[0])\n\n    with torch.profiler.profile() as prof:\n        for i in range(1, 4):\n            fwd_bwd(inputs[i])\n            prof.step()\n\n    prof.export_chrome_trace(\"trace_break.json\")\n```\n\n```{figure} _static/img/profiling_torch_compile/graph_breaks_with_torch_compiled_region.png\n:alt: Visualization in the chrome://trace viewer, showing nested Torch-Compiled Region events and multiple CompiledFunction events - indicating graph breaks.\n```",
    "1893": "一级标题：Profiling to understand torch.compile performance\n二级标题：Operator Kernels\n内容：\nWhen an operator is launched, we expect to see a few events:\n\n1. CPU-side event\n2. Kernel launch (if dealing with a GPU kernel)\n3. GPU-side event\n\n```{figure} _static/img/profiling_torch_compile/kernel_launch_labeled.png\n:alt: Visualization in the chrome://trace viewer, showing the three types of events - CPU-side event, kernel launch, and GPU-side event\n```\n\n**Inductor-generated Triton kernels:**\n1. The **CPU-side event** should appear as an event prefixed with \"triton\\_\". The events currently have minimal information - the kernel name and a launch, but less information than typical aten kernel launches (which contain input shapes, types, etc.).\n2. The **kernel launch** should appear as cuLaunchKernel instead of cudaLaunchKernel (cudaLaunchKernel is typical for aten ops)\n3. The **GPU-side event** should appear, and how descriptive the name will be depends on the inductor config for unique_kernel_names\n\n```{figure} _static/img/profiling_torch_compile/triton_kernel_launch.png\n```\n\n**Non-Inductor generated Triton kernels:**\n\n1. The **CPU-side** event may not appear in traces; the machinery for automatically inserting a profiler event is currently implemented at the Inductor level, so Triton kernels that bypass Inductor may not appear in traces, unless users have annotated them manually\n2. The **kernel launch** should appear s cuLaunchKernel instead of cudaLaunchKernel (cudaLaunchKernel is typical for aten ops)\n3. The **GPU-side** event should appear, named similarly to the triton kernel that was authored.\n\n```{figure} _static/img/profiling_torch_compile/noninductor_triton_kernel.png\n```\n\n**Inductor-generated CPU kernels:**\n\n1. The **CPU-side event** will not appear in traces; we haven't added profiling for this yet.\n2. The **kernel launch** and **GPU-side events** don't exist\n\n**Non-Triton kernels** (i.e. aten kernels or custom ops) should also be expected to sometimes appear in traces. Sometimes, Inductor will fall back to the original op implementation, in which case you will see a call to the aten op.",
    "1894": "一级标题：Profiling to understand torch.compile performance\n二级标题：Launch overhead\n内容：\nOne common issue is bad GPU utilization. A quick way to identify this is if there are large gaps between kernels on the GPU:\n\n```{figure} _static/img/profiling_torch_compile/cpu_bound.png\n:alt: Visualization in the chrome://trace viewer, showing large gaps between GPU kernels. This indicates that the model is CPU bound, likely due to overhead during kernel launches.\n```\n\nThis is often the result of CPU overhead, e.g. if the amount of time spent on the CPU between kernel launches is larger than the amount of time spent by the GPU to process the kernels. The issue is more common for small batch sizes.\n\nWhen using inductor, enabling CUDA graphs can often help improve performance when launch overhead is a concern.",
    "1895": "一级标题：Writing Graph Transformations on ATen IR\n二级标题：无\n内容：",
    "1896": "一级标题：Writing Graph Transformations on ATen IR\n二级标题：Passes\n内容：\nSince the ATen IR sits at the FX Graph/GraphModule level, any\ntransformations written for FX Graphs can be easily applied onto the\nATen IR. If you’re familiar with writing FX graph transformations, then\nthis will be the same.\n\nThe most direct way of writing transformations is by looping through the\ngiven graph and directly manipulating the nodes within the graph.\n\nFor example, let’s say we want to replace\n`torch.ops.aten.add.Tensor()` calls with\n`torch.ops.aten.mul.Tensor()` calls:\n\n```python\nimport torch\n\ndef replace_add_with_mul(gm: torch.fx.GraphModule) -> torch.fx.GraphModule:\n    for node in gm.graph.nodes:\n        if node.op == \"call_function\" and node.target == torch.ops.aten.add.Tensor:\n            node.target = torch.ops.aten.mul.Tensor\n```\n\nWe can also delete and append new nodes through FX utility functions\nthat can be found in the\n[Graph](https://pytorch.org/docs/stable/fx.html#torch.fx.Graph)\ndocumentation. For example, if we want to insert a\n`torch.ops.aten.relu.default()` after the `add` call:\n\n```python\nimport torch\n\ndef insert_relu_after_add(gm: torch.fx.GraphModule) -> torch.fx.GraphModule:\n    for node in gm.graph.nodes:\n        if node.op == \"call_function\" and node.target == torch.ops.aten.add.Tensor:\n\n            # Specifies the insertion point. Any nodes added to the graph within\n            # this scope will be inserted after `node`\n            with gm.graph.inserting_after(node):\n                # Insert a new `call_function` node with op `torch.ops.aten.relu.default`\n                new_relu_node = gm.graph.call_function(torch.ops.aten.relu.default, args=(node,))\n                # Replace all the places that use `node` to now use the `new_relu_node`\n                node.replace_all_uses_with(new_relu_node)\n```\n\nIn general, transformations can be roughly categorized into a couple of\naxis:\n\nAxis A: 1. Creating one-to-X mapping (eg. decomposition) 2. Creating\nmany-to-one mapping (eg. fusion)\n\nAxis B: 1. Doing forwards iteration (eg. shape propagation) 2. Doing\nbackwards iteration (eg. dead code elimination)\n\nAxis C: 1. Dependent on local node information (eg. out-variant\nconversion) 2. Dependent on global graph information (eg. memory\nplanning)\n\nOur projection on the frequency of these use cases are: 1. A.1, B.1, C.1\n2\\. A.2 3. B.2, C.2\n\nAlthough we can make all graph transformations through directly\nmanipulating the graph, we also provide some helper utilities for some\nease of use for the level 1 and 2 use-cases.\n\n### Transformer\n\nFor level 1 uses cases (creating one-to-X mappings, doing forwards\niterations, and looking at local node information), we can utilize the\n[Transformer](https://pytorch.org/docs/stable/fx.html#torch.fx.Transformer)\nclass to execute each node and recreate a graph, except with the\ntransformations specified.\n\n#### One-to-One Pass\n\nAn example for one-to-one mappings, if we wanted to replace an op A with\nanother op B, we can run the GraphModule, and very time we see op A,\nreturn op B.\n\nAn example is:\n\n```python\nclass ReplaceAddWithMul(torch.fx.Transformer):\n    def call_function(self, target, args, kwargs):\n        if target != torch.ops.aten.add.Tensor:\n            return super().call_function(target, args, kwargs)\n        return super().call_function(torch.ops.aten.mul.Tensor, args, kwargs)\n\ntransformed_graph_module = ReplaceAddWithMul(graph_module).transform()\n```\n\nThe `super().call_function(target, args, kwargs, meta)` call creates a\n`call_function` FX node, and returns the result of running the\noperator with the given arguments.\n\n#### One-to-X Pass\n\nIf we wanted to do one-to-X mappings, like replacing op A with 2 other\nops B and C, we would then make 2 calls to `super().call_function` to\ncreate 2 FX nodes, one with op B and another with op C, and return the\nresult of running op C.\n\nFor example:\n\n```python\nclass ReplaceAddWithMulSub(torch.fx.Transformer):\n    \"\"\"\n    Original:\n        def f(x, y):\n            return x + y\n\n    After pass:\n        def f(x, y):\n            z = x * y\n            return z - y\n    \"\"\"\n    def call_function(self, target, args, kwargs):\n        if target != torch.ops.aten.add.Tensor:\n            return super().call_function(target, args, kwargs)\n\n        x, y = args\n\n        mul_res = super().call_function(torch.ops.aten.mul.Tensor, args, {})\n        return super().call_function(torch.ops.aten.sub.Tensor, (mul_res, y), {})\n\ntransformed_graph_module = ReplaceAddWithMulSub(graph_module).transform()\n```\n\n#### One-to-None Pass\n\nIf we wanted to remove an op, we can just return the value passed into\nthe function:\n\n```python\nclass RemoveDetachPass(torch.fx.Transformer):\n    def call_function(self, target, args, kwargs):\n        if target not in (\n            torch.ops.aten.detach.default,\n            torch.ops.aten.detach_copy.default,\n        ):\n            return super().call_function(target, args, kwargs, meta)\n\n        assert len(args) == 1\n        return args[0]\n\ntransformed_graph_module = RemoveDetachPass(graph_module).transform()\n```\n\n#### Utilizing Local Information\n\nAn example of utilizing local node information is, if we wanted to\nconvert all the scalars within the graph to tensors, we can run the\ngiven `fx.GraphModule`, and for every argument that contains a scalar,\nwe convert it to a tensor. It might look something like:\n\n```python\ndef args_map(target, fn, args, kwargs):\n    assert isinstance(args, tuple)\n    assert isinstance(kwargs, dict)\n    args = list(args)\n    kwargs = kwargs.copy()\n\n    # Update the argument based on the function passed\n    def update(key, args, schema):\n        args[key] = fn(args[key], schema)\n\n    # Update each argument in the schema\n    for i, schema in enumerate(target._schema.arguments):\n        if schema.name in kwargs:\n            update(schema.name, kwargs, schema)\n        elif not schema.kwarg_only and i < len(args):\n            update(i, args, schema)\n    return tuple(args), kwargs\n\nclass ScalarToTensorPass(torch.fx.Transformer):\n    def call_function(self, target, args, kwargs):\n        breakpoint()\n        def try_coerce(value, arg):\n            return (\n                torch.tensor(value)\n                if isinstance(value, (float, int, bool))\n                and type(arg.type) == torch.TensorType\n                else value\n            )\n\n        args, kwargs = args_map(target, try_coerce, args, kwargs)\n        return super().call_function(target, args, kwargs)\n\ntransformed_graph_module = ScalarToTensorPass(graph_module).transform()\n```\n\n### Subgraph Rewriter\n\nFor creating many-to-one mappings, we can utilize FX’s [subgraph\nrewriter](https://github.com/pytorch/pytorch/blob/main/torch/fx/subgraph_rewriter.py).\nGiven a `pattern`, it creates a subgraph of operators matching to the\npattern, and then replaces each matched subgraph with the\n`replacement`.\n\nNote:\n\n```\nThis is an inplace operation.\n```\n\nThe `pattern` and `replacement` inputs must be callable functions or\nGraphModules containing the same operators that are used within the\ngraph (ATen ops) so that the subgraph rewriter can find the correct\npattern in the graph. Inputs to the pattern/replacement callables will\nbe treated as wildcards when matching.\n\nAn example:\n\n```python\nfrom torch.fx import subgraph_rewriter\n\ndef replace_patterns(graph_module):\n    def pattern(x, y):\n        x = torch.ops.aten.add.Tensor(x, y)\n        x = torch.ops.aten.mul.Tensor(x, y)\n        return x\n\n    def replacement(x, y):\n        return torch.ops.aten.sub.Tensor(x, y)\n\nreplaced_patterns = subgraph_rewriter.replace_pattern_with_filters(\n    traced_module, pattern, replacement\n)\n```\n\nThe subgraph rewriter returns a list of `ReplacedPatterns`:\n\n```python\n@dataclass\nclass ReplacedPatterns:\n    # Node from which the match was found\n    anchor: Node\n    # Maps nodes in the pattern subgraph to nodes in the larger graph\n    nodes_map: Dict[Node, Node]\n    # List of nodes that were added into the graph\n    replacements: List[Node]\n```\n\nNote:\n\n```\nThe nodes created by the subgraph rewriter will not have the metadata that\nis populated in the matched nodes, but you can use\n`ReplacedPatterns.nodes_map` to find the nodes in the original graph that\nwere matched, and `ReplacedPatterns.replacements` to find the nodes that\nwere replaced in the transformed graph.\n```",
    "1897": "一级标题：Writing Graph Transformations on ATen IR\n二级标题：Pass Manager\n内容：\nThe\n[PassManager](https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/infra/pass_manager.py)\nis a class used to run multiple passes on a given graph module. When\ninitializing a `PassManager` instance, we pass in a list of passes\nthat we want to run and set a couple of flags. To run the collection of\npasses on a graph module, we can pass the graph module directly to the\n`PassManager` instance.\n\nAn example:\n\n```python\nfrom torch.fx.passes.infra.pass_manager import PassManager\n\npm = PassManager(\n    passes=[replace_add_with_div, replace_div_with_mul],\n    run_checks_after_each_pass=True,\n    suppress_check_failures=False,\n)\ngraph_module_out = pm(graph_module)\n```\n\nTo add a common set of checks that are run after each pass, we can call\nthe function `set_checks(check: Callable)` which takes in a callable\nfunction as input. If the `run_checks_after_each_pass` flag is set,\nthe `check` will be called after each pass is run on the graph module.\n\nAn example:\n\n```python\npm = PassManager(passes=[replace_add_with_div, replace_div_with_mul])\n\ndef check_div_target(graph_module):\n    for node in graph_module.graph.nodes:\n        if node.op == \"call_function\" and node.target != torch.div:\n            raise ValueError(\"Target should be div!\")\n\npm.add_checks(check_div_target)\n\npm(graph_module)    # raises ValueError after replace_div_with_mul pass\n```",
    "1898": "一级标题：Writing Graph Transformations on ATen IR\n二级标题：Partitioner\n内容：\nThere are a couple of common FX graph based partitioners we can use to\npartition the graph.\n\n### Subgraph Matcher\n\nFor finding subgraphs within a graph that match a specific pattern, we\ncan utilize FX’s\n[`SubgraphMatcher`](https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/utils/matcher_utils.py).\n\nClass Attributes:\n\n- `pattern (Graph)`: The targeted matching pattern. Placeholder nodes\n  in the graph will be treated as wildcards when matching.\n- `match_output (bool)`: If True, output node in the pattern graph\n  will be treated as a part of the targeted pattern. If False, output\n  node is ignored during match.\n- `match_placeholder (bool)`: If True, placeholder node in the\n  pattern graph will be treated as a part of the targeted pattern. If\n  False, placeholder nodes will be used a wildcard.\n- `remove_overlapping_matches (bool)`: If True, in the case of\n  overlapping matches, only the first match will be returned.\n- `ignore_literals (bool)`: If True, will not check if literals are\n  equal and will instead treat them as wildcards.\n\nAn example:\n\n```python\nfrom torch.fx.passes.utils.matcher_utils import SubgraphMatcher\n\nclass LargeModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self._weight = torch.nn.Parameter(torch.ones(3, 3))\n        self._bias = torch.nn.Parameter(torch.ones(3, 3))\n\n    def forward(self, x):\n        return torch.ops.aten.addmm.default(self._bias, x, self._weight)\n\nlarge_model_graph = torch.export(LargeModel(), inputs).graph\n\nclass PatternModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self._weight_1 = torch.nn.Parameter(torch.ones(5, 5))\n        self._bias_1 = torch.nn.Parameter(torch.ones(5, 5))\n\n    def forward(self, x):\n        return torch.ops.aten.addmm.default(self._bias_1, x, self._weight_1)\n\npattern_graph = torch.export(PatternModel(), inputs).graph\n\nsubgraph_matcher = SubgraphMatcher(pattern_graph)\nmatch_result = subgraph_matcher.match(large_model_graph)\n```\n\nThe `match` function returns a list of `InternalMatch`:\n\n```python\n@dataclass\nclass InternalMatch():\n    # Nodes from which the match was found\n    anchors: List[Node]\n    # Maps nodes in the pattern subgraph to nodes in the larger graph\n    nodes_map: Dict[Node, Node] = field(default_factory=dict)\n    # Nodes in target graph that are matched placeholder in pattern\n    placeholder_nodes: List[Node] = field(default_factory=list)\n    # Nodes in matched subgraph returned by output\n    returning_nodes: List[Node] = field(default_factory=list)\n```\n\n### Capability Based Partitioner\n\nTo find the largest subgraphs of nodes that support a specific\ninvariant, we can utilize FX’s\n[`CapabilityBasedPartitioner`](https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/infra/partitioner.py#L34).\n\nClass Attributes\n\n- `graph_module (torch.fx.GraphModule)`: The graph module we are\n  partitioning on.\n- `operator_support (OperatorSupportBase)`: The object used to\n  determine if a node in the graph is supported in the partition.\n- `allows_single_node_partition (bool)`: If True, allows single node\n  partitions to be formed.\n- `non_compute_ops (Optional[Sequence[str]])`: A set of ops that are\n  considered to be “non-compute” (ex `torch.ops.aten.view` and\n  `_operator.getitem`, so that the partitioner will not create graphs\n  that only contain these non-compute ops\n- `allowed_single_node_partition_ops (Optional[Sequence[str]])`: A\n  set of ops that are allowed to be in a single node partition.\n\nThe\n[`OperatorSupportBase`](https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/operator_support.py#LL28C1-L28C1)\nclass is used by the partitioner to determine if a specific node in the\ngraph belongs in the partition. This is done by overriding the\n`is_node_supported` function. You can chain multiple\n`OperatorSupportBase` by using\n[`chain`](https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/operator_support.py#L150) (which\nreturns False if any of the OperatorSupportBase return False) and\n[`any_chain`](https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/operator_support.py#L164)\n(which returns True if any of the OperatorSupportBase returns True).\n\nAn example:\n\n```python\nfrom torch.fx.passes.infra.partitioner import CapabilityBasedPartitioner\nfrom torch.fx.passes.operator_support import any_chain, OperatorSupportBase\n\nclass AddMulOperatorSupport(OperatorSupportBase):\n    def is_node_supported(self, submodules, node: torch.fx.Node) -> bool:\n        return node.op == \"call_function\" and node.target in [\n            torch.ops.aten.add.Tensor, torch.ops.aten.mul.Tensor,\n        ]\n\ncapability_partitioner = CapabilityBasedPartitioner(\n    graph_module,\n    op_support,\n)\n\n# Returns a list of partitions (list of nodes that belong in each partition)\npartition_list = capability_partitioner.propose_partitions()\n# Fuses the partitions into graph modules and inserts `call_module` nodes in the graph\nfused_graph_module = capability_partitioner.fuse_partitions(partition_list)\n```",
    "1899": "一级标题：torch.compile Troubleshooting\n二级标题：无\n内容：\nYou're trying to use `torch.compile` on your PyTorch model to enhance its performance\nbut it's not working as expected. Perhaps performance isn't improving, crashes are happening, or compilation time is too long. This article provides tips, workarounds, and debugging tools to help you overcome these challenges.\n\n**Contents**\n\n```{contents}\n:local: true\n```",
    "1900": "一级标题：torch.compile Troubleshooting\n二级标题：Setting Expectations\n内容：\n`torch.compile` is designed as a general-purpose PyTorch compiler.\nUnlike the previous compiler solution, TorchScript, `torch.compile`\nrequires fewer code changes, meaning models typically don't need to be rewritten from scratch.\nIt also manages unsupported code more gracefully - unsupported code results in a lost optimization opportunity rather than a crash.\n\nIn the ideal world, one can simply apply `torch.compile` to any PyTorch model and enjoy automatic speedups.\nHowever, in reality, code complexities can lead to one of three scenarios:\n\n1. `torch.compile` works seamlessly, providing speedups.\n2. Some code modifications are necessary. `torch.compile` doesn't crash or take too long,\n   but you might not be seeing significant performance gains.\n3. Extensive changes to your code are required.\n\nWe anticipate most code will fall under scenarios (1) and (2).\nThis document provides tips, arranged by level of involvement, to help address code issues in scenario (2).\n\n### Compile times\n\n`torch.compile` functions as a just-in-time compiler, so the initial one or two runs\nof the compiled function are expected to be significantly slower. Recompilations, which can occur under certain conditions (detailed below),\nwill also make runs slower. Various `torch.compile` components cache results to\nreduce compilation time for future invocations, even in different processes.\nCold-start (uncached) compilation time typically ranges from seconds to minutes for common or benchmarked models.\nLarger models may take upwards of 30 minutes to a few hours.",
    "1901": "一级标题：torch.compile Troubleshooting\n二级标题：Terminology\n内容：\nThe following terms are relevant to troubleshooting `torch.compile` problems.\n\n### Graph break\n\n`torch.compile` traces your code and attempts to capture your PyTorch code into a\nsingle computation graph of PyTorch operators (FX graph). However, this is not always possible.\nWhen encountering code that can't be traced, a \"graph break\" occurs.\nA graph break involves compiling the FX graph has been determined so far, running the unsupported code,\nthen resuming tracing after the unsupported code with a new FX graph.\nBecause the computation graph is broken up, we lose optimization opportunities,\nso model code should avoid graph breaks whenever possible.\nGraph breaks occur on things like:\n\n- Data-dependent if-statements\n- Many Python built-in functions\n- C functions\n\nBelow is an example of a graph break due to the function `copy.deepcopy` from a Python builtin library\n(exact output may differ).\n\n```py\nimport torch\n\n@torch.compile\ndef fn(x):\n    x = x + 1\n    with open(\"test.txt\", \"r\") as f:\n        return x + len(f.read())\n\nfn(torch.ones(3, 3))\n```\n\n```\n$TORCH_LOGS=\"graph_breaks\" python playground.py\nGraph break in user code at /data/users/williamwen/pytorch/playground.py:7\nReason: Unsupported: builtin: open [<class 'torch._dynamo.variables.constant.ConstantVariable'>, <class 'torch._dynamo.variables.constant.ConstantVariable'>] False\nUser code traceback:\nFile \"/data/users/williamwen/pytorch/playground.py\", line 7, in fn\n    with open(\"test.txt\", \"r\") as f:\nTraceback (most recent call last):\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py\", line 635, in wrapper\n    return inner_fn(self, inst)\n        ^^^^^^^^^^^^^^^^^^^^\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py\", line 2414, in CALL\n    self._call(inst)\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py\", line 2408, in _call\n    self.call_function(fn, args, kwargs)\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py\", line 962, in call_function\n    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/variables/builtin.py\", line 997, in call_function\n    return handler(tx, args, kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/variables/builtin.py\", line 831, in <lambda>\n    return lambda *args: unimplemented(error_msg)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/exc.py\", line 313, in unimplemented\n    raise Unsupported(msg, case_name=case_name)\ntorch._dynamo.exc.Unsupported: builtin: open [<class 'torch._dynamo.variables.constant.ConstantVariable'>, <class 'torch._dynamo.variables.constant.ConstantVariable'>] False\n```\n\n### Guards\n\n`torch.compile` makes some assumptions about runtime values as we trace through code.\nDuring tracing, we generate \"guards\", which are runtime checks for these assumptions.\nGuards are run in future calls to the compiled function to determine if we can reuse previously compiled code.\nExamples of runtime checks are constant values, types, and object IDs.\n\nBelow is an example of generated guards. The `TENSOR_MATCH` guard checks for the input's type, device, dtype, shape, etc.\n\n```py\nimport torch\n\n@torch.compile\ndef fn(x):\n    return x + 1\n\nfn(torch.ones(3, 3))\n```\n\n```\n$ TORCH_LOGS=\"guards\" python playground.py\nGUARDS:\n\nTREE_GUARD_MANAGER:\n+- RootGuardManager\n| +- DEFAULT_DEVICE: utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:471 in init_ambient_guards\n| +- GLOBAL_STATE: ___check_global_state()\n| +- TORCH_FUNCTION_MODE_STACK: ___check_torch_function_mode_stack()\n| +- GuardManager: source=L['x'], accessed_by=DictGetItemGuardAccessor(x)\n| | +- TENSOR_MATCH: check_tensor(L['x'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=False, size=[3, 3], stride=[3, 1])  # return x + 1  # playground.py:6 in fn\n| | +- NO_HASATTR: hasattr(L['x'], '_dynamo_dynamic_indices') == False           # return x + 1  # playground.py:6 in fn\n```\n\n### Recompilation\n\nIf the guards fail for every instance of previously compiled code,\nthen `torch.compile` must \"recompile\" the function, requiring the original code to be traced again.\n\nIn the example below, recompilation is necessary because the guard checking the tensor argument's shape failed.\n\n```py\nimport torch\n\n@torch.compile\ndef fn(x):\n    return x + 1\n\nfn(torch.ones(3, 3))\nfn(torch.ones(4, 4))\n```\n\n```\n$ TORCH_LOGS=\"recompiles\" python playground.py\nRecompiling function fn in /data/users/williamwen/pytorch/playground.py:3\n    triggered by the following guard failure(s):\n    - 0/0: tensor 'L['x']' size mismatch at index 0. expected 3, actual 4\n```\n\n### Dynamic Shapes\n\n`torch.compile` initially assumes tensor shapes are static/constant and guards based on these assumptions.\nBy using \"dynamic shapes,\" we can get `torch.compile` to produce compiled code that can accept\ntensor inputs with different shapes - we avoid recompiling every time shapes differ.\nBy default, automatic dynamic shapes are enabled `torch.compile(dynamic=None)` -\nif compilation fails due to shape mismatch, recompilation is attempted with dynamic shapes.\nDynamic shapes can also be fully enabled `dynamic=True` or disabled `dynamic=False`.\n\nBelow, we enable dynamic shapes and note that we no longer need to recompile.\n\n```py\nimport torch\n\n@torch.compile(dynamic=True)\ndef fn(x):\n    return x + 1\n\nfn(torch.ones(3, 3))\nfn(torch.ones(4, 4))\n```\n\n```\n$ TORCH_LOGS=\"dynamic,recompiles\" python playground.py\ncreate_symbol s0 = 3 for L['x'].size()[0] [2, int_oo] at playground.py:5 in fn (_dynamo/variables/builder.py:2718 in <lambda>), for more info run with TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL=\"s0\"\nproduce_guards\nproduce_guards\n```\n\nFor more information on dynamic shapes, see [The dynamic shapes manual](https://docs.google.com/document/d/1GgvOe7C8_NVOMLOCwDaYV1mXXyHMXY7ExoewHqooxrs/edit#heading=h.fh8zzonyw8ng).",
    "1902": "一级标题：torch.compile Troubleshooting\n二级标题：Logging Tools\n内容：\n(tlparse-torch-trace)=\n\n### tlparse / TORCH_TRACE\n\n`tlparse` / `TORCH_TRACE` are a pair of tools that produce compilation reports that look like this:\n<https://web.mit.edu/~ezyang/Public/bhack-20240609-tlparse/index.html>.\n\nTraces are very easy to collect. To collect a trace, run your reproduction command with\n\n```\nTORCH_TRACE=\"/tmp/tracedir\" python foo.py\npip install tlparse\ntlparse /tmp/tracedir\n```\n\nThis approach works even if you are running a distributed job, providing a trace for each rank.\nIt will open your browser with HTML similar to what's generated above.\nIf you are making a bug report for a complicated problem that you don't have a standalone reproduction for,\nyou can still greatly assist PyTorch developers by attaching the trace log generated in `/tmp/tracedir`.\n\n```{warning}\nThe trace log contains all of your model code.\nDo not share the trace log if the model you are working on is sensitive. The trace log does NOT contain weights.\n```\n\n```{raw} html\n    <style>\n        .red {background-color:#ff0000;}\n        .green {background-color:#00ff00;}\n        .dark-green {background-color:#027f02;}\n    </style>\n```\n\n```{eval-rst}\n.. role:: red\n\n.. role:: green\n\n.. role:: dark-green\n```\n\nThe output of `tlparse` is primarily aimed for PyTorch developers,\nand the log format is easy to upload and share on GitHub.\nHowever,  as a non-PyTorch developer, you can still extract useful information from it.\nWe recommend starting with the inline help text in the report, which explains its contents.\nHere are some insights you can gain from a `tlparse`:\n\n- What model code was compiled by looking at the stack trie?\n  This is especially useful if you're not familiar with the codebase being compiled!\n- How many graph breaks / distinct compilation regions are there?\n  (Each distinct compile is its own color coded block like {dark-green}`[0/0]`).\n  Frames that are potentially graph-broken are light green {green}`[2/4]`.\n  If there are a lot of frames, that is suspicious, and suggests that you had some catastrophic graph breaks,\n  or maybe your code isn't a good match for `torch.compile`.\n- How many times did I recompile a particular frame? Something that recompiled a lot will look like:\n  {dark-green}`[10/0]` {dark-green}`[10/1]` {dark-green}`[10/2]`\n  \\- if something is being recompiled a lot, that is very suspicious and worth looking into, even if it isn't the root cause of your problem.\n- Was there a compilation error? Frames that errored will look like {red}`[0/1]`.\n- What intermediate compiler products did I generate for a given frame?\n  For example, you can look at the high-level generated FX graph or the generated Triton code.\n- Is there relevant information for a particular frame? You can find these in `compilation_metrics`.\n\n(torch-logs)=\n\n### TORCH_LOGS\n\nYou can use the `TORCH_LOGS` environment variable to selectively enable parts of the `torch.compile` stack to log.\n`TORCH_LOGS` is in fact the source of logs for `tlparse`. The format of the `TORCH_LOGS` environment variable looks like this:\n\n```\nTORCH_LOGS=\"<option1>,<option2>,...\" python foo.py\n```\n\nUseful high-level options include:\n\n- `graph_breaks`: logs locations of graph breaks in user code and the reason for the graph break\n- `guards`: logs guards that are generated\n- `recompiles`: logs which function recompiled and the guards that failed, leading to the recompilation\n- `dynamic`: logs related to dynamic shapes\n\nAlso, you can programmatically set logging options using `torch._logging.set_logs`:\n\n```py\nimport logging\ntorch._logging.set_logs(graph_breaks=True)\n...\n```\n\nMore `TORCH_LOGS` options are {ref}`troubleshooting-torch-logs-options`.\nFor the full list of options, see [torch.\\_logging](https://pytorch.org/docs/stable/logging.html)\nand [torch.\\_logging.set_logs](https://pytorch.org/docs/stable/generated/torch._logging.set_logs.html#torch._logging.set_logs).\n\n### tlparse vs. TORCH_LOGS\n\nGenerally, we suggest first using `tlparse` when encountering issues.\n`tlparse` is ideal for debugging large models and gaining a high-level overview of how your model was compiled.\nOn the other hand, `TORCH_LOGS` is preferred for small examples and fine-grained debugging detail,\nwhen we already have an idea of which `torch.compile` component is causing the problem.",
    "1903": "一级标题：torch.compile Troubleshooting\n二级标题：Simple Workarounds\n内容：\nHere, we describe some workarounds to `torch.compile` issues involving small code modifications\nor changing some `torch.compile` settings.\n\n### Where to apply torch.compile?\n\nWe recommend applying `torch.compile` to the highest-level function that doesn't cause excessive problems.\nTypically, it is your train or eval step with the optimizer but without the loop, your top-level `nn.Module`,\nor some sub-``` nn.Module``s. ``torch.compile ``` specifically doesn't handle distributed wrapper modules like\nDDP or FSDP very well, so consider applying `torch.compile` to the inner module passed to the wrapper.\n\n```py\n# inference\nmodel = ...\nopt_model = torch.compile(model)\n\nfor _ in range(N_ITERS):\n    inp = ...\n    out = opt_model(inp)\n```\n\n```py\n# training\nmodel = ...\nopt = torch.optim.Adam(model.parameters())\n\n@torch.compile\ndef train(mod, data):\n    opt.zero_grad(True)\n    pred = mod(data[0])\n    loss = torch.nn.CrossEntropyLoss()(pred, data[1])\n    loss.backward()\n    opt.step()\n\nfor _ in range(N_ITERS):\n    inp = ...\n    train(model, inp)\n```\n\n```py\n# DistributedDataParallel\nmodel = ...\nopt_model = torch.compile(model)\nmodel_ddp = DistributedDataParallel(opt_model, ...)\n\nfor _ in range(N_ITERS):\n    inp = ...\n    out = model_ddp(inp)\n```\n\n### Disabling and Suppressing Errors\n\nFor some model architectures, there are portions of the model which are particularly difficult to compile\n\\- either there are many graph breaks, or there are crashes. You may want to explicitly disable these\nportions of the model which are problematic so that you can apply `torch.compile` to the parts that work.\nYou can do this by using the `@torch.compiler.disable` decorator. When `torch.compile` attempts to call a\ndisabled function, it breaks the graph and skips tracing the disabled function, resuming tracing after the call.\nBy default, all recursive calls made from a disabled function are also disabled. Use the `recursive=False`\noption to allow compilation for recursive calls.\n\n```py\ndef bad1_inner(...):\n    # skipped\n\n@torch.compiler.disable\ndef bad1_outer(...):\n    # skipped\n    bad1_inner(...)\n\ndef bad2_inner(...)\n    # traced\n\n@torch.compiler.disable(recursive=False)\ndef bad2_outer(...):\n    # skipped\n    bad2_inner(...)\n\n@torch.compile\ndef fn(...):\n    # graph break\n    bad1_outer(...)\n        ...\n    # graph break\n    bad2_outer(...)\n```\n\nFor example, we use `torch.compiler.disable` to disable `torch.compile` on sparse architecture in\nrecommendation models, as the sparse arch is difficult to compile. Preprocessing and logging functions\nare other examples of functions that typically cause a lot of graph breaks and do not get value from being compiled.\n\nIf you are experiencing compiler crashes and you want to continue regardless, you can set\n`torch._dynamo.config.suppress_errors = True`. When the compiler crashes, we will just skip tracing\nthe function and try again later. This is not best practice - it is better to eventually manually add\ndisable annotations as necessary.\n\n### Resolving graph breaks\n\nTo maximize optimization opportunities, it's important to reduce the number of graph breaks.\nRecall that you can see what graph breaks are happening using `tlparse` or `TORCH_LOGS=\"graph_breaks\"`.\nIn general, graph breaks are caused by one of the following:\n\n1. You're trying to do something that fundamentally cannot be traced, such as data-dependent control flow.\n2. You're trying to do something not yet supported. .\n   For example, we currently have limited support for tracing code that uses the built-in Python `inspect` module.\n3. Your code has an error in it. For example, you may have tried calling a function with an incorrect number of arguments.\n\nGraph break logs will tell you the user code location and reason for the graph break.\nUnfortunately, many graph breaks are not actionable without a deeper understanding of Dynamo.\nIt can even be challenging to determine which of the three causes was the true cause of your graph break.\nWe are working on making graph break messages more actionable.\n\nAdditionally, the impact of lost optimization opportunities differs between graph breaks.\nFor example, graph breaks that happen in the middle of your model's `forward` are likely to have a more negatie impact than\ngraph breaks in a preprocessing part at the beginning of the `forward`. So it is not crucial to prevent *every single*\nbreak, but rather to prevent the ones that cause significant performance hits.\n\nIf a graph break message doesn't suggest any action, you suspect that the cause of your graph break is (2),\nand you believe that the graph break is causing performance hits,\nthen please report the graph break as an issue. If a function has many graph breaks,\nconsider disabling compilation on that function, as the overhead cost for the graph breaks may become prohibitive.\n\nBelow are some common graph breaks and some workarounds.\n\n#### Data-dependent operations\n\n`torch.compile` graph breaks on data-dependent operations such as data-dependent control flow\n(if-statements, loops with tensors) and direct tensor data accesses (`.item`, `.data_ptr`).\n\n```py\nimport torch\n\n@torch.compile\ndef fn(x):\n    y = x.sum()\n    if y > 0:\n        return x + y.item()\n    return x - y.item()\n\nfn(torch.ones(3, 3))\n```\n\n```\n$ TORCH_LOGS=\"graph_breaks\" python playground.py\nGraph break in user code at /data/users/williamwen/pytorch/playground.py:6\nReason: Data-dependent jump\nUser code traceback:\nFile \"/data/users/williamwen/pytorch/playground.py\", line 6, in fn\n    if y > 0:\n\nGraph break in user code at /data/users/williamwen/pytorch/playground.py:7\nReason: Unsupported: Tensor.item\nUser code traceback:\nFile \"/data/users/williamwen/pytorch/playground.py\", line 7, in torch_dynamo_resume_in_fn_at_6\n    return x + y.item()\nTraceback (most recent call last):\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py\", line 616, in wrapper\n    return inner_fn(self, inst)\n        ^^^^^^^^^^^^^^^^^^^^\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py\", line 2288, in CALL\n    self._call(inst)\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py\", line 2282, in _call\n    self.call_function(fn, args, kwargs)\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py\", line 838, in call_function\n    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/variables/misc.py\", line 1038, in call_function\n    return self.obj.call_method(tx, self.name, args, kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/variables/tensor.py\", line 527, in call_method\n    result = handler_method(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/variables/tensor.py\", line 773, in method_item\n    unimplemented(\"Tensor.item\")\nFile \"/data/users/williamwen/pytorch/torch/_dynamo/exc.py\", line 304, in unimplemented\n    raise Unsupported(msg, case_name=case_name)\ntorch._dynamo.exc.Unsupported: Tensor.item\n```\n\nThe general workaround for these graph breaks is to avoid doing data-dependent operations. Some specific workarounds are:\n\n- If your control flow doesn't actually depend on data values, consider modifying your code to perform control flow on constants.\n\n```py\n# old\nx = torch.randn(3, 3)\n@torch.compile\ndef fn(y):\n    if x.sum() > 0:\n        return y + x\n    else:\n        return y - x\n\n# new\nx = torch.randn(3, 3)\ncond = (x.sum() > 0).item()\n@torch.compile\ndef fn(y):\n    if cond:\n        return y + x\n    else:\n        return y - x\n```\n\n- Use higher-order ops like `torch.cond` (<https://pytorch.org/docs/main/cond.html>) in place of data-dependent control flow\n\n```py\n# old\n@torch.compile\ndef fn(x):\n    if x.sum() > 0:\n        return x + 1\n    return x - 1\n\n# new\n@torch.compile\ndef fn(x):\n    return torch.cond(\n        x.sum() > 0,\n        lambda x: x + 1,\n        lambda x: x - 1,\n        (x,),\n    )\n```\n\n- If you have a `.item()` call, try `torch._dynamo.config.capture_scalar_outputs = True` or `TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1`\n- Wrap problematic parts of the function in a custom op\n\n#### Custom ops\n\nIf you have code that `torch.compile` has trouble tracing through, either due to missing support or fundamental incompatibility,\nyou can consider wrapping the problematic code in a custom op.\n\nCustom ops require a little bit of additional work to get them to be compatible with `torch.compile`.\nSee <https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html> for more details.\n\n#### Printing\n\nPrinting/logging/issuing warnings will result in a graph break. If you have a function that makes many logging calls,\nfor example, a function that logs data about a training iteration, consider applying `torch.compiler.disable` on it.\n\nAlternatively, you can try using `torch._dynamo.config.reorderable_logging_functions`.\nThis config is used to reorder logging functions so that they are called at the end of the traced function,\nthus avoiding a graph break. However, the logged contents may differ if, for example, a mutation occurs.\n\n```py\nimport torch\n\ntorch._dynamo.config.reorderable_logging_functions.add(print)\n\n@torch.compile\ndef fn(x):\n    x += 1\n    print(\"log!\")\n    return torch.sin(x)\n\nfn(torch.ones(3, 3))\n```\n\n```\n$ TORCH_LOGS=\"graph_breaks\" python playground.py\nlog!\n```\n\n#### Incorrect code\n\nYour code may be wrong, or is otherwise encountering an error from outside `torch.compile`.\nIn the code below, we made a typo in the `torch.sin` call by providing an extra argument.\n\n```py\nimport torch\n\n@torch.compile\ndef fn(x):\n    y = torch.sin(x, x)\n    return y\n\nfn(torch.ones(3, 3))\n```\n\n```\n$ TORCH_LOGS=\"graph_breaks\" python playground.py\nGraph break in user code at /data/users/williamwen/pytorch/playground.py:5\nReason: Unsupported: TypeError <built-in method sin of type object at 0x7fd6fd764600>: sin() takes 1 positional argument but 2 were given\nUser code traceback:\nFile \"/data/users/williamwen/pytorch/playground.py\", line 5, in fn\n    y = torch.sin(x, x)\n...\n```\n\nIt can be difficult to tell from the logs if the error is caused by your code or because of a `torch.compile` bug.\nIn order to differentiate, we recommend trying to run your code without `torch.compile` to see if you still get the error.\n\n### Dealing with recompilations\n\nYou can view recompilations and their reasons using `tlparse` or `TORCH_LOGS=recompiles`.\n\n#### Is dynamic shapes enabled?\n\nRecompilations due to mismatched shapes are in the form:\n\n```\ntensor 'L['x']' size mismatch at index 0. expected 3, actual 4\n```\n\nMake sure that the `dynamic` option of `torch.compile` is not set to `False`.\nThe default option, `dynamic=None`, will only attempt dynamic shapes after the first compilation.\nYou can set `dynamic=True` to upfront compile as dynamic as possible.\n\nFor more information on dynamic shapes, see [The dynamic shapes manual](https://docs.google.com/document/d/1GgvOe7C8_NVOMLOCwDaYV1mXXyHMXY7ExoewHqooxrs/edit#heading=h.fh8zzonyw8ng).\n\n#### Changing the cache size limit\n\nThere is a limit to how many times a function can be recompiled, determined by `torch._dynamo.config.recompile_limit`\nand `torch._dynamo.config.accumulated_recompile_limit`.\nIf either limit is exceeded, then we will not attempt to compile the function again and instead will run the function eagerly.\n`torch.compile` will also issue a warning containing the affected function and which limit was hit.\nIn the example below, each function call results in a recompile attempt.\nWhen we hit the cache size limit (8), we stop attempting to recompile.\n\n```py\nimport torch\n\n@torch.compile(dynamic=False)\ndef fn(x):\n    return x + 1\n\nfor i in range(1, 10):\n    fn(torch.ones(i))\n```\n\n```\n$ python playground.py\ntorch._dynamo hit config.recompile_limit (8)\n    function: 'fn' (/data/users/williamwen/pytorch/playground.py:5)\n    last reason: 0/0: tensor 'L['x']' size mismatch at index 0. expected 1, actual 9\n```\n\nIf you know that the number of recompilations has a reasonable constant upper bound, you can raise the cache size limit.\nIf the cost of recompilation outweighs the benefit of compilation, then you can consider lowering the cache size limit.\n\n#### Wrapping constants with tensors\n\nBy default, `int` / `float` variables are treated as constants and are guarded as such.\nIn the below example, we have a recompilation for each function call.\n\n```py\nimport torch\n\n@torch.compile\ndef fn(x, c):\n    return x + c\n\nfor i in range(1, 10):\n    fn(torch.ones(i), 0.5 + i)\n```\n\n```\n$ TORCH_LOGS=\"recompiles\" python playground.py\nRecompiling function fn in /data/users/williamwen/pytorch/playground.py:3\n    triggered by the following guard failure(s):\n    - 0/7: L['c'] == 8.5\n    - 0/6: L['c'] == 7.5\n    - 0/5: L['c'] == 6.5\n    - 0/4: L['c'] == 5.5\n    - 0/3: L['c'] == 4.5\n    - 0/2: L['c'] == 3.5\n    - 0/1: L['c'] == 2.5\n    - 0/0: L['c'] == 1.5\ntorch._dynamo hit config.recompile_limit (8)\n    function: 'fn' (/data/users/williamwen/pytorch/playground.py:3)\n    last reason: 0/0: L['c'] == 1.5\n```\n\nIn particular, for LR schedulers, initializing with a constant can lead to recompilations:\n\n```py\nimport torch\n\nmod = torch.nn.Linear(3, 3)\nopt = torch.optim.Adam(mod.parameters(), lr=0.01)\nsched = torch.optim.lr_scheduler.ExponentialLR(opt, 0.9)\n\n@torch.compile\ndef fn(inp):\n    opt.zero_grad(True)\n    out = mod(inp).sum()\n    out.backward()\n    opt.step()\n    sched.step()\n\nfor i in range(1, 10):\n    fn(torch.ones(3, 3))\n```\n\n```\n$ TORCH_LOGS=\"recompiles\" python playground.py\nRecompiling function step in /data/users/williamwen/pytorch/torch/optim/adam.py:189\n    triggered by the following guard failure(s):\n    - 3/7: L['self'].param_groups[0]['lr'] == 0.004782969000000002\n    - 3/6: L['self'].param_groups[0]['lr'] == 0.005314410000000002\n    - 3/5: L['self'].param_groups[0]['lr'] == 0.005904900000000002\n    - 3/4: L['self'].param_groups[0]['lr'] == 0.006561000000000002\n    - 3/3: L['self'].param_groups[0]['lr'] == 0.007290000000000001\n    - 3/2: L['self'].param_groups[0]['lr'] == 0.008100000000000001\n    - 3/1: L['self'].param_groups[0]['lr'] == 0.009000000000000001\n    - 3/0: L['self'].param_groups[0]['lr'] == 0.01\ntorch._dynamo hit config.recompile_limit (8)\n    function: 'step' (/data/users/williamwen/pytorch/torch/optim/adam.py:189)\n    last reason: 3/0: L['self'].param_groups[0]['lr'] == 0.01\n```\n\nIn both examples, we can wrap float variables in tensors in order to prevent recompilations.\n\n```py\n# first example\nfor i in range(1, 10):\n    fn(torch.ones(i), torch.tensor(0.5 + i))\n\n# second example\nopt = torch.optim.Adam(mod.parameters(), lr=torch.tensor(0.01))\nsched = torch.optim.lr_scheduler.ExponentialLR(opt, torch.tensor(0.9))\n```",
    "1904": "一级标题：torch.compile Troubleshooting\n二级标题：Reporting Issues\n内容：\nIf the workarounds provided above were not enough to get `torch.compile` working,\nthen you should consider reporting the issue to PyTorch.\nBut there are a few things that you can do to make our lives significantly easier.\n\n### Ablation\n\nCheck which component of the `torch.compile` stack is the one causing the issue using the `backend=` option for `torch.compile`.\nIn particular, try:\n\n- `torch.compile(fn, backend=\"eager\")`, which only runs TorchDynamo, the graph capture component of `torch.compile`.\n- `torch.compile(fn, backend=\"aot_eager\")`, which runs TorchDynamo and AOTAutograd, which additionally generates the backward graph during compilation.\n- `torch.compile(fn, backend=\"aot_eager_decomp_partition\")`, which runs TorchDynamo and AOTAutograd with operator decompositions/partitions.\n- `torch.compile(fn, backend=\"inductor\")`, which runs TorchDynamo, AOTAutograd, and TorchInductor, the backend ML compiler that generates compiled kernels.\n\nIf you only fail with the Inductor backend, you can additionally test various Inductor modes:\n\n- `torch.compile(fn, backend=\"inductor\", mode=\"default\")`\n- `torch.compile(fn, backend=\"inductor\", mode=\"reduce-overhead\")`\n- `torch.compile(fn, backend=\"inductor\", mode=\"max-autotune\")`\n\nYou can also check if dynamic shapes is causing issues with any backend:\n\n- `torch.compile(fn, dynamic=True)` (always use dynamic shapes)\n- `torch.compile(fn, dynamic=False)` (never use dynamic shapes)\n- `torch.compile(fn, dynamic=None)` (automatic dynamic shapes)\n\n### Bisecting\n\nDid you try on the latest nightly? Did something work in the past but now no longer works?\nCan you bisect to determine the first nightly where your issue occurs?\nBisecting is especially helpful for performance, accuracy, or compile time regressions,\nwhere it is not immediately obvious where the problem originates from.\n\n### Creating a reproducer\n\nCreating reproducers is a lot of work, and it is perfectly fine if you do not have the time to do it.\nHowever, if you are a motivated user unfamiliar with the internals of `torch.compile`,\ncreating a standalone reproducer can have a huge impact on our ability to fix the bug.\nWithout a reproducer, your bug report must contain enough information for us to identify the root cause of the problem and write a reproducer from scratch.\n\nHere's a list of useful reproducers, ranked from most to least preferred:\n\n1. **Self-contained, small reproducer:** A script with no external dependencies, under 100 lines of code, that reproduces the problem when run.\n2. **Self-contained, large reproducer:** Even if it's large, being self-contained is a huge advantage!\n3. **Non-self-contained reproducer with manageable dependencies:**\n   For example, if you can reproduce the problem by running a script after `pip install transformers`,\n   that's manageable. We can likely run it and investigate.\n4. **Non-self-contained reproducer requiring substantial setup:** This might involve downloading datasets,\n   multiple environment setup steps, or specific system library versions requiring a Docker image.\n   The more complex the setup, the harder it is for us to recreate the environment.\n\n   :::{note}\n       Docker simplifies setup but complicates changes to the environment, so it's not a perfect solution, though we'll use it if necessary.\n   :::\n\nSomewhat orthogonally, a reproducer that can be run in a single process is better than a reproducer\nthat requires multiprocess training (but once again, if you only have a multiprocess reproducer, we'll take it!).\n\nAdditionally, below is a non-exhaustive list of aspects to check in your\nissue that you can attempt to replicate in your reproducer:\n\n- **Autograd**. Did you have tensor inputs with `requires_grad=True`? Did you call `backward()` on the output?\n- **Dynamic shapes**. Did you set `dynamic=True`? Or did you run the test code multiple times with varying shapes?\n- **Custom operators**. Is there a custom operator involved in the real workflow?\n  Can you replicate some of its important characteristics using the Python custom operator API?\n- **Configuration**. Did you set all the same configuration?\n  This includes `torch._dynamo.config` and `torch._inductor.config` settings,\n  as well as arguments to `torch.compile` like `backend` / `mode`.\n- **Context managers**. Did you replicate any active context managers?\n  This could be `torch.no_grad`, automatic mixed precision, `TorchFunctionMode` / `TorchDispatchMode`,\n  activation checkpointing, compiled autograd etc.\n- **Tensor subclasses**. Is there a tensor subclass involved?\n\n### Minifier\n\nThe minifier is an early `torch.compile` tool that, given an FX graph that crashes when we attempt to run or compile it,\nfinds a subgraph that also crashes and outputs the code that performs that subgraph's operations.\nEssentially, the minifier finds a minimal repro for a certain class of `torch.compile`-related crashes.\nThis assumes that we were able to successfully trace through code.\n\nUnfortunately, most of the time nowadays, the minifier doesn't work as expected, and alternative methods may be necessary.\nThis is likely because bugs that can be automatically reproduced in this manner are generally easier to fix\nand have already been addressed, leaving more complex issues that do not reproduce easily.\nHowever, it is straightforward to attempt using the minifier, so it is worth trying even if it may not succeed.\n\nInstructions for operating the minifier can be found [here](https://pytorch.org/docs/stable/torch.compiler_troubleshooting_old.html).\nIf the compiler is crashing, you can set `TORCHDYNAMO_REPRO_AFTER=\"dynamo\"` or `TORCHDYNAMO_REPRO_AFTER=\"aot\"`\nThe `aot` option is more likely to succeed, although it may not identify the `AOTAutograd` issues. This will generate the `repro.py` file which may help to diagnose the problem.\nFor accuracy-related issues, consider setting `TORCHDYNAMO_REPRO_LEVEL=4`. Please note that this may not always successfully identify the problematic subgraph.",
    "1905": "一级标题：torch.compile Troubleshooting\n二级标题：Debugging Deeper\n内容：\nThis section provides tools and techniques for independently debugging `torch.compile` issues\nor for gaining a deeper understanding of the `torch.compile` stack.\nThese methods are more involved than those presented above and are used by PyTorch developers regularly\nto debug real `torch.compile` issues.\n\nBelow is a high-level overview of the stack:\n\n![Torch Dynamo Stack](_static/img/dynamo/td_stack.png)\n\nThe stack comprises three main components: TorchDynamo, AOTAutograd, and Inductor.\nOur debugging strategy involves first identifying the component in which the error occurs\nand then individually debugging the component. To determine the component responsible for the issue,\nsee the `Ablation` section under `Reporting Issues` above. For guidance on debugging a specific component, consult the sections below.\n\n### TorchDynamo\n\n#### Logging what Dynamo is tracing\n\nThe `TORCH_LOGS=trace_bytecode` option enables you to view the precise bytecode instructions that Dynamo is tracing,\nas well as a symbolic representation of the Python interpreter stack. When encountering a graph break or crash,\nit is advisable to inspect the last few bytecode instructions traced.\n\nYou can also use `TORCH_LOGS=trace_source` to see which lines of source code Dynamo is tracing through.\nThis is useful in combination with `trace_bytecode` to see the line of source code each traced bytecode instruction corresponds to.\n\nFinally, you can use `TORCH_LOGS=graph_code` to see the Python code representing the FX graph that Dynamo traced.\nYou can view this code to double check that the correct ops are being traced.\n\n```py\nimport torch\n\ndef g(x, y):\n    return x + y\n\n@torch.compile(backend=\"eager\")\ndef f(x):\n    x = torch.sin(x)\n    x = g(x, x)\n    return x\n\nf(torch.ones(3, 3))\n```\n\n```\n$ TORCH_LOGS=\"trace_bytecode,trace_source,graph_code\" python playground.py\nTRACE starts_line /data/users/williamwen/pytorch/playground.py:6 in f ()\n    @torch.compile(backend=\"eager\")\nTRACE RESUME 0 []\nTRACE starts_line /data/users/williamwen/pytorch/playground.py:8 in f (f)\n        x = torch.sin(x)\nTRACE LOAD_GLOBAL torch []\nTRACE LOAD_ATTR sin [NullVariable(), PythonModuleVariable(<module 'torch' from '/data/users/williamwen/pytorch/torch/__init__.py'>)]\nTRACE LOAD_FAST x [NullVariable(), TorchInGraphFunctionVariable(<built-in method sin of type object at 0x7f00f6964600>)]\nTRACE CALL 1 [NullVariable(), TorchInGraphFunctionVariable(<built-in method sin of type object at 0x7f00f6964600>), LazyVariableTracker()]\nTRACE STORE_FAST x [TensorVariable()]\nTRACE starts_line /data/users/williamwen/pytorch/playground.py:9 in f (f)\n        x = g(x, x)\nTRACE LOAD_GLOBAL g []\nTRACE LOAD_FAST x [NullVariable(), UserFunctionVariable()]\nTRACE LOAD_FAST x [NullVariable(), UserFunctionVariable(), TensorVariable()]\nTRACE CALL 2 [NullVariable(), UserFunctionVariable(), TensorVariable(), TensorVariable()]\nTRACE starts_line /data/users/williamwen/pytorch/playground.py:3 in g (g) (inline depth: 1)\n    def g(x, y):\nTRACE RESUME 0 []\nTRACE starts_line /data/users/williamwen/pytorch/playground.py:4 in g (g) (inline depth: 1)\n        return x + y\nTRACE LOAD_FAST x []\nTRACE LOAD_FAST y [TensorVariable()]\nTRACE BINARY_OP 0 [TensorVariable(), TensorVariable()]\nTRACE RETURN_VALUE None [TensorVariable()]\nTRACE STORE_FAST x [TensorVariable()]\nTRACE starts_line /data/users/williamwen/pytorch/playground.py:10 in f (f)\n        return x\nTRACE LOAD_FAST x []\nTRACE RETURN_VALUE None [TensorVariable()]\nTRACED GRAPH\n===== __compiled_fn_1 =====\n/data/users/williamwen/pytorch/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n    def forward(self, L_x_: \"f32[3, 3][3, 1]cpu\"):\n        l_x_ = L_x_\n\n        # File: /data/users/williamwen/pytorch/playground.py:8 in f, code: x = torch.sin(x)\n        x: \"f32[3, 3][3, 1]cpu\" = torch.sin(l_x_);  l_x_ = None\n\n        # File: /data/users/williamwen/pytorch/playground.py:4 in g, code: return x + y\n        x_1: \"f32[3, 3][3, 1]cpu\" = x + x;  x = None\n        return (x_1,)\n```\n\n#### Breakpointing Dynamo tracing\n\nInserting a breakpoint in Dynamo/user code is helpful at times to see what the state of Dynamo is when tracing through user code.\nUnfortunately, inserting a breakpoint in the normal Python fashion will result in a graph break in TorchDynamo,\nso we will not be able to view the state of Dynamo at the point where we intended to breakpoint.\n\nThe first method for setting a breakpoint is to insert it within the Dynamo source code. Three recommended locations to place a breakpoint are:\n\n- In `torch/_dynamo/symbolic_convert.py`, breakpoint at functions that are named after the problematic bytecode instruction,\n  such as `def CALL_FUNCTION` and `def STORE_ATTR`. You can conditionally breakpoint depending on inputs,\n  for example, the `argval` of the instruction, or the name of the object at the top of the stack since some bytecode opcodes are frequently used.\n- Breakpoint where the graph break or error originates from. Typically, graph breaks are emitted from a call to `unimplemented(...)`.\n- Breakpoint in `torch/_dynamo/variables/builder.py, function:_wrap`. You will likely have to conditionally breakpoint on the input.\n  This function determines how to symbolically represent a given value. Consider breakpointing here if you suspect that a value is represented incorrectly.\n\nThe second way to insert a breakpoint is to use `torch._dynamo.comptime.comptime.breakpoint`:\n\n```py\nfrom torch._dynamo.comptime import comptime\n\n@torch.compile\ndef f(...):\n    ...\n    comptime.breakpoint()\n    ...\n```\n\nA comptime breakpoint is convenient as it enables you to inspect the Dynamo state at a specific location within the user code being traced.\nIt does not require you to insert a breakpoint in the Dynamo source or to conditionally breakpoint based on variables.\n\nWhen a comptime breakpoint is triggered, you can do the following:\n\n- `ctx.print_bt()` to print the user stack trace\n- `ctx.print_locals()` to print all current locals\n- `ctx.print_graph()` to print the currently traced graph\n- `ctx.disas()` to print the currently traced function's bytecode\n- Use standard `pdb` commands, such as `bt/u/d/n/s/r`, - you can go up the `pdb` stack to inspect more Dynamo internals\n\n```py\nimport torch\nfrom torch._dynamo.comptime import comptime\n\n@torch.compile(backend=\"eager\")\ndef f(x):\n    y = x + 1\n    comptime.breakpoint()\n    y = y + 1\n    return y\n\nf(torch.ones(3, 3))\n```\n\n```\n$ python playground.py\n--Return--\n> /data/users/williamwen/pytorch/torch/_dynamo/comptime.py(392)inner()->None\n-> builtins.breakpoint()\n(Pdb) ctx.print_bt()\nFile \"/data/users/williamwen/pytorch/playground.py\", line 7, in f\n    comptime.breakpoint()\n\n(Pdb) ctx.print_locals()\nx = FakeTensor(..., size=(3, 3))\ny = FakeTensor(..., size=(3, 3))\n(Pdb) bt\n...\n/data/users/williamwen/pytorch/torch/_dynamo/symbolic_convert.py(826)call_function()\n-> self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n/data/users/williamwen/pytorch/torch/_dynamo/variables/misc.py(331)call_function()\n-> func(ComptimeContext(tx))\n> /data/users/williamwen/pytorch/torch/_dynamo/comptime.py(392)inner()->None\n-> builtins.breakpoint()\n(Pdb) ctx.print_graph()\n\n\n\ndef forward(self, L_x_: \"f32[3, 3]\"):\n    l_x_ = L_x_\n\n    # File: /data/users/williamwen/pytorch/playground.py:6 in f, code: y = x + 1\n    y: \"f32[3, 3]\" = l_x_ + 1;  l_x_ = y = None\n```\n\n% TODO(uncomment/update once we improve this API)\n% Debugging large models\n% ^^^^^^^^^^^^^^^^^^^^^^\n%\n% Debugging TorchDynamo on large models can be tricky, mainly because Dynamo traces through large amounts of code.\n% It can be difficult to find the problematic function, or to determine where to place a breakpoint.\n% Even if we've found the problematic function, we don't want to deal with logging spam.\n% Fortunately, you can use ``TORCHDYNAMO_DEBUG_FUNCTION=<function name>``, which limits dynamo tracing to only functions with a specific name\n% (exact match). This will allow you to filter all of the functions in the model to the function(s) of interest.\n% Use this in combination with the above debugging strategies.\n\n#### Bytecode generation errors\n\nAlthough uncommon, Dynamo may generate incorrect bytecode. This may occur if you determine the following:\n\n- Ablation reveals the error is happening at the TorchDynamo level\n- The error is not being emitted from TorchDynamo stack frames\n- The error looks more like a user error rather than a Dynamo error, or is a segmentation fault\n- The error does not occur without `torch.compile`\n\nBytecode generation bugs are generally tricky to fix and we recommend submitting an issue instead of trying to fix those yourself.\nIf you are interested in seeing the bytecode that Dynamo generates, you can use `TORCH_LOGS=bytecode`.\nYou can see a high-level overview on what bytecode Dynamo generates [here](https://docs.google.com/presentation/d/1tMZOoAoNKF32CAm1C-WfzdVVgoEvJ3lp/edit?usp=sharing&ouid=114922067987692817315&rtpof=true&sd=true).\n\n### AOTAutograd\n\nAOTAutograd errors are typically difficult to debug - we recommend just submitting an issue.\nAOTAutograd logging output is primarily helpful to see what the input to Inductor is.\n\n% TODO\n% TorchInductor\n% -------------\n\n% TODO\n\n(troubleshooting-torch-logs-options)=\n\n### Summary of TORCH_LOGS options\n\nA summary of helpful `TORCH_LOGS` options is:\n\n```{eval-rst}\n.. list-table::\n    :widths: 25 50\n    :header-rows: 1\n\n    * - Option\n      - Description\n    * - +all\n      - Output debug logs from all ``torch.compile`` components\n    * - +dynamo\n      - Output debug logs from TorchDynamo\n    * - +aot\n      - Output debug logs from AOTAutograd\n    * - +inductor\n      - Output debug logs from TorchInductor\n    * - dynamic\n      - Output logs from dynamic shapes\n    * - graph_code\n      - Output the Python code for the FX graph that Dynamo generated\n    * - graph_sizes\n      - Output the tensor sizes of the FX graph that Dynamo generated\n    * - trace_bytecode\n      - Output the bytecode instructions that Dynamo is tracing through and the symbolic interpreter stack Dynamo is keeping track of\n    * - trace_source\n      - Output the line of code in the original source that Dynamo is currently tracing through\n    * - bytecode\n      - Output Dynamo-generated bytecode\n    * - guards\n      - Output generated guards\n    * - recompiles\n      - Output recompilation reasons (only the first guard check that fails)\n    * - recompiles_verbose\n      - Output all guard checks that fail when a recompilation occurs\n    * - aot_graphs\n      - Output graph generated by AOTAutograd\n    * - aot_joint_graphs\n      - Output the joint forward-backward graph generated by AOTAutograd\n    * - output_code\n      - Output code generated by Inductor\n    * - kernel_code\n      - Output code generated by Inductor on a per-kernel basis\n    * - schedule\n      - Output Inductor scheduling logs\n    * - perf_hints\n      - Output Inductor perf hint logs\n    * - fusion\n      - Output Inductor fusion logs\n```\n\nFor the full list of options, see [torch.\\_logging](https://pytorch.org/docs/stable/logging.html)\nand [torch.\\_logging.set_logs](https://pytorch.org/docs/stable/generated/torch._logging.set_logs.html#torch._logging.set_logs).",
    "1906": "一级标题：torch.compile Troubleshooting\n二级标题：Related Articles\n内容：\n- [torch.compile tutorial](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)\n- [torch.compile fine-grained APIs](https://pytorch.org/docs/stable/torch.compiler_fine_grain_apis.html)\n- [torch.compile FAQ](https://pytorch.org/docs/stable/torch.compiler_faq.html)\n- [torch.compiler namespace overview](https://pytorch.org/docs/stable/torch.compiler.html#torch-compiler-overview)\n- [torch.compiler API reference](https://pytorch.org/docs/stable/torch.compiler_api.html)\n- [Profiling torch.compile](https://pytorch.org/docs/stable/torch.compiler_profiling_torch_compile.html)\n- [torch.compile missing manual](https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit?usp=sharing)\n- [The dynamic shapes manual](https://docs.google.com/document/d/1GgvOe7C8_NVOMLOCwDaYV1mXXyHMXY7ExoewHqooxrs/edit#heading=h.fh8zzonyw8ng)\n- [TorchInductor caching tutorial](https://pytorch.org/tutorials/recipes/torch_compile_caching_tutorial.html)",
    "1907": "一级标题：PyTorch 2.0 Troubleshooting (old)\n二级标题：无\n内容：\n**Author**: [Michael Lazos](https://github.com/mlazos)\n\n:::{note}\nThis document is outdated and is now mainly a primary resource on how to run the `torch.compile` minifier.\nPlease see the [updated troubleshooting document](https://pytorch.org/docs/main/torch.compiler_troubleshooting.html).\nThere is also a more [comprehensive manual for torch.compile](https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit#heading=h.ivdr7fmrbeab)\navailable.\n:::\n\nWe are actively developing debug tools, profilers, and improving our\nerror and warning messages. Below is a table of the available\ntools and their typical usage. For additional help see\n{ref}`diagnosing-runtime-errors`.\n\n```{eval-rst}\n.. list-table:: Title\n   :widths: 25 25 50\n   :header-rows: 1\n\n   * - Tool\n     - Purpose\n     - Usage\n   * - Info logging\n     - View summarized steps of compilation\n     - ``torch._logging.set_logs(dynamo = logging.INFO)`` or ``TORCH_LOGS=\"dynamo\"``\n   * - Debug logging\n     - View detailed steps of compilation (print every instruction traced)\n     - ``torch._logging.set_logs(dynamo = logging.DEBUG)`` and\n       ``torch._dynamo.config.verbose = True``, or ``TORCH_LOGS=\"+dynamo\" TORCHDYNAMO_VERBOSE=1``\n   * - Minifier for any backend\n     - Find smallest subgraph which reproduces errors for any backend\n     - set environment variable ``TORCHDYNAMO_REPRO_AFTER=\"dynamo\"``\n   * - Minifier for ``TorchInductor``\n     - If the error is known to occur after ``AOTAutograd`` find\n       smallest subgraph which reproduces errors during ``TorchInductor`` lowering\n     - set environment variable ``TORCHDYNAMO_REPRO_AFTER=\"aot\"``\n   * - Dynamo accuracy minifier\n     - Finds the smallest subgraph which reproduces an accuracy issue\n       between an eager mode model and optimized model, when you\n       suspect the problem is in ``AOTAutograd``\n     - ``TORCHDYNAMO_REPRO_AFTER=\"dynamo\" TORCHDYNAMO_REPRO_LEVEL=4``\n   * - Inductor accuracy minifier\n     - Finds the smallest subgraph which reproduces an accuracy issue\n       between an eager mode model and optimized model, when you\n       suspect the problem is in the backend (e.g., inductor).\n       If this doesn't work, try the Dynamo accuracy minifier\n       instead.\n     - ``TORCHDYNAMO_REPRO_AFTER=\"aot\" TORCHDYNAMO_REPRO_LEVEL=4``\n   * - ``torch._dynamo.explain``\n     - Find graph breaks and display reasoning for them\n     - ``torch._dynamo.explain(fn)(*inputs)``\n   * - Record/Replay\n     - Record and replay frames which to reproduce errors during graph capture\n     - ``torch._dynamo.config.replay_record_enabled = True``\n   * - TorchDynamo function name filtering\n     - Only compile functions with the given name to reduce noise when\n       debugging an issue\n     - set environment variable ``TORCHDYNAMO_DEBUG_FUNCTION=<name>``\n   * - TorchInductor Debug logging\n     - Print general TorchInductor debug info and generated Triton/C++ code\n     - ``torch._inductor.config.debug = True``\n   * - TorchInductor Tracing\n     - Show time taken in each TorchInductor stage + output code and graph\n       visualization\n     - set the environment variable TORCH_COMPILE_DEBUG=1 or\n       ``torch._inductor.config.trace.enabled = True``\n```\n\nIn addition to info and debug logging,\nyou can use [torch.\\_logging](https://pytorch.org/docs/main/logging.html)\nfor more fine-grained logging.\n\n(diagnosing-runtime-errors)=",
    "1908": "一级标题：PyTorch 2.0 Troubleshooting (old)\n二级标题：Diagnosing Runtime Errors\n内容：\nAt a high level, the TorchDynamo stack consists of a graph capture from\nPython code (TorchDynamo) and a backend compiler. For example, a\nbackend compiler may consist of backward graph tracing (AOTAutograd) and\ngraph lowering (TorchInductor)\\*. Errors can occur in any component of\nthe stack and will provide full stack traces.\n\nTo determine in which component an error occurred,\nyou may use info-level logging\n`torch._logging.set_logs(dynamo = logging.INFO)` or `TORCH_LOGS=\"dynamo\"`\nand look for `Step #: ...` outputs. Logs are made at the beginning and end of\neach step, so the step that an error should correspond to is the most recently\nlogged step whose end has not yet been logged. The steps correspond to the\nfollowing parts of the stack:\n\n| Step | Component        |\n| ---- | ---------------- |\n| 1    | TorchDynamo      |\n| 2    | Compiler Backend |\n| 3    | TorchInductor    |\n\nIf info logging is insufficient, you can use available backend\noptions. These options include:\n\n- `\"eager\"`: only runs TorchDynamo forward graph capture and then\n  runs the captured graph with PyTorch. This provides an indication as\n  to whether TorchDynamo is raising the error.\n- `\"aot_eager\"`: runs TorchDynamo to capture a forward graph, and\n  then AOTAutograd to trace the backward graph without any additional\n  backend compiler steps. PyTorch eager will then be used to run the\n  forward and backward graphs. This is useful to narrow down the issue\n  to AOTAutograd.\n\nThe general procedure to narrow down an issue is the following:\n\n1. Run your program with the `\"eager\"` backend. If the error no longer\n   occurs, the issue is in the backend compiler that is being used (if\n   using TorchInductor, proceed to step 2. If not, see\n   {ref}`minifying-backend-compiler-errors`). If the error still\n   occurs with the `\"eager\"` backend, it is due to\n   {ref}`torchdynamo-errors`.\n2. This step is only necessary if `TorchInductor` is used as the backend\n   compiler. Run the model with the `\"aot_eager\"` backend. If this\n   backend raises an error then the error is occurring during\n   AOTAutograd tracing. If the error no longer occurs with this backend,\n   then {ref}`minifying-torchinductor-errors`.\n\nEach of these cases are analyzed in the following sections.\n\n:::{note}\nThe TorchInductor backend consists of\nboth AOTAutograd tracing and the TorchInductor compiler itself. We will\ndisambiguate by referring to `TorchInductor` as the backend, and\nTorchInductor lowering as the phase which lowers the graph traced by\nAOTAutograd.\n:::\n\n(torchdynamo-errors)=\n\n### Torchdynamo Errors\n\nIf the error that is generated occurs with the `\"eager\"` backend, then\nTorchDynamo is most likely the source of the error. Here is a sample code\nwhich will generate an error.\n\n```py\nimport torch\n\nimport torch._dynamo as dynamo\n\n\ndef test_assertion_error():\n    y = torch.ones(200, 200)\n    z = {y: 5}\n    return z\n\ncompiled_test_assertion_error = torch.compile(test_assertion_error, backend=\"eager\")\n\ncompiled_test_assertion_error()\n```\n\nThe code above generates the following error:\n\n```\ntorch._dynamo.convert_frame: [ERROR] WON'T CONVERT test_assertion_error /scratch/mlazos/torchdynamo/../test/errors.py line 26\ndue to:\nTraceback (most recent call last):\n  File \"/scratch/mlazos/torchdynamo/torchdynamo/symbolic_convert.py\", line 837, in BUILD_MAP\n    assert isinstance(k, ConstantVariable) or (\nAssertionError\n\nfrom user code:\n   File \"/scratch/mlazos/torchdynamo/../test/errors.py\", line 34, in test_assertion_error\n    z = {y: 5}\n\nSet torch._dynamo.config.verbose=True for more information\n==========\n```\n\nAs the message suggests you can set\n`torch._dynamo.config.verbose=True` to get a full stack trace to both\nthe error in TorchDynamo and the user code. In addition to this flag,\nyou can also set the `log_level` of TorchDynamo through\n`torch._logging.set_logs(dynamo = logging.INFO)` or `TORCH_LOGS=\"dynamo\"`. These levels include:\n\n- `logging.DEBUG` or `TORCH_LOGS=\"+dynamo\"`: Print every instruction that is\n  encountered in addition to all the log levels listed below.\n- `logging.INFO`:\n  Print each function that is compiled (original and modified bytecode)\n  and the graph that is captured in addition to all the log levels listed below.\n- `logging.WARNING` (default): Print graph breaks in addition to all\n  the log levels listed below.\n- `logging.ERROR`: Print errors only.\n\nIf a model is very large, the logs can become overwhelming. If\nan error occurs deep within a model's Python code, it can be useful to\nexecute only the frame in which the error occurs to enable easier\ndebugging. There are two tools available to enable this:\n\n- Setting the environment variable `TORCHDYNAMO_DEBUG_FUNCTION`\n  to the desired function name will only run torchdynamo on functions with that\n  name.\n- Enabling the record/replay tool (set `torch._dynamo.config.replay_record_enabled = True`)\n  which dumps an execution record when an error is encountered. This record can\n  then be replayed to run only the frame where an error occurred.\n\n### Diagnosing TorchInductor Errors\n\nIf the error does not occur with the `\"eager\"` backend, then the\nbackend compiler is the source of the error ([example\nerror](https://gist.github.com/mlazos/2f13681e3cc6c43b3911f336327032de)).\nThere are [different choices](./torch.compiler.md)\nfor backend compilers for TorchDynamo, with TorchInductor\nfitting the needs of most users. This section focuses on TorchInductor\nas the motivating example, but some tools can also be used with other\nbackend compilers.\n\nBelow is the portion of the stack which we are focusing on:\n\nWith TorchInductor as the chosen backend, AOTAutograd is used to\ngenerate the backward graph from the forward graph captured by\ntorchdynamo. It is important to note that errors can occur during this\ntracing and also while TorchInductor lowers the forward and backward\ngraphs to GPU code or C++. A model can often consist of hundreds or\nthousands of FX nodes, so narrowing the exact nodes where this problem\noccurred can be very difficult. Fortunately, there are tools available to\nautomatically minify these input graphs to the nodes which are causing\nthe issue. The first step is to determine whether the error occurs\nduring tracing of the backward graph with AOTAutograd or during\nTorchInductor lowering. As mentioned above in step 2, the\n`\"aot_eager\"` backend can be used to run only AOTAutograd in isolation\nwithout lowering. If the error still occurs with this backend, this\nindicates that the error is occurring during AOTAutograd tracing.\n\nHere is an example:\n\n```py\nimport torch\n\nimport torch._dynamo as dynamo\n\nmodel = torch.nn.Sequential(*[torch.nn.Linear(200, 200) for _ in range(5)])\n\ndef test_backend_error():\n\n    y = torch.ones(200, 200)\n    x = torch.ones(200, 200)\n    z = x + y\n    a = torch.ops.aten._foobar(z)  # dummy function which errors\n    return model(a)\n\n\ncompiled_test_backend_error = torch.compile(test_backend_error, backend=\"inductor\")\ncompiled_test_backend_error()\n```\n\nRunning this should give you this error with a longer stack trace below\nit:\n\n```\nTraceback (most recent call last):\n  File \"/scratch/mlazos/torchdynamo/torchinductor/graph.py\", line 246, in call_function\n    return lowerings[target](*args, **kwargs)\n  File \"/scratch/mlazos/torchdynamo/torchinductor/lowering.py\", line 185, in wrapped\n    return decomp_fn(*args, **kwargs)\n  File \"/scratch/mlazos/torchdynamo/torchinductor/lowering.py\", line 810, in _foobar\n    assert False\nAssertionError\n...\n```\n\n[error with full stack\ntrace](https://gist.github.com/mlazos/d6947854aa56d686800259a164c62100)\n\nIf you then change `torch.compile(backend=\"inductor\")` to\n`torch.compile(backend=\"aot_eager\")`, it will run without error, because\n[the\nissue](https://github.com/pytorch/torchdynamo/blob/d09e50fbee388d466b5252a63045643166006f77/torchinductor/lowering.py#:~:text=%23%20This%20shouldn%27t%20be,assert%20False)\nis in the TorchInductor lowering process, not in AOTAutograd.\n\n(minifying-torchinductor-errors)=\n\n### Minifying TorchInductor Errors\n\nFrom here, let’s run the minifier to get a minimal repro. Setting the\nenvironment variable `TORCHDYNAMO_REPRO_AFTER=\"aot\"` (or setting\n`torch._dynamo.config.repro_after=\"aot\"` directly) will generate a\nPython program which reduces the graph produced by AOTAutograd to the\nsmallest subgraph which reproduces the error. (See below for an example\nwhere we minify the graph produced by TorchDynamo) Running the program\nwith this environment variable should show nearly [identical\noutput](https://gist.github.com/mlazos/0458ab828aa403c779fe73c012aa5982),\nwith an additional line indicating where `minifier_launcher.py` has\nbeen written to. The output directory is configurable by setting\n`torch._dynamo.config.base_dir` to a valid directory name. The final\nstep is to run the minifier and check that it runs successfully. A\nsuccessful run looks like\n[this](https://gist.github.com/mlazos/e6ea41ccce68a7b1b8a7a09acb1b206a).\nIf the minifier runs successfully, it generates runnable python code\nwhich reproduces the exact error. For our example this is the following\ncode:\n\n```python\nimport torch\nfrom torch import tensor, device\nimport torch.fx as fx\nfrom torch._dynamo.testing import rand_strided\nfrom math import inf\nfrom torch.fx.experimental.proxy_tensor import make_fx\n\n# torch version: 1.13.0a0+gitfddfc44\n# torch cuda version: 11.6\n# torch git version: fddfc4488afb207971c54ad4bf58130fdc8a4dc5\n\n\n# CUDA Info:\n# nvcc: NVIDIA (R) Cuda compiler driver\n# Copyright (c) 2005-2022 NVIDIA Corporation\n# Built on Thu_Feb_10_18:23:41_PST_2022\n# Cuda compilation tools, release 11.6, V11.6.112\n# Build cuda_11.6.r11.6/compiler.30978841_0\n\n# GPU Hardware Info:\n# NVIDIA A100-SXM4-40GB : 8\n\nfrom torch.nn import *\n\nclass Repro(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, add):\n        _foobar = torch.ops.aten._foobar.default(add);  add = None\n        return (_foobar,)\n\nargs = [((200, 200), (200, 1), torch.float32, 'cpu')]\nargs = [rand_strided(shape, stride, dtype, device) for shape, stride, dtype, device in args]\nmod = make_fx(Repro())(*args)\nfrom torch._inductor.compile_fx import compile_fx_inner\n\ncompiled = compile_fx_inner(mod, args)\ncompiled(*args)\n```\n\nThe `forward` method of the `Repro` module contains the exact op\nwhich causes the issue. When filing an issue, please include any\nminified repros to aid in debugging.\n\n(minifying-backend-compiler-errors)=\n\n### Minifying Backend Compiler Errors\n\nWith backend compilers other than TorchInductor the process for finding\nthe subgraph causing the error is nearly identical to the procedure in\n{ref}`minifying-torchinductor-errors` with one important\ncaveat. Namely, that the minifier will now be run on the graph that is\ntraced by TorchDynamo, not the output graph of AOTAutograd. Let’s walk\nthrough an example.\n\n```py\nimport torch\n\nimport torch._dynamo as dynamo\n\nmodel = torch.nn.Sequential(*[torch.nn.Linear(200, 200) for _ in range(5)])\n# toy compiler which fails if graph contains relu\ndef toy_compiler(gm: torch.fx.GraphModule, _):\n    for node in gm.graph.nodes:\n        if node.target == torch.relu:\n            assert False\n\n    return gm\n\n\ndef test_backend_error():\n    y = torch.ones(200, 200)\n    x = torch.ones(200, 200)\n    z = x + y\n    a = torch.relu(z)\n    return model(a)\n\n\ncompiled_test_backend_error = torch.compile(test_backend_error, backend=toy_compiler)\ncompiled_test_backend_error()\n```\n\nIn order to run the code after TorchDynamo has traced the forward graph,\nyou can use the `TORCHDYNAMO_REPRO_AFTER` environment variable. Running\nthis program with `TORCHDYNAMO_REPRO_AFTER=\"dynamo\"` (or\n`torch._dynamo.config.repro_after=\"dynamo\"`) should produce [this\noutput](https://gist.github.com/mlazos/244e3d5b53667e44078e194762c0c92b)and\nthe following code in `{torch._dynamo.config.base_dir}/repro.py`.\n\n:::{note}\nThe other option for TORCHDYNAMO_REPRO_AFTER is `\"aot\"`, which\nwill run the minifier after the backward graph has been generated.\n:::\n\n```python\nimport torch\nimport torch._dynamo as dynamo\nfrom torch import tensor, device\nimport torch.fx as fx\nfrom torch._dynamo.testing import rand_strided\nfrom math import inf\nfrom torch._dynamo.debug_utils import run_fwd_maybe_bwd\n\nfrom torch.nn import *\n\nclass Repro(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, add):\n        relu = torch.relu(add);  add = None\n        return (relu,)\n\n\nmod = Repro().cuda()\nopt_mod = torch.compile(mod, backend=\"None\")\n\n\nargs = [((200, 200), (200, 1), torch.float32, 'cpu', False)]\nargs = [rand_strided(sh, st, dt, dev).requires_grad_(rg) for (sh, st, dt, dev, rg) in args]\n\n\nwith torch.cuda.amp.autocast(enabled=False):\n    ref = run_fwd_maybe_bwd(mod, args)\n    res = run_fwd_maybe_bwd(opt_mod, args)\n```\n\nThe minifier successfully reduced the graph to the op that raises the\nerror in `toy_compiler`. The other difference from the procedure in\n{ref}`minifying-torchinductor-errors` is that the minifier is\nautomatically run after encountering a backend compiler error. After a\nsuccessful run, the minifier writes `repro.py` to\n`torch._dynamo.config.base_dir`.",
    "1909": "一级标题：PyTorch 2.0 Troubleshooting (old)\n二级标题：Performance Profiling\n内容：\n### Accessing TorchDynamo Profiler\n\nTorchDynamo has a built-in stats function for collecting and displaying\nthe time spent in each compilation phase. These stats can be accessed by\ncalling `torch._dynamo.utils.compile_times()` after executing\nTorch.\\_Dynamo. By default, this returns a string representation of the\ncompile times spent in each TorchDynamo function by name.\n\n### TorchInductor Debugging using TORCH_COMPILE_DEBUG\n\nTorchInductor has a builtin stats and trace function for displaying time\nspent in each compilation phase, output code, output graph visualization\nand IR dump. This is a debugging tool designed to make it easier to\nunderstand and troubleshoot the internals of TorchInductor.\n\nLet's run an example with the following test program (`repro.py`):\n\n```\nimport torch\n\n@torch.compile()\ndef test_model(x):\n    model = torch.nn.Sequential(\n        torch.nn.Linear(10, 10),\n        torch.nn.LayerNorm(10),\n        torch.nn.ReLU(),\n    )\n    return model(x)\n\n\ny = test_model(torch.ones(10, 10))\n```\n\nSetting the environment variable `TORCH_COMPILE_DEBUG=1` will cause a\ndebug trace directory to be created, by default this directory will be in the\ncurrent directory and named torch_compile_debug (this can be overridden in\nthe torchdynamo configuration field `debug_dir_root` and also the\n`env var TORCH_COMPILE_DEBUG_DIR`). Inside this directory, each run will\nhave a separate folder named with the timestamp and process id of the run:\n\n```\n$ env TORCH_COMPILE_DEBUG=1 python repro.py\n$ cd torch_compile_debug\n$ ls\nrun_2023_03_01_08_20_52_143510-pid_180167\n```\n\nIn the run folder there will be a `torchdynamo` directory which contains\ndebug logs, and an `torchinductor` folder which contains a subfolder for each\ncompiled kernel with inductor debug artifacts.\n\n```\n$ cd\nrun_2023_03_01_08_20_52_143510-pid_180167\n$ ls\ntorchinductor  torchdynamo\n```\n\nMoving further into the `torchinductor` directory, the `\\*.log` files are\nlogs from the AOT Autograd phase of compilation, `model__0_forward_1.0` contains\nthe inductor debug artifacts.\n\n```\n$ cd torchinductor\n$ ls\naot_model___0_debug.log  model__0_forward_1.0\n$ cd model__0_forward_1.0\n$ ls\ndebug.log  fx_graph_readable.py  fx_graph_runnable.py  fx_graph_transformed.py  ir_post_fusion.txt  ir_pre_fusion.txt  output_code.py\n```\n\nHere is a summary of the contents:\n\n- `fx_graph_readable.py` and `fx_graph_runnable.py` are the readable and\n  runnable versions of the `fx_graph` received by inductor.\n- `fx_graph_transformed.py` is the fx graph after inductor has run all fx passes.\n- `ir\\*.txt` is the inductor ir pre and post fusion.\n- `output_code.py` is the compiled triton kernel for the subgraph.\n\nHere are [example debug directory contents](https://gist.github.com/jansel/f4af078791ad681a0d4094adeb844396)\nfor the test program:\n\n```\nimport torch\n\n@torch.compile()\ndef test_model(x):\n    model = torch.nn.Sequential(\n        torch.nn.Linear(10, 10),\n        torch.nn.LayerNorm(10),\n        torch.nn.ReLU(),\n    )\n    return model(x)\n\n\ny = test_model(torch.ones(10, 10))\n```\n\nEach file in that debug trace can be enabled and disabled through\n`torch._inductor.config.trace.*`. The profile and the diagram are both\ndisabled by default since they are expensive to generate.\n\nA single node in this new debug format looks like:\n\n```\nbuf1: SchedulerNode(ComputedBuffer)\nbuf1.writes =\n    {   MemoryDep(name='buf1', index=0, size=()),\n        MemoryDep(name='buf1', index=0, size=(s0,))}\nbuf1.unmet_dependencies = {MemoryDep(name='buf0', index=c0, size=(s0,))}\nbuf1.met_dependencies = {MemoryDep(name='primals_2', index=c0, size=(s0,))}\nbuf1.group.device = cuda:0\nbuf1.group.iteration = (1, s0)\nbuf1.sizes = ([], [s0])\nclass buf1_loop_body:\n    var_ranges = {z0: s0}\n    index0 = z0\n    index1 = 0\n    def body(self, ops):\n        get_index = self.get_index('index0')\n        load = ops.load('buf0', get_index, False)\n        get_index_1 = self.get_index('index0')\n        load_1 = ops.load('primals_2', get_index_1, False)\n        add = ops.add(load, load_1)\n        get_index_2 = self.get_index('index1')\n        reduction = ops.reduction('buf1', torch.float32, torch.float32, 'sum', get_index_2, add)\n        return reduction\n```\n\nSee the [example debug directory\noutput](https://gist.github.com/jansel/f4af078791ad681a0d4094adeb844396)\nfor more examples.\n\n% _Memory Profiling\n% ----------------\n%\n% TBD\n\n### Graph Breaks\n\nGiven a program like this:\n\n```python\ndef some_fun(x):\n    ...\n\ncompiled_fun = torch.compile(some_fun, ...)\n...\n```\n\nTorchDynamo will attempt to compile all of the torch/tensor operations\nwithin some_fun into a single FX graph, but it may fail to capture\neverything into one graph.\n\nSome graph break reasons are insurmountable to TorchDynamo, and can't be\neasily fixed. - calling into a C extension other than torch is invisible\nto torchdynamo, and could do arbitrary things without TorchDynamo being\nable to introduce necessary guards (see {ref}`making-dynamo-sound-guards`)\nto ensure that the compiled program would be safe to reuse. Graph breaks\ncan hinder performance if the resulting fragments are small. To maximize\nperformance, it's important to have as few graph breaks as possible.",
    "1910": "一级标题：PyTorch 2.0 Troubleshooting (old)\n二级标题：Identifying the Cause of a Graph Break\n内容：\nTo identify all graph breaks in a program and the associated reasons for\nthe breaks, `torch._dynamo.explain` can be used. This tool runs\nTorchDynamo on the supplied function and aggregates the graph breaks\nthat are encountered. Here is an example usage:\n\n```python\nimport torch\nimport torch._dynamo as dynamo\ndef toy_example(a, b):\n    x = a / (torch.abs(a) + 1)\n    print(\"woo\")\n    if b.sum() < 0:\n        b = b * -1\n    return x * b\nexplanation = dynamo.explain(toy_example)(torch.randn(10), torch.randn(10))\nprint(explanation_verbose)\n\"\"\"\nGraph Count: 3\nGraph Break Count: 2\nOp Count: 5\nBreak Reasons:\n  Break Reason 1:\n    Reason: builtin: print [<class 'torch._dynamo.variables.constant.ConstantVariable'>] False\n    User Stack:\n      <FrameSummary file foo.py, line 5 in toy_example>\n  Break Reason 2:\n    Reason: generic_jump TensorVariable()\n    User Stack:\n      <FrameSummary file foo.py, line 6 in torch_dynamo_resume_in_toy_example_at_5>\nOps per Graph:\n  ...\nOut Guards:\n  ...\n\"\"\"\n```\n\nOutputs include:\n\n- `out_guards` - a list of lists where each sublist contains the guards that must pass to ensure the traced graphs are valid.\n- `graphs` - a list of graph modules which were successfully traced.\n- `ops_per_graph` - a list of lists where each sublist contains the ops that are run in the graph.\n\nTo throw an error on the first graph break encountered, use the `fullgraph`\nmode. This mode disables TorchDynamo’s Python fallback, and only\nsucceeds if the entire program is convertible into a single graph. Example\nusage:\n\n```python\ndef toy_example(a, b):\n   ...\n\ncompiled_toy = torch.compile(toy_example, fullgraph=True, backend=<compiler>)(a, b)\n```\n\n### Excessive Recompilation\n\nWhen TorchDynamo compiles a function (or part of one), it makes certain\nassumptions about locals and globals in order to allow compiler\noptimizations, and expresses these assumptions as guards that check\nparticular values at runtime. If any of these guards fail, Dynamo will\nrecompile that function (or part) up to\n`torch._dynamo.config.recompile_limit` times. If your program is\nhitting the cache limit, you will first need to determine which guard is\nfailing and what part of your program is triggering it.\n\nIf your program exhibits a bounded amount of dynamism, you may be able\nto tune the TorchDynamo cache limit to allow for each variation to be\ncompiled and cached, but if the cache limit is too high you may find the\ncost of recompilation outweighs any optimization benefits.\n\n```\ntorch._dynamo.config.recompile_limit = <your desired cache limit>\n```\n\nTorchDynamo plans to support many common cases of dynamic tensor shapes,\nsuch as varying batch size or sequence length. It does not plan to\nsupport rank-dynamism. In the meantime, setting a specific cache limit\ncan be used in coordination with bucketing techniques to achieve an\nacceptable number of recompilations for some dynamic models.",
    "1911": "一级标题：PyTorch 2.0 Troubleshooting (old)\n二级标题：Accuracy Debugging\n内容：\nAccuracy issues can also be minified if you set the environment variable\n`TORCHDYNAMO_REPRO_LEVEL=4`, it operates with a similar git bisect\nmodel and a full repro might be something like\n`TORCHDYNAMO_REPRO_AFTER=\"aot\" TORCHDYNAMO_REPRO_LEVEL=4` the reason\nwe need this is downstream compilers will codegen code whether it’s\nTriton code or the C++ backend, the numerics from those downstream\ncompilers can be different in subtle ways yet have dramatic impact on\nyour training stability. So the accuracy debugger is very useful for us\nto detect bugs in our codegen or with a backend compiler.\n\nIf you'd like to ensure that random number generation is the same across both torch\nand triton then you can enable `torch._inductor.config.fallback_random = True`",
    "1912": "一级标题：PyTorch 2.0 Troubleshooting (old)\n二级标题：Extended Debugging\n内容：\nExtended debugging can be enabled by using the following experimental flags.\n\n`TORCHDYNAMO_EXTENDED_DEBUG_GUARD_ADDED` - provides extended debug information if the\nstring representation of a guard matches this flag value. For example, set it to\n\"Ne(s0, 10)\" to generate full Python and C++ backtrace whenever guard was issued.\n`TORCHDYNAMO_EXTENDED_DEBUG_CREATE_SYMBOL` - provides extended debug information when\na particular symbol is allocated. For example, set this to \"u2\" to generate full Python\nand C++ backtrace whenever this symbol was created.\n`TORCHDYNAMO_EXTENDED_DEBUG_CPP` - provides extended debug information (C++ backtrace)\nfor all extended debug settings as well as errors. For example, set this to \"1\". The C++\nbacktrace is slow and very spammy so it is not included by default with extended debugging.",
    "1913": "一级标题：PyTorch 2.0 Troubleshooting (old)\n二级标题：Cold Start Timing and Cache Corruption Debugging\n内容：\nIn order to measure the cold start compilation time or debug a cache corruption,\nit is possible pass `TORCHINDUCTOR_FORCE_DISABLE_CACHES=1` or set\n`torch.compiler.config.force_disable_caches = True` which will override any\nother caching config option and disable all compile time caching.",
    "1914": "一级标题：torch.overrides\n二级标题：无\n内容：\n```{eval-rst}\n.. py:module:: torch.overrides\n```\n\nThis module exposes various helper functions for the ``__torch_function__``\nprotocol. See {ref}`extending-torch-python` for more details on the\n``__torch_function__`` protocol.",
    "1915": "一级标题：torch.overrides\n二级标题：Functions\n内容：\n```{eval-rst}\n.. autofunction::  get_ignored_functions\n```\n\n```{eval-rst}\n.. autofunction::  get_overridable_functions\n```\n\n```{eval-rst}\n.. autofunction::  resolve_name\n```\n\n```{eval-rst}\n.. autofunction::  get_testing_overrides\n```\n\n```{eval-rst}\n.. autofunction::  handle_torch_function\n```\n\n```{eval-rst}\n.. autofunction::  has_torch_function\n```\n\n```{eval-rst}\n.. autofunction::  is_tensor_like\n```\n\n```{eval-rst}\n.. autofunction::  is_tensor_method_or_property\n```\n\n```{eval-rst}\n.. autofunction::  wrap_torch_function\n```",
    "1916": "一级标题：Understanding CUDA Memory Usage\n二级标题：无\n内容：\nTo debug CUDA memory use, PyTorch provides a way to generate memory snapshots that record the state of allocated CUDA memory\nat any point in time, and optionally record the history of allocation events that led up to that snapshot.\n\nThe generated snapshots can then be drag and dropped onto the interactiver viewer hosted at [pytorch.org/memory_viz](https://pytorch.org/memory_viz) which\ncan be used to explore the snapshot.\n\n```{note}\nThe memory profiler and visualizer described in this document only have visibility into the CUDA memory that is\nallocated and managed through the PyTorch allocator.  Any memory allocated directly from CUDA APIs will not be\nvisible in the PyTorch memory profiler.\n\nNCCL (used for distributed communication on CUDA devices) is a common example of a library that allocates some\nGPU memory that is invisible to the PyTorch memory profiler.  See {ref}`non_pytorch_alloc` for more info.\n```",
    "1917": "一级标题：Understanding CUDA Memory Usage\n二级标题：Generating a Snapshot\n内容：\nThe common pattern for recording a snapshot is to enable memory history, run the code to be observed, and then save a file with a pickled snapshot:\n\n```python\n# enable memory history, which will\n# add tracebacks and event history to snapshots\ntorch.cuda.memory._record_memory_history()\n\nrun_your_code()\ntorch.cuda.memory._dump_snapshot(\"my_snapshot.pickle\")\n```",
    "1918": "一级标题：Understanding CUDA Memory Usage\n二级标题：Using the visualizer\n内容：\nOpen [pytorch.org/memory_viz](https://pytorch.org/memory_viz) and drag/drop the pickled snapshot file into the visualizer.\nThe visualizer is a javascript application that runs locally on your computer. It does not upload any snapshot data.",
    "1919": "一级标题：Understanding CUDA Memory Usage\n二级标题：Active Memory Timeline\n内容：\nThe Active Memory Timeline shows all the live tensors over time in the snapshot on a particular GPU. Pan/Zoom over the plot to look at smaller allocations.\nMouse over allocated blocks to see a stack trace for when that block was allocated, and details like its address. The detail slider can be adjusted to\nrender fewer allocations and improve performance when there is a lot of data.\n\n```{image} _static/img/torch_cuda_memory/active_memory_timeline.png\n```",
    "1920": "一级标题：Understanding CUDA Memory Usage\n二级标题：Allocator State History\n内容：\nThe Allocator State History shows individual allocator events in a timeline on the left. Select an event in the timeline to see a visual summary of the\nallocator state at that event. This summary shows each individual segment returned from cudaMalloc and how it is split up into blocks of individual allocations\nor free space. Mouse over segments and blocks to see the stack trace when the memory was allocated. Mouse over events to see the stack trace when the event occurred,\nsuch as when a tensor was freed. Out of memory errors are reported as OOM events. Looking at the state of memory during an OOM may provide insight into why\nan allocation failed even though reserved memory still exists.\n\n```{image} _static/img/torch_cuda_memory/allocator_state_history.png\n```\n\nThe stack trace information also reports the address at which an allocation occurred.\nThe address b7f064c000000_0 refers to the (b)lock at address 7f064c000000 which is the \"_0\"th time this address was allocated.\nThis unique string can be looked up in the Active Memory Timeline and searched\nin the Active State History to examine the memory state when a tensor was allocated or freed.\n\n(non_pytorch_alloc)=",
    "1921": "一级标题：Understanding CUDA Memory Usage\n二级标题：Identifying Non-PyTorch allocations\n内容：\nIf you suspect CUDA memory is being allocated outside of PyTorch, you can collect the raw CUDA allocation info using\nthe pynvml package, and compare that to the allocation reported by pytorch.\n\n\nTo collect raw memory usage outside pytorch, use {func}`device_memory_used`\n\n```python\nimport torch\ndevice_idx = ...\nprint(torch.cuda.device_memory_used(device_idx))\n```",
    "1922": "一级标题：Understanding CUDA Memory Usage\n二级标题：Snapshot API Reference\n内容：\n```{eval-rst}\n.. currentmodule:: torch.cuda.memory\n```\n\n```{eval-rst}\n.. autofunction:: _record_memory_history\n```\n\n```{eval-rst}\n.. autofunction:: _snapshot\n```\n\n\n```{eval-rst}\n.. autofunction:: _dump_snapshot\n```",
    "1923": "一级标题：Torch Environment Variables\n二级标题：无\n内容：\nPyTorch leverages environment variables for adjusting various settings that influence its runtime behavior.\nThese variables offer control over key functionalities, such as displaying the C++ stack trace upon encountering errors, synchronizing the execution of CUDA kernels,\nspecifying the number of threads for parallel processing tasks and many more.\n\nMoreover, PyTorch leverages several high-performance libraries, such as MKL and cuDNN,\nwhich also utilize environment variables to modify their functionality.\nThis interplay of settings allows for a highly customizable development environment that can be\noptimized for efficiency, debugging, and computational resource management.\n\nPlease note that while this documentation covers a broad spectrum of environment variables relevant to PyTorch and its associated libraries, it is not exhaustive.\nIf you find anything in this documentation that is missing, incorrect, or could be improved, please let us know by filing an issue or opening a pull request.\n\n\n```{eval-rst}\n.. toctree::\n   :maxdepth: 1\n\n   threading_environment_variables\n   cuda_environment_variables\n   mps_environment_variables\n   debugging_environment_variables\n   miscellaneous_environment_variables\n   logging\n   torch_nccl_environment_variables\n\n```",
    "1924": "一级标题：PYTORCH ProcessGroupNCCL Environment Variables\n二级标题：无\n内容：\nFor more information on the environment variables, see [ProcessGroupNCCL Environment Variables](https://github.com/pytorch/pytorch/blob/main/torch/csrc/distributed/c10d/ProcessGroupNCCL.hpp).\n\n```{list-table}\n:header-rows: 1\n\n* - **Variable**\n  - **Description**\n* - ``TORCH_NCCL_ASYNC_ERROR_HANDLING``\n  - Control how we perform Async Error Handling with NCCL when an exception is observed in watchdog. If set to 0, no handling of asynchronous NCCL errors. If set to 1, aborting NCCL communicator and tearing down process upon error. If set to 2, only abort NCCL communicator and if set to 3, tearing down process without aborting NCCL communicator. By default, it is set to 3.\n* - ``TORCH_NCCL_HIGH_PRIORITY``\n  - Control whether to use high priority stream for the NCCL communicator.\n* - ``TORCH_NCCL_BLOCKING_WAIT``\n  - Control whether or not wait() is blocking or non-blocking.\n* - ``TORCH_NCCL_DUMP_ON_TIMEOUT``\n  - Control whether dumping debug info on watchdog timeout or exception is detected. This variable must be set together with TORCH_NCCL_TRACE_BUFFER_SIZE larger than 0.\n* - ``TORCH_NCCL_DESYNC_DEBUG``\n  - Control whether Desync Debug is enabled. This is helpful in figuring out the culprit rank of collective desync.\n* - ``TORCH_NCCL_ENABLE_TIMING``\n  - If set to ``1``, enable recording start-events for all ProcessGroupNCCL collectives, and compute accurate collective timing per-collective.\n* - ``TORCH_NCCL_ENABLE_MONITORING``\n  - If set to ``1``,enable monitoring thread which aborts the process when the ProcessGroupNCCL Watchdog thread gets stuck and no heartbeat is detected after TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC. This can happen due to calling CUDA/NCCL APIs that may hang. It is Useful to prevent jobs being stuck for a prolonged time than necessary tying up cluster resources.\n* - ``TORCH_NCCL_HEARTBEAT_TIMEOUT_SEC``\n  - Control the watchdog heartbeat timeout period after which the monitoring thread will abort the process.\n* - ``TORCH_NCCL_TRACE_BUFFER_SIZE``\n  - The maximum number of events we store in the flight recorder's ring buffer. One event could be the start or end of a collective, for example. Set to 0 to disable the tracebuffer and debugging info dump.\n* - ``TORCH_NCCL_TRACE_CPP_STACK``\n  - Whether to collect cpp stack traces for flight recorder. Default value is False.\n* - ``TORCH_NCCL_COORD_CHECK_MILSEC``\n  - Control the interval inside the monitoring thread to check the coordinated signal from other ranks, e.g. to dump the debugging information. Default value is 1000 ms.\n* - ``TORCH_NCCL_WAIT_TIMEOUT_DUMP_MILSEC``\n  - Control how much extra time we will wait for dumping the debugging info before we exit and throws timeout exception.\n* - ``TORCH_NCCL_DEBUG_INFO_TEMP_FILE``\n  - The file into which the debugging info would be dumped.\n* - ``TORCH_NCCL_DEBUG_INFO_PIPE_FILE``\n  - The pipe file to trigger debugging dump manually, write anything into the pipe would trigger the dump.\n* - ``TORCH_NCCL_NAN_CHECK``\n  - Control whether to enable NAN check for the input, Error would be thrown if NAN is detected.\n```",
    "1925": "一级标题：Type Info\n二级标题：无\n内容：\nThe numerical properties of a {class}`torch.dtype` can be accessed through either the {class}`torch.finfo` or the {class}`torch.iinfo`.\n\n(finfo-doc)=",
    "1926": "一级标题：Type Info\n二级标题：torch.finfo\n内容：\n```{eval-rst}\n.. class:: torch.finfo\n```\n\nA {class}`torch.finfo` is an object that represents the numerical properties of a floating point\n{class}`torch.dtype`, (i.e. ``torch.float32``, ``torch.float64``, ``torch.float16``, and ``torch.bfloat16``).\nThis is similar to [numpy.finfo](https://numpy.org/doc/stable/reference/generated/numpy.finfo.html).\n\nA {class}`torch.finfo` provides the following attributes:\n\n| Name            | Type  | Description                                                                                 |\n| :-------------- | :---- | :------------------------------------------------------------------------------------------ |\n| bits            | int   | The number of bits occupied by the type.                                                    |\n| eps             | float | The difference between 1.0 and the next smallest representable float larger than 1.0.       |\n| max             | float | The largest representable number.                                                           |\n| min             | float | The smallest representable number (typically ``-max``).                                     |\n| tiny            | float | The smallest positive normal number. Equivalent to ``smallest_normal``.                     |\n| smallest_normal | float | The smallest positive normal number. See notes.                                             |\n| resolution      | float | The approximate decimal resolution of this type, i.e., ``10**-precision``.                  |\n\n```{note}\n  The constructor of {class}`torch.finfo` can be called without argument,\n  in which case the class is created for the pytorch default dtype (as returned by {func}`torch.get_default_dtype`).\n```\n\n```{note}\n  `smallest_normal` returns the smallest *normal* number, but there are smaller\n  subnormal numbers. See https://en.wikipedia.org/wiki/Denormal_number\n  for more information.\n```\n\n(iinfo-doc)=",
    "1927": "一级标题：Type Info\n二级标题：torch.iinfo\n内容：\n```{eval-rst}\n.. class:: torch.iinfo\n```\n\nA {class}`torch.iinfo` is an object that represents the numerical properties of a integer\n{class}`torch.dtype` (i.e. ``torch.uint8``, ``torch.int8``, ``torch.int16``, ``torch.int32``, and ``torch.int64``).\nThis is similar to [numpy.iinfo](https://numpy.org/doc/stable/reference/generated/numpy.iinfo.html).\n\nA {class}`torch.iinfo` provides the following attributes:\n\n| Name | Type | Description                              |\n| :--- | :--- | :--------------------------------------- |\n| bits | int  | The number of bits occupied by the type. |\n| max  | int  | The largest representable number.        |\n| min  | int  | The smallest representable number.       |",
    "1928": "一级标题：User Guide\n二级标题：无\n内容：\nPyTorch provides a flexible and efficient platform for building deep\nlearning models, offering dynamic computation graphs and a rich\necosystem of tools and libraries. This guide will help you harness the power\nof PyTorch to create and deploy machine learning models effectively.\n\n```{note}\nThis guide is a work in progress.\n```\n\n```{toctree}\n:maxdepth: 1\n:caption: Introduction\n\nPytorch Overview <https://docs.pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html>\nGet Started <https://pytorch.org/get-started/locally/>\nLearn the Basics <https://docs.pytorch.org/tutorials/beginner/basics/intro.html>\n```\n\n```{toctree}\n:maxdepth: 1\n:caption: Core Concepts\n\npytorch_main_components\n```\n\n```{toctree}\n:maxdepth: 1\n:caption: Beyond the Basics\n\n```\n\n```{toctree}\n:maxdepth: 1\n:caption: Developer Notes\n\n../notes\n```",
    "1929": "一级标题：PyTorch Main Components\n二级标题：无\n内容：\nPyTorch is a flexible and powerful library for deep learning that provides a comprehensive set of tools for building, training, and deploying machine learning models.",
    "1930": "一级标题：PyTorch Main Components\n二级标题：PyTorch Components for Basic Deep Learning\n内容：\nSome of the basic PyTorch components include:\n\n* **Tensors** - N-dimensional arrays that serve as PyTorch's fundamental\ndata structure. They support automatic differentiation, hardware acceleration, and provide a comprehensive API for mathematical operations.\n\n* **Autograd** - PyTorch's automatic differentiation engine\nthat tracks operations performed on tensors and builds a computational\ngraph dynamically to be able to compute gradients.\n\n* **Neural Network API** - A modular framework for building neural networks with pre-defined layers,\nactivation functions, and loss functions. The {mod}`nn.Module` base class provides a clean interface\nfor creating custom network architectures with parameter management.\n\n* **DataLoaders** - Tools for efficient data handling that provide\nfeatures like batching, shuffling, and parallel data loading. They abstract away the complexities\nof data preprocessing and iteration, allowing for optimized training loops.",
    "1931": "一级标题：PyTorch Main Components\n二级标题：PyTorch Compiler\n内容：\nThe PyTorch compiler is a suite of tools that optimize model execution and\nreduce resource requirements. You can learn more about the PyTorch compiler [here](https://docs.pytorch.org/docs/stable/torch.compiler_get_started.html).",
    "1932": "一级标题：torch.utils\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.utils\n```\n\n```{eval-rst}\n.. currentmodule:: torch.utils\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    rename_privateuse1_backend\n    generate_methods_for_privateuse1_backend\n    get_cpp_backtrace\n    set_module\n    swap_tensors\n```\n\n<!-- This module needs to be documented. Adding here in the meantime\nfor tracking purposes -->\n```{eval-rst}\n.. py:module:: torch.utils.backend_registration\n.. py:module:: torch.utils.benchmark.examples.compare\n.. py:module:: torch.utils.benchmark.examples.fuzzer\n.. py:module:: torch.utils.benchmark.examples.op_benchmark\n.. py:module:: torch.utils.benchmark.examples.simple_timeit\n.. py:module:: torch.utils.benchmark.examples.spectral_ops_fuzz_test\n.. py:module:: torch.utils.benchmark.op_fuzzers.binary\n.. py:module:: torch.utils.benchmark.op_fuzzers.sparse_binary\n.. py:module:: torch.utils.benchmark.op_fuzzers.sparse_unary\n.. py:module:: torch.utils.benchmark.op_fuzzers.spectral\n.. py:module:: torch.utils.benchmark.op_fuzzers.unary\n.. py:module:: torch.utils.benchmark.utils.common\n.. py:module:: torch.utils.benchmark.utils.compare\n.. py:module:: torch.utils.benchmark.utils.compile\n.. py:module:: torch.utils.benchmark.utils.cpp_jit\n.. py:module:: torch.utils.benchmark.utils.fuzzer\n.. py:module:: torch.utils.benchmark.utils.sparse_fuzzer\n.. py:module:: torch.utils.benchmark.utils.timer\n.. py:module:: torch.utils.benchmark.utils.valgrind_wrapper.timer_interface\n.. py:module:: torch.utils.bundled_inputs\n.. py:module:: torch.utils.checkpoint\n.. py:module:: torch.utils.collect_env\n.. py:module:: torch.utils.cpp_backtrace\n.. py:module:: torch.utils.cpp_extension\n.. py:module:: torch.utils.data.backward_compatibility\n.. py:module:: torch.utils.data.dataloader\n.. py:module:: torch.utils.data.datapipes.dataframe.dataframe_wrapper\n.. py:module:: torch.utils.data.datapipes.dataframe.dataframes\n.. py:module:: torch.utils.data.datapipes.dataframe.datapipes\n.. py:module:: torch.utils.data.datapipes.dataframe.structures\n.. py:module:: torch.utils.data.datapipes.datapipe\n.. py:module:: torch.utils.data.datapipes.gen_pyi\n.. py:module:: torch.utils.data.datapipes.iter.callable\n.. py:module:: torch.utils.data.datapipes.iter.combinatorics\n.. py:module:: torch.utils.data.datapipes.iter.combining\n.. py:module:: torch.utils.data.datapipes.iter.filelister\n.. py:module:: torch.utils.data.datapipes.iter.fileopener\n.. py:module:: torch.utils.data.datapipes.iter.grouping\n.. py:module:: torch.utils.data.datapipes.iter.routeddecoder\n.. py:module:: torch.utils.data.datapipes.iter.selecting\n.. py:module:: torch.utils.data.datapipes.iter.sharding\n.. py:module:: torch.utils.data.datapipes.iter.streamreader\n.. py:module:: torch.utils.data.datapipes.iter.utils\n.. py:module:: torch.utils.data.datapipes.map.callable\n.. py:module:: torch.utils.data.datapipes.map.combinatorics\n.. py:module:: torch.utils.data.datapipes.map.combining\n.. py:module:: torch.utils.data.datapipes.map.grouping\n.. py:module:: torch.utils.data.datapipes.map.utils\n.. py:module:: torch.utils.data.datapipes.utils.common\n.. py:module:: torch.utils.data.datapipes.utils.decoder\n.. py:module:: torch.utils.data.datapipes.utils.snapshot\n.. py:module:: torch.utils.data.dataset\n.. py:module:: torch.utils.data.distributed\n.. py:module:: torch.utils.data.graph\n.. py:module:: torch.utils.data.graph_settings\n.. py:module:: torch.utils.data.sampler\n.. py:module:: torch.utils.dlpack\n.. py:module:: torch.utils.file_baton\n.. py:module:: torch.utils.flop_counter\n.. py:module:: torch.utils.hipify.constants\n.. py:module:: torch.utils.hipify.cuda_to_hip_mappings\n.. py:module:: torch.utils.hipify.hipify_python\n.. py:module:: torch.utils.hipify.version\n.. py:module:: torch.utils.hooks\n.. py:module:: torch.utils.jit.log_extract\n.. py:module:: torch.utils.mkldnn\n.. py:module:: torch.utils.mobile_optimizer\n.. py:module:: torch.utils.show_pickle\n.. py:module:: torch.utils.tensorboard.summary\n.. py:module:: torch.utils.tensorboard.writer\n.. py:module:: torch.utils.throughput_benchmark\n.. py:module:: torch.utils.weak\n```",
    "1933": "一级标题：torch.xpu\n二级标题：无\n内容：\n```{eval-rst}\n.. automodule:: torch.xpu\n```\n```{eval-rst}\n.. currentmodule:: torch.xpu\n```\n\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    StreamContext\n    current_device\n    current_stream\n    device\n    device_count\n    device_of\n    get_arch_list\n    get_device_capability\n    get_device_name\n    get_device_properties\n    get_gencode_flags\n    get_stream_from_external\n    init\n    is_available\n    is_initialized\n    set_device\n    set_stream\n    stream\n    synchronize\n```",
    "1934": "一级标题：torch.xpu\n二级标题：Random Number Generator\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    get_rng_state\n    get_rng_state_all\n    initial_seed\n    manual_seed\n    manual_seed_all\n    seed\n    seed_all\n    set_rng_state\n    set_rng_state_all\n```",
    "1935": "一级标题：torch.xpu\n二级标题：Streams and events\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n    Event\n    Stream\n```\n\n```{eval-rst}\n.. automodule:: torch.xpu.memory\n```\n```{eval-rst}\n.. currentmodule:: torch.xpu.memory\n```",
    "1936": "一级标题：torch.xpu\n二级标题：Memory management\n内容：\n```{eval-rst}\n.. autosummary::\n    :toctree: generated\n    :nosignatures:\n\n     empty_cache\n     max_memory_allocated\n     max_memory_reserved\n     mem_get_info\n     memory_allocated\n     memory_reserved\n     memory_stats\n     memory_stats_as_nested_dict\n     reset_accumulated_memory_stats\n     reset_peak_memory_stats\n```\n\n<!-- This module needs to be documented. Adding here in the meantime\nfor tracking purposes -->\n```{eval-rst}\n.. py:module:: torch.xpu.random\n.. py:module:: torch.xpu.streams\n```"
}