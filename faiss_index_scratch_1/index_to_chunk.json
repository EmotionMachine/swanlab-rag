{
    "0": "一级标题：API文档\n二级标题：无\n内容：\nAPI文档分为以下几个部分：\n\n- `CLI`：命令行部分\n- `Python`：Python SDK部分\n- `其他`：环境变量",
    "1": "一级标题：API文档\n二级标题：CLI\n内容：\n- [swanlab watch](/api/cli-swanlab-watch.md): 启动离线实验看板\n- [swanlab login](/api/cli-swanlab-login.md): 登录SwanLab\n- [swanlab logout](/api/cli-swanlab-logout.md): 登出SwanLab\n- [swanlab convert](/api/cli-swanlab-convert.md): 将其他产品的日志转换为SwanLab项目\n- [swanlab sync](/api/cli-swanlab-sync.md): 将本地日志同步到SwanLab云端/私有化部署端",
    "2": "一级标题：API文档\n二级标题：Python SDK\n内容：\n- [init](/api/py-init.md)\n- [login](/api/py-login.md)\n- [Image](/api/py-Image.md)\n- [Audio](/api/py-Audio.md)\n- [Text](/api/py-Text.md)\n- [Video](/api/py-video.md)\n- [ECharts](/api/py-echarts.md)\n- [Object3D](/api/py-object3d.md)\n- [Molecule](/api/py-molecule.md)\n- [pr_curve](/api/py-pr_curve.md)\n- [roc_curve](/api/py-roc_curve.md)\n- [confusion_matrix](/api/py-confusion_matrix.md)\n- [run](/api/py-run.md)\n- [convert](/api/py-converter.md)\n- [sync_wandb](/api/py-sync-wandb.md)\n- [sync_tensorboard](/api/py-sync-tensorboard.md)\n- [sync_mlflow](/api/py-sync-mlflow.md)\n- [register_callback](/api/py-register-callback.md)",
    "3": "一级标题：API文档\n二级标题：其他\n内容：\n- [开放接口](/api/py-openapi.md)\n- [环境变量](/api/environment-variable.md)",
    "4": "一级标题：swanlab convert\n二级标题：无\n内容：\n```bash\nswanlab convert [OPTIONS]\n```\n\n| 选项 | 描述 |\n| --- | --- |\n| `-t`, `--type` | 选择转换类型，可选`tensorboard`、`wandb`、`mlflow`，默认为`tensorboard`。 |\n| `-p`, `--project` | 设置转换创建的SwanLab项目名，默认为None。 |\n| `-w`, `--workspace` | 设置SwanLab项目所在空间，默认为None。 |\n| `-l`, `--logdir` | 设置SwanLab项目的日志文件保存路径，默认为None。 |\n| `--cloud` | 设置SwanLab项目是否将日志上传到云端，默认为True。 |\n| `--tb-logdir` | 需要转换的Tensorboard日志文件路径(tfevent) |\n| `--wb-project` | 需要转换的Wandb项目名 |\n| `--wb-entity` | 需要转换的Wandb项目所在实体 |\n| `--wb-runid` | 需要转换的Wandb Run的id |\n| `--mlflow-uri` | 需要转换的MLFlow项目URI |\n| `--mlflow-exp` | 需要转换的MLFlow实验ID |",
    "5": "一级标题：swanlab convert\n二级标题：介绍\n内容：\n将其他日志工具的内容转换为SwanLab项目。  \n支持转换的工具包括：`Tensorboard`、`Weights & Biases`、`MLFlow`。",
    "6": "一级标题：swanlab convert\n二级标题：使用案例\n内容：\n### Tensorboard\n\n[集成-Tensorboard](/guide_cloud/integration/integration-tensorboard.md)\n\n### Weights & Biases\n\n[集成-Weights & Biases](/guide_cloud/integration/integration-wandb.md)\n\n### MLFlow\n\n[集成-MLFlow](/guide_cloud/integration/integration-mlflow.md)",
    "7": "一级标题：swanlab login\n二级标题：无\n内容：\n``` bash\nswanlab login [OPTIONS]\n```\n\n| 选项 | 描述 |\n| --- | --- |\n| `-r`, `--relogin` | 重新登录。|\n| `-h`, `--host` | 指定SwanLab服务所在的主机。比如`http://localhost:8000`。|\n| `-k`, `--api-key` | 指定API Key。如果您不喜欢使用命令行来输入 API 密钥，这将允许自动登录。|\n| `-w`, `--web-host` | 指定SwanLab前端所在的Web主机。|",
    "8": "一级标题：swanlab login\n二级标题：介绍\n内容：\n登录SwanLab账号，以同步实验到云端。\n\n执行下面的命令后，如果第一次登录，会让你填写[API_KEY](https://swanlab.cn/settings)：\n\n```bash\nswanlab login\n```\n\n登录过一次后，凭证会保存到本地，并覆盖之前登录过的凭证，无需再次通过`swanlab.login`或`swanlab login`登录。\n\n> 如果你不希望凭证保存在本地，请在python脚本中使用[swanlab.login()](./py-login.md)进行登录。\n\n如果你的电脑不太适合命令行粘贴API Key（比如一些Windows CMD）的方式登录，可以使用：\n\n```bash\nswanlab login -k <api-key>\n```",
    "9": "一级标题：swanlab login\n二级标题：重新登录\n内容：\n如果需要登录一个别的账号，则用下面的命令：\n\n```bash\nswanlab login --relogin\n```\n\n这会让你输入一个新的API Key以重新登录。",
    "10": "一级标题：swanlab login\n二级标题：退出登录\n内容：\n```bash\nswanlab logout\n```",
    "11": "一级标题：swanlab login\n二级标题：登录到私有化服务\n内容：\n```bash\nswanlab login --host <host>\n```",
    "12": "一级标题：swanlab logout\n二级标题：无\n内容：\n```bash\nswanlab logout\n```\n\n在编程环境上退出账号。",
    "13": "一级标题：其他CLI命令\n二级标题：无\n内容：\n- `swanlab -v`：查看SwanLab库版本\n- `swanlab --help`：API帮助",
    "14": "一级标题：swanlab sync\n二级标题：无\n内容：\n```bash\nswanlab sync [options] [logdir]\n```\n\n| 选项 | 描述 |\n| --- | --- |\n| `-k`, `--api-key` | 用于身份验证的API密钥。如果未指定，将使用环境中的默认API密钥。如果指定，将使用此API密钥登录但不会保存密钥。|\n| `-h`, `--host` | 同步日志的主机地址。如果未指定，将使用默认主机(`https://swanlab.cn`)。|\n| `-w`, `--workspace` | 同步日志的工作空间。如果未指定，将使用默认工作空间。|\n| `-p`, `--project` | 同步日志的项目。如果未指定，将使用默认项目。|\n| `-i`, `--id` | 同步日志的实验ID。仅当路径为单个目录时可用。|",
    "15": "一级标题：swanlab sync\n二级标题：介绍\n内容：\n将本地日志，同步上传到SwanLab云端/私有化部署端。",
    "16": "一级标题：swanlab sync\n二级标题：版本对照\n内容：\n> 版本对照仅适用于`swanlab sync`命令\n\n| swanlab库版本 | 特性 | 支持的日志文件 |\n| --- | --- | --- |\n| >=0.6.8 | 支持同步训练异常终端的日志文件；支持`id`参数 | 由`>=0.6.8`版本的swanlab库产生 |\n| <0.6.8 | - | 由`<0.6.8`版本的swanlab库产生 |",
    "17": "一级标题：swanlab sync\n二级标题：命令行示例\n内容：\n找到你需要上传到云端的日志文件目录（默认是`swanlog`下的以`run-`开头的目录），然后执行命令：\n\n```bash\nswanlab sync ./swanlog/run-xxx\n```\n\n::: info\n默认同步到的项目的是日志文件中记录的`project`，即跑该实验时设置的`project`。  \n如果想要同步到其他项目，可以使用`-p`选项指定项目。\n:::\n\n看到下面的打印信息，则表示同步成功：\n\n![swanlab sync](./cli-swanlab-sync/console.png)\n\n完成sync操作后，项目上会多出一个新的实验。",
    "18": "一级标题：swanlab sync\n二级标题：Python代码示例\n内容：\n```python\nimport swanlab\n\nswanlab.login(api_key=\"你的API Key\")\n\nswanlab.sync(\n    dir_path=\"./swanlog/run-xxx\",\n    workspace=\"swanlab\",\n    project_name=\"sync_test\",\n)\n```",
    "19": "一级标题：swanlab sync\n二级标题：批量上传\n内容：\n```bash\nswanlab sync ./swanlog/run-*\n```",
    "20": "一级标题：swanlab sync\n二级标题：resume式同步\n内容：\n如果你不希望创建1个新实验，而是在原本的实验上同步（会自行比对数据，增加差异的部分），可以使用`--id`参数：\n\n```bash\nswanlab sync ./swanlog/run-xxx --id <实验ID>\n```\n\n实验ID获取方式见：[恢复实验/断点续训](/guide_cloud/experiment_track/resume-experiment.md)",
    "21": "一级标题：swanlab watch\n二级标题：无\n内容：\n``` bash\nswanlab watch [OPTIONS]\n```\n\n| 选项 | 描述 | 例子 |\n| --- | --- | --- |\n| `-p`, `--port` | 设置实验看板Web服务运行的端口，默认为**5092**。 | `swanlab watch -p 8080`：将实验看板Web服务设置为8080端口 |\n| `-h`, `--host` | 设置实验看板Web服务运行的IP地址，默认为**127.0.0.1**。 | `swanlab watch -h 0.0.0.0`：将实验看板Web服务的IP地址设置为0.0.0.0 |\n| `-l`, `--logdir` | 设置实验看板Web服务读取的日志文件路径，默认为`swanlog`。 | `swanlab watch --logdir ./logs`：将当前目录下的logs文件夹设置为日志文件读取路径 |\n| `--help` | 查看终端帮助信息。 | `swanlab watch --help` |",
    "22": "一级标题：swanlab watch\n二级标题：介绍\n内容：\n本地启动SwanLab[离线看板](/zh/guide_cloud/self_host/offline-board.md)。  \n在创建SwanLab实验时（并设置mode=\"local\"），会在本地目录下创建一个日志文件夹（默认名称为`swanlog`），使用`swanlab watch`可以本地离线打开实验看板，查看指标图表和配置。",
    "23": "一级标题：swanlab watch\n二级标题：使用案例\n内容：\n### 打开SwanLab离线看板\n\n首先，我们找到日志文件夹（默认名称为`swanlog`），然后在命令行执行下面的命令：\n\n```bash\nswanlab watch -l [logfile_path]\n```\n\n其中`logfile_path`是日志文件夹的路径，可以是绝对路径或相对路径。如果你的日志文件夹名称是默认的`swanlog`，那么也可以直接用`swanlab watch`启动而无需`-l`选项。\n\n执行命令后，会看到下面的输出：\n```bash{6}\nswanlab watch -l [logfile_path]\n\n*swanlab: Try to explore the swanlab experiment logs in: [logfile_path]\n*swanlab: SwanLab Experiment Dashboard ready in 465ms\n\n        ➜  Local:   http://127.0.0.1:5092\n```\n\n访问提供的URL，即可访问SwanLab离线看板。\n\n### 设置IP和端口号\n\n我们可以通过`-h`参数设置IP，`-p`参数设置端口号。  \n比如我们希望能够在本地访问云服务器上的离线看板，那么需要在云服务器上开启实验看板时，设置IP为0.0.0.0：\n\n```bash\nswanlab watch -h 0.0.0.0\n```\n\n如果需要设置端口的话：\n```bash\nswanlab watch -h 0.0.0.0 -p 8080\n```",
    "24": "一级标题：环境变量\n二级标题：无\n内容：\n[⚙️完整环境变量1 -> Github](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/env.py)、[⚙️完整环境变量2 -> Github](https://github.com/SwanHubX/SwanLab-Toolkit/blob/main/swankit/env.py)",
    "25": "一级标题：环境变量\n二级标题：全局配置\n内容：\n| 环境变量 | 描述 | 默认值 |\n| --- | --- | --- |\n| `SWANLAB_SAVE_DIR` | SwanLab 全局文件夹保存的路径 | 用户主目录下的 `.swanlab` 文件夹 |\n| `SWANLAB_LOG_DIR` | SwanLab 解析日志文件保存的路径 | 当前运行目录的 `swanlog` 文件夹 |\n| `SWANLAB_MODE` | SwanLab 的解析模式，涉及操作员注册的回调。目前有三种模式：`local`、`cloud`、`disabled`。**注意：大小写敏感** | `cloud` |",
    "26": "一级标题：环境变量\n二级标题：服务配置\n内容：\n| 环境变量 | 描述 | \n| --- | --- |\n| `SWANLAB_BOARD_PORT` | CLI 离线看板 `swanboard` 服务的端口 |\n| `SWANLAB_BOARD_HOST` | CLI 离线看板 `swanboard` 服务的地址 |\n| `SWANLAB_WEB_HOST` | SwanLab 云端环境的 Web 地址 |\n| `SWANLAB_API_HOST` | SwanLab 云端环境的 API 地址 |",
    "27": "一级标题：环境变量\n二级标题：实验配置\n内容：\n| 环境变量 | 描述 |\n| --- | --- |\n| `SWANLAB_PROJ_NAME` | 项目名称，效果等价于 `swanlab.init(project=\"...\")` |\n| `SWANLAB_WORKSPACE` | 工作空间名称，效果等价于 `swanlab.init(workspace=\"...\")` |\n| `SWANLAB_EXP_NAME` | 实验名称，效果等价于 `swanlab.init(experiment_name=\"...\")` |\n| `SWANLAB_RUN_ID` | 实验运行ID，效果等价于 `swanlab.init(id=\"...\")` |\n| `SWANLAB_RESUME` | 是否断点续训，效果等价于 `swanlab.init(resume=...)`，可选值为 `must`、`allow`、`never` |",
    "28": "一级标题：环境变量\n二级标题：登录认证\n内容：\n| 环境变量 | 描述 |\n| --- | --- | \n| `SWANLAB_API_KEY` | 云端 API Key。登录时会首先查找此环境变量，如果不存在，判断用户是否已登录，未登录则进入登录流程。<br>- 如果 `login` 接口传入字符串，此环境变量无效<br>- 如果用户已登录，此环境变量的优先级高于本地存储的登录信息 |",
    "29": "一级标题：环境变量\n二级标题：其他\n内容：\n| 环境变量 | 描述 |\n| --- | --- |\n| `SWANLAB_WEBHOOK` | Webhook 地址。<br> SwanLab 初始化完毕时，如果此环境变量存在，会调用此地址发送消息 |",
    "30": "一级标题：swanlab.Audio\n二级标题：无\n内容：\n[Github源代码](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/modules/audio.py)\n\n```python\nAudio(\n    data_or_path: Union[str, np.ndarray],\n    sample_rate: int = 44100,\n    caption: str = None,\n) -> None\n```\n\n| 参数          | 描述                                                                                                     |\n|-------------|--------------------------------------------------------------------------------------------------------|\n| data_or_path | (Union[str, np.ndarray]) 接收音频文件路径、numpy数组。Audio类将判断接收的数据类型做相应的转换。 |\n| sample_rate | (int) 音频的采样率，默认为44100。                                             |\n| caption     | (str) 音频的标签。用于在实验看板中展示音频时进行标记。                                                      |",
    "31": "一级标题：swanlab.Audio\n二级标题：介绍\n内容：\n对各种类型的音频数据做转换，以被`swanlab.log()`记录。\n\n![](/assets/media-audio-1.jpg)\n\n### 从numpy array创建\n\n记录单个音频：\n\n```python\nimport numpy as np\nimport swanlab\n\nrun = swanlab.init()\n\n# 创建一个numpy array类型的音频\nwhite_noise = np.random.randn(2, 100000)\n# 传入swanlab.Audio，设置采样率\naudio = swanlab.Audio(white_noise, caption=\"white_noise\")\n\nrun.log({\"examples\": audio})\n```\n\n记录多个音频：\n\n```python\nimport numpy as np\nimport swanlab\n\nrun = swanlab.init()\n\n# 创建一个列表\nexamples = []\nfor i in range(3):\n    white_noise = np.random.randn(100000)\n    audio = swanlab.Audio(white_noise, caption=\"audio_{i}\")\n    # 列表中添加swanlab.Audio类型对象\n    examples.append(audio)\n\nrun.log({\"examples\": examples})\n```\n\n### 从文件路径创建\n\n```python\nimport swanlab\n\nrun = swanlab.init()\naudio = swanlab.Audio(\"path/to/file\")\n\nrun.log({\"examples\": audio})\n```",
    "32": "一级标题：swanlab.Image\n二级标题：无\n内容：\n[Github源代码](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/modules/image.py)\n\n```python\nImage(\n    data_or_path: Union[str, np.ndarray, PILImage.Image],\n    mode: str = \"RGB\",\n    caption: str = None,\n    file_type: str = None,\n    size: Union[int, list, tuple] = None,\n) -> None\n```\n\n| 参数        | 描述                                                                                                                                                                   |\n|-----------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| data_or_path | (Union[str, np.ndarray, PILImage.Image]) 接收图像文件路径、numpy数组、或者PIL图像。Image类将判断接收的数据类型做相应的转换。                                      |\n| mode      | (str) 图像的 PIL 模式。最常见的是 \"L\"、\"RGB\"、\"RGBA\"。完整解释请参阅：[Pillow mode](https://pillow.readthedocs.io/en/stable/handbook/concepts.html#modes)                         |\n| caption   | (str) 图像的标签。用于在实验看板中展示图像时进行标记。                                                                                                                 |\n| file_type | (str) 设置图片的格式，可选['png', 'jpg', 'jpeg', 'bmp']，默认为'png'                                                                                                   |\n| size      | (Union[int, list, tuple]) 设置图像的尺寸，默认保持原图尺寸。如果size设置为int类型，如512，将根据最长边不超过512的标准做图像缩放, [size更多用法](#对传入图像做resize)|",
    "33": "一级标题：swanlab.Image\n二级标题：介绍\n内容：\n对各种类型的图像数据做转换，以被`swanlab.log()`记录。\n\n![](/assets/media-image-1.jpg)\n\n### 从numpy array创建\n\n记录单张图像：\n\n```python\nimport numpy as np\nimport swanlab\n\nrun = swanlab.init()\n\n# 1. 创建一个numpy array\nrandom_image = np.random.randint(low=0, high=256, size=(100, 100, 3), dtype=np.uint8)\n# 2. 传入swanlab.Image\nimage = swanlab.Image(random_image, caption=\"random image\")\n\nrun.log({\"examples\": image})\n```\n\n记录多张图像：\n\n```python\nimport numpy as np\nimport swanlab\n\nrun = swanlab.init()\n\n# 创建一个列表\nexamples = []\nfor i in range(3):\n    random_image = np.random.randint(low=0, high=256, size=(100, 100, 3), dtype=np.uint8)\n    image = swanlab.Image(random_image, caption=\"random image\")\n    # 列表中添加swanlab.Image类型对象\n    examples.append(image)\n\n# 记录图列\nrun.log({\"examples\": examples})\n```\n\n### 从PyTorch Tensor创建\n\n`swanlab.Image`支持传入尺寸为[B, C, H, W]与[C, H, W]的Tensor。\n\n```python\nimport torch\nimport swanlab\n\nrun = swanlab.init()\n···\nfor batch, ground_truth in train_dataloader():\n    # 假设batch是尺寸为[16, 3, 256, 256]的tensor\n    tensors = swanlab.Image(batch)\n    run.log({\"examples\": tensors})\n```\n\n\n### 从PIL Image创建\n\n```python\nimport numpy as np\nfrom PIL import Image\nimport swanlab\n\nrun = swanlab.init()\n\n# 创建一个列表\nexamples = []\nfor i in range(3):\n    random_image = np.random.randint(low=0, high=256, size=(100, 100, 3), dtype=np.uint8)\n    pil_image = Image.fromarray(random_image)\n    image = swanlab.Image(pil_image, caption=\"random image\")\n    examples.append(image)\n\nrun.log({\"examples\": examples})\n```\n\n### 从文件路径创建\n\n```python\nimport swanlab\n\nrun = swanlab.init()\nimage = swanlab.Image(\"path/to/file\", caption=\"random image\")\n\nrun.log({\"examples\": image})\n```\n\n`swanlab.Image`在默认情况下，是以`png`的格式做图像转换与存储。\n\n如果想要用`jpg`格式：\n\n```python{3}\nimage = swanlab.Image(\"path/to/file\",\n                      caption=\"random image\",\n                      file_type=\"jpg\")\n```\n\n### 对传入图像做Resize\n\n在默认情况，`swanlab.Image`不对图像做任何尺寸缩放。  \n\n如果需要放缩图像，我们可以通过设置`size`参数，来调节图像尺寸。\n\n放缩规则为：  \n\n1. 默认: 不对图像做任何缩放\n\n2. `size`为int类型: 如果最长边超过`size`, 则将最长边设为`size`, 另一边等比例缩放; 否则不缩放\n\n3. `size`为list/tuple类型: \n\n    - (int, int): 将图像缩放到宽为size[0], 高为size[1]\n    - (int, None): 将图像缩放到宽为size[0], 高等比例缩放\n    - (None, int): 将缩放缩放到高为size[1], 宽等比例缩放\n\n```python\nprint(im_array.shape)\n# [1024, 512, 3]\n\nim1 = swanlab.Image(im_array, size=512)\n# [512, 256, 3]\n\nim2 = swanlab.Image(im_array, size=(512, 512))\n# [512, 512, 3]\n\nim3 = swanlab.Image(im_array, size=(None, 1024))\n# [2048, 1024, 3]\n\nim4 = swanlab.Image(im_array, size=(256, None))\n# [256, 128, 3]\n```\n\n### 记录Matplotlib图表\n\n```python\nimport swanlab\nimport matplotlib.pyplot as plt\n\n# 定义横纵坐标的数据\nx = [1, 2, 3, 4, 5]\ny = [2, 3, 5, 7, 11]\n\n# plt创建折线图\nplt.plot(x, y)\n\n# 添加标题和标签\nplt.title(\"Examples\")\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")\n\nswanlab.init()\n\n# 记录plt\nswanlab.log({\"example\": swanlab.Image(plt)})\n```",
    "34": "一级标题：swanlab.Text\n二级标题：无\n内容：\n[Github源代码](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/modules/text.py)\n\n```python\nText(\n    data: Union[str],\n    caption: str = None,\n) -> None\n```\n\n| 参数    | 描述                                                              |\n|-------|-----------------------------------------------------------------|\n| data  | (Union[str]) 接收字符串。                                      |\n| caption | (str) 文本的标签。用于在实验看板中对data进行标记。                     |",
    "35": "一级标题：swanlab.Text\n二级标题：介绍\n内容：\n对文本数据做转换，以被`swanlab.log()`记录。\n\n![](./py-text/show.png)\n\n### 记录字符串文本\n\n记录单个字符串文本：\n\n```python{4}\nimport swanlab\n\nswanlab.init()\ntext = swanlab.Text(\"an awesome text.\")\nswanlab.log({\"examples\": text})\n```\n\n记录多个字符串文本：\n\n```python\nimport swanlab\n\nswanlab.init()\n\nexamples = []\nfor i in range(3):\n    text = swanlab.Text(\"an awesome text.\")\n    examples.append(text)\n\nswanlab.log({\"examples\": examples})\n```",
    "36": "一级标题：swanlab.converter\n二级标题：无\n内容：\n将其他日志工具的内容转换为SwanLab项目的API。\n\n- [swanlab.converter.TFBConverter](/guide_cloud/integration/integration-tensorboard)\n- [swanlab.converter.WandbConverter](/guide_cloud/integration/integration-wandb)\n- [swanlab.converter.MLFlowConverter](/guide_cloud/integration/integration-mlflow)",
    "37": "一级标题：swanlab.echarts\n二级标题：无\n内容：\n<!--@include: @zh/shared/custom-charts.md-->\n\n<!--@include: @zh/shared/custom-charts-3d.md-->",
    "38": "一级标题：swanlab.init\n二级标题：无\n内容：\n```python\ninit(\n    project: str = None,\n    workspace: str = None,\n    experiment_name: str = None,\n    description: str = None,\n    tags: List[str] = None,\n    config: Union[dict, str] = None,\n    logdir: str = None,\n    mode: str = \"cloud\",\n    load: str = None,\n    public: bool = None,\n    callbacks: list = None,\n    settings: Settings = None,\n    id: str = None,\n    resume: Union[Literal['must', 'allow', 'never'], bool] = None,\n    reinit: bool = None,\n    **kwargs,\n)\n```\n\n| 参数         | 描述 |\n|-------------|------|\n| project |(str)项目名，如果不指定则取运行目录的名称。|\n| workspace |(str)工作空间，默认将实验同步到你的个人空间下，如果要上传到组织，则填写组织的username。|\n| experiment_name | (str) 实验名称, 如果不指定则取\"swan-1\"这样的`动物名+序号`作为实验名。 |\n| tags       | (list) 实验标签。可以传入多个字符串组成的列表，标签会显示在实验顶部的标签栏。|\n| description   | (str) 实验描述, 如果不指定默认为None。                                   |\n| config       | (dict, str) 实验配置，在此处可以记录一些实验的超参数等信息。支持传入配置文件路径，支持yaml和json文件。                   |\n| logdir       | (str) 离线看板日志文件存储路径，默认为`swanlog `。                                 |\n| mode       | (str) 设置swanlab实验创建的模式，可选\"cloud\"、\"local\"、\"offline\"、\"disabled\"，默认设置为\"cloud\"。<br>`cloud`：将实验上传到云端。（公有云和私有化部署）<br>`offline`：仅将实验数据保存到本地。<br>`local`：不上传到云端，但会记录实验数据和一些可被`swanlab watch`打开的数据到本地。<br>`disabled`：不上传也不记录。|\n| load       | (str) 加载的配置文件路径，支持yaml和json文件。|\n| public       | (bool) 设置使用代码直接创建SwanLab项目的可见性，默认为False即私有。|\n| callbacks       | (list) 设置实验回调函数，支持`swankit.callback.SwanKitCallback`的子类。|\n| name       | (str) 与experiment_name效果一致，优先级低于experiment_name。|\n| notes       | (str) 与description效果一致，优先级低于description。|\n| settings       | (dict) 实验配置。支持传入1个`swanlab.Settings`对象。|\n| id       | (str) 上次实验的运行ID，用于恢复上次实验。ID必须为21位字符串。|\n| resume       | (str) 断点续训模式，可选True、False、\"must\"、\"allow\"、\"never\"，默认取None。<br>`True`： 效果同`resume=\"allow\"`。<br>`False`：效果同`resume=\"never\"`。<br>`must`：你必须传递 `id` 参数，并且实验必须存在。<br>`allow`：如果存在实验，则会resume该实验，否则将创建新的实验。<br>`never`：你不能传递 `id` 参数，将会创建一个新的实验。(即不开启resume的效果)|\n| reinit       | (bool) 是否重新创建实验，如果为True，则每次调用`swanlab.init()`时，会把上一次实验`finish`掉；默认取None。|",
    "39": "一级标题：swanlab.init\n二级标题：介绍\n内容：\n- 在机器学习训练流程中，我们可以将`swandb.init()`添加到训练脚本和测试脚本的开头，SwanLab将跟踪机器学习流程的每个环节。\n\n- `swanlab.init()`会生成一个新的后台进程来将数据记录到实验中，默认情况下，它还会将数据同步到swanlab.cn，以便你可以在线实时看到可视化结果。\n\n- 在使用`swanlab.log()`记录数据之前，需要先调用`swanlab.init()`：\n\n```python\nimport swanlab\n\nswanlab.init()\nswanlab.log({\"loss\": 0.1846})\n```\n\n- 调用`swanlab.init()`会返回一个`SwanLabRun`类型的对象，同样可以执行`log`操作：\n\n```python\nimport swanlab\n\nrun = swanlab.init()\nrun.log({\"loss\": 0.1846})\n```\n\n- 在脚本运行结束时，我们将自动调用`swanlab.finish`来结束SwanLab实验。但是，如果从子进程调用`swanlab.init()`，如在jupyter notebook中，则必须在子进程结束时显式调用`swanlab.finish`。\n\n```python\nimport swanlab\n\nswanlab.init()\nswanlab.finish()\n```",
    "40": "一级标题：swanlab.init\n二级标题：更多用法\n内容：\n### 设置项目、实验名、描述\n\n```python\nswanlab.init(\n    project=\"cats-detection\",\n    experiment_name=\"YoloX-baseline\",\n    description=\"YoloX检测模型的基线实验，主要用于后续对比。\",\n)\n```\n\n### 设置标签\n\n```python\nswanlab.init(\n    tags=[\"yolo\", \"detection\", \"baseline\"]\n)\n```\n\n### 设置日志文件保存位置\n\n下面的代码展示了如何将日志文件保存到自定义的目录下：\n\n```python\nswanlab.init(\n    logdir=\"path/to/my_custom_dir\",\n)\n```\n\n### 将实验相关的元数据添加到实验配置中\n\n```python\nswanlab.init(\n    config={\n        \"learning-rate\": 1e-4,\n        \"model\": \"CNN\",\n    }\n)\n\n```\n\n### 上传到组织\n\n```python\nswanlab.init(\n    workspace=\"[组织的username]\"\n)\n```\n\n### 插件\n\n关于插件的更多信息，请参考[插件](/plugin/plugin-index.md)。\n\n```python\nfrom swanlab.plugin.notification import EmailCallback\n\nemail_callback = EmailCallback(...)\n\nswanlab.init(\n    callbacks=[email_callback]\n)\n```\n\n### 断点续训\n\n断点续训的意思是，如果你之前有一个状态为`完成`或`中断`的实验，需要补一些实验数据，那么你可以通过`resume`和`id`参数来恢复这个实验。\n\n```python\nswanlab.init(\n    resume=True,\n    id=\"14pk4qbyav4toobziszli\",  # id必须为21位字符串\n)\n```\n\n实验id可以在实验的「环境」选项卡或URL中找到，必须为1个21位字符串。\n\n\n:::tip resume使用场景\n\n1. 之前的训练进程断了，基于checkpoint继续训练时，希望实验图表能和之前的swanlab实验续上，而非创建1个新swanlab实验\n2. 训练和评估分为了两个进程，但希望评估和训练记录在同一个swanlab实验中\n3. config中有一些参数填写有误，希望更新config参数\n\n:::\n\n:::warning ⚠️注意\n\n1. 由项目克隆产生的实验，不能被resume\n\n:::\n\n\n断点续训可以选择三种模式：\n\n1. `allow`：如果项目下存在`id`对应的实验，则会resume该实验，否则将创建新的实验。\n2. `must`：如果项目下存在`id`对应的实验，则会resume该实验，否则将报错\n3. `never`：不能传递 `id` 参数，将会创建一个新的实验。(即不开启resume的效果)\n\n::: info\n`resume=True` 效果同 `resume=\"allow\"`。<br>\n`resume=False` 效果同 `resume=\"never\"`。\n:::\n\n测试代码：\n\n```python\nimport swanlab\n\nrun = swanlab.init()\nswanlab.log({\"loss\": 2, \"acc\":0.4})\nrun.finish()\n\nrun = swanlab.init(resume=True, id=run.id)\nswanlab.log({\"loss\": 0.2, \"acc\": 0.9})\n```",
    "41": "一级标题：swanlab.init\n二级标题：过期参数\n内容：\n- `cloud`：在v0.3.4被`mode`参数取代。参数仍然可用，且会覆盖掉`mode`的设置。",
    "42": "一级标题：swanlab.integration\n二级标题：无\n内容：\n[源代码](https://github.com/SwanHubX/SwanLab/tree/main/swanlab/integration)\n\nSwanLab与外部项目的集成API。\n\n- [swanlab.integration.accelerate](/guide_cloud/integration/integration-huggingface-accelerate.md)\n- [swanlab.integration.fastai](/guide_cloud/integration/integration-fastai.md)\n- [swanlab.integration.keras](/guide_cloud/integration/integration-keras.md)\n- [swanlab.integration.lightgbm](/guide_cloud/integration/integration-lightgbm.md)\n- [swanlab.integration.mmengine](/guide_cloud/integration/integration-mmengine.md)\n- [swanlab.integration.pytorch_lightning](/guide_cloud/integration/integration-pytorch-lightning.md)\n- [swanlab.integration.sb3](/guide_cloud/integration/integration-sb3.md)\n- [swanlab.integration.torchtune](/guide_cloud/integration/integration-pytorch-torchtune.md)\n- [swanlab.integration.transformers](/guide_cloud/integration/integration-huggingface-transformers.md)\n- [swanlab.integration.ultralytics](/guide_cloud/integration/integration-ultralytics.md)\n- [swanlab.integration.xgboost](/guide_cloud/integration/integration-xgboost.md)",
    "43": "一级标题：log\n二级标题：无\n内容：\n[Github源代码](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/sdk.py)\n\n```python\nlog(\n    data: Dict[str, DataType],\n    step: int = None,\n    print_to_console: bool = False,\n)\n```\n\n| 参数   | 描述                                       |\n|--------|------------------------------------------|\n| data   | (Dict[str, DataType]) 必须。传入一个键值对字典，key为指标名，value为指标值。value支持int、float、可被float()转换的类型、或任何`BaseType`类型。 |\n| step   | (int) 可选，该参数设置了data的步数。如不设置step，则将以0开始，后续每1次step累加1。 |\n| print_to_console | (bool) 可选，默认值为False。当设置为True时，会将data的key和value以字典的形式打印到终端。 |",
    "44": "一级标题：log\n二级标题：介绍\n内容：\n`swanlab.log`是指标记录的核心API，使用它记录实验中的数据，例如标量、图像、音频和文本。  \n\n最基本的用法是如下面代码所示，这将会将准确率与损失值记录到实验中，生成可视化图表并更新这些指标的汇总值（summary）。：\n\n```python\nswanlab.log({\"acc\": 0.9, \"loss\":0.1462})\n```\n\n除了标量以外，`swanlab.log`支持记录多媒体数据，包括图像、音频、文本等，并在UI上有很好的显示效果。",
    "45": "一级标题：log\n二级标题：打印传入的字典\n内容：\n`swanlab.log`支持打印传入的`data`的`key`和`value`到终端，默认情况下不打印。要开启打印的话，需要设置`print_to_console=True`。\n\n```python\nswanlab.log({\"acc\": 0.9, \"loss\":0.1462}, print_to_console=True)\n```\n\n当然，你也可以用这种方式打印：\n\n```python\nprint(swanlab.log({\"acc\": 0.9, \"loss\":0.1462}))\n```",
    "46": "一级标题：log\n二级标题：更多用法\n内容：\n- 记录[图像](/api/py-Image.md)\n- 记录[音频](/api/py-Audio.md)\n- 记录[文本](/api/py-Text.md)",
    "47": "一级标题：swanlab.login\n二级标题：无\n内容：\n``` bash\nlogin(\n    api_key: str = None,\n    host: str = None,\n    web_host: str = None,\n    save: bool = False\n):\n```\n\n| 参数 | 描述 |\n| --- | --- |\n| `api_key` | (str) 身份验证密钥，如果未提供，密钥将从密钥文件中读取。|\n| `host` | (str) SwanLab服务所在的API主机，如果未提供，将使用默认主机（即云端版）|\n| `web_host` | (str) SwanLab服务所在的Web主机，如果未提供，将使用默认主机（即云端版）|\n| `save` | (bool) 是否将API密钥保存到密钥文件中，默认值为False。|",
    "48": "一级标题：swanlab.login\n二级标题：介绍\n内容：\n在Python代码中登录SwanLab账号，以将实验上传到指定的云端服务器。API Key从你的SwanLab「设置」-「常规」页面中获取。",
    "49": "一级标题：swanlab.login\n二级标题：登录到公有云\n内容：\n```python\nimport swanlab\n\nswanlab.login(api_key='your-api-key', save=True)\n```\n\n默认将登录到`swanlab.cn`，即SwanLab公有云服务。\n\n如果需要登录到其他主机，可以指定`host`参数，如`http://localhost:8000`。\n\n将`save`参数设置为`True`，会将登录凭证保存到本地（会覆盖之前保存的凭证），无需再次通过`swanlab.login`或`swanlab login`登录。\n\n**如果你在公共机器上使用，请将`save`参数设置为`False`**，这样不会泄露你的API Key，也避免其他人不小心上传数据到你的空间。",
    "50": "一级标题：swanlab.login\n二级标题：登录到私有化服务\n内容：\n```python\nswanlab.login(api_key='your-api-key', host='your-private-host')\n```",
    "51": "一级标题：swanlab.Molecule\n二级标题：无\n内容：\n[源代码](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/modules/object3d/molecule.py)\n\n| 参数        | 描述       |\n|-----------|------------------------------------------------------------------------------------------------|\n| pdb_data | (str) 接收的PDB数据（字符串形式）                                 |      \n| caption   | (str) 分子对象的标签。用于在实验看板中展示分子对象时进行标记。                |",
    "52": "一级标题：swanlab.Molecule\n二级标题：简介\n内容：\n对各种类型的生物化学分子做转换，以被`swanlab.log()`记录。\n\n![molecule gif](/assets/molecule.gif)",
    "53": "一级标题：swanlab.Molecule\n二级标题：从RDKit Mol对象创建\n内容：\n```python\nfrom rdkit import Chem\nimport swanlab\n\nmol = Chem.MolFromSmiles(\"CCO\")\nmolecule = swanlab.Molecule.from_mol(mol, caption=\"Ethanol\")\n\nswanlab.init(project=\"molecule_demo\")\nswanlab.log({\"molecule\": molecule})\n```",
    "54": "一级标题：swanlab.Molecule\n二级标题：从PDB文件创建\n内容：\n```python\nimport swanlab\n\nmolecule = swanlab.Molecule.from_pdb(\"path/to/your/pdb/file.pdb\")\n\nswanlab.init(project=\"molecule_demo\")\nswanlab.log({\"molecule\": molecule})\n```",
    "55": "一级标题：swanlab.Molecule\n二级标题：从SDF文件创建\n内容：\n```python\nimport swanlab\n\nmolecule = swanlab.Molecule.from_sdf(\"path/to/your/sdf/file.sdf\")\n\nswanlab.init(project=\"molecule_demo\")\nswanlab.log({\"molecule\": molecule})\n```",
    "56": "一级标题：swanlab.Molecule\n二级标题：从SMILES字符串创建\n内容：\n```python\nimport swanlab\n\nmolecule = swanlab.Molecule.from_smiles(\"CCO\")\n\nswanlab.init(project=\"molecule_demo\")\nswanlab.log({\"molecule\": molecule})\n```",
    "57": "一级标题：swanlab.Molecule\n二级标题：从MOL文件创建\n内容：\n```python\nimport swanlab\n\nmolecule = swanlab.Molecule.from_mol(\"path/to/your/mol/file.mol\")\n\nswanlab.init(project=\"molecule_demo\")\nswanlab.log({\"molecule\": molecule})\n```",
    "58": "一级标题：swanlab.Object3D\n二级标题：无\n内容：\n[Github源代码](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/modules/object3d/object3d.py)\n\n| 参数        | 描述   |\n|-----------|---------------|\n| data | (Union[np.ndarray, str, Path]) 接收点云文件路径、numpy数组。Object3D类将判断接收的数据类型做相应的转换。                                      |              |\n| caption   | (str) 3D对象的标签。用于在实验看板中展示3D对象时进行标记。                                                                                                                 |",
    "59": "一级标题：swanlab.Object3D\n二级标题：介绍\n内容：\n对各种类型的点云数据做转换，以被`swanlab.log()`记录。\n\n![](./py-object3d/demo.png)\n\n### 从文件/字典创建\n\n::: warning 示例文件\ndata.swanlab.pts.json：[Google Drive下载](https://drive.google.com/file/d/1mFill-BXw3cirPHwIHndb1wNX4pWvSXb/view)\n:::\n\n文件的格式为`json`，内容格式如下：\n\n```json\n{\n    \"points\": [\n        [x1, y1, z1, r1, g1, b1],\n        [x2, y2, z2, r2, g2, b2],\n        ...\n    ],\n    // （可选）检测框，用于点云检测等任务，会框住对应位置\n    \"boxes\": [\n        {\n            \"color\": [r, g, b],\n            \"corners\": [[x1,y1,z1], ..., [x8,y8,z8]],\n            // （可选）检测框的标签文本，会在视图中显示\n            \"label\": \"class_name\",\n            // （可选）置信度，会在视图中显示\n            \"score\": 0.95,\n        },\n        ...\n    ]\n}\n```\n\n**json文件参数详细解释：**\n\n* **`points`**：\n    * 这是一个数组，用于存储3D点云数据。\n    * 每个元素都是一个包含6个数值的数组 `[x, y, z, r, g, b]`，分别代表：\n        * `x`, `y`, `z`：点的三维坐标。\n        * `r`, `g`, `b`：点的颜色，分别代表红、绿、蓝三个通道的数值，通常取值范围为0-255。\n\n* **`boxes`**（可选）：\n    * 这是一个数组，用于存储3D检测框数据。\n    * 每个元素都是一个对象，代表一个检测框，包含以下字段：\n        * **`color`**：检测框的颜色，`[r, g, b]` 数组，代表红、绿、蓝三个通道的数值。\n        * **`corners`**：检测框的八个顶点坐标，`[[x1, y1, z1], ..., [x8, y8, z8]]` 数组，每个元素是一个三维坐标 `[x, y, z]`。\n        * **`label`**（可选）：检测框的标签文本，字符串类型，用于在视图中显示检测框的类别。\n        * **`score`**（可选）：检测框的置信度，数值类型，通常取值范围为0-1，用于表示检测框的可靠程度。\n\n---\n\n使用SwanLab从`json`文件中记录3D点云数据：\n\n::: code-group\n\n```python [Object3D]\nimport swanlab\n\nswanlab.init()\n\nobj = swanlab.Object3D(\"data.swanlab.pts.json\", caption=\"3d_point_cloud\")\nswanlab.log({\"examples\": obj})\n```\n\n```python [Object3D.from_point_data]\nimport swanlab\n\nswanlab.init()\n\nwith open(\"data.swanlab.pts.json\", \"r\") as f:\n    cloud_point = json.load(f)\n\nobj = swanlab.Object3D.from_point_data(\n    points=cloud_point[\"points\"],\n    boxes=cloud_point[\"boxes\"],\n    caption=\"3d_point_cloud\"\n)\n\nswanlab.log({\"examples\": obj})\n```\n:::\n\n\n<video controls src=\"./py-object3d/video.mp4\"></video>\n\n<br>\n\n### 从numpy数组创建\n\n::: code-group\n\n```python [从坐标创建]\nimport numpy as np\n\n# Example 1: Create point cloud from coordinates\npoints_xyz = np.array([\n    [0, 0, 0],  # Point1: x=0, y=0, z=0\n    [1, 1, 1],  # Point2: x=1, y=1, z=1\n    [2, 0, 1]   # Point3: x=2, y=0, z=1\n])\n\ncloud_xyz = swanlab.Object3D(points_xyz, caption=\"Basic XYZ Points\")\nswanlab.log({\"examples\": cloud_xyz})\n```\n\n```python [从坐标和类别创建]\nimport numpy as np\n\n# Example 2: Create point cloud with categories\npoints_xyzc = np.array([\n    [0, 0, 0, 0],  # Point1: xyz + category 0\n    [1, 1, 1, 1],  # Point2: xyz + category 1\n    [2, 0, 1, 2]   # Point3: xyz + category 2\n])\n\ncloud_xyzc = swanlab.Object3D(points_xyzc, caption=\"Points with Categories\")\nswanlab.log({\"examples\": cloud_xyzc})\n```\n\n```python [从坐标和RGB创建]\nimport numpy as np\n\n# Example 3: Create point cloud with RGB colors\npoints_xyzrgb = np.array([\n    [0, 0, 0, 255, 0, 0],    # Point1: xyz + red\n    [1, 1, 1, 0, 255, 0],    # Point2: xyz + green\n    [2, 0, 1, 0, 0, 255]     # Point3: xyz + blue\n])\n\ncloud_xyzrgb = swanlab.Object3D(points_xyzrgb, caption=\"Colored Points\")\nswanlab.log({\"examples\": cloud_xyzrgb})\n```\n:::\n\n### 单步记录多个点云\n\n```python\nimport swanlab\n\n...\n\ncloud1 = swanlab.Object3D(points1, caption=\"cloud1\")\ncloud2 = swanlab.Object3D(points2, caption=\"cloud2\")\ncloud3 = swanlab.Object3D(points3, caption=\"cloud3\")\n\n...\n\nswanlab.log({\"examples\": [cloud1, cloud2, cloud3, ...]})\n```",
    "60": "一级标题：swanlab.OpenApi\n二级标题：无\n内容：\n基于 SwanLab 云端功能, 在 SDK 端提供访问 **开放 API（OpenAPI）** 的能力, 允许用户通过编程方式在本地环境中操作云端 **实验/项目/工作空间** 资源。\n\n![](./py-openapi/logo.jpg)\n\n通过开放 API 的形式, 用户可以在本地编程环境中:\n\n- 获取实验数据、个人信息、工作空间信息、项目列表等\n- 进行实验的自动管理（如查询、组织、元数据编辑等）\n- 更方便地与其他工具集成（如 CI/CD、实验调度等）\n\n利用好此特性可极大提升 SDK 的灵活性和可扩展性, 方便构建高级用法或扩展体系",
    "61": "一级标题：swanlab.OpenApi\n二级标题：支持的API列表\n内容：\n下表列出了SwanLab OpenAPI支持的所有方法，点击API名称可跳转到详细说明：\n\n| API名称 | 分类 | 功能描述 | Ready |\n|---------|------|----------|------|\n| [`list_workspaces`](#list-workspaces) | WorkSpace | 获取当前用户的所有工作空间(组织)列表 | ✅ |\n| [`list_projects`](#list-projects) | Project | 获取指定工作空间下的所有项目列表 | ✅ |\n| [`delete_project`](#delete-project) | Project | 删除一个项目 | ✅ |\n| [`list_experiments`](#list-experiments) | Experiment | 获取指定项目下的所有实验列表 | ✅ |\n| [`get_experiment`](#get-experiment) | Experiment | 获取一个实验的详细信息（实验名、配置、环境等） | ✅ |\n| [`get_summary`](#get-summary) | Experiment | 获取一个实验的Summary信息，包含实验跟踪指标的最终值和最大最小值 | ✅ |\n| [`get_metrics`](#get-metrics) | Experiment | 获取一个实验指标的值 |  ✅ |\n| [`delete_experiment`](#delete-experiment) | Experiment | 删除一个实验 | ✅ |",
    "62": "一级标题：swanlab.OpenApi\n二级标题：介绍\n内容：\n> 前置条件：需要在编程环境下登录过SwanLab账号。\n\n要使用 SwanLab 的开放 API, 只需实例化一个 `OpenApi` 对象。\n\n```python\nfrom swanlab import OpenApi\n\nmy_api = OpenApi() # 使用本地登录信息\nprint(my_api.list_workspaces().data) # 获取当前用户的工作空间列表\n```\n\n如果你需要获取其他用户的数据：\n```python\nfrom swanlab import OpenApi\n\nother_api = OpenApi(api_key='other_api_key') # 使用另一个账户的api_key\nprint(other_api.list_workspaces().data)\n```\n\n\n具体来说, **OpenApi**的认证逻辑如下：\n\n1. 如果显式提供了`api_key`参数, 则优先使用该`api_key`进行身份认证, 可以在[这里](https://swanlab.cn/space/~/settings)查看自己的 API 密钥；\n2. 否则,使用本地的认证信息。",
    "63": "一级标题：swanlab.OpenApi\n二级标题：常用参数\n内容：\n### 实验ID `exp_id`\n\n实验的唯一标识符**CUID**, 即`exp_id`, 可通过`list_experiments`方法获取对应的`cuid`字段\n\n要查看某一个实验的CUID, 可在云端版网页的\"环境\"标签页查看\"实验ID\"一行, 点击即可复制此实验的CUID\n\n![](./py-openapi/exp_id.png)\n\n### 工作空间名 `username`\n\n工作空间名即`username`, 用于标识用户所在的工作空间:\n\n- 若为个人空间, `username`即为用户的用户名\n- 若为组织空间, `username`为该组织的组织ID\n\n`username`可以通过`list_workspaces`方法获取, 返回的工作空间列表中每个元素的`username`字段即为工作空间名\n\n一般的, 若在开放API调用中不指定`username`, 则**默认**为当前用户的个人空间",
    "64": "一级标题：swanlab.OpenApi\n二级标题：模型定义\n内容：\n在使用开放 API 时, 获取到的部分云端资源组成较为复杂, 如实验、项目等, 难以用简单的Python数据类型表示\n\n因此, 这些资源在开放API的返回值中被定义为了对象, 支持 IDE 的自动补全与类型检查, 从而方便用户进行操作\n\n例如, 要获取一个实验对象的开始时间, 可以用:\n\n```python\napi_response: ApiResponse = my_api.get_experiment(project=\"project1\", exp_cuid=\"cuid1\")\nmy_exp: Experiment = api_response.data\ncreated_time: str = my_exp.createdAt\n```\n\n或者, 要获取一个项目对象所属工作空间的名字, 可以用:\n\n```python\napi_response: ApiResponse = my_api.list_projects()\nmy_project: Project = api_response.data[0]\nworkspace_name: str = my_project.group[\"name\"]\n```\n\n对于一个模型, 其属性可通过以下三种方式访问:\n\n- `my_exp.createdAt`\n- `my_exp[\"createdAt\"]`\n- `my_exp.get(\"createdAt\")`\n\n> Note: 模型可以通过字典风格访问, 但不是真正的字典, 可以通过`my_exp_dict: Dict = my_exp.model_dump()`获取此时模型对应的字典\n\n### API 响应 `ApiResponse`\n\n开放 API 方法返回`swanlab.api.openapi.types.ApiResponse`对象, 包含以下字段:\n\n| 字段 | 类型 |描述 |\n| --- | --- | --- |\n| `code` | `int` | HTTP 状态码 |\n| `errmsg` | `str` | 错误信息, 如果状态码不为`2XX`则非空 |\n| `data` | `Any` | 返回的具体数据, 下面API文档中提到的返回值即为该字段 |\n\n### 实验模型 `Experiment`\n\n实验对象的类型为`swanlab.api.openapi.types.Experiment`, 包含以下字段:\n\n| 字段 | 类型 | 描述 |\n| --- | --- | --- |\n| `cuid` | `str` | 实验CUID, 唯一标识符 |\n| `name` | `str` | 实验名 |\n| `description` | `str` | 实验描述 |\n| `state` | `str` | 实验状态, `FINISHED` 或 `RUNNING` |\n| `show` | `bool` | 显示状态 |\n| `createdAt` | `str` | 创建时间, 格式如 `2024-11-23T12:28:04.286Z` |\n| `finishedAt` | `str` | 完成时间, 格式如 `2024-11-23T12:28:04.286Z`, 若不存在则为 None |\n| `user` | `Dict[str, str]` | 实验创建者, 包含 `username` 与 `name` |\n| `profile` | `dict` | 详细包含了实验的所有配置信息, 如用户自定义配置与Python运行环境等 |\n\n### 项目模型 `Project`\n\n项目对象的类型为`swanlab.api.openapi.types.Project`, 包含以下字段:\n\n| 字段 | 类型 | 描述 |\n| --- | --- | --- |\n| `cuid` | `str` | 项目CUID, 唯一标识符 |\n| `name` | `str` | 项目名 |\n| `description` | `str` | 项目描述 |\n| `visibility` | `str` | 可见性, `PUBLIC` 或 `PRIVATE` |\n| `createdAt` | `str` | 创建时间, 格式如 `2024-11-23T12:28:04.286Z` |\n| `updatedAt` | `str` | 更新时间, 格式如 `2024-11-23T12:28:04.286Z` |\n| `group` | `Dict[str, str]` | 工作空间信息, 包含 `type`, `username`, `name` |\n| `count` | `Dict[str, int]` | 项目的统计信息, 如实验个数, 协作者数量等 |",
    "65": "一级标题：swanlab.OpenApi\n二级标题：OpenAPIs\n内容：\n每个开放 API 都是`OpenApi`对象的一个方法\n\n下面是所有可用的SwanLab 开放 API\n\n### WorkSpace\n\n#### `list_workspaces`\n\n获取当前用户的所有工作空间(组织)列表。\n\n**返回值**\n\n`data` `(List[Dict])`: 用户加入的工作空间列表, 每个元素是一个字典, 包含工作空间的基础信息:\n\n| 字段 | 类型 | 描述 |\n| --- | --- | --- |\n| `name` | `str` | 工作空间名称 |\n| `username` | `str` | 工作空间唯一标识(用于组织相关的 URL) |\n| `role` | `str` | 用户在该工作空间中的角色, 为 `OWNER` 或 `MEMBER` |\n\n**示例**\n\n::: code-group\n\n```python [获取工作区列表]\nfrom swanlab import OpenApi\nmy_api = OpenApi()\n\nmy_api.list_workspaces().data\n\"\"\"\n[\n    {\n        \"name\": \"workspace1\",\n        \"username\": \"kites-test3\",\n        \"role\": \"OWNER\"\n    },\n    {\n        \"name\": \"hello-openapi\",\n        \"username\": \"kites-test2\",\n        \"role\": \"MEMBER\"\n    }\n]\n\"\"\"\n```\n\n```python [获取第一个工作区名称]\nfrom swanlab import OpenApi\nmy_api = OpenApi()\n\nmy_api.list_workspaces().data[0][\"name\"]\n\"\"\"\n\"workspace1\"\n\"\"\"\n```\n\n```python [获取响应状态码]\nfrom swanlab import OpenApi\nmy_api = OpenApi()\n\nmy_api.list_workspaces().code\n\"\"\"\n200\n\"\"\"\n```\n\n:::\n\n<br>\n\n---\n\n### Experiment\n\n#### `list_experiments`\n\n获取指定项目下的所有实验列表\n\n**方法参数**\n\n| 参数  | 类型 | 描述 |\n| --- | --- | --- |\n| `project` | `str` | 项目名 |\n| `username` | `str` | 工作空间名, 默认为用户个人空间 |\n\n**返回值**\n\n`data` `(List[Experiment])`: 包含实验[(Experiment)](#实验模型-experiment)对象的列表\n\n**示例**\n\n::: code-group\n\n```python [获取实验列表]\nmy_api.list_experiments(project=\"project1\").data\n\"\"\"\n[\n    {\n        \"cuid\": \"cuid1\",\n        \"name\": \"experiment1\",\n        \"description\": \"Description 1\",\n        \"state\": \"RUNNING\",\n        \"show\": true,\n        \"createdAt\": \"2024-11-23T12:28:04.286Z\",\n        \"finishedAt\": null,\n        \"user\": {\n            \"username\": \"kites-test3\",\n            \"name\": \"Kites Test\"\n        },\n        \"profile\": {\n            \"config\": {\n                \"lr\": 0.001,\n                \"epochs\": 10\n            }\n        }\n    },\n    ...\n]\n\"\"\"\n```\n\n```python [获取第一个实验的CUID]\nmy_api.list_experiments(project=\"project1\").data[0].cuid\n\"\"\"\n\"cuid1\"\n\"\"\"\n```\n\n```python [获取第一个实验的名称]\nmy_api.list_experiments(project=\"project1\").data[0].name\n\"\"\"\n\"experiment1\"\n\"\"\"\n```\n\n:::\n\n<br>\n\n#### `get_experiment`\n\n获取一个实验的详细信息\n\n**方法参数**\n\n| 参数 | 类型 | 描述 |\n| --- | --- | --- |\n| `project` | `str` | 项目名 |\n| `exp_id` | `str` | 实验CUID, 唯一标识符, 可通过`list_experiments`获取, 也可在云端版实验\"环境\"标签页查看 |\n| `username` | `str` | 工作空间名, 默认为用户个人空间 |\n\n**返回值**\n\n`data` `(Experiment)`: 返回一个实验[(Experiment)](#实验模型-experiment)类型的对象, 包含实验的详细信息\n\n**示例**\n\n::: code-group\n\n```python [获取实验信息]\nmy_api.get_experiment(project=\"project1\", exp_id=\"cuid1\").data\n\"\"\"\n{\n    \"cuid\": \"cuid1\",\n    \"name\": \"experiment1\",\n    \"description\": \"This is a test experiment\",\n    \"state\": \"FINISHED\",\n    \"show\": true,\n    \"createdAt\": \"2024-11-23T12:28:04.286Z\",\n    \"finishedAt\": \"2024-11-25T15:56:48.123Z\",\n    \"user\": {\n        \"username\": \"kites-test3\",\n        \"name\": \"Kites Test\"\n    },\n    \"profile\": {\n        \"conda\": \"...\",\n        \"requirements\": \"...\",\n        ...\n    }\n}\n\"\"\"\n```\n\n```python [获取实验的状态]\nmy_api.get_experiment(project=\"project1\", exp_id=\"cuid1\").data.state\n\"\"\"\n\"FINISHED\"\n\"\"\"\n```\n\n```python [获取实验的创建者用户名]\nmy_api.get_experiment(project=\"project1\", exp_id=\"cuid1\").data.user[\"username\"]\n\"\"\"\n\"kites-test3\"\n\"\"\"\n```\n\n:::\n\n<br>\n\n#### `delete_experiment`\n\n删除一个实验\n\n**方法参数**\n| 参数 | 类型 | 描述 |\n| --- | --- | --- |\n| `project` | `str` | 项目名 |\n| `exp_id` | `str` | 实验CUID, 唯一标识符, 可通过`list_experiments`获取, 也可在云端版实验\"环境\"标签页查看 |\n| `username` | `str` | 工作空间名, 默认为用户个人空间 |\n\n**返回值**\n`data` `(dict)`: 空字典, 仅表示删除操作成功\n\n**示例**\n\n::: code-group\n\n```python [删除实验]\nmy_api.delete_experiment(project=\"project1\", exp_id=\"cuid1\")\n```\n\n:::\n\n<br>\n\n#### `get_summary`\n\n获取一个实验的概要信息, 包含实验跟踪指标的最终值和最大最小值, 以及其对应的步数\n\n**方法参数**\n\n| 参数 | 类型 | 描述 |\n| --- | --- | --- |\n| `project` | `str` | 项目名 |\n| `exp_id` | `str` | 实验CUID, 唯一标识符, 可通过`list_experiments`获取, 也可在云端版实验\"环境\"标签页查看 |\n| `username` | `str` | 工作空间名, 默认为用户个人空间 |\n\n**返回值**\n\n`data` `(Dict[str, Dict])`: 返回一个字典, 包含实验的概要信息\n\n字典中的每个键是一个指标名称, 值是一个结构如下的字典:\n\n| 字段 | 类型 | 描述 |\n| --- | --- | --- |\n| `step` | `int` | 最后一个步数 |\n| `value` | `float` | 最后一个步数的指标值 |\n| `min` | `Dict[str, float]` | 最小值对应的步数和指标值 |\n| `max` | `Dict[str, float]` | 最大值对应的步数和指标值 |\n\n\n**示例**\n\n::: code-group\n\n```python [获取实验概要信息]\nmy_api.get_summary(project=\"project1\", exp_id=\"cuid1\").data\n\"\"\"\n{\n    \"loss\": {\n        \"step\": 47,\n        \"value\": 0.1907215012216071,\n        \"min\": {\n            \"step\": 33,\n            \"value\": 0.1745886406861026\n        },\n        \"max\": {\n            \"step\": 0,\n            \"value\": 0.7108771095136294\n        }\n    },\n    ...\n}\n\"\"\"\n```\n\n\n```python [获取指标的最大值]\nmy_api.get_summary(project=\"project1\", exp_id=\"cuid1\").data[\"loss\"][\"max\"][\"value\"]\n\"\"\"\n0.7108771095136294\n\"\"\"\n```\n\n```python [获取指标最小值所在步]\nmy_api.get_summary(project=\"project1\", exp_id=\"cuid1\").data[\"loss\"][\"min\"][\"step\"]\n\"\"\"\n33\n\"\"\"\n```\n:::\n\n<br>\n\n#### get_metrics\n\n获取一个实验的指标值\n\n**方法参数**\n\n| 参数 | 类型 | 描述 |\n| --- | --- | --- |\n| `exp_id` | `str` | 实验CUID, 唯一标识符, 可通过`list_experiments`获取, 也可在云端版实验\"环境\"标签页查看 |\n| `keys` | `Union[str, List[str]]` | 指标名列表, 即swanlab.log({key: value})中的key, 可在网站查看, 也可通过`get_summary`获取 |\n\n**返回值**\n\n`data` `(DataFrame)`: 返回一个DataFrame, 包含实验的指标值\n\n**示例**\n\n::: code-group\n\n```python [获取实验指标]\nmy_api.get_metrics(exp_id=\"cuid1\", keys=[\"loss\", \"acc\"]).data\n\"\"\"\n          loss  loss_timestamp       acc  acc_timestamp\nstep                                                   \n1     0.336772   1751712864853  0.670422  1751712864852\n2     0.338035   1751712864858  0.830018  1751712864857\n3     0.282654   1751712864862  0.794594  1751712864862\n4     0.258216   1751712864866  0.832750  1751712864866\n5     0.097542   1751712864871  0.901684  1751712864871\n6     0.092955   1751712864875  0.907544  1751712864875\n7     0.149327   1751712864879  0.942524  1751712864879\n8     0.131631   1751712864884  0.921309  1751712864883\n\"\"\"\n```\n\n:::\n\n\n<br>\n\n---\n\n\n### Project\n\n#### `list_projects`\n\n获取指定工作空间下的所有项目列表\n\n**方法参数**\n\n| 参数  | 类型 | 描述 |\n| --- | --- | --- |\n| `username` | `str` | 工作空间名, 默认为用户个人空间 |\n| `detail` | `bool` | 是否项目统计信息, 默认为 True |\n\n**返回值**\n\n`data` `(List[Project])`: 包含项目[(Project)](#项目模型-project)对象的列表\n\n**示例**\n\n::: code-group\n\n```python [获取项目列表]\nmy_api.list_projects().data\n\"\"\"\n[\n    {\n        \"cuid\": \"project1\",\n        \"name\": \"Project 1\",\n        \"description\": \"Description 1\",\n        \"visibility\": \"PUBLIC\",\n        \"createdAt\": \"2024-11-23T12:28:04.286Z\",\n        \"updatedAt\": null,\n        \"group\": {\n            \"type\": \"PERSON\",\n            \"username\": \"kites-test3\",\n            \"name\": \"Kites Test\"\n        },\n        \"count\": {\n            \"experiments\": 4,\n            \"contributors\": 1,\n            \"children\": 0,\n            \"runningExps\": 0\n        }\n    },\n    ...\n]\n\"\"\"\n```\n\n:::\n\n#### `delete_project`\n\n删除一个项目\n\n**方法参数**\n\n| 参数 | 类型 | 描述 |\n| --- | --- | --- |\n| `project` | `str` | 项目名 |\n| `username` | `str` | 工作空间名, 默认为用户个人空间 |\n\n**返回值**\n\n`data` `(dict)`: 空字典, 仅表示删除操作成功\n\n**示例**\n\n::: code-group\n\n```python [删除项目]\nmy_api.delete_project(project=\"project1\")\n```\n\n:::\n\n<br>",
    "66": "一级标题：其他Python API\n二级标题：无\n内容：",
    "67": "一级标题：其他Python API\n二级标题：get_run\n内容：\n获取当前运行的实验对象（`SwanLabRun`）。\n\n```python\nrun = swanlab.init(...)\n\n...\n\nrun = swanlab.get_run()\n```",
    "68": "一级标题：其他Python API\n二级标题：get_url\n内容：\n获取实验的URL（cloud模式，否则为None）。\n\n```python\nprint(swanlab.get_url())\n```",
    "69": "一级标题：其他Python API\n二级标题：get_project_url\n内容：\n获取项目的URL（cloud模式，否则为None）。\n\n```python\nprint(swanlab.get_project_url())\n```",
    "70": "一级标题：swanlab.register_callback\n二级标题：无\n内容：\n```python\n@should_call_before_init(\"After calling swanlab.init(), you can't call it again.\")\ndef register_callbacks(\n    self,\n    callbacks: List[SwanKitCallback]\n) -> None:\n```\n\n| 参数 | 类型 | 描述 |\n| --- | --- | --- |\n| `callbacks` | `List[SwanKitCallback]` | 回调函数列表 |",
    "71": "一级标题：swanlab.register_callback\n二级标题：介绍\n内容：\n使用`swanlab.register_callbacks()`注册回调函数，以在SwanLab的执行生命周期中调用。\n\n```python {3}\nfrom swanlab.plugin.writer import EmailCallback\nemail_callback = EmailCallback(...)\nswanlab.register_callbacks([email_callback])\n\nswanlab.init(...)\n```\n\n效果等价于：\n\n```python\nfrom swanlab.plugin.writer import EmailCallback\nemail_callback = EmailCallback(...)\n\nswanlab.init(\n    ...\n    callbacks=[email_callback]\n)\n```\n\n**场景**：比如你使用时的是SwanLab与Transformers的集成，那么你要找到`swanlab.init()`是不容易的。那么，你可以在`trainer.train()`调用前，用`swanlab.register_callbacks()`注册回调函数，实现插件的注入。",
    "72": "一级标题：run\n二级标题：无\n内容：\nrun 指的是 `swanlab.init()` 返回的 `SwanLabRun` 对象，这里介绍run具有的一些方法。  \n(逐步更新中...)",
    "73": "一级标题：run\n二级标题：public\n内容：\npublic存储了SwanLabRun的一些公共信息，包括：\n- `project_name`: 项目名称\n- `version`: 版本\n- `run_id`: 实验ID\n- `swanlog_dir`: swanlog日志目录的路径\n- `run_dir`: 运行目录的路径\n- `cloud`: 云端信息\n    - `project_name`: 项目名称（仅在cloud模式时有效）\n    - `project_url`: 项目在云端的URL（仅在cloud模式时有效）\n    - `experiment_name`: 实验名称（仅在cloud模式时有效）\n    - `experiment_url`: 实验在云端的URL（仅在cloud模式时有效）\n\n以字典形式获取public信息：\n\n```python\nimport swanlab\nrun = swanlab.init()\nprint(run.public.json())\n```\n\n比如，你想要获取实验的URL，可以这样：\n\n```python\nprint(run.public.cloud.experiment_url)\n```",
    "74": "一级标题：swanlab.Settings\n二级标题：无\n内容：\n```python\nSettings(\n    model_config = ConfigDict(frozen=True),\n    metadata_collect: StrictBool = True,\n    collect_hardware: StrictBool = True,\n    collect_runtime: StrictBool = True,\n    security_mask: StrictBool = True,\n    requirements_collect: StrictBool = True,\n    conda_collect: StrictBool = False,\n    hardware_monitor: StrictBool = True,\n    disk_io_dir: DirectoryPath = Field(...),\n    upload_interval: PositiveInt = 1,\n    max_log_length: int = Field(ge=500, le=4096, default=1024),\n    log_proxy_type: Literal[\"all\", \"stdout\", \"stderr\", \"none\"] = \"all\",\n)\n```\n\n| 参数                     | 类型            | 描述                                                                              |\n|:-----------------------|:--------------|:--------------------------------------------------------------------------------|\n| `metadata_collect`     | StrictBool    | 是否开启元数据采集。默认值为 `True`。                                                          |\n| `collect_hardware`     | StrictBool    | 是否采集当前系统环境的硬件信息。默认值为 `True`。                                                    |\n| `collect_runtime`      | StrictBool    | 是否采集运行时信息。默认值为 `True`。                                                          |\n| `security_mask`        | StrictBool    | 是否自动隐藏隐私信息，如 api_key 等。开启后将在检测到隐私信息时，自动将其替换为加密字符（****）。默认值为 `True`。             |\n| `requirements_collect` | StrictBool    | 是否采集 Python 环境信息 (`pip list`)。默认值为 `True`。                                      |\n| `conda_collect`        | StrictBool    | 是否采集 Conda 环境信息。默认值为 `False`。                                                   |\n| `hardware_monitor`     | StrictBool    | 是否开启硬件监控。如果 `metadata_collect` 关闭，则此项无效。默认值为 `True`。                            |\n| `disk_io_dir`          | DirectoryPath | 磁盘 IO 监控的路径。默认值为系统根目录 (`/` 或 `C:\\`)。                                            |\n| `hardware_interval`    | PositiveInt   | 硬件监控采集间隔，以秒为单位，最小值为5秒。                                                          |\n| `backup`               | PositiveInt   | 日志备份开启功能，默认值为 `True`。开启后，日志将被备份到本地（默认为`swanlog`目录）。      |\n| `upload_interval`      | PositiveInt   | 日志上传间隔（单位：秒）。默认值为 `1`。                                                          |\n| `max_log_length`       | int           | 终端日志上传单行最大字符数（范围：500-4096）。默认值为 `1024`。                                         |\n| `log_proxy_type`       | Literal       | 日志代理类型，会影响实验的日志选项卡记录的内容。默认值为 `\"all\"`。\"stdout\" 表示只代理标准输出流，\"stderr\" 表示只代理标准错误流，\"all\" 表示代理标准输出流和标准错误流，\"none\" 表示不代理日志。|",
    "75": "一级标题：swanlab.Settings\n二级标题：介绍\n内容：\n- `swanlab.Settings`类用于和管理 SwanLab 的全局功能开关和设置。\n\n- 在`import swanlab`时，会创建一个默认的全局设置，各个设置及其默认值详见上表。\n\n- 如果我们要对某些设置进行调整，需要通过新建一个`Settings`实例如`new_settings`，在实例化时传入想要修改的配置参数，然后要通过运行`swanlab.merge_settings(new_settings)`来对全局设置进行更新。\n\n- 值得注意的是，`merge_settings()`方法只在`swanlab.init()`被调用之前可用，这意味着，在使用`swanlab`的过程中，一旦`swanlab.init()`被调用，全局设置将不再能被更改。",
    "76": "一级标题：swanlab.Settings\n二级标题：更多用法\n内容：\n### 更新全局设置\n\n::: code-group\n\n```python [方式一]\nimport swanlab\n\n# 创建新的设置对象\nnew_settings = swanlab.Settings(\n    metadata_collect=False,\n    hardware_monitor=False,\n    upload_interval=5\n)\n\nswanlab.init(settings=new_settings)\n...\n```\n\n```python [方式二]\nimport swanlab\n\n# 创建新的设置对象\nnew_settings = swanlab.Settings(\n    metadata_collect=False,\n    hardware_monitor=False,\n    upload_interval=5\n)\n\n# 更新全局设置\nswanlab.merge_settings(new_settings)\n\nswanlab.init()\n...\n```\n\n:::\n\n### 记录 conda 环境信息\n\n```python\nimport swanlab\nfrom swanlab import Settings\n\n# 创建新的设置对象\nnew_settings = Settings(\n    conda_collect=True  # 默认不开启\n)\n\n# 更新全局设置\nswanlab.merge_settings(new_settings)\n\nswanlab.init()\n...\n```",
    "77": "一级标题：swanlab.sync_mlflow\n二级标题：无\n内容：\n将MLFlow项目同步到SwanLab，[文档](/guide_cloud/integration/integration-mlflow.md)",
    "78": "一级标题：swanlab.sync_tensorboard\n二级标题：无\n内容：\n将tensorboard/tensorboardX的指标同步到SwanLab, [文档](/guide_cloud/integration/integration-tensorboard.md)",
    "79": "一级标题：swanlab.sync_wandb\n二级标题：无\n内容：\n将wandb的指标同步到SwanLab, [文档](/guide_cloud/integration/integration-wandb.md)",
    "80": "一级标题：音频分类\n二级标题：无\n内容：\n:::info\n音频分类、音频处理入门\n:::\n\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/PyTorch\\_Audio\\_Classification-simple/charts)\n\n音频分类任务是指将音频信号按照其内容的类别归属进行划分。例如，区分一段音频是音乐、语音、环境声音（如鸟鸣、雨声、机器运转声）还是动物叫声等。其目的是通过自动分类的方式，高效地对大量音频数据进行组织、检索和理解。\n\n![alt text](/assets/examples/audio_classification/example-audio-classification-1.png)\n\n在现在音频分类的应用场景，比较多的是在音频标注、音频推荐这一块。同时，这也是一个非常好的入门音频模型训练的任务。\n\n在本文中，我们会基于PyTorch框架，使用 ResNet系列模型在 GTZAN 数据集上进行训练，同时使用[SwanLab](https://swanlab.cn)监控训练过程、评估模型效果。\n\n* Github：[https://github.com/Zeyi-Lin/PyTorch-Audio-Classification](https://github.com/Zeyi-Lin/PyTorch-Audio-Classification)\n* 数据集：[https://pan.baidu.com/s/14CTI_9MD1vXCqyVxmAbeMw?pwd=1a9e](https://pan.baidu.com/s/14CTI_9MD1vXCqyVxmAbeMw?pwd=1a9e) 提取码: 1a9e\n* SwanLab实验日志：[https://swanlab.cn/@ZeyiLin/PyTorch\\_Audio\\_Classification-simple/charts](https://swanlab.cn/@ZeyiLin/PyTorch\\_Audio\\_Classification-simple/charts)\n* 更多实验日志：[https://swanlab.cn/@ZeyiLin/PyTorch\\_Audio\\_Classification/charts](https://swanlab.cn/@ZeyiLin/PyTorch\\_Audio\\_Classification/charts)",
    "81": "一级标题：音频分类\n二级标题：1. 音频分类逻辑\n内容：\n本教程对音频分类任务的逻辑如下：\n\n1. 载入音频数据集，数据集为音频WAV文件与对应的标签\n2. 以8:2的比例划分训练集和测试集\n3. 使用`torchaudio`库，将音频文件转换为梅尔频谱图，本质将其转换为图像分类任务\n4. 使用ResNet模型对梅尔频谱图进行训练迭代\n5. 使用SwanLab记录训练和测试阶段的loss、acc变化，并对比不同实验之间的效果差异",
    "82": "一级标题：音频分类\n二级标题：2. 环境安装\n内容：\n本案例基于**Python>=3.8**，请在您的计算机上安装好Python。\n\n我们需要安装以下这几个Python库：\n\n```python\ntorch\ntorchvision\ntorchaudio\nswanlab\npandas\nscikit-learn\n```\n\n一键安装命令：\n\n```shellscript\npip install torch torchvision torchaudio swanlab pandas scikit-learn\n```",
    "83": "一级标题：音频分类\n二级标题：3. GTZAN数据集准备\n内容：\n本任务使用的数据集为GTZAN，这是一个在音乐流派识别研究中常用的公开数据集。GTZAN数据集包含 1000 个音频片段，每个音频片段的时长为 30 秒，共分为 10 种音乐流派：包括布鲁斯（Blues）、古典（Classical）、乡村（Country）、迪斯科（Disco）、嘻哈（Hip Hop）、爵士（Jazz）、金属（Metal）、流行（Pop）、雷鬼（Reggae）、摇滚（Rock），且每种流派都有 100 个音频片段。\n\n![alt text](/assets/examples/audio_classification/example-audio-classification-2.png)\n\nGTZAN数据集是在 2000-2001 年从各种来源收集的，包括个人 CD、收音机、麦克风录音等，代表了各种录音条件下的声音。\n\n**数据下载方式（大小1.4GB）：**\n\n1. 百度网盘下载：链接: [https://pan.baidu.com/s/14CTI\\_9MD1vXCqyVxmAbeMw?pwd=1a9e](https://pan.baidu.com/s/14CTI\\_9MD1vXCqyVxmAbeMw?pwd=1a9e) 提取码: 1a9e\n2. 通过Kaggle下载：[https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification](https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification)\n3. 在Hyper超神经网站下载BT种子进行下载：[https://hyper.ai/cn/datasets/32001](https://hyper.ai/cn/datasets/32001)\n\n> 注意，数据集中有一个音频是损坏的，在百度网盘版本里已经将其剔除。\n\n下载完成后，解压到项目根目录下即可。",
    "84": "一级标题：音频分类\n二级标题：4. 生成数据集CSV文件\n内容：\n我们将数据集中的音频文件路径和对应的标签，处理成一个`audio_dataset.csv`文件，其中第一列为文件路径，第二列为标签：\n\n（这一部分先不执行，在完整代码里会带上）\n\n```python\nimport os\nimport pandas as pd\n\ndef create_dataset_csv():\n    # 数据集根目录\n    data_dir = './GTZAN/genres_original'\n    data = []\n    \n    # 遍历所有子目录\n    for label in os.listdir(data_dir):\n        label_dir = os.path.join(data_dir, label)\n        if os.path.isdir(label_dir):\n            # 遍历子目录中的所有wav文件\n            for audio_file in os.listdir(label_dir):\n                if audio_file.endswith('.wav'):\n                    audio_path = os.path.join(label_dir, audio_file)\n                    data.append([audio_path, label])\n    \n    # 创建DataFrame并保存为CSV\n    df = pd.DataFrame(data, columns=['path', 'label'])\n    df.to_csv('audio_dataset.csv', index=False)\n    return df\n\n\n# 生成或加载数据集CSV文件\nif not os.path.exists('audio_dataset.csv'):\n    df = create_dataset_csv()\nelse:\n    df = pd.read_csv('audio_dataset.csv')\n```\n\n处理后，你会在根目录下看到一个`audio_dataset.csv`文件：\n\n![alt text](/assets/examples/audio_classification/example-audio-classification-3.png)",
    "85": "一级标题：音频分类\n二级标题：5. 配置训练跟踪工具SwanLab\n内容：\nSwanLab 是一款开源、轻量的 AI 实验跟踪工具，提供了一个跟踪、比较、和协作实验的平台。SwanLab 提供了友好的 API 和漂亮的界面，结合了超参数跟踪、指标记录、在线协作、实验链接分享等功能，让您可以快速跟踪 AI 实验、可视化过程、记录超参数，并分享给伙伴。\n\n![alt text](/assets/examples/audio_classification/example-audio-classification-4.png)\n\n配置SwanLab的方式很简单：\n\n1. 注册一个账号：[https://swanlab.cn](https://swanlab.cn)\n2. 在安装好swanlab后（pip install swanlab），登录：\n\n```bash\nswanlab login\n```\n\n在提示输入API Key时，去[设置页面](https://swanlab.cn/settings/overview)复制API Key，粘贴后按回车即可。\n\n![alt text](/assets/examples/audio_classification/example-audio-classification-5.png)",
    "86": "一级标题：音频分类\n二级标题：6. 完整代码\n内容：\n开始训练时的目录结构：\n\n```\n|--- train.py\n|--- GTZAN\n```\n\ntrain.py做的事情包括：\n\n1. 生成数据集csv文件\n2. 加载数据集和resnet18模型（ImageNet预训练）\n3. 训练20个epoch，每个epoch进行训练和评估\n4. 记录loss和acc，以及学习率的变化情况，在swanlab中可视化\n\n\n\ntrain.py：\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchaudio\nimport torchvision.models as models\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport swanlab\n\n\ndef create_dataset_csv():\n    # 数据集根目录\n    data_dir = './GTZAN/genres_original'\n    data = []\n    \n    # 遍历所有子目录\n    for label in os.listdir(data_dir):\n        label_dir = os.path.join(data_dir, label)\n        if os.path.isdir(label_dir):\n            # 遍历子目录中的所有wav文件\n            for audio_file in os.listdir(label_dir):\n                if audio_file.endswith('.wav'):\n                    audio_path = os.path.join(label_dir, audio_file)\n                    data.append([audio_path, label])\n    \n    # 创建DataFrame并保存为CSV\n    df = pd.DataFrame(data, columns=['path', 'label'])\n    df.to_csv('audio_dataset.csv', index=False)\n    return df\n\n# 自定义数据集类\nclass AudioDataset(Dataset):\n    def __init__(self, df, resize, train_mode=True):\n        self.audio_paths = df['path'].values\n        # 将标签转换为数值\n        self.label_to_idx = {label: idx for idx, label in enumerate(df['label'].unique())}\n        self.labels = [self.label_to_idx[label] for label in df['label'].values]\n        self.resize = resize\n        self.train_mode = train_mode  # 添加训练模式标志\n    def __len__(self):\n        return len(self.audio_paths)\n    \n    def __getitem__(self, idx):\n        # 加载音频文件\n        waveform, sample_rate = torchaudio.load(self.audio_paths[idx])\n        \n        # 将音频转换为梅尔频谱图\n        transform = torchaudio.transforms.MelSpectrogram(\n            sample_rate=sample_rate,\n            n_fft=2048,\n            hop_length=640,\n            n_mels=128\n        )\n        mel_spectrogram = transform(waveform)\n\n        # 确保数值在合理范围内\n        mel_spectrogram = torch.clamp(mel_spectrogram, min=0)\n        \n        # 转换为3通道图像格式 (为了适配ResNet)\n        mel_spectrogram = mel_spectrogram.repeat(3, 1, 1)\n        \n        # 确保尺寸一致\n        resize = torch.nn.AdaptiveAvgPool2d((self.resize, self.resize))\n        mel_spectrogram = resize(mel_spectrogram)\n        \n        return mel_spectrogram, self.labels[idx]\n\n# 修改ResNet模型\nclass AudioClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super(AudioClassifier, self).__init__()\n        # 加载预训练的ResNet\n        self.resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n        # 修改最后的全连接层\n        self.resnet.fc = nn.Linear(512, num_classes)\n        \n    def forward(self, x):\n        return self.resnet(x)\n\n# 训练函数\ndef train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n    \n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for i, (inputs, labels) in enumerate(train_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            \n            loss.backward()\n\n            optimizer.step()\n            optimizer.zero_grad()\n            \n            running_loss += loss.item()\n            \n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n        \n        train_loss = running_loss/len(train_loader)\n        train_acc = 100.*correct/total\n        \n        # 验证阶段\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                \n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n        \n        val_loss = val_loss/len(val_loader)\n        val_acc = 100.*correct/total\n        \n        current_lr = optimizer.param_groups[0]['lr']\n        \n        # 记录训练和验证指标\n        swanlab.log({\n            \"train/loss\": train_loss,\n            \"train/acc\": train_acc,\n            \"val/loss\": val_loss,\n            \"val/acc\": val_acc,\n            \"train/epoch\": epoch,\n            \"train/lr\": current_lr\n        })\n            \n        print(f'Epoch {epoch+1}:')\n        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n        print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n        print(f'Learning Rate: {current_lr:.6f}')\n\n# 主函数\ndef main():\n    # 设置设备\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    run = swanlab.init(\n        project=\"PyTorch_Audio_Classification-simple\",\n        experiment_name=\"resnet18\",\n        config={\n            \"batch_size\": 16,\n            \"learning_rate\": 1e-4,\n            \"num_epochs\": 20,\n            \"resize\": 224,\n        },\n    )\n    \n    # 生成或加载数据集CSV文件\n    if not os.path.exists('audio_dataset.csv'):\n        df = create_dataset_csv()\n    else:\n        df = pd.read_csv('audio_dataset.csv')\n    \n    # 划分训练集和验证集\n    train_df = pd.DataFrame()\n    val_df = pd.DataFrame()\n    \n    for label in df['label'].unique():\n        label_df = df[df['label'] == label]\n        label_train, label_val = train_test_split(label_df, test_size=0.2, random_state=42)\n        train_df = pd.concat([train_df, label_train])\n        val_df = pd.concat([val_df, label_val])\n    \n    # 创建数据集和数据加载器 \n    train_dataset = AudioDataset(train_df, resize=run.config.resize, train_mode=True)\n    val_dataset = AudioDataset(val_df, resize=run.config.resize, train_mode=False)\n    \n    train_loader = DataLoader(train_dataset, batch_size=run.config.batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n    \n    # 创建模型\n    num_classes = len(df['label'].unique())  # 根据实际分类数量设置\n    print(\"num_classes\", num_classes)\n    model = AudioClassifier(num_classes).to(device)\n    \n    # 定义损失函数和优化器\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=run.config.learning_rate)  \n    \n    # 训练模型\n    train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=run.config.num_epochs, device=device)\n\nif __name__ == \"__main__\":\n    main()\n```\n\n看到下面的输出，则代表训练开始：\n\n![alt text](/assets/examples/audio_classification/example-audio-classification-6.png)\n\n访问打印的swanlab链接，可以看到训练的全过程：\n\n![alt text](/assets/examples/audio_classification/example-audio-classification-7.png)\n\n可以看到Reset18模型，且不加任何策略的条件下，在训练集的准确率为99.5%，验证集的准确率最高为71.5%，val loss在第3个epoch开始反而在上升，呈现「过拟合」的趋势。",
    "87": "一级标题：音频分类\n二级标题：7. 进阶代码\n内容：\n下面是我训出验证集准确率87.5%的实验，具体策略包括：\n\n1. 将模型换成resnext101\\_32x8d\n2. 将梅尔顿图的resize提高到512\n3. 增加warmup策略\n4. 增加时间遮蔽、频率屏蔽、高斯噪声、随机响度这四种数据增强策略\n5. 增加学习率梯度衰减策略\n\n![alt text](/assets/examples/audio_classification/example-audio-classification-8.png)\n\n进阶代码（需要24GB显存，如果要降低显存消耗的话，可以调低batch\\_size）：\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchaudio\nimport torchvision.models as models\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport swanlab\nimport random\nimport numpy as np\n\n# 设置随机种子\ndef set_seed(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndef create_dataset_csv():\n    # 数据集根目录\n    data_dir = './GTZAN/genres_original'\n    data = []\n    \n    # 遍历所有子目录\n    for label in os.listdir(data_dir):\n        label_dir = os.path.join(data_dir, label)\n        if os.path.isdir(label_dir):\n            # 遍历子目录中的所有wav文件\n            for audio_file in os.listdir(label_dir):\n                if audio_file.endswith('.wav'):\n                    audio_path = os.path.join(label_dir, audio_file)\n                    data.append([audio_path, label])\n    \n    # 创建DataFrame并保存为CSV\n    df = pd.DataFrame(data, columns=['path', 'label'])\n    df.to_csv('audio_dataset.csv', index=False)\n    return df\n\n# 自定义数据集类\nclass AudioDataset(Dataset):\n    def __init__(self, df, resize, train_mode=True):\n        self.audio_paths = df['path'].values\n        # 将标签转换为数值\n        self.label_to_idx = {label: idx for idx, label in enumerate(df['label'].unique())}\n        self.labels = [self.label_to_idx[label] for label in df['label'].values]\n        self.resize = resize\n        self.train_mode = train_mode  # 添加训练模式标志\n    def __len__(self):\n        return len(self.audio_paths)\n    \n    def __getitem__(self, idx):\n        # 加载音频文件\n        waveform, sample_rate = torchaudio.load(self.audio_paths[idx])\n        \n        # 将音频转换为梅尔频谱图\n        transform = torchaudio.transforms.MelSpectrogram(\n            sample_rate=sample_rate,\n            n_fft=2048,\n            hop_length=640,\n            n_mels=128\n        )\n        mel_spectrogram = transform(waveform)\n        \n        # 仅在训练模式下进行数据增强\n        if self.train_mode:\n            # 1. 时间遮蔽 (Time Masking)：通过随机选择一个时间步，然后遮蔽掉20个时间步\n            time_mask = torchaudio.transforms.TimeMasking(time_mask_param=20)\n            mel_spectrogram = time_mask(mel_spectrogram)\n            \n            # 2. 频率遮蔽 (Frequency Masking)：通过随机选择一个频率步，然后遮蔽掉20个频率步\n            freq_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param=20)\n            mel_spectrogram = freq_mask(mel_spectrogram)\n            \n            # 3. 随机增加高斯噪声\n            if random.random() < 0.5:\n                noise = torch.randn_like(mel_spectrogram) * 0.01\n                mel_spectrogram = mel_spectrogram + noise\n            \n            # 4. 随机调整响度\n            if random.random() < 0.5:\n                gain = random.uniform(0.8, 1.2)\n                mel_spectrogram = mel_spectrogram * gain\n\n        # 确保数值在合理范围内\n        mel_spectrogram = torch.clamp(mel_spectrogram, min=0)\n        \n        # 转换为3通道图像格式 (为了适配ResNet)\n        mel_spectrogram = mel_spectrogram.repeat(3, 1, 1)\n        \n        # 确保尺寸一致\n        resize = torch.nn.AdaptiveAvgPool2d((self.resize, self.resize))\n        mel_spectrogram = resize(mel_spectrogram)\n        \n        return mel_spectrogram, self.labels[idx]\n\n# 修改ResNet模型\nclass AudioClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super(AudioClassifier, self).__init__()\n        # 加载预训练的ResNet\n        self.resnet = models.resnext101_32x8d(weights=models.ResNeXt101_32X8D_Weights.IMAGENET1K_V1)\n        # 修改最后的全连接层\n        self.resnet.fc = nn.Linear(2048, num_classes)\n        \n    def forward(self, x):\n        return self.resnet(x)\n\n# 训练函数\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device, run):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        # 前5个epoch进行warmup\n        if epoch < 5:\n            warmup_factor = (epoch + 1) / 5\n            for param_group in optimizer.param_groups:\n                param_group['lr'] = run.config.learning_rate * warmup_factor\n        \n        # optimizer.zero_grad()  # 移到循环外部\n        \n        for i, (inputs, labels) in enumerate(train_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            \n            optimizer.step()\n            optimizer.zero_grad()\n            \n            running_loss += loss.item()\n            \n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n        \n        train_loss = running_loss\n        train_acc = 100.*correct/total\n        \n        # 验证阶段\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                \n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n        \n        val_loss = val_loss/len(val_loader)\n        val_acc = 100.*correct/total\n        \n        # 只在warmup结束后使用学习率调度器\n        if epoch >= 5:\n            scheduler.step()\n        current_lr = optimizer.param_groups[0]['lr']\n        \n        # 记录训练和验证指标\n        swanlab.log({\n            \"train/loss\": train_loss,\n            \"train/acc\": train_acc,\n            \"val/loss\": val_loss,\n            \"val/acc\": val_acc,\n            \"train/epoch\": epoch,\n            \"train/lr\": current_lr\n        })\n            \n        print(f'Epoch {epoch+1}:')\n        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n        print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n        print(f'Learning Rate: {current_lr:.6f}')\n\n# 主函数\ndef main():\n    # 设置随机种子\n    set_seed(42)\n    \n    # 设置设备\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    run = swanlab.init(\n        project=\"PyTorch_Audio_Classification-simple\",\n        experiment_name=\"😄resnext101_32x8d\",\n        config={\n            \"batch_size\": 16,\n            \"learning_rate\": 1e-4,\n            \"num_epochs\": 30,\n            \"resize\": 512,\n            \"weight_decay\": 0  # 添加到配置中\n        },\n    )\n    \n    # 生成或加载数据集CSV文件\n    if not os.path.exists('audio_dataset.csv'):\n        df = create_dataset_csv()\n    else:\n        df = pd.read_csv('audio_dataset.csv')\n    \n    # 划分训练集和验证集\n    train_df = pd.DataFrame()\n    val_df = pd.DataFrame()\n    \n    for label in df['label'].unique():\n        label_df = df[df['label'] == label]\n        label_train, label_val = train_test_split(label_df, test_size=0.2, random_state=42)\n        train_df = pd.concat([train_df, label_train])\n        val_df = pd.concat([val_df, label_val])\n    \n    # 创建数据集和数据加载器 \n    train_dataset = AudioDataset(train_df, resize=run.config.resize, train_mode=True)\n    val_dataset = AudioDataset(val_df, resize=run.config.resize, train_mode=False)\n    \n    train_loader = DataLoader(train_dataset, batch_size=run.config.batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n    \n    # 创建模型\n    num_classes = len(df['label'].unique())  # 根据实际分类数量设置\n    model = AudioClassifier(num_classes).to(device)\n    \n    # 定义损失函数和优化器\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(\n        model.parameters(), \n        lr=run.config.learning_rate,\n        weight_decay=run.config.weight_decay\n    )  # Adam优化器\n    \n    # 添加学习率调度器\n    scheduler = optim.lr_scheduler.StepLR(\n        optimizer,\n        step_size=10,  # 在第10个epoch衰减\n        gamma=0.1,     # 衰减率为0.1\n        verbose=True\n    )\n    \n    # 训练模型\n    train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, \n                num_epochs=run.config.num_epochs, device=device, run=run)\n    \n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n![alt text](/assets/examples/audio_classification/example-audio-classification-9.png)\n\n可以看到提升的非常明显\n\n> 期待有训练师能把eval acc刷上90！",
    "88": "一级标题：音频分类\n二级标题：8. 相关链接\n内容：\n* Github：[https://github.com/Zeyi-Lin/PyTorch-Audio-Classification](https://github.com/Zeyi-Lin/PyTorch-Audio-Classification)\n* 数据集：[https://pan.baidu.com/s/14CTI_9MD1vXCqyVxmAbeMw?pwd=1a9e](https://pan.baidu.com/s/14CTI_9MD1vXCqyVxmAbeMw?pwd=1a9e) 提取码: 1a9e\n* SwanLab实验日志：[https://swanlab.cn/@ZeyiLin/PyTorch\\_Audio\\_Classification-simple/charts](https://swanlab.cn/@ZeyiLin/PyTorch\\_Audio\\_Classification-simple/charts)\n* 更多实验日志：[https://swanlab.cn/@ZeyiLin/PyTorch\\_Audio\\_Classification/charts](https://swanlab.cn/@ZeyiLin/PyTorch\\_Audio\\_Classification/charts)\n* SwanLab官网：[https://swanlab.cn](https://swanlab.cn)",
    "89": "一级标题：BERT文本分类\n二级标题：无\n内容：\n:::info\n自然语言处理、文本分类、机器学习入门\n:::\n\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/BERT/charts)\n\n[在线Demo](https://swanlab.cn/@ZeyiLin/BERT/charts) ｜ [知乎](https://zhuanlan.zhihu.com/p/699441531)  | [美团外卖评论分类](https://zhuanlan.zhihu.com/p/701460910)",
    "90": "一级标题：BERT文本分类\n二级标题：概述\n内容：\n**BERT**（Bidirectional Encoder Representations from Transformers）是由Google提出的一种自然语言处理预训练模型，广泛应用于各种自然语言处理任务。BERT 通过在大规模语料库上进行预训练，能够捕捉词汇之间的上下文关系，从而在很多任务上取得了优秀的效果。\n\n在这个任务中，我们将使用 BERT 模型对 IMDB 电影评论进行情感分类，具体来说是将电影评论分类为“正面”或“负面”。\n\n![IMDB](/assets/example-bert-1.png)\n\n**IMDB 电影评论数据集**包含50,000条电影评论，分为25,000条训练数据和25,000条测试数据，每部分又包含50%正面评论和50%负面评论。我们将使用预训练的 BERT 模型，通过微调(finetuning)的方式，来对这些评论进行情感分类。",
    "91": "一级标题：BERT文本分类\n二级标题：环境安装\n内容：\n本案例基于`Python>=3.8`，请在您的计算机上安装好Python。 环境依赖：\n\n```txt\ntransformers\ndatasets\nswanlab\n```\n\n快速安装命令：\n\n```bash\npip install transformers datasets swanlab\n```\n\n> 本文的代码测试于transformers==4.41.0、datasets==2.19.1、swanlab==0.3.3",
    "92": "一级标题：BERT文本分类\n二级标题：完整代码\n内容：\n```python\n\"\"\"\n用预训练的Bert模型微调IMDB数据集，并使用SwanLabCallback回调函数将结果上传到SwanLab。\nIMDB数据集的1是positive，0是negative。\n\"\"\"\n\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom swanlab.integration.transformers import SwanLabCallback\nimport swanlab\n\ndef predict(text, model, tokenizer, CLASS_NAME):\n    inputs = tokenizer(text, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        predicted_class = torch.argmax(logits).item()\n\n    print(f\"Input Text: {text}\")\n    print(f\"Predicted class: {int(predicted_class)} {CLASS_NAME[int(predicted_class)]}\")\n    return int(predicted_class)\n\n# 加载IMDB数据集\ndataset = load_dataset('imdb')\n\n# 加载预训练的BERT tokenizer\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n\n# 定义tokenize函数\ndef tokenize(batch):\n    return tokenizer(batch['text'], padding=True, truncation=True)\n\n# 对数据集进行tokenization\ntokenized_datasets = dataset.map(tokenize, batched=True)\n\n# 设置模型输入格式\ntokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\ntokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n\n# 加载预训练的BERT模型\nmodel = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n\n# 设置训练参数\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    eval_strategy='epoch',\n    save_strategy='epoch',\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    logging_first_step=100,\n    # 总的训练轮数\n    num_train_epochs=3,\n    weight_decay=0.01,\n    report_to=\"none\",\n    # 单卡训练\n)\n\nCLASS_NAME = {0: \"negative\", 1: \"positive\"}\n\n# 设置swanlab回调函数\nswanlab_callback = SwanLabCallback(project='BERT',\n                                   experiment_name='BERT-IMDB',\n                                   config={'dataset': 'IMDB', \"CLASS_NAME\": CLASS_NAME})\n\n# 定义Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['test'],\n    callbacks=[swanlab_callback],\n)\n\n# 训练模型\ntrainer.train()\n\n# 保存模型\nmodel.save_pretrained('./sentiment_model')\ntokenizer.save_pretrained('./sentiment_model')\n\n# 测试模型\ntest_reviews = [\n    \"I absolutely loved this movie! The storyline was captivating and the acting was top-notch. A must-watch for everyone.\",\n    \"This movie was a complete waste of time. The plot was predictable and the characters were poorly developed.\",\n    \"An excellent film with a heartwarming story. The performances were outstanding, especially the lead actor.\",\n    \"I found the movie to be quite boring. It dragged on and didn't really go anywhere. Not recommended.\",\n    \"A masterpiece! The director did an amazing job bringing this story to life. The visuals were stunning.\",\n    \"Terrible movie. The script was awful and the acting was even worse. I can't believe I sat through the whole thing.\",\n    \"A delightful film with a perfect mix of humor and drama. The cast was great and the dialogue was witty.\",\n    \"I was very disappointed with this movie. It had so much potential, but it just fell flat. The ending was particularly bad.\",\n    \"One of the best movies I've seen this year. The story was original and the performances were incredibly moving.\",\n    \"I didn't enjoy this movie at all. It was confusing and the pacing was off. Definitely not worth watching.\"\n]\n\nmodel.to('cpu')\ntext_list = []\nfor review in test_reviews:\n    label = predict(review, model, tokenizer, CLASS_NAME)\n    text_list.append(swanlab.Text(review, caption=f\"{label}-{CLASS_NAME[label]}\"))\n\nif text_list:\n    swanlab.log({\"predict\": text_list})\n\nswanlab.finish()\n```",
    "93": "一级标题：BERT文本分类\n二级标题：演示效果\n内容：\n![](/assets/example-bert-2.png)",
    "94": "一级标题：猫狗分类\n二级标题：无\n内容：\n:::info\n图像分类、机器学习入门、RGB图像、自定义数据集\n:::\n\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/Cats_Dogs_Classification/runs/jzo93k112f15pmx14vtxf/chart)\n\n[知乎](https://zhuanlan.zhihu.com/p/676430630)\n\n猫狗分类是计算机视觉最基础的任务之一——如果说完成MNIST手写体识别是实现CV的“Hello World”，那猫狗分类就是旅程的下一站～。这篇文章我将带大家使用SwanLab、PyTorch、Gradio三个开源工具，完成从数据集准备、代码编写、可视化训练到构建Demo网页的全过程。\n\n![](./cats_dogs/01.png)\n\n- 实验过程可看这个网页：[猫狗分类｜SwanLab](https://swanlab.cn/@ZeyiLin/Cats_Dogs_Classification/runs/jzo93k112f15pmx14vtxf/chart)\n- 代码：[Github](https://github.com/Zeyi-Lin/Resnet50-cats_vs_dogs) \n- 在线Demo：[HuggingFace](https://huggingface.co/spaces/TheEeeeLin/Resnet50-cats_vs_dogs)\n- 数据集：[百度云](https://pan.baidu.com/s/1qYa13SxFM0AirzDyFMy0mQ) 提取码: 1ybm\n- 三个开源库：[SwanLab](https://github.com/swanhubx/swanlab)、[Gradio](https://github.com/gradio-app/gradio)、[PyTorch](https://github.com/pytorch/pytorch)",
    "95": "一级标题：猫狗分类\n二级标题：1. 准备部分\n内容：\n### 1.1 安装Python库\n需要安装下面这4个库：\n```bash\ntorch>=1.12.0\ntorchvision>=0.13.0\nswanlab\ngradio\n```\n安装命令：\n```bash\npip install torch>=1.12.0 torchvision>=0.13.0 swanlab gradio\n```\n\n### 1.2 创建文件目录\n现在打开1个文件夹，新建下面这5个文件：\n\n![在这里插入图片描述](./cats_dogs/02.png)\n\n它们各自的作用分别是： \n| 文件 | 用途 |\n| --- | --- |\n| `checkpoint` | 这个文件夹用于存储训练过程中生成的模型权重。 |\n| `datasets` | 这个文件夹用于放置数据集。 |\n| `app.py` | 运行Gradio Demo的Python脚本。 |\n| `load_datasets.py` | 负责载入数据集，包含了数据的预处理、加载等步骤，确保数据以适当的格式提供给模型使用。 |\n| `train.py` | 模型训练的核心脚本。它包含了模型的载入、训练循环、损失函数的选择、优化器的配置等关键组成部分，用于指导如何使用数据来训练模型。 |\n\n### 1.3 下载猫狗分类数据集\n\n数据集来源是Modelscope上的[猫狗分类数据集](https://modelscope.cn/datasets/tany0699/cats_and_dogs/summary)，包含275张图像的数据集和70张图像的测试集，一共不到10MB。\n我对数据做了一些整理，所以更推荐使用下面的百度网盘链接下载：\n> 百度网盘：链接: https://pan.baidu.com/s/1qYa13SxFM0AirzDyFMy0mQ 提取码: 1ybm\n\n![在这里插入图片描述](./cats_dogs/03.png)\n\n将数据集放入`datasets`文件夹：\n\n![在这里插入图片描述](./cats_dogs/04.png)\n\nok，现在我们开始训练部分！\n> ps：如果你想要用更大规模的数据来训练猫狗分类模型，请前往文末的相关链接。",
    "96": "一级标题：猫狗分类\n二级标题：2. 训练部分\n内容：\nps：如果想直接看完整代码和效果，可直接跳转到第2.9。\n\n### 2.1 load_datasets.py\n我们首先需要创建1个类`DatasetLoader`，它的作用是完成数据集的读取和预处理，我们将它写在`load_datasets.py`中。 \n在写这个类之前，先分析一下数据集。\n在datasets目录下，`train.csv`和`val.csv`分别记录了训练集和测试集的图像相对路径（第一列是图像的相对路径，第二列是标签，0代表猫，1代表狗）： \n![在这里插入图片描述](./cats_dogs/05.png)\n![左图作为train.csv，右图为train文件夹中的cat文件夹中的图像](./cats_dogs/06.png)\n左图作为train.csv，右图为train文件夹中的cat文件夹中的图像。\n\n那么我们的目标就很明确： \n1. 解析这两个csv文件，获取图像相对路径和标签 \n2. 根据相对路径读取图像\n3. 对图像做预处理\n4. 返回预处理后的图像和对应标签\n\n明确了目标后，现在我们开始写`DatasetLoader`类：\n\n```python\nimport csv\nimport os\nfrom torchvision import transforms\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n\nclass DatasetLoader(Dataset):\n    def __init__(self, csv_path):\n        self.csv_file = csv_path\n        with open(self.csv_file, 'r') as file:\n            self.data = list(csv.reader(file))\n\n        self.current_dir = os.path.dirname(os.path.abspath(__file__))\n\n    def preprocess_image(self, image_path):\n        full_path = os.path.join(self.current_dir, 'datasets', image_path)\n        image = Image.open(full_path)\n        image_transform = transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n        return image_transform(image)\n\n    def __getitem__(self, index):\n        image_path, label = self.data[index]\n        image = self.preprocess_image(image_path)\n        return image, int(label)\n\n    def __len__(self):\n        return len(self.data)\n   ```\n   \n`DatasetLoader`类由四个部分组成：\n1. `__init__`：包含1个输入参数csv_path，在外部传入`csv_path`后，将读取后的数据存入`self.data`中。`self.current_dir`则是获取了当前代码所在目录的绝对路径，为后续读取图像做准备。\n\n2. `preprocess_image`：此函数用于图像预处理。首先，它构造图像文件的绝对路径，然后使用PIL库打开图像。接着，定义了一系列图像变换：调整图像大小至256x256、转换图像为张量、对图像进行标准化处理，最终，返回预处理后的图像。\n\n3. `__getitem__`：当数据集类被循环调用时，`__getitem__`方法会返回指定索引index的数据，即图像和标签。首先，它根据索引从`self.data`中取出图像路径和标签。然后，调用`preprocess_image`方法来处理图像数据。最后，将处理后的图像数据和标签转换为整型后返回。\n\n4. `__len__`：用于返回数据集的总图像数量。\n\n### 2.2 载入数据集\n> 从本节开始，代码将写在train.py中。\n```python\nfrom torch.utils.data import DataLoader\nfrom load_datasets import DatasetLoader\n\nbatch_size = 8\n\nTrainDataset = DatasetLoader(\"datasets/train.csv\")\nValDataset = DatasetLoader(\"datasets/val.csv\")\nTrainDataLoader = DataLoader(TrainDataset, batch_size=batch_size, shuffle=True)\nValDataLoader = DataLoader(ValDataset, batch_size=batch_size, shuffle=False)\n```\n\n我们传入那两个csv文件的路径实例化`DatasetLoader`类，然后用PyTorch的`DataLoader`做一层封装。`DataLoader`可以再传入两个参数：\n- `batch_size`：定义了每个数据批次包含多少张图像。在深度学习中，我们通常不会一次性地处理所有数据，而是将数据划分为小批次。这有助于模型更快地学习，并且还可以节省内存。在这里我们定义batch_size = 8，即每个批次将包含8个图像。\n- `shuffle`：定义了是否在每个循环轮次（epoch）开始时随机打乱数据。这通常用于训练数据集以保证每个epoch的数据顺序不同，从而帮助模型更好地泛化。如果设置为True，那么在每个epoch开始时，数据将被打乱。在这里我们让训练时打乱，测试时不打乱。\n\n### 2.3 载入ResNet50模型\n\n模型我们选用经典的**ResNet50**，模型的具体原理本文就不细说了，重点放在工程实现上。\n我们使用**torchvision**来创建1个resnet50模型，并载入在Imagenet1k数据集上预训练好的权重：\n\n```python\nfrom torchvision.models import ResNet50_Weights\n\n# 加载预训练的ResNet50模型\nmodel = torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n```\n\n因为猫狗分类是个2分类任务，而torchvision提供的resnet50默认是1000分类，所以我们需要把模型最后的全连接层的输出维度替换为2：\n\n```python\nfrom torchvision.models import ResNet50_Weights\n\nnum_classes=2\n\n# 加载预训练的ResNet50模型\nmodel = torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n\n# 将全连接层的输出维度替换为num_classes\nin_features = model.fc.in_features\nmodel.fc = torch.nn.Linear(in_features, num_classes)\n```\n\n### 2.4 设置cuda/mps/cpu\n如果你的电脑是**英伟达显卡**，那么cuda可以极大加速你的训练；\n如果你的电脑是**Macbook Apple Sillicon（M系列芯片）**，那么mps同样可以极大加速你的训练；\n如果都不是，那就选用cpu：\n```python\n#检测是否支持mps\ntry:\n    use_mps = torch.backends.mps.is_available()\nexcept AttributeError:\n    use_mps = False\n\n#检测是否支持cuda\nif torch.cuda.is_available():\n    device = \"cuda\"\nelif use_mps:\n    device = \"mps\"\nelse:\n    device = \"cpu\"\n```\n\n将模型加载到对应的device中：\n\n```python\nmodel.to(torch.device(device))\n```\n\n### 2.5 设置超参数、优化器、损失函数\n\n**超参数**\n设置训练轮次为20轮，学习率为1e-4，训练批次为8，分类数为2分类。\n\n```python\nnum_epochs = 20\nlr = 1e-4\nbatch_size = 8\nnum_classes = 2\n```\n### 损失函数与优化器\n设置损失函数为交叉熵损失，优化器为Adam。\n\n```python\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n```\n\n### 2.6 初始化SwanLab\n\n在训练中我们使用`swanlab`库作为实验管理与指标可视化工具。\n[swanlab](https://github.com/SwanHubX/SwanLab)是一个类似Tensorboard的开源训练图表可视化库，有着更轻量的体积与更友好的API，除了能记录指标，还能自动记录训练的logging、硬件环境、Python环境、训练时间等信息。\n\n![在这里插入图片描述](./cats_dogs/07.png)\n\n#### 2.6.1 设置初始化配置参数\nswanlab库使用`swanlab.init`设置实验名、实验介绍、记录超参数以及日志文件的保存位置。\n后续打开可视化看板需要根据日志文件完成。\n\n```python\nimport swanlab\n\nswanlab.init(\n    # 设置实验名\n    experiment_name=\"ResNet50\",\n    # 设置实验介绍\n    description=\"Train ResNet50 for cat and dog classification.\",\n    # 记录超参数\n    config={\n        \"model\": \"resnet50\",\n        \"optim\": \"Adam\",\n        \"lr\": lr,\n        \"batch_size\": batch_size,\n        \"num_epochs\": num_epochs,\n        \"num_class\": num_classes,\n        \"device\": device,\n    }\n)\n```\n\n#### 2.6.2 跟踪关键指标\nswanlab库使用`swanlab.log`来记录关键指标，具体使用案例见2.7和2.8。\n\n### 2.7 训练函数\n\n我们定义1个训练函数`train`：\n```python\ndef train(model, device, train_dataloader, optimizer, criterion, epoch):\n    model.train()\n    for iter, (inputs, labels) in enumerate(train_loader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        print('Epoch [{}/{}], Iteration [{}/{}], Loss: {:.4f}'.format(epoch, num_epochs, iter + 1, len(TrainDataLoader),\n                                                                      loss.item()))\n        swanlab.log({\"train_loss\": loss.item()})\n```\n\n训练的逻辑很简单：我们循环调用`train_dataloader`，每次取出1个batch_size的图像和标签，传入到resnet50模型中得到预测结果，将结果和标签传入损失函数中计算交叉熵损失，最后根据损失计算反向传播，Adam优化器执行模型参数更新，循环往复。\n在训练中我们最关心的指标是损失值`loss`，所以我们用`swanlab.log`跟踪它的变化。\n  \n### 2.8 测试函数\n我们定义1个测试函数`test`：\n```python\ndef test(model, device, test_dataloader, epoch):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in test_dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    accuracy = correct / total * 100\n    print('Accuracy: {:.2f}%'.format(accuracy))\n    swanlab.log({\"test_acc\": accuracy})\n```\n\n测试的逻辑同样很简单：我们循环调用`test_dataloader`，将测试集的图像传入到resnet50模型中得到预测结果，与标签进行对比，计算整体的准确率。\n在测试中我们最关心的指标是准确率`accuracy`，所以我们用`swanlab.log`跟踪它的变化。\n\n### 2.9 完整训练代码\n\n我们一共训练`num_epochs`轮，每4轮进行测试，并在最后保存权重文件：\n\n```python\nfor epoch in range(1, num_epochs + 1):\n    train(model, device, TrainDataLoader, optimizer, criterion, epoch)\n    if epoch % 4 == 0: \n        accuracy = test(model, device, ValDataLoader, epoch)\n\nif not os.path.exists(\"checkpoint\"):\n    os.makedirs(\"checkpoint\")\ntorch.save(model.state_dict(), 'checkpoint/latest_checkpoint.pth')\nprint(\"Training complete\")\n```\n\n组合后的完整`train.py`代码：\n\n```python\nimport torch\nimport torchvision\nfrom torchvision.models import ResNet50_Weights\nimport swanlab\nfrom torch.utils.data import DataLoader\nfrom load_datasets import DatasetLoader\nimport os\n\n# 定义训练函数\ndef train(model, device, train_dataloader, optimizer, criterion, epoch):\n    model.train()\n    for iter, (inputs, labels) in enumerate(train_dataloader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        print('Epoch [{}/{}], Iteration [{}/{}], Loss: {:.4f}'.format(epoch, num_epochs, iter + 1, len(TrainDataLoader),\n                                                                      loss.item()))\n        swanlab.log({\"train_loss\": loss.item()})\n\n\n# 定义测试函数\ndef test(model, device, test_dataloader, epoch):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in test_dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    accuracy = correct / total * 100\n    print('Accuracy: {:.2f}%'.format(accuracy))\n    swanlab.log({\"test_acc\": accuracy})\n\n\nif __name__ == \"__main__\":\n    num_epochs = 20\n    lr = 1e-4\n    batch_size = 8\n    num_classes = 2\n    \n    # 设置device\n    try:\n        use_mps = torch.backends.mps.is_available()\n    except AttributeError:\n        use_mps = False\n\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    elif use_mps:\n        device = \"mps\"\n    else:\n        device = \"cpu\"\n\n    # 初始化swanlab\n    swanlab.init(\n        experiment_name=\"ResNet50\",\n        description=\"Train ResNet50 for cat and dog classification.\",\n        config={\n            \"model\": \"resnet50\",\n            \"optim\": \"Adam\",\n            \"lr\": lr,\n            \"batch_size\": batch_size,\n            \"num_epochs\": num_epochs,\n            \"num_class\": num_classes,\n            \"device\": device,\n        }\n    )\n\n    TrainDataset = DatasetLoader(\"datasets/train.csv\")\n    ValDataset = DatasetLoader(\"datasets/val.csv\")\n    TrainDataLoader = DataLoader(TrainDataset, batch_size=batch_size, shuffle=True)\n    ValDataLoader = DataLoader(ValDataset, batch_size=batch_size, shuffle=False)\n\n    # 载入ResNet50模型\n    model = torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n\n    # 将全连接层替换为2分类\n    in_features = model.fc.in_features\n    model.fc = torch.nn.Linear(in_features, num_classes)\n\n    model.to(torch.device(device))\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    \n    # 开始训练\n    for epoch in range(1, num_epochs + 1):\n        train(model, device, TrainDataLoader, optimizer, criterion, epoch)  # Train for one epoch\n\n        if epoch % 4 == 0:  # Test every 4 epochs\n            accuracy = test(model, device, ValDataLoader, epoch)\n    \n    # 保存权重文件\n    if not os.path.exists(\"checkpoint\"):\n        os.makedirs(\"checkpoint\")\n    torch.save(model.state_dict(), 'checkpoint/latest_checkpoint.pth')\n    print(\"Training complete\")\n```\n   \n### 2.10 开始训练！\n\n🔥实验过程可看这个网页：[猫狗分类｜SwanLab](https://swanlab.cn/@ZeyiLin/Cats_Dogs_Classification/runs/jzo93k112f15pmx14vtxf/chart)\n如果你第一次使用SwanLab，你需要先登录账号，在终端输入：\n\n```bash\nswanlab login\n```\n\n会让你填一个API Key，去[SwanLab](https://swanlab.cn)官网登录一下账号，在设置页面复制API Key，粘贴过来就可以：\n\n![在这里插入图片描述](./cats_dogs/08.png)\n\n然后，我们运行`train.py`：    \n\n![在这里插入图片描述](./cats_dogs/09.png)\n\n这时候你会在看到在开头会给到你两个链接，我们点击第一个，里面包含了这个项目的信息和一个对比实验表格：\n\n![在这里插入图片描述](./cats_dogs/10.png)\n\n我们点开1个进行中的实验，会看到train_loss和test_acc整体的变化曲线，以及我们测试集里的图像和它们对应的预测标签：\n\n![在这里插入图片描述](./cats_dogs/11.png)\n\n切换到实验卡片，这里记录了实验的各种信息，包括超参数、最终的实验指标、实验状态、训练时长、Git仓库链接、主机名、操作系统、Python版本、硬件配置等等。\n\n![在这里插入图片描述](./cats_dogs/12.png)\n\n可以看到模型在中已经达到了100%的测试准确率，但是在最后反而拉了——这可能因为过拟合、也可能是常规的波动，就看后续如何优化啦～\n![在这里插入图片描述](./cats_dogs/13.png)",
    "97": "一级标题：猫狗分类\n二级标题：3. Gradio演示程序\n内容：\nGradio是一个开源的Python库，旨在帮助数据科学家、研究人员和从事机器学习领域的开发人员快速创建和共享用于机器学习模型的用户界面。\n在这里我们使用Gradio来构建一个猫狗分类的Demo界面，编写`app.py`程序：\n```python\nimport gradio as gr\nimport torch\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nimport torchvision\n\n\n# 加载与训练中使用的相同结构的模型\ndef load_model(checkpoint_path, num_classes):\n    # 加载预训练的ResNet50模型\n    try:\n        use_mps = torch.backends.mps.is_available()\n    except AttributeError:\n        use_mps = False\n\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    elif use_mps:\n        device = \"mps\"\n    else:\n        device = \"cpu\"\n\n    model = torchvision.models.resnet50(weights=None)\n    in_features = model.fc.in_features\n    model.fc = torch.nn.Linear(in_features, num_classes)\n    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n    model.eval()  # Set model to evaluation mode\n    return model\n\n\n# 加载图像并执行必要的转换的函数\ndef process_image(image, image_size):\n    # Define the same transforms as used during training\n    preprocessing = transforms.Compose([\n        transforms.Resize((image_size, image_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n    image = preprocessing(image).unsqueeze(0)\n    return image\n\n\n# 预测图像类别并返回概率的函数\ndef predict(image):\n    classes = {'0': 'cat', '1': 'dog'}  # Update or extend this dictionary based on your actual classes\n    image = process_image(image, 256)  # Using the image size from training\n    with torch.no_grad():\n        outputs = model(image)\n        probabilities = F.softmax(outputs, dim=1).squeeze()  # Apply softmax to get probabilities\n    # Mapping class labels to probabilities\n    class_probabilities = {classes[str(i)]: float(prob) for i, prob in enumerate(probabilities)}\n    return class_probabilities\n\n\n# 定义到您的模型权重的路径\ncheckpoint_path = 'checkpoint/latest_checkpoint.pth'\nnum_classes = 2\nmodel = load_model(checkpoint_path, num_classes)\n\n# 定义Gradio Interface\niface = gr.Interface(\n    fn=predict,\n    inputs=gr.Image(type=\"pil\"),\n    outputs=gr.Label(num_top_classes=num_classes),\n    title=\"Cat vs Dog Classifier\",\n)\n\nif __name__ == \"__main__\":\n    iface.launch()\n```\n\n运行程序后，会出现以下输出：\n\n![在这里插入图片描述](./cats_dogs/14.png)\n点开链接，出现猫狗分类的Demo网页：\n\n![在这里插入图片描述](./cats_dogs/15.png)\n\n用猫和狗的图片试试：\n\n![在这里插入图片描述](./cats_dogs/16.png)\n\n![在这里插入图片描述](./cats_dogs/17.png)\n\n效果很完美！\n\n至此，我们完成了用PyTorch、SwanLab、Gradio三个开源工具训练1个猫狗分类模型的全部过程，更多想了解的可以参考相关链接或评论此文章。\n\n如果有帮助，请点个赞和收藏吧～",
    "98": "一级标题：猫狗分类\n二级标题：4. 相关链接\n内容：\n- 在线看实验过程：[猫狗分类 · SwanLab](https://swanlab.cn/@ZeyiLin/Cats_Dogs_Classification/runs/jzo93k112f15pmx14vtxf/chart)\n- SwanLab：[Github](https://github.com/SwanHubX/SwanLab)\n- 猫狗分类代码：[Github](https://github.com/xiaolin199912/Resnet50-cats_vs_dogs)\n- 在线Demo：[HuggingFace](https://huggingface.co/spaces/TheEeeeLin/Resnet50-cats_vs_dogs)\n- 猫狗分类数据集（300张图像）：[ModelScope](https://modelscope.cn/datasets/tany0699/cats_and_dogs/summary)\n  - 百度云下载：[链接](https://pan.baidu.com/s/1qYa13SxFM0AirzDyFMy0mQ) 提取码: 1ybm\n- 猫狗分类数据集（10k张图像）：[ModelScope](https://modelscope.cn/datasets/XCsunny/cat_vs_dog_class/summary)",
    "99": "一级标题：CIFAR10 图像分类\n二级标题：无\n内容：\n:::info\n图像分类、机器学习入门\n:::\n\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/CIFAR10/runs/5q3sh20ni2zs6p28ja8qm/chart)",
    "100": "一级标题：CIFAR10 图像分类\n二级标题：概述\n内容：\nCIFAR-10是一个经典的图像分类数据集，包含60,000张32×32像素的彩色图像，分为10个类别（如飞机、汽车、鸟类等），其中50,000张用于训练，10,000张用于测试。\n\n![](./cifar10/dataset.png)\n\nCIFAR-10常被用于图像分类训练任务。该任务是构建模型对输入图像进行10分类，输出每个类别的概率。由于图像分辨率低、背景复杂且数据量有限，该数据集常被用于测试模型的泛化能力和特征提取效果，成为深度学习入门基准。典型方法包括CNN（如ResNet、AlexNet），配合数据增强和交叉熵损失优化，最高准确率可达95%以上。CIFAR-10的轻量级特性使其广泛用于教学和研究，并衍生出更复杂的变体（如CIFAR-100）。\n\nCIFAR-10 包含来自 10 个类别的图像。这些类别包括：\n\n- 飞机 (airplane)\n- 汽车 (automobile)\n- 鸟类 (bird)\n- 猫 (cat)\n- 鹿 (deer)\n- 狗 (dog)\n- 青蛙 (frog)\n- 马 (horse)\n- 船 (ship)\n- 卡车 (truck)\n\n本案例主要：\n\n- 使用`pytorch`进行[ResNet50](https://arxiv.org/abs/1512.03385)(残差神经网络)网络的构建、模型训练与评估\n- 使用`swanlab` 跟踪超参数、记录指标和可视化监控整个训练周期",
    "101": "一级标题：CIFAR10 图像分类\n二级标题：环境安装\n内容：\n本案例基于`Python>=3.8`，请在您的计算机上安装好Python。\n环境依赖：\n```\ntorch\ntorchvision\nswanlab\n```\n快速安装命令：\n```bash\npip install torch torchvision swanlab\n```",
    "102": "一级标题：CIFAR10 图像分类\n二级标题：完整代码\n内容：\n```python\nimport os\nimport random\nimport numpy as np\nimport torch\nfrom torch import nn, optim, utils\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.transforms import ToTensor, Compose, Resize, Lambda\nimport swanlab\n\ndef set_seed(seed=42):\n    \"\"\"设置所有随机种子以确保可重复性\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    # 设置CUDA的随机种子\n    if torch.cuda.is_available():\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n# 捕获并可视化前20张图像\ndef log_images(loader, num_images=16):\n    images_logged = 0\n    logged_images = []\n    for images, labels in loader:\n        # images: batch of images, labels: batch of labels\n        for i in range(images.shape[0]):\n            if images_logged < num_images:\n                # 使用swanlab.Image将图像转换为wandb可视化格式\n                logged_images.append(swanlab.Image(images[i], caption=f\"Label: {labels[i]}\", size=(128, 128)))\n                images_logged += 1\n            else:\n                break\n        if images_logged >= num_images:\n            break\n    swanlab.log({\"Preview/CIFAR10\": logged_images}) \n\n\nif __name__ == \"__main__\":\n    # 设置随机种子\n    set_seed(42)\n\n    # 设置device\n    try:\n        use_mps = torch.backends.mps.is_available()\n    except AttributeError:\n        use_mps = False\n\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    elif use_mps:\n        device = \"mps\"\n    else:\n        device = \"cpu\"\n\n    # 初始化swanlab\n    run = swanlab.init(\n        project=\"CIFAR10\",\n        experiment_name=\"resnet50-pretrained\",\n        config={\n            \"model\": \"Resnet50\",\n            \"optim\": \"Adam\",\n            \"lr\": 1e-4,\n            \"batch_size\": 32,\n            \"num_epochs\": 5,\n            \"train_dataset_num\": 45000,\n            \"val_dataset_num\": 5000,\n            \"device\": device,\n            \"num_classes\": 10,\n        },\n    )\n\n    # 定义转换：调整大小并转换为3通道\n    transform = Compose([\n        ToTensor(),\n        Resize((224, 224), antialias=True),  # ResNet期望224x224的输入\n        # Lambda(lambda x: x.repeat(3, 1, 1))  # 将单通道转换为3通道\n    ])\n\n    # 设置训练集、验证集和测试集\n    dataset = CIFAR10(os.getcwd(), train=True, download=True, transform=transform)\n    \n    # 确保划分数量正确\n    total_size = len(dataset)  # 应该是50000\n    train_dataset, val_dataset = utils.data.random_split(\n        dataset, \n        [run.config.train_dataset_num, run.config.val_dataset_num],\n        generator=torch.Generator().manual_seed(42)  # 保持划分的随机性一致\n    )\n\n    train_loader = utils.data.DataLoader(train_dataset, batch_size=run.config.batch_size, shuffle=True)\n    val_loader = utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False)\n\n    # 初始化模型、损失函数和优化器\n    if run.config.model == \"Resnet18\":\n        from torchvision.models import resnet18\n        model = resnet18(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n    elif run.config.model == \"Resnet34\":\n        from torchvision.models import resnet34\n        model = resnet34(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n    elif run.config.model == \"Resnet50\":\n        from torchvision.models import resnet50\n        model = resnet50(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n    elif run.config.model == \"Resnet101\":\n        from torchvision.models import resnet101\n        model = resnet101(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n    elif run.config.model == \"Resnet152\":\n        from torchvision.models import resnet152\n        model = resnet152(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n\n    model.to(torch.device(device))\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=run.config.lr)\n\n    # （可选）看一下数据集的前16张图像\n    log_images(train_loader, 8)\n\n    # 开始训练\n    for epoch in range(1, run.config.num_epochs+1):\n        swanlab.log({\"train/epoch\": epoch}, step=epoch)\n        model.train()  # 确保模型处于训练模式\n        train_correct = 0\n        train_total = 0\n        \n        # 训练循环\n        for iter, batch in enumerate(train_loader):\n            x, y = batch\n            x, y = x.to(device), y.to(device)\n            optimizer.zero_grad()\n            output = model(x)\n            loss = criterion(output, y)\n            loss.backward()\n            optimizer.step()\n\n            # 计算训练准确率\n            _, predicted = torch.max(output, 1)\n            train_total += y.size(0)\n            train_correct += (predicted == y).sum().item()\n\n            if iter % 40 == 0:\n                print(\n                    f\"Epoch [{epoch}/{run.config.num_epochs}], Iteration [{iter + 1}/{len(train_loader)}], Loss: {loss.item()}\"\n                )\n                swanlab.log({\"train/loss\": loss.item()}, step=(epoch - 1) * len(train_loader) + iter)\n\n        # 记录每个epoch的训练准确率\n        train_accuracy = train_correct / train_total\n        swanlab.log({\"train/acc\": train_accuracy}, step=(epoch - 1) * len(train_loader) + iter)\n\n        # 评估\n        model.eval()\n        correct = 0\n        total = 0\n        val_loss = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                x, y = batch\n                x, y = x.to(device), y.to(device)\n                output = model(x)\n                # 计算验证损失\n                loss = criterion(output, y)\n                val_loss += loss.item()\n                # 计算验证准确率\n                _, predicted = torch.max(output, 1)\n                total += y.size(0)\n                correct += (predicted == y).sum().item()\n\n        accuracy = correct / total\n        avg_val_loss = val_loss / len(val_loader)\n        swanlab.log({\n            \"val/acc\": accuracy,\n            \"val/loss\": avg_val_loss,\n            }, step=(epoch - 1) * len(train_loader) + iter)\n```",
    "103": "一级标题：CIFAR10 图像分类\n二级标题：切换其他ResNet模型\n内容：\n上面的代码支持切换以下ResNet模型：\n- ResNet18\n- ResNet34\n- ResNet50\n- ResNet101\n- ResNet152\n\n切换方式非常简单，只需要将`config`的`model`参数修改为对应的模型名称即可，如切换为ResNet50：\n\n```python (5)\n    # 初始化swanlab\n    run = swanlab.init(\n        ...\n        config={\n            \"model\": \"Resnet50\",\n        ...\n        },\n    )\n```\n\n- `config`是如何发挥作用的？ 👉 [设置实验配置](/guide_cloud/experiment_track/set-experiment-config)",
    "104": "一级标题：CIFAR10 图像分类\n二级标题：效果演示\n内容：\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/CIFAR10/runs/5q3sh20ni2zs6p28ja8qm/chart)\n\n![](./cifar10/show.png)",
    "105": "一级标题：DQN-CartPole\n二级标题：无\n内容：\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/RL-All-In-One/runs/vjbnl6y3l99k0sqrd0f2s/chart)\n\n\n> 训练过程：[RL-All-In-One](https://swanlab.cn/@ZeyiLin/RL-All-In-One/runs/vjbnl6y3l99k0sqrd0f2s/chart)\n>\n> 代码：[Zeyi-Lin/SwanBook-RL](https://github.com/Zeyi-Lin/SwanBook-RL/blob/main/dqn-cartpole.py)\n>\n> 硬件环境：纯CPU可训，实测M1 Max训练3分30秒",
    "106": "一级标题：DQN-CartPole\n二级标题：一、什么是DQN？\n内容：\nDQN（Deep Q-Network，深度Q网络）是Q-Learning的**深度学习扩展**，通过神经网络替代Q表的方式来解决高维状态空间问题（例如图像输入），开启了**深度强化学习时代**。它在2013年由DeepMind提出，并在**Atari**游戏上取得了突破性表现。\n\n![image](https://imagebed-1301372061.cos.ap-beijing.myqcloud.com/blogs/image-20250207132403-hte9grx.png)\n\n传统的Q-Learning方法很好，但是Q表是个离散的结构，无法处理状态是连续的任务；以及一些状态空间巨大的任务（比如视频游戏），Q表的开销也是无法接受的，所以DQN应运而生。DQN用神经网络（称为QNet）**近似Q函数**，输入状态S，输出所有动作的Q值。\n\n**DQN还做了以下改进：**\n\n1. **经验回放（Experience Replay）** ：存储历史经验(st,at,rt+1,st+1)(st,at,rt+1,st+1)到缓冲区，训练时随机采样，打破数据相关性。\n2. **目标网络（Target Network）** ：使用独立的网络计算目标Q值，减少训练波动。\n3. **端到端训练**：直接从原始输入（如像素）学习，无需人工设计状态特征。\n\n具体DQN原理本文不做过多赘述，结合本文提供的代码和网上其他教程/DeepSeek R1学习，会有更好效果。",
    "107": "一级标题：DQN-CartPole\n二级标题：二、什么是CartPole推车倒立摆任务？\n内容：\n**CartPole（推车倒立摆）**  是强化学习中经典的基准测试任务，因为其直观可视、方便调试、状态和动作空间小等特性，常用于入门教学和算法验证。它的目标是训练一个智能体（agent）通过左右移动小车，使车顶的杆子尽可能长时间保持竖直不倒。\n\n![image](https://imagebed-1301372061.cos.ap-beijing.myqcloud.com/blogs/20250207134541.png)\n\n* **环境**：小车（cart）可以在水平轨道上左右移动，顶部通过关节连接一根自由摆动的杆子（pole）。\n* **目标**：通过左右移动小车，使杆子的倾斜角度不超出阈值（±12°或±15°），同时小车不超出轨道范围（如轨道长度的±2.4单位）。简单理解为，就是杆子不会倒下里，小车不会飞出屏幕。\n* **状态**：状态空间包含4个连续变量，分别是小车位置（x），小车速度（v），杆子角度（θ），杆子角速度（ω）\n* **动作**：动作空间只有2个离线动作，分别是0（向左移动）或1（向右移动）\n* **奖励机制**：每成功保持杆子不倒+1分，目前是让奖励最大化，即杆子永远不倒\n\n使用`gymnasium`库，启动cartpole环境非常容易，下面是一个简单的示例代码：\n\n```python\nimport gymnasium as gym\n\nenv = gym.make(\"CartPole-v1\")\nstate = env.reset()\ndone = False\n\nwhile not done:\n    action = 0 if state[2] < 0 else 1  # 根据杆子角度简单决策\n    next_state, reward, done, _ = env.step(action)\n    state = next_state\n    env.render()\n```",
    "108": "一级标题：DQN-CartPole\n二级标题：三、安装环境\n内容：\n首先你需要1个Python>=3.8的环境，然后安装下面的库：\n\n```txt\nswanlab\ngymnasium\nnumpy\ntorch\npygame\nmoviepy\n```\n\n一键安装命令：\n\n```bash\npip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\npip install swanlab gymnasium numpy torch pygame moviepy\n```",
    "109": "一级标题：DQN-CartPole\n二级标题：四、定义QNet\n内容：\nDQN使用神经网络来近似QLearning中的Q表，这个神经网络被称为QNetwork。\n\nQNetwork的输入是状态向量，输出是动作向量，这里用一个非常简单的神经网络：\n\n```python\nclass QNetwork(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(state_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, action_dim)\n        )\n        self.to(device)  # 将网络移到指定设备\n  \n    def forward(self, x):\n        return self.fc(x)\n```",
    "110": "一级标题：DQN-CartPole\n二级标题：五、定义DQNAgent\n内容：\nDQNAgent定义了一系列强化学习训练的行为，代码略长，我拿部分内容进行解读：\n\n### 初始配置\n\n```python\nclass DQNAgent:\n    def __init__(self, state_dim, action_dim):\n        self.q_net = QNetwork(state_dim, action_dim)       # 当前网络\n        self.target_net = QNetwork(state_dim, action_dim)  # 目标网络\n        self.target_net.load_state_dict(self.q_net.state_dict())  # 将目标网络和当前网络初始化一致，避免网络不一致导致的训练波动\n        self.optimizer = optim.Adam(self.q_net.parameters(), lr=1e-3)\n        self.replay_buffer = deque(maxlen=10000)           # 经验回放缓冲区\n\t\tself.update_target_freq = 100  \n```\n\nDQN会定义2个神经网络，分别是q_net和target_net，结构是完全相同的。训练过程中，target_net负责计算预期值，即 **reward + target_net(next_state).max(1)[0]** ，q_net负责计算当前值，训练时将两个值送到MSELoss里计算差值，反向传播后更新q_net的参数；每过update_target_freq步，将q_net的参数赋给target_net。\n\n优化器使用Adam；经验回访缓冲区是最大长度为10000的队列，用于存储历史经验用于训练。\n\n### 动作选择（ε-贪婪策略）\n\n动作选择的ε-贪婪策略，指的是在当前状态下，选择下一个动作时的两种方式：\n\nA. 随机选择1个动作，这种被称为探索\n\nB. 按照先前训练得到的知识选择动作。\n\n在强化学习训练中，每一步会以epsilon（即ε）的概率选择A，否则选择B：\n\n```python\n    def choose_action(self, state):\n        if np.random.rand() < self.epsilon:\n            return np.random.randint(0, 2)  # CartPole有2个动作（左/右）\n        else:\n            state_tensor = torch.FloatTensor(state).to(device)\n            q_values = self.q_net(state_tensor)\n            return q_values.cpu().detach().numpy().argmax()\n```\n\n在训练中，开始时以高概率随机探索环境，逐渐转向利用学到的知识。",
    "111": "一级标题：DQN-CartPole\n二级标题：六、完整代码\n内容：\n**下面是DQN训练的完整代码，做了这些事：**\n\n1. 开启gymnasium中的CartPole环境\n2. QAgent按照ε-贪婪策略选择动作，更新状态，训练模型更新q_net参数\n3. 每隔固定的步数，同步target_net的参数\n4. 一共训练600轮，每10轮会进行一次评估，并使用swanlab记录参数\n5. 保存评估时最高reward的模型权重\n6. 使用了经验回放与ε衰减策略\n7. 训练完成后，进行测试，并保存测试视频到本地目录下\n\n**完整代码如下：**\n\n```python\nimport gymnasium as gym\nfrom gymnasium.wrappers import RecordVideo\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom collections import deque\nimport random\nimport swanlab\nimport os\n\n# 设置随机数种子\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# 定义Q网络\nclass QNetwork(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(state_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, action_dim)\n        )\n  \n    def forward(self, x):\n        return self.fc(x)\n\n# DQN Agent\nclass DQNAgent:\n    def __init__(self, state_dim, action_dim):\n        self.q_net = QNetwork(state_dim, action_dim)       # 当前网络\n        self.target_net = QNetwork(state_dim, action_dim)  # 目标网络\n        self.target_net.load_state_dict(self.q_net.state_dict())  # 将目标网络和当前网络初始化一致，避免网络不一致导致的训练波动\n        self.best_net = QNetwork(state_dim, action_dim)\n        self.optimizer = optim.Adam(self.q_net.parameters(), lr=1e-3)\n        self.replay_buffer = deque(maxlen=10000)           # 经验回放缓冲区\n        self.batch_size = 64\n        self.gamma = 0.99\n        self.epsilon = 0.1\n        self.update_target_freq = 100  # 目标网络更新频率\n        self.step_count = 0\n        self.best_reward = 0\n        self.best_avg_reward = 0\n        self.eval_episodes = 5  # 评估时的episode数量\n\n    def choose_action(self, state):\n        if np.random.rand() < self.epsilon:\n            return np.random.randint(0, 2)  # CartPole有2个动作（左/右）\n        else:\n            state_tensor = torch.FloatTensor(state)\n            q_values = self.q_net(state_tensor)\n            return q_values.cpu().detach().numpy().argmax()\n\n    def store_experience(self, state, action, reward, next_state, done):\n        self.replay_buffer.append((state, action, reward, next_state, done))\n\n    def train(self):\n        if len(self.replay_buffer) < self.batch_size:\n            return\n      \n        # 从缓冲区随机采样\n        batch = random.sample(self.replay_buffer, self.batch_size)\n        states, actions, rewards, next_states, dones = zip(*batch)\n\n        states = torch.FloatTensor(np.array(states))\n        actions = torch.LongTensor(actions)\n        rewards = torch.FloatTensor(rewards)\n        next_states = torch.FloatTensor(np.array(next_states))\n        dones = torch.FloatTensor(dones)\n\n        # 计算当前Q值\n        current_q = self.q_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n\n        # 计算目标Q值（使用目标网络）\n        with torch.no_grad():\n            next_q = self.target_net(next_states).max(1)[0]\n            target_q = rewards + self.gamma * next_q * (1 - dones)\n\n        # 计算损失并更新网络\n        loss = nn.MSELoss()(current_q, target_q)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        # 定期更新目标网络\n        self.step_count += 1\n        if self.step_count % self.update_target_freq == 0:\n            # 使用深拷贝更新目标网络参数\n            self.target_net.load_state_dict({\n                k: v.clone() for k, v in self.q_net.state_dict().items()\n            })\n\n    def save_model(self, path=\"./output/best_model.pth\"):\n        if not os.path.exists(\"./output\"):\n            os.makedirs(\"./output\")\n        torch.save(self.q_net.state_dict(), path)\n        print(f\"Model saved to {path}\")\n      \n    def evaluate(self, env):\n        \"\"\"评估当前模型的性能\"\"\"\n        original_epsilon = self.epsilon\n        self.epsilon = 0  # 关闭探索\n        total_rewards = []\n\n        for _ in range(self.eval_episodes):\n            state = env.reset()[0]\n            episode_reward = 0\n            while True:\n                action = self.choose_action(state)\n                next_state, reward, done, _, _ = env.step(action)\n                episode_reward += reward\n                state = next_state\n                if done or episode_reward > 2e4:\n                    break\n            total_rewards.append(episode_reward)\n\n        self.epsilon = original_epsilon  # 恢复探索\n        return np.mean(total_rewards)\n\n# 训练过程\nenv = gym.make('CartPole-v1')\nstate_dim = env.observation_space.shape[0]\naction_dim = env.action_space.n\nagent = DQNAgent(state_dim, action_dim)\n\n\n# 初始化SwanLab日志记录器\nswanlab.init(\n    project=\"RL-All-In-One\",\n    experiment_name=\"DQN-CartPole-v1\",\n    config={\n        \"state_dim\": state_dim,\n        \"action_dim\": action_dim,\n        \"batch_size\": agent.batch_size,\n        \"gamma\": agent.gamma,\n        \"epsilon\": agent.epsilon,\n        \"update_target_freq\": agent.update_target_freq,\n        \"replay_buffer_size\": agent.replay_buffer.maxlen,\n        \"learning_rate\": agent.optimizer.param_groups[0]['lr'],\n        \"episode\": 600,\n        \"epsilon_start\": 1.0,\n        \"epsilon_end\": 0.01,\n        \"epsilon_decay\": 0.995,\n    },\n    description=\"增加了初始化目标网络和当前网络一致，避免网络不一致导致的训练波动\"\n)\n\n# ========== 训练阶段 ==========\n\nagent.epsilon = swanlab.config[\"epsilon_start\"]\n\nfor episode in range(swanlab.config[\"episode\"]):\n    state = env.reset()[0]\n    total_reward = 0\n  \n    while True:\n        action = agent.choose_action(state)\n        next_state, reward, done, _, _ = env.step(action)\n        agent.store_experience(state, action, reward, next_state, done)\n        agent.train()\n\n        total_reward += reward\n        state = next_state\n        if done or total_reward > 2e4:\n            break\n  \n    # epsilon是探索系数，随着每一轮训练，epsilon 逐渐减小\n    agent.epsilon = max(swanlab.config[\"epsilon_end\"], agent.epsilon * swanlab.config[\"epsilon_decay\"])  \n  \n    # 每10个episode评估一次模型\n    if episode % 10 == 0:\n        eval_env = gym.make('CartPole-v1')\n        avg_reward = agent.evaluate(eval_env)\n        eval_env.close()\n      \n        if avg_reward > agent.best_avg_reward:\n            agent.best_avg_reward = avg_reward\n            # 深拷贝当前最优模型的参数\n            agent.best_net.load_state_dict({k: v.clone() for k, v in agent.q_net.state_dict().items()})\n            agent.save_model(path=f\"./output/best_model.pth\")\n            print(f\"New best model saved with average reward: {avg_reward}\")\n\n    print(f\"Episode: {episode}, Train Reward: {total_reward}, Best Eval Avg Reward: {agent.best_avg_reward}\")\n  \n    swanlab.log(\n        {\n            \"train/reward\": total_reward,\n            \"eval/best_avg_reward\": agent.best_avg_reward,\n            \"train/epsilon\": agent.epsilon\n        },\n        step=episode,\n    )\n\n# 测试并录制视频\nagent.epsilon = 0  # 关闭探索策略\ntest_env = gym.make('CartPole-v1', render_mode='rgb_array')\ntest_env = RecordVideo(test_env, \"./dqn_videos\", episode_trigger=lambda x: True)  # 保存所有测试回合\nagent.q_net.load_state_dict(agent.best_net.state_dict())  # 使用最佳模型\n\nfor episode in range(3):  # 录制3个测试回合\n    state = test_env.reset()[0]\n    total_reward = 0\n    steps = 0\n  \n    while True:\n        action = agent.choose_action(state)\n        next_state, reward, done, _, _ = test_env.step(action)\n        total_reward += reward\n        state = next_state\n        steps += 1\n      \n        # 限制每个episode最多1500步,约30秒,防止录制时间过长\n        if done or steps >= 1500:\n            break\n  \n    print(f\"Test Episode: {episode}, Reward: {total_reward}\")\n\ntest_env.close()\n```\n\n---\n\n训练用的是SwanLab的记录过程，能更好地分析和总结知识。\n\n在开始训练之前，如果你没有使用过[SwanLab](https://swanlab.cn)，需要去它的官网（[https://swanlab.cn](https://swanlab.cn)）注册一下，然后按下面的步骤复制API Key：\n\n![image](https://imagebed-1301372061.cos.ap-beijing.myqcloud.com/blogs/20250207150845.png)\n\n接下来打开命令行，敲下面的命令：\n\n```python\nswanlab login\n```\n\n在弹出的提示中，把API Key粘贴进去（粘贴进去不会显示任何东西，放心这是正常的），然后按回车，登录完毕！\n\n然后，就可以运行训练代码了。",
    "112": "一级标题：DQN-CartPole\n二级标题：七、训练结果\n内容：\n训练过程可以看：[RL-All-In-One - SwanLab](https://swanlab.cn/@ZeyiLin/RL-All-In-One/runs/vjbnl6y3l99k0sqrd0f2s/chart)\n\n我的机器是Macbook M1 Max，大概训练了3分30秒。\n\n![image](https://imagebed-1301372061.cos.ap-beijing.myqcloud.com/blogs/20250207151927.png)\n\n可以看到train的reward波动的很厉害，因为随机探索的缘故，但eval（关闭随机探索）可以看到是很快达到了20000分的上限。\n\n下面是训练好的Agent控制倒立摆的30s视频：\n\n<video controls src=\"/assets/rl-video-episode-0.mp4\"></video>",
    "113": "一级标题：FashionMNIST\n二级标题：无\n内容：\n:::info\n图像分类、机器学习入门、灰度图像\n:::\n\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/FashionMNIST/overview)",
    "114": "一级标题：FashionMNIST\n二级标题：概述\n内容：\nFashionMNIST 是一个广泛用于测试机器学习算法的图像数据集，特别是在图像识别领域。它由 Zalando 发布，旨在替代传统的 MNIST 数据集，后者主要包含手写数字的图片。FashionMNIST 的设计初衷是提供一个稍微更具挑战性的问题，同时保持与原始 MNIST 数据集相同的图像大小（28x28 像素）和结构（训练集60,000张图片，测试集10,000张图片）。\n\n![fashion-mnist](/assets/example-fashionmnist.png)\n\nFashionMNIST 包含来自 10 个类别的服装和鞋类商品的灰度图像。这些类别包括：\n\n1. T恤/上衣（T-shirt/top）\n2. 裤子（Trouser）\n3. 套头衫（Pullover）\n4. 裙子（Dress）\n5. 外套（Coat）\n6. 凉鞋（Sandal）\n7. 衬衫（Shirt）\n8. 运动鞋（Sneaker）\n9. 包（Bag）\n10. 短靴（Ankle boot）\n\n每个类别都有相同数量的图像，使得这个数据集成为一个平衡的数据集。这些图像的简单性和标准化尺寸使得 FashionMNIST 成为计算机视觉和机器学习领域入门级的理想选择。数据集被广泛用于教育和研究，用于测试各种图像识别方法的效果。\n\n本案例主要：\n\n- 使用`pytorch`进行[ResNet34](https://arxiv.org/abs/1512.03385)(残差神经网络)网络的构建、模型训练与评估\n- 使用`swanlab` 跟踪超参数、记录指标和可视化监控整个训练周期",
    "115": "一级标题：FashionMNIST\n二级标题：环境安装\n内容：\n本案例基于`Python>=3.8`，请在您的计算机上安装好Python。\n环境依赖：\n```\ntorch\ntorchvision\nswanlab\n```\n快速安装命令：\n```bash\npip install torch torchvision swanlab\n```",
    "116": "一级标题：FashionMNIST\n二级标题：完整代码\n内容：\n```python\nimport os\nimport random\nimport numpy as np\nimport torch\nfrom torch import nn, optim, utils\nfrom torchvision.datasets import FashionMNIST\nfrom torchvision.transforms import ToTensor, Compose, Resize, Lambda\nimport swanlab\n\n\ndef set_seed(seed=42):\n    \"\"\"设置所有随机种子以确保可重复性\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    # 设置CUDA的随机种子\n    if torch.cuda.is_available():\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n# 捕获并可视化前20张图像\ndef log_images(loader, num_images=16):\n    images_logged = 0\n    logged_images = []\n    for images, labels in loader:\n        # images: batch of images, labels: batch of labels\n        for i in range(images.shape[0]):\n            if images_logged < num_images:\n                # 使用swanlab.Image将图像转换为wandb可视化格式\n                logged_images.append(swanlab.Image(images[i], caption=f\"Label: {labels[i]}\", size=(128, 128)))\n                images_logged += 1\n            else:\n                break\n        if images_logged >= num_images:\n            break\n    swanlab.log({\"Preview/FashionMNIST\": logged_images})\n\n\nif __name__ == \"__main__\":\n    # 设置随机种子\n    set_seed(42)\n\n    # 设置device\n    try:\n        use_mps = torch.backends.mps.is_available()\n    except AttributeError:\n        use_mps = False\n\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    elif use_mps:\n        device = \"mps\"\n    else:\n        device = \"cpu\"\n\n    # 初始化swanlab\n    run = swanlab.init(\n        project=\"FashionMNIST\",\n        experiment_name=\"resnet50\",\n        config={\n            \"model\": \"Resnet50\",\n            \"optim\": \"Adam\",\n            \"lr\": 1e-4,\n            \"batch_size\": 32,\n            \"num_epochs\": 10,\n            \"train_dataset_num\": 55000,\n            \"val_dataset_num\": 5000,\n            \"device\": device,\n            \"num_classes\": 10,\n        },\n    )\n\n    # 定义转换：调整大小并转换为3通道\n    transform = Compose([\n        ToTensor(),\n        Resize((224, 224), antialias=True),  # ResNet期望224x224的输入\n        Lambda(lambda x: x.repeat(3, 1, 1))  # 将单通道转换为3通道\n    ])\n\n    # 设置训练集、验证集和测试集\n    dataset = FashionMNIST(os.getcwd(), train=True, download=True, transform=transform)\n    train_dataset, val_dataset = utils.data.random_split(\n        dataset, [run.config.train_dataset_num, run.config.val_dataset_num]\n    )\n\n    train_loader = utils.data.DataLoader(train_dataset, batch_size=run.config.batch_size, shuffle=True)\n    val_loader = utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False)\n\n    # 初始化模型、损失函数和优化器\n    if run.config.model == \"Resnet18\":\n        from torchvision.models import resnet18\n        model = resnet18(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n    elif run.config.model == \"Resnet34\":\n        from torchvision.models import resnet34\n        model = resnet34(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n    elif run.config.model == \"Resnet50\":\n        from torchvision.models import resnet50\n        model = resnet50(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n    elif run.config.model == \"Resnet101\":\n        from torchvision.models import resnet101\n        model = resnet101(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n    elif run.config.model == \"Resnet152\":\n        from torchvision.models import resnet152\n        model = resnet152(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n\n    model.to(torch.device(device))\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=run.config.lr)\n\n    # （可选）看一下数据集的前8张图像\n    log_images(train_loader, 8)\n\n    # 开始训练\n    for epoch in range(1, run.config.num_epochs+1):\n        swanlab.log({\"train/epoch\": epoch}, step=epoch)\n        model.train()  # 确保模型处于训练模式\n        train_correct = 0\n        train_total = 0\n        \n        # 训练循环\n        for iter, batch in enumerate(train_loader):\n            x, y = batch\n            x, y = x.to(device), y.to(device)\n            optimizer.zero_grad()\n            output = model(x)\n            loss = criterion(output, y)\n            loss.backward()\n            optimizer.step()\n\n            # 计算训练准确率\n            _, predicted = torch.max(output, 1)\n            train_total += y.size(0)\n            train_correct += (predicted == y).sum().item()\n\n            if iter % 40 == 0:\n                print(\n                    f\"Epoch [{epoch}/{run.config.num_epochs}], Iteration [{iter + 1}/{len(train_loader)}], Loss: {loss.item()}\"\n                )\n                swanlab.log({\"train/loss\": loss.item()}, step=(epoch - 1) * len(train_loader) + iter)\n\n        # 记录每个epoch的训练准确率\n        train_accuracy = train_correct / train_total\n        swanlab.log({\"train/acc\": train_accuracy}, step=(epoch - 1) * len(train_loader) + iter)\n\n        #\n        model.eval()\n        correct = 0\n        total = 0\n        val_loss = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                x, y = batch\n                x, y = x.to(device), y.to(device)\n                output = model(x)\n                # 计算验证损失\n                loss = criterion(output, y)\n                val_loss += loss.item()\n                # 计算验证准确率\n                _, predicted = torch.max(output, 1)\n                total += y.size(0)\n                correct += (predicted == y).sum().item()\n\n        accuracy = correct / total\n        avg_val_loss = val_loss / len(val_loader)\n        swanlab.log({\n            \"val/acc\": accuracy,\n            \"val/loss\": avg_val_loss,\n            }, step=(epoch - 1) * len(train_loader) + iter)\n```",
    "117": "一级标题：FashionMNIST\n二级标题：切换其他ResNet模型\n内容：\n上面的代码支持切换以下ResNet模型：\n- ResNet18\n- ResNet34\n- ResNet50\n- ResNet101\n- ResNet152\n\n切换方式非常简单，只需要将`config`的`model`参数修改为对应的模型名称即可，如切换为ResNet50：\n\n```python (5)\n    # 初始化swanlab\n    run = swanlab.init(\n        ...\n        config={\n            \"model\": \"Resnet50\",\n        ...\n        },\n    )\n```\n\n- `config`是如何发挥作用的？ 👉 [设置实验配置](/guide_cloud/experiment_track/set-experiment-config)",
    "118": "一级标题：FashionMNIST\n二级标题：效果演示\n内容：\n![](/assets/example-fashionmnist-show.jpg)",
    "119": "一级标题：使用ChatGLM4进行大模型指令遵从微调（附代码和测试脚本）\n二级标题：无\n内容：\n[![](/assets/badge1.svg)](https://swanlab.cn/@ShaohonChen/chatglm-finetune/)\n\n作者：情感机器实验室-陈少宏 邮箱：<shaohon_chen@115lab.club>\n\n[[toc]]",
    "120": "一级标题：使用ChatGLM4进行大模型指令遵从微调（附代码和测试脚本）\n二级标题：摘要\n内容：\n![instruct](./images/glm4-instruct/instruct.png)\n\n本教程主要实现了一个大模型的指令遵从微调方法。为了便于实现，减少代码量，本文使用了🤗HuggingFace的TRL框架实现。该框架除了支持SFT外，对DPO、PPO、GRPO等流行的强化微调算法都有很好的支持。\n\n虽然使用框架能够极大的减少工作量，但是不可避免的为新手学习带来了困扰。因此本教程会尽量附上完整的文档引用来帮助读者进一步学习框架。诚然从使用pytorch实现微调过程能够极大的提升对过程的理解，社区也有相当多优秀的项目。但是笔者仍推荐大家多使用框架来完成训练，这样可以减少大量的时间来让大家更专注于创新。\n\n因此本教程建议对🤗HuggingFace Transformers框架有一定基础的读者阅读～。\n\n注意：由于ChatGLM的模型相对较大，实际运行大概需要显存>=16G\n\n🎉 **SwanLab被官方集成进入了🤗HuggingFace Transformers：** 如果本地环境安装了SwanLab会默认开启！也可以通过`report_to=\"swanlab\"`开启训练跟踪。\n\n![swanlabxhuggingface](./images/glm4-instruct/swanlabxhuggingface.png)\n\n**参考资料：**\n\n* 智谱AI官网：[https://www.zhipuai.cn/](https://www.zhipuai.cn/)\n\n* ChatGLM-9B基座模型：[https://huggingface.co/THUDM/glm-4-9b-hf](https://huggingface.co/THUDM/glm-4-9b-hf/tree/main)\n\n* ChatGLM-9B-Chat模型：[https://huggingface.co/THUDM/glm-4-9b-chat-hf](https://huggingface.co/THUDM/glm-4-9b-chat-hf/tree/main)\n\n* Alpaca数据集中文版：[https://huggingface.co/datasets/llamafactory/alpaca_gpt4_zh](https://huggingface.co/datasets/llamafactory/alpaca_gpt4_zh)\n\n* 本博客开源项目链接：[https://github.com/SwanHubX/glm4-finetune](https://github.com/SwanHubX/glm4-finetune)\n\n* SwanLab训练日志查看：[https://swanlab.cn/@ShaohonChen/chatglm-finetune/](https://swanlab.cn/@ShaohonChen/chatglm-finetune/)",
    "121": "一级标题：使用ChatGLM4进行大模型指令遵从微调（附代码和测试脚本）\n二级标题：TRL包介绍+环境准备\n内容：\n![trl](./images/glm4-instruct/trl.png)\n\n本教程使用[🤗HuggingFace TRL](https://huggingface.co/docs/trl/index)框架来完成微调代码的实现。TRL是一个强大且便于使用的微调框架，除了支持SFT外，也能轻松的通过接口调用DPO、PPO、GRPO等流行的强化微调算法。此外也完美兼容Transformers架构。\n\n首先是安装本教程的环境，安装命令如下：\n\n```bash\npip install transformers trl datasets peft swanlab\n```\n\n其中`transformers trl peft`用于模型的加载和训练，`datasets`用于导入数据集，`swanlab`用于对训练过程可视化跟踪。\n\n下面列举一个简单的微调案例来介绍HF TRL框架的使用方法：\n\n```python\nfrom datasets import load_dataset\nfrom trl import SFTConfig, SFTTrainer\n\ndataset = load_dataset(\"stanfordnlp/imdb\", split=\"train\")   # 设置微调数据集，此处使用IMDB电影评论分类数据\n\ntraining_args = SFTConfig(  # 设置微调参数\n    max_length=512,\n    output_dir=\"/tmp\",\n)\ntrainer = SFTTrainer(   # 设置模型，此处使用facebook的opt-350M，参数量比较小便于下载\n    \"facebook/opt-350m\",\n    train_dataset=dataset,\n    args=training_args,\n)\ntrainer.train() # 开始训练，流程和TRL一样\n```\n\n上面的代码来自HF官方文档[https://huggingface.co/docs/trl/sft_trainer](https://huggingface.co/docs/trl/sft_trainer)，增加了注释便于读者理解。\n\n简单来说TRL包的使用方法和Transformers类似，不过多了两步：\n\n* 导入`SFTConfig`模块，这个模块基于`transformers`的`TrainingArguments`，不过针对SFT引入了一点额外的参数，以及lora的支持参数\n\n* 导入`SFTTrainer`模块，这个模块包含了SFT的代码实现，还有一些对`peft`的lora支持和数据集格式转换代码。\n\n后文将完整的介绍如何使用TRL包完成大模型的指令遵从功能。",
    "122": "一级标题：使用ChatGLM4进行大模型指令遵从微调（附代码和测试脚本）\n二级标题：GLM4介绍+模型准备\n内容：\n![chatglm_history](images/glm4-instruct/chatglm_history.png)\n\nGLM-4-9B是[智谱AI](https://www.zhipuai.cn/)推出的最新一代预训练模型GLM-4系列中的开源版本。ChatGLM发布了多个版本，其中GLM-4-9B是第四代基座模型，其微调版本GLM-4-9B-Chat具备网页浏览、代码执行、自定义工具调用（Function Call）和长文本推理（支持最大 128K 上下文）等高级功能。\n\n本教程使用GLM-4-9B模型进行指令遵从功能微调，并使用SwanLab进行模型的结果跟踪。\n\n⚠️注意：ChatGLM为了配合Huggingface Transformers更新，发布了两个版本权重`THUDM/glm-4-9b`和`THUDM/glm-4-9b-hf`，后者对应更为新版本的transformers，因此本教程使用后者的权重。\n\n本教程以经提供好了下载模型的脚本，下载模型的方法如下：\n\n```bash\nhuggingface-cli download --local-dir ./weights/glm-4-9b-hf THUDM/glm-4-9b-hf\n```\n\n模型将会下载在项目目录下的`./weights/glm-4-9b-hf`中\n\n下面列举一个使用`transformers`加载ChatGLM模型并进行推理的代码：\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ndevice = \"cuda\"\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/glm-4-9b-chat-hf\")\nmodel = AutoModelForCausalLM.from_pretrained(\"THUDM/glm-4-9b-chat-hf\").eval().to(device)\ninputs = tokenizer.encode(\"我是ChatGLM，是\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n```\n\n由于是基座模型，没经过微调，因此模型只会完成`\"我是ChatGLM，是\"`这段文本的后续补全，运行后会生成如下代码：\n\n```bash\nLoading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.35it/s]\n[gMASK]<sop>我是ChatGLM，是人工智能助手。我是ChatGLM，是人工智能助手。我是ChatGLM，是人工智能助手\n```\n\n当然上面的例子是一个基座模型推理的例子，该模型只能进行文本生成，如果希望使用对话能力，还是需要加载已经微调好的对话模型，代码如下：\n\n```python\nfrom transformers import pipeline\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"你是谁\"},\n]\npipe = pipeline(\"text-generation\", model=\"THUDM/glm-4-9b-chat-hf\")\nprint(pipe(messages))\n```\n\n此处我们换了种推理接口，直接使用pipeline完成推理，运行后将会生成如下信息\n\n```bash\nLoading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.24it/s]\nDevice set to use cuda:0\n[{'generated_text': [{'role': 'user', 'content': '你是谁'}, {'role': 'assistant', 'content': '\\n我是一个人工智能助手，名为 ChatGLM。我是基于清华大学 KEG 实验室和'}]}]\n```\n\n使用`print(model)`将模型的结构打印出来，展示如下：\n\n```text\nGlmForCausalLM(\n  (model): GlmModel(\n    (embed_tokens): Embedding(151552, 4096, padding_idx=151329)\n    (layers): ModuleList(\n      (0-39): 40 x GlmDecoderLayer(\n        (self_attn): GlmAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n          (k_proj): Linear(in_features=4096, out_features=256, bias=True)\n          (v_proj): Linear(in_features=4096, out_features=256, bias=True)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n        )\n        (mlp): GlmMLP(\n          (gate_up_proj): Linear(in_features=4096, out_features=27392, bias=False)\n          (down_proj): Linear(in_features=13696, out_features=4096, bias=False)\n          (activation_fn): SiLU()\n        )\n        (input_layernorm): GlmRMSNorm((4096,), eps=1.5625e-07)\n        (post_attention_layernorm): GlmRMSNorm((4096,), eps=1.5625e-07)\n      )\n    )\n    (norm): GlmRMSNorm((4096,), eps=1.5625e-07)\n    (rotary_emb): GlmRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=151552, bias=False)\n)\n```\n\n可以看到GLM模型的层数达到了惊人的40层😂，因此本身使用Lora进行微调时其可训练参数会比其他模型大一些。",
    "123": "一级标题：使用ChatGLM4进行大模型指令遵从微调（附代码和测试脚本）\n二级标题：数据集准备\n内容：\n数据集我已经提前包括在了github项目当中，可以直接使用如下命令下载完整的实验代码\n\n```bash\ngit clone https://github.com/SwanHubX/glm4-finetune.git\n```\n\n如果只想下载数据集，可以直接下载如下文件：\n\n```bash\nwget https://github.com/SwanHubX/glm4-finetune/blob/main/data/alpaca_gpt4_data_zh.json\n```\n\n也可以通过🤗huggingface上下载：[https://huggingface.co/datasets/llamafactory/alpaca_gpt4_zh](https://huggingface.co/datasets/llamafactory/alpaca_gpt4_zh)",
    "124": "一级标题：使用ChatGLM4进行大模型指令遵从微调（附代码和测试脚本）\n二级标题：代码说明+超参数调整\n内容：\n完整的微调代码公开在了GitHub上，使用如下命令即可下载\n\n```bash\ngit clone https://github.com/SwanHubX/glm4-finetune.git\n```\n\n文章的附件中也有完整的实现代码[#代码附件](#附件完整代码)\n\n本文接下来重点介绍各个代码的功能模块\n\n加载模型的超参数设置，这里可以重点关注lora参数的设置，本文lora参数参考了ChatGLM官方微调代码的lora参数设置\n\n这里要注意学习率为5e-4，如果是全量微调要小一个数量级。\n\n```python",
    "125": "# Model kwargs",
    "126": "@dataclass\nclass ChatGLM4ModelConfig(ModelConfig):\n    model_name_or_path: Optional[str] = field(\n        default=\"./weights/glm-4-9b-hf\",\n        metadata={\n            \"help\": \"Model checkpoint for weights initialization. default used glm4\"\n        },\n    )\n    torch_dtype: Optional[str] = field(\n        default=\"bfloat16\",\n        metadata={\n            \"help\": \"Override the default `torch.dtype` and load the model under this dtype.\",\n            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n        },\n    )\n    use_peft: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to use PEFT for training. Default true\"},\n    )\n    lora_r: int = field(\n        default=8,\n        metadata={\"help\": \"LoRA R value.\"},\n    )\n    lora_alpha: int = field(\n        default=32,\n        metadata={\"help\": \"LoRA alpha.\"},\n    )\n    lora_dropout: float = field(\n        default=0.1,\n        metadata={\"help\": \"LoRA dropout.\"},\n    )\n    lora_target_modules: Optional[list[str]] = field(\n        default_factory=lambda: [\"q_proj\", \"k_proj\", \"v_proj\"],\n        metadata={\"help\": \"LoRA target modules.\"},\n    )\n```\n\n数据集超参数设置，这里比较简单，只是加载了本地的数据集\n\n```python",
    "127": "# Datasets kwargs",
    "128": "@dataclass\nclass DataTrainingArguments:\n    data_files: Optional[str] = field(\n        default=\"./data/alpaca_gpt4_data_zh.json.json\",\n        metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"},\n    )\n```\n\n不过为了方便读者理解数据集长什么样，仍旧提供数据集展示脚本\n\n```python\nimport datasets\nraw_dataset=datasets.load_dataset(\"json\", data_files=\"data/glaive_toolcall_zh_1k.json\")\nprint(raw_dataset)\n\"\"\"打印内容\nDatasetDict({\n    train: Dataset({\n        features: ['instruction', 'input', 'output'],\n        num_rows: 42677\n    })\n})\n\"\"\"\n```\n\n可以看到数据一共有1000条，并且包括`'conversations', 'tools'`两个字段\n\n进一步选取其中一条打印：\n\n```python\nprint(raw_dataset[\"train\"][0])\n```\n\n输出如下：\n\n```json\n{\n    \"instruction\": \"保持健康的三个提示。\",\n    \"input\": \"\",\n    \"output\": \"以下是保持健康的三个提示：\\n\\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\\n\\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\\n\\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。\"\n}\n```\n\n这里大家会注意到为什么会有Instruct和input两部分。实际上早期针对指令遵从的研究是为了获得一个通用的任务处理模型（比如既能做翻译又能做计算这样），因此我们通常把对任务的描述放到instruct中，将实际的任务文本放在input中。\n但是随着ChatGPT这种通用的AI助理出现，大家已经逐渐习惯直接下指令让其执行了。因此instruct和prompt的这种分离就显得没那么有必要了。实际上无论分离和不分离模型的本质都是根据前文补后文。因此分离不分离对模型的最终结果不会有太大影响，无非就是格式的不同。\n现在的开源Chat大语言模型流行把“人设”放在“system prompt”中，把用户的指令放在input中，因此后文我们会将Alpaca数据集处理成更适应于主流Chat的格式。\n\nChatGLM提供的推荐输入微调数据结构如下：\n\n```json\n{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"类型#裤*材质#牛仔布*风格#性感\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"3x1的这款牛仔裤采用浅白的牛仔面料为裤身材质，其柔然的手感和细腻的质地，在穿着舒适的同时，透露着清纯甜美的个性气质。除此之外，流畅的裤身剪裁将性感的腿部曲线彰显的淋漓尽致，不失为一款随性出街的必备单品。\"\n    }\n  ]\n}\n```\n\n这里可能有一定经验的读者会说，不对呀，我们从0训练我们当然可以定义自己的数据结构。这么想是对的，但是让我们能够直接使用ChatGLM原生的`chat_template`，我还是建议咱们遵守chatglm官方定义的数据格式，这么做的话既能兼容ChatGLM的很多工具，又能充分利用官方定义的special_token。\n\n我们可以通过HuggingFace上开源的`glm-4-9b-chat-hf`的`tokenizer_config.json`中可以找到他们的原生`chat_template`，下面的脚本提供一个打印`chat_template`的代码\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ndevice = \"cuda\"\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/glm-4-9b-chat-hf\")\nprint(tokenizer.chat_template)\n```\n\n获取tokenizer配置的链接[https://huggingface.co/THUDM/glm-4-9b-chat-hf/blob/main/tokenizer_config.json](https://huggingface.co/THUDM/glm-4-9b-chat-hf/blob/main/tokenizer_config.json)\n\n这里我们简单打印一下转换完成后数据集最终的一个效果，参考脚本如下：\n\n```python\ndef formatting_func(example):\n    \"\"\"\n    process data format\n    \"\"\"\n    prompt = example[\"instruction\"]\n    if len(example[\"input\"]) != 0:\n        prompt += \"\\n\\n\" + example[\"input\"]\n    conversations = [\n        {\"role\": \"user\", \"content\": prompt},\n        {\"role\": \"assistant\", \"content\": example[\"output\"]},\n    ]\n    output_text = tokenizer.apply_chat_template(\n        conversation=conversations, tokenize=False\n    )\n    return output_text\n```\n\n输出效果如下，以下字段便是实际运用于模型微调时，输入给模型的数据样式：\n\n```text\n[gMASK]<sop><|user|>\n保持健康的三个提示。<|assistant|>\n以下是保持健康的三个提示：\n\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\n\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\n\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。\n```\n\n最后便是训练的超参数设置和训练过程的实现，这里由于数据规模比较小，我们训练600个steps，每个GPU实际batch大小为1*4：\n\n```python",
    "129": "# Train kwargs",
    "130": "@dataclass\nclass MySFTConfig(SFTConfig):\n    output_dir: Optional[str] = field(\n        default=\"./output/lora-glm4-9b-alpaca\",\n        metadata={\n            \"help\": \"The output directory where the model predictions and checkpoints will be written. Defaults to 'lora-glm4-9b-toolcall' if not provided.\"\n        },\n    )\n    num_train_epochs: float = field(\n        default=3.0, metadata={\"help\": \"Total number of training epochs to perform.\"}\n    )\n    per_device_train_batch_size: int = field(\n        default=2,\n        metadata={\"help\": \"Batch size per GPU/TPU/MPS/NPU core/CPU for training.\"},\n    )\n    per_device_eval_batch_size: int = field(\n        default=4,\n        metadata={\"help\": \"Batch size per GPU/TPU/MPS/NPU core/CPU for evaluation.\"},\n    )\n    gradient_accumulation_steps: int = field(\n        default=1,\n        metadata={\n            \"help\": \"Number of updates steps to accumulate before performing a backward/update pass.\"\n        },\n    )\n    learning_rate: float = field(\n        default=5e-4, metadata={\"help\": \"The initial learning rate for AdamW.\"}\n    )\n    bf16: bool = field(\n        default=True,\n        metadata={\n            \"help\": (\n                \"Whether to use bf16 (mixed) precision instead of 32-bit. Requires Ampere or higher NVIDIA\"\n                \" architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change.\"\n            )\n        },\n    )\n    bf16_full_eval: bool = field(\n        default=True,\n        metadata={\n            \"help\": (\n                \"Whether to use full bfloat16 evaluation instead of 32-bit. This is an experimental API and it may\"\n                \" change.\"\n            )\n        },\n    )\n    max_seq_length: Optional[int] = field(\n        default=512,\n        metadata={\n            \"help\": \"Maximum length of the tokenized sequence. Sequences longer than `max_seq_length` are truncated \"\n            \"from the right. If `None`, no truncation is applied. When packing is enabled, this value sets the \"\n            \"sequence length.\"\n        },\n    )\n    eval_strategy: Union[str] = field(\n        default=\"steps\",\n        metadata={\"help\": \"The evaluation strategy to use.\"},\n    )\n    eval_steps: Optional[float] = field(\n        default=0.1,\n        metadata={\n            \"help\": (\n                \"Run an evaluation every X steps. Should be an integer or a float in range `[0,1)`. \"\n                \"If smaller than 1, will be interpreted as ratio of total training steps.\"\n            )\n        },\n    )\n    logging_steps: float = field(\n        default=10,\n        metadata={\n            \"help\": (\n                \"Log every X updates steps. Should be an integer or a float in range `[0,1)`. \"\n                \"If smaller than 1, will be interpreted as ratio of total training steps.\"\n            )\n        },\n    )\n    save_steps: float = field(\n        default=0.1,\n        metadata={\n            \"help\": (\n                \"Save checkpoint every X updates steps. Should be an integer or a float in range `[0,1)`. \"\n                \"If smaller than 1, will be interpreted as ratio of total training steps.\"\n            )\n        },\n    )\n```\n\n训练的流程这块如下,使用HF TRL后流程变得非常简洁。\n\n```python",
    "131": "# Training",
    "132": "trainer = SFTTrainer(\n    model=model_args.model_name_or_path,\n    args=training_args,\n    data_collator=None,\n    train_dataset=raw_datasets[\"train\"],\n    eval_dataset=(\n        raw_datasets[\"test\"] if training_args.eval_strategy != \"no\" else None\n    ),\n    processing_class=tokenizer,\n    peft_config=get_peft_config(model_args),\n    formatting_func=formatting_func,\n    callbacks=[SavePredictCallback()],\n)\ntrainer.train()\n```",
    "133": "一级标题：使用ChatGLM4进行大模型指令遵从微调（附代码和测试脚本）\n二级标题：启动训练+效果评测\n内容：\n本代码在实现训练时默认是开启[SwanLab](https://swanlab.cn)的。SwanLab被官方集成进入了🤗HuggingFace Transformers。可以通过`report_to=\"swanlab\"`开启训练跟踪。如果本地环境安装了SwanLab会默认开启！\n\n启动训练的命令如下：\n\n```bash\npython instruct_train.py\n```\n\n可以看到如下启动信息\n\n![train](images/glm4-instruct/train.png)\n\n如果没登录SwanLab可能会弹出登录提示，这里推荐选择1并在[https://swanlab.cn](https://swanlab.cn)完成注册。即可在线查看到训练进展。\n\n登陆命令如下\n\n```bash\nswanlab login\n```\n\n点击打印出的链接即可通过看板查看训练日志：\n\n![swanlab](images/glm4-instruct/swanlab.png)\n\n通过配置`callback`，SwanLab还能自动记录模型的预测输出，代码和效果如下：\n\n```python",
    "134": "# Print prediction text callback",
    "135": "class SavePredictCallback(TrainerCallback):\n    def __init__(self, num_steps=10):\n        self.num_steps = num_steps\n\n    def on_save(self, args, state, control, model, processing_class, **kwargs):\n        if state.is_world_process_zero:\n            tokenizer = processing_class\n            batch_test_message = [\n                [{\"role\": \"user\", \"content\": \"你好，告诉我你的名字。\"}],\n                [{\"role\": \"user\", \"content\": \"告诉我1+2等于多少？\"}],\n            ]\n            batch_inputs_text = tokenizer.apply_chat_template(\n                batch_test_message,\n                return_tensors=\"pt\",\n                return_dict=True,\n                padding=True,\n                padding_side=\"left\",\n                add_generation_prompt=True,\n            ).to(model.device)\n\n            # print(batch_inputs_text)\n            outputs = model.generate(**batch_inputs_text, max_new_tokens=512)\n            batch_reponse = tokenizer.batch_decode(\n                outputs, skip_special_tokens=False\n            )\n            log_text_list = [swanlab.Text(response) for response in batch_reponse]\n            swanlab.log({\"Prediction\": log_text_list}, step=state.global_step)\n```\n\n![swanlab-text](images/glm4-instruct/swanlab-text.png)\n\n**多卡实验**\n\n如果你的卡数比较多，推荐使用多卡训练来极大提升训练速度！首先安装huggingface accelerate和deepspeed来方便的开启zero2多卡训练：\n\n```bash\npip install accelerate deepspeed\n```\n\n接下来使用如下命令来开启多卡训练（默认8GPU，可更改num_processes参数为实际卡数）：\n\n```bash\naccelerate launch --num_processes 8 --config_file configs/zero2.yaml instruct_train.py\n```\n\n关于zero2的详细设置在`configs/zero2.yaml`中。\n\n模型将会保存在`output/lora-glm4-9b-alpaca`，由于笔者的硬盘空间有限，因此仅仅保存Lora权重，推理加载时也要记得加载原始模型。\n\n**推理+效果对比**\n\n可以通过使用如下命令进行命令行聊天：\n\n```bash\nbash chat_cli.py\n```\n\n效果如下，我个人感觉有点overfit，因此建议大家使用早一点的checkpoints来做推理：\n\n![chat_cli](images/glm4-instruct/chat_cli.png)",
    "136": "一级标题：使用ChatGLM4进行大模型指令遵从微调（附代码和测试脚本）\n二级标题：附件：完整代码\n内容：\n完整代码如下，推荐还是通过使用github获得完整的代码\n\n[https://github.com/SwanHubX/glm4-finetune](https://github.com/SwanHubX/glm4-finetune)\n\n记得帮忙点个star🌟\n\n```python\n\"\"\"\nRefer: https://huggingface.co/docs/trl/sft_trainer#add-special-tokens-for-chat-format for more advance tools\n\"\"\"\n\nimport argparse\nfrom typing import Optional, Union, List\nfrom dataclasses import dataclass, field\n\nimport datasets\nfrom transformers import AutoTokenizer, TrainerCallback\nfrom trl import (\n    ModelConfig,\n    SFTConfig,\n    SFTTrainer,\n    TrlParser,\n    get_kbit_device_map,\n    get_peft_config,\n    get_quantization_config,\n)\nimport swanlab",
    "137": "# Model kwargs",
    "138": "@dataclass\nclass ChatGLM4ModelConfig(ModelConfig):\n    model_name_or_path: Optional[str] = field(\n        default=\"./weights/glm-4-9b-hf\",\n        metadata={\n            \"help\": \"Model checkpoint for weights initialization. default used glm4\"\n        },\n    )\n    torch_dtype: Optional[str] = field(\n        default=\"bfloat16\",\n        metadata={\n            \"help\": \"Override the default `torch.dtype` and load the model under this dtype.\",\n            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n        },\n    )\n    use_peft: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to use PEFT for training. Default true\"},\n    )\n    lora_r: int = field(\n        default=8,\n        metadata={\"help\": \"LoRA R value.\"},\n    )\n    lora_alpha: int = field(\n        default=32,\n        metadata={\"help\": \"LoRA alpha.\"},\n    )\n    lora_dropout: float = field(\n        default=0.1,\n        metadata={\"help\": \"LoRA dropout.\"},\n    )\n    lora_target_modules: Optional[list[str]] = field(\n        default_factory=lambda: [\"q_proj\", \"k_proj\", \"v_proj\"],\n        metadata={\"help\": \"LoRA target modules.\"},\n    )",
    "139": "# Datasets kwargs",
    "140": "@dataclass\nclass DataTrainingArguments:\n    data_files: Optional[str] = field(\n        default=\"./data/alpaca_gpt4_data_zh.json\",\n        metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"},\n    )",
    "141": "# Train kwargs",
    "142": "@dataclass\nclass MySFTConfig(SFTConfig):\n    output_dir: Optional[str] = field(\n        default=\"./output/lora-glm4-9b-alpaca\",\n        metadata={\n            \"help\": \"The output directory where the model predictions and checkpoints will be written. Defaults to 'lora-glm4-9b-toolcall' if not provided.\"\n        },\n    )\n    num_train_epochs: float = field(\n        default=3.0, metadata={\"help\": \"Total number of training epochs to perform.\"}\n    )\n    per_device_train_batch_size: int = field(\n        default=2,\n        metadata={\"help\": \"Batch size per GPU/TPU/MPS/NPU core/CPU for training.\"},\n    )\n    per_device_eval_batch_size: int = field(\n        default=4,\n        metadata={\"help\": \"Batch size per GPU/TPU/MPS/NPU core/CPU for evaluation.\"},\n    )\n    gradient_accumulation_steps: int = field(\n        default=1,\n        metadata={\n            \"help\": \"Number of updates steps to accumulate before performing a backward/update pass.\"\n        },\n    )\n    learning_rate: float = field(\n        default=5e-4, metadata={\"help\": \"The initial learning rate for AdamW.\"}\n    )\n    bf16: bool = field(\n        default=True,\n        metadata={\n            \"help\": (\n                \"Whether to use bf16 (mixed) precision instead of 32-bit. Requires Ampere or higher NVIDIA\"\n                \" architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change.\"\n            )\n        },\n    )\n    bf16_full_eval: bool = field(\n        default=True,\n        metadata={\n            \"help\": (\n                \"Whether to use full bfloat16 evaluation instead of 32-bit. This is an experimental API and it may\"\n                \" change.\"\n            )\n        },\n    )\n    max_seq_length: Optional[int] = field(\n        default=512,\n        metadata={\n            \"help\": \"Maximum length of the tokenized sequence. Sequences longer than `max_seq_length` are truncated \"\n            \"from the right. If `None`, no truncation is applied. When packing is enabled, this value sets the \"\n            \"sequence length.\"\n        },\n    )\n    eval_strategy: Union[str] = field(\n        default=\"steps\",\n        metadata={\"help\": \"The evaluation strategy to use.\"},\n    )\n    eval_steps: Optional[float] = field(\n        default=0.1,\n        metadata={\n            \"help\": (\n                \"Run an evaluation every X steps. Should be an integer or a float in range `[0,1)`. \"\n                \"If smaller than 1, will be interpreted as ratio of total training steps.\"\n            )\n        },\n    )\n    logging_steps: float = field(\n        default=10,\n        metadata={\n            \"help\": (\n                \"Log every X updates steps. Should be an integer or a float in range `[0,1)`. \"\n                \"If smaller than 1, will be interpreted as ratio of total training steps.\"\n            )\n        },\n    )\n    save_steps: float = field(\n        default=0.1,\n        metadata={\n            \"help\": (\n                \"Save checkpoint every X updates steps. Should be an integer or a float in range `[0,1)`. \"\n                \"If smaller than 1, will be interpreted as ratio of total training steps.\"\n            )\n        },\n    )",
    "143": "# Print prediction text callback",
    "144": "class SavePredictCallback(TrainerCallback):\n    def __init__(self, num_steps=10):\n        self.num_steps = num_steps\n\n    def on_save(self, args, state, control, model, processing_class, **kwargs):\n        if state.is_world_process_zero:\n            tokenizer = processing_class\n            batch_test_message = [\n                [{\"role\": \"user\", \"content\": \"你好，告诉我你的名字。\"}],\n                [{\"role\": \"user\", \"content\": \"告诉我1+2等于多少？\"}],\n            ]\n            batch_inputs_text = tokenizer.apply_chat_template(\n                batch_test_message,\n                return_tensors=\"pt\",\n                return_dict=True,\n                padding=True,\n                padding_side=\"left\",\n                add_generation_prompt=True,\n            ).to(model.device)\n\n            # print(batch_inputs_text)\n            outputs = model.generate(**batch_inputs_text, max_new_tokens=512)\n            batch_reponse = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n            log_text_list = [swanlab.Text(response) for response in batch_reponse]\n            swanlab.log({\"Prediction\": log_text_list}, step=state.global_step)\n\n\ndef main(model_args, data_args, training_args):",
    "145": "# Model init kwargs & Tokenizer",
    "146": "quantization_config = get_quantization_config(model_args)\n    model_kwargs = dict(\n        trust_remote_code=model_args.trust_remote_code,\n        attn_implementation=model_args.attn_implementation,\n        torch_dtype=model_args.torch_dtype,\n        use_cache=False if training_args.gradient_checkpointing else True,\n        device_map=get_kbit_device_map() if quantization_config is not None else None,\n        quantization_config=quantization_config,\n    )\n    training_args.model_init_kwargs = model_kwargs\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_args.model_name_or_path,\n        trust_remote_code=model_args.trust_remote_code,\n        use_fast=True,\n    )\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    if tokenizer.chat_template is None:\n        tokenizer.chat_template = \"[gMASK]<sop>{% for item in messages %}{% if item['tools'] is defined %}<|system|>\\n你是一个名为 ChatGLM 的人工智能助手。你是基于智谱AI训练的语言模型 GLM-4 模型开发的，你的任务是针对用户的问题和要求提供适当的答复和支持。\\n\\n# 可用工具{% set tools = item['tools'] %}{% for tool in tools %}{% if tool['type'] == 'function' %}\\n\\n## {{ tool['function']['name'] }}\\n\\n{{ tool['function'] | tojson(indent=4) }}\\n在调用上述函数时，请使用 Json 格式表示调用的参数。{% elif tool['type'] == 'python' %}\\n\\n## python\\n\\n当你向 `python` 发送包含 Python 代码的消息时，该代码将会在一个有状态的 Jupyter notebook 环境中执行。\\n`python` 返回代码执行的输出，或在执行 60 秒后返回超时。\\n`/mnt/data` 将会持久化存储你的文件。在此会话中，`python` 无法访问互联网。不要使用 `python` 进行任何网络请求或者在线 API 调用，这些在线内容的访问将不会成功。{% elif tool['type'] == 'simple_browser' %}\\n\\n## simple_browser\\n\\n你可以使用 `simple_browser` 工具。该工具支持以下函数：\\n`search(query: str, recency_days: int)`：使用搜索引擎进行查询并显示结果，可以使用 `recency_days` 参数控制搜索内容的时效性。\\n`mclick(ids: list[int])`：获取一系列指定 id 的页面内容。每次调用时，须选择3-10个页面。选择多个角度的页面，同时尽可能选择可信任的信息来源。考虑到部分页面是无法加载的，你也可以多打开一些可能有用的页面而不用担心内容过多。\\n`open_url(url: str)`：打开指定的 URL。\\n\\n使用 `【{引用 id}†{引用文本}】` 来引用内容。\\n\\n操作步骤：1. 使用 `search` 来获得信息列表; 2. 使用 `mclick` 来获取指定 ID 页面的内容; 3. 根据获得的内容进行回复。在回复中应当引用信息来源。\\n 如果用户提供了 URL，也可以用 `open_url` 直接打开页面。\\n如果初次搜索结果没有找到合适的信息，也可以再次使用 `search` 进行搜索。{% elif tool['type'] == 'cogview' %}\\n\\n## cogview\\n\\n如果用户的请求中包含了对图像的描述，你可以使用 `cogview` 来生成图像并展示给用户。你需要向 `cogview` 发送图像描述，规则：\\n- 发送给 `cogview` 的消息必须使用英语。用户的中文描述必须完全翻译为英语。\\n- 应当尽可能详细地描述图像生成的需求，需求描述约 100 英文单词。\\n- 保持用户原始描述的意图。不要虚构内容或者没见过的人物。\\n- 如无特殊说明，所在地为中国，持有中国立场并遵循中国社会主义价值观。{% endif %}{% endfor %}{% endif %}{% if item['content'] %}<|{{ item['role'] }}|>{{ item['metadata'] }}\\n{{ item['content'] }}{% endif %}{% endfor %}{% if add_generation_prompt %}<|assistant|>{% endif %}\"",
    "147": "# Dataset",
    "148": "raw_datasets = datasets.load_dataset(\"json\", data_files=data_args.data_files)\n    raw_datasets = raw_datasets[\"train\"].train_test_split(0.05)  # split train test data\n\n    def formatting_func(example):\n        \"\"\"\n        process data format\n        \"\"\"\n        prompt = example[\"instruction\"]\n        if len(example[\"input\"]) != 0:\n            prompt += \"\\n\\n\" + example[\"input\"]\n        conversations = [\n            {\"role\": \"user\", \"content\": prompt},\n            {\"role\": \"assistant\", \"content\": example[\"output\"]},\n        ]\n        output_text = tokenizer.apply_chat_template(\n            conversation=conversations, tokenize=False\n        )\n        return output_text",
    "149": "# Training",
    "150": "trainer = SFTTrainer(\n        model=model_args.model_name_or_path,\n        args=training_args,\n        data_collator=None,\n        train_dataset=raw_datasets[\"train\"],\n        eval_dataset=(\n            raw_datasets[\"test\"] if training_args.eval_strategy != \"no\" else None\n        ),\n        processing_class=tokenizer,\n        peft_config=get_peft_config(model_args),\n        formatting_func=formatting_func,\n        callbacks=[SavePredictCallback()],\n    )\n    trainer.train()\n\n    # Save\n    trainer.save_model(training_args.output_dir)\n\n\ndef make_parser(subparsers: argparse._SubParsersAction = None):\n    dataclass_types = (ChatGLM4ModelConfig, DataTrainingArguments, MySFTConfig)\n    if subparsers is not None:\n        parser = subparsers.add_parser(\n            \"sft\", help=\"Run the SFT training script\", dataclass_types=dataclass_types\n        )\n    else:\n        parser = TrlParser(dataclass_types)\n    return parser\n\n\nif __name__ == \"__main__\":\n    parser = make_parser()\n    model_args, data_args, training_args = parser.parse_args_and_config()\n    main(model_args, data_args, training_args)\n```",
    "151": "一级标题：Hello World\n二级标题：无\n内容：\n[![](/assets/colab.svg)](https://colab.research.google.com/drive/1RWsrY_1bS8ECzaHvYtLb_1eBkkdzekR3?usp=sharing)\n\n这是一个入门案例，是一个最简的深度学习训练模拟。",
    "152": "一级标题：Hello World\n二级标题：环境准备\n内容：\n```bash\npip install swanlab\n```",
    "153": "一级标题：Hello World\n二级标题：完整代码\n内容：\n```python\nimport swanlab\nimport random\n\noffset = random.random() / 5\n\n# 初始化SwanLab\nrun = swanlab.init(\n    project=\"my-project\",\n    config={\n        \"learning_rate\": 0.01,\n        \"epochs\": 10,\n    },\n)\n\n# 模拟训练过程\nfor epoch in range(2, run.config.epochs):\n    acc = 1 - 2**-epoch - random.random() / epoch - offset\n    loss = 2**-epoch + random.random() / epoch + offset\n    print(f\"epoch={epoch}, accuracy={acc}, loss={loss}\")\n\n    swanlab.log({\"accuracy\": acc, \"loss\": loss})  # 记录指标\n```",
    "154": "一级标题：LSTM股票预测\n二级标题：无\n内容：\n:::info\n时间序列、量化交易、时序模型\n:::\n\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/Google-Stock-Prediction/runs/0c2ci59aje4rb54r2z4y5/chart)\n\n[在线Demo](https://swanlab.cn/@ZeyiLin/Google-Stock-Prediction/runs/0c2ci59aje4rb54r2z4y5/chart) | [知乎教程](https://zhuanlan.zhihu.com/p/702114810)",
    "155": "一级标题：LSTM股票预测\n二级标题：概述\n内容：\nLSTM（Long Short-Term Memory），即长短时记忆网络，是一种特殊的RNN（递归神经网络），它改进了传统RNN在处理长序列数据时存在的梯度消失和梯度爆炸问题。LSTM由Hochreiter和Schmidhuber于1997年提出，已成为处理**时间序列数据**的经典模型之一。\n\n![](/assets/example-lstm-1.png)\n\n股票预测任务指的是根据一支股票的过去一段时间的数据，通过AI模型预测现在以及未来的股价变化，也是一种实用的时间序列任务。这里我们使用2016～2021年的Google股价数据数据集来进行训练和推理。",
    "156": "一级标题：LSTM股票预测\n二级标题：环境安装\n内容：\n本案例基于`Python>=3.8`，请在您的计算机上安装好Python。 环境依赖：\n\n```txt\npandas\ntorch\nmatplotlib\nswanlab\nscikit-learn\n```\n\n快速安装命令：\n\n```bash\npip install pandas torch matplotlib swanlab scikit-learn\n```\n\n> 本代码测试于torch==2.3.0、pandas==2.0.3、matplotlib==3.8.2、swanlab==0.3.8、scikit-learn==1.3.2",
    "157": "一级标题：LSTM股票预测\n二级标题：完整代码\n内容：\n请先在[Kaggle](https://www.kaggle.com/datasets/shreenidhihipparagi/google-stock-prediction)下载Google Stock Prediction数据集到根目录下。\n\n```python\nimport os\nimport swanlab\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nfrom copy import deepcopy as dc\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nclass LSTMModel(nn.Module):\n    \"\"\"\n    定义模型类\n    \"\"\"\n    def __init__(self, input_size=1, hidden_size1=50, hidden_size2=64, fc1_size=32, fc2_size=16, output_size=1):\n        super(LSTMModel, self).__init__()\n        self.lstm1 = nn.LSTM(input_size, hidden_size1, batch_first=True)\n        self.lstm2 = nn.LSTM(hidden_size1, hidden_size2, batch_first=True)\n        self.fc1 = nn.Linear(hidden_size2, fc1_size)\n        self.fc2 = nn.Linear(fc1_size, fc2_size)\n        self.fc3 = nn.Linear(fc2_size, output_size)\n\n    def forward(self, x):\n        x, _ = self.lstm1(x)\n        x, _ = self.lstm2(x)\n        x = self.fc1(x[:, -1, :])\n        x = self.fc2(x)\n        x = self.fc3(x)\n        return x\n\n\nclass TimeSeriesDataset(Dataset):\n    \"\"\"\n    定义数据集类\n    \"\"\"\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, i):\n        return self.X[i], self.y[i]\n\n\ndef prepare_dataframe_for_lstm(df, n_steps):\n    \"\"\"\n    处理数据集，使其适用于LSTM模型\n    \"\"\"\n    df = dc(df)\n    df['date'] = pd.to_datetime(df['date'])\n    df.set_index('date', inplace=True)\n    \n    for i in range(1, n_steps+1):\n        df[f'close(t-{i})'] = df['close'].shift(i)\n        \n    df.dropna(inplace=True)\n    return df\n\n\ndef get_dataset(file_path, lookback, split_ratio=0.9):\n    \"\"\"\n    归一化数据、划分训练集和测试集\n    \"\"\"\n    data = pd.read_csv(file_path)\n    data = data[['date','close']]\n    \n    shifted_df_as_np = prepare_dataframe_for_lstm(data, lookback)\n\n    scaler = MinMaxScaler(feature_range=(-1,1))\n    shifted_df_as_np = scaler.fit_transform(shifted_df_as_np)\n\n    X = shifted_df_as_np[:, 1:]\n    y = shifted_df_as_np[:, 0]\n\n    X = dc(np.flip(X,axis=1))\n\n    # 划分训练集和测试集\n    split_index = int(len(X) * split_ratio)\n    \n    X_train = X[:split_index]\n    X_test = X[split_index:]\n\n    y_train = y[:split_index]\n    y_test = y[split_index:]\n\n    X_train = X_train.reshape((-1, lookback, 1))\n    X_test = X_test.reshape((-1, lookback, 1))\n\n    y_train = y_train.reshape((-1, 1))\n    y_test = y_test.reshape((-1, 1))\n\n    # 转换为Tensor\n    X_train = torch.tensor(X_train).float()\n    y_train = torch.tensor(y_train).float()\n    X_test = torch.tensor(X_test).float()\n    y_test = torch.tensor(y_test).float()\n    \n    return scaler, X_train, X_test, y_train, y_test\n\n\ndef train(model, train_loader, optimizer, criterion):\n        model.train()\n        running_loss = 0\n        # 训练\n        for i, batch in enumerate(train_loader):\n            x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n            y_pred = model(x_batch)\n            loss = criterion(y_pred, y_batch)\n            running_loss += loss.item()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        \n        avg_loss_epoch = running_loss / len(train_loader)\n        print(f'Epoch: {epoch}, Batch: {i}, Avg. Loss: {avg_loss_epoch}')\n        swanlab.log({\"train/loss\": running_loss}, step=epoch)\n        running_loss = 0\n\n\ndef validate(model, test_loader, criterion, epoch):\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for _, batch in enumerate(test_loader):\n            x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n            y_pred = model(x_batch)\n            loss = criterion(y_pred, y_batch)\n            val_loss += loss.item()\n        avg_val_loss = val_loss / len(test_loader)\n        print(f'Epoch: {epoch}, Validation Loss: {avg_val_loss}')\n        swanlab.log({\"val/loss\": avg_val_loss}, step=epoch)\n       \n       \ndef inverse_transform_and_extract(scaler, data, lookback):\n    dummies = np.zeros((data.shape[0], lookback + 1))\n    dummies[:, 0] = data.flatten()\n    return dc(scaler.inverse_transform(dummies)[:, 0])\n\n\ndef plot_predictions(actual, predicted, title, xlabel='Date', ylabel='Close Price'):\n    \"\"\"\n    绘制最后的股价预测与真实值的对比图\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.plot(actual, color='red', label='Actual Close Price')\n    plt.plot(predicted, color='blue', label='Predicted Close Price', alpha=0.5)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(title)\n    plt.legend()\n    return swanlab.Image(plt, caption=title)\n\n\ndef visualize_predictions(train_predictions, val_predictions, scaler, y_train, y_test, lookback):    \n    train_predictions = inverse_transform_and_extract(scaler, train_predictions, lookback)\n    val_predictions = inverse_transform_and_extract(scaler, val_predictions, lookback)\n    new_y_train = inverse_transform_and_extract(scaler, y_train, lookback)\n    new_y_test = inverse_transform_and_extract(scaler, y_test, lookback)\n\n    plt_image = []\n    plt_image.append(plot_predictions(new_y_train, train_predictions, '(TrainSet) Google Stock Price Prediction with LSTM'))\n    plt_image.append(plot_predictions(new_y_test, val_predictions, '(TestSet) Google Stock Price Prediction with LSTM'))\n\n    swanlab.log({\"Prediction\": plt_image})\n\n\nif __name__ == '__main__':\n    # ------------------- 初始化一个SwanLab实验 -------------------\n    swanlab.init(\n        project='Google-Stock-Prediction',\n        experiment_name=\"LSTM\",\n        description=\"根据前7天的数据预测下一日股价\",\n        config={ \n            \"learning_rate\": 1e-3,\n            \"epochs\": 100,\n            \"batch_size\": 32,\n            \"lookback\": 7,\n            \"spilt_ratio\": 0.9, \n            \"save_path\": \"./checkpoint\",\n            \"optimizer\": \"Adam\",\n            \"device\": 'cuda' if torch.cuda.is_available() else 'cpu',\n        },\n    )\n    \n    config = swanlab.config\n    device = torch.device(config.device)\n    \n    # ------------------- 定义数据集 -------------------\n    scaler, X_train, X_test, y_train, y_test = get_dataset(file_path='./GOOG.csv',\n                                                           lookback=config.lookback,\n                                                           split_ratio=config.spilt_ratio,)\n    \n    train_dataset = TimeSeriesDataset(X_train, y_train)\n    test_dataset = TimeSeriesDataset(X_test, y_test)\n\n    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n\n    # ------------------- 定义模型、超参数 -------------------\n    model = LSTMModel(input_size=1, output_size=1)\n\n    model = model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n    criterion = nn.MSELoss()\n\n    # ------------------- 训练与验证 -------------------\n    for epoch in range(1, config.epochs+1):\n        train(model, train_loader, optimizer, criterion)\n        validate(model, test_loader, criterion, epoch)\n        \n    # ------------------- 使用最佳模型推理，与生成可视化结果 -------------------\n    with torch.no_grad():\n        model.eval()\n        train_predictions = model(X_train.to(device)).to('cpu').numpy()\n        val_predictions = model(X_test.to(device)).to('cpu').numpy()\n        visualize_predictions(train_predictions, val_predictions, scaler, y_train, y_test, config.lookback)\n    \n    # ------------------- 保存模型 -------------------\n    model_save_path = os.path.join(config.save_path, 'lstm.pth')\n    if not os.path.exists(config.save_path):\n        os.makedirs(config.save_path)\n    torch.save(model.state_dict(), model_save_path)\n```",
    "158": "一级标题：LSTM股票预测\n二级标题：演示效果\n内容：\n![](/assets/example-lstm-2.png)",
    "159": "一级标题：MNIST手写体识别\n二级标题：无\n内容：\n:::info\n图像分类、机器学习入门、灰度图像\n:::\n\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/MNIST-example/runs/4plp6w0qehoqpt0uq2tcy/chart)\n\n[![](/assets/colab.svg)](https://colab.research.google.com/drive/1Au8aXxU2o0QNWSzGXGsTdHggighXQMNu?usp=sharing)",
    "160": "一级标题：MNIST手写体识别\n二级标题：概述\n内容：\nMNIST手写体识别是深度学习最经典的入门任务之一，由 LeCun 等人提出。  \n该任务基于[MNIST数据集](https://paperswithcode.com/dataset/mnist)，研究者通过构建机器学习模型，来识别10个手写数字（0～9）。\n\n![mnist](/assets/mnist.jpg)\n\n本案例主要：\n- 使用`pytorch`进行CNN（卷积神经网络）的构建、模型训练与评估\n- 使用`swanlab`跟踪超参数、记录指标和可视化监控整个训练周期",
    "161": "一级标题：MNIST手写体识别\n二级标题：环境安装\n内容：\n本案例基于`Python>=3.8`，请在您的计算机上安装好Python。  \n环境依赖：\n```\ntorch\ntorchvision\nswanlab\n```\n快速安装命令：\n```bash\npip install torch torchvision swanlab\n```",
    "162": "一级标题：MNIST手写体识别\n二级标题：完整代码\n内容：\n```python\nimport os\nimport torch\nfrom torch import nn, optim, utils\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nimport swanlab\n\n# CNN网络构建\nclass ConvNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # 1,28x28\n        self.conv1 = nn.Conv2d(1, 10, 5)  # 10, 24x24\n        self.conv2 = nn.Conv2d(10, 20, 3)  # 128, 10x10\n        self.fc1 = nn.Linear(20 * 10 * 10, 500)\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x):\n        in_size = x.size(0)\n        out = self.conv1(x)  # 24\n        out = F.relu(out)\n        out = F.max_pool2d(out, 2, 2)  # 12\n        out = self.conv2(out)  # 10\n        out = F.relu(out)\n        out = out.view(in_size, -1)\n        out = self.fc1(out)\n        out = F.relu(out)\n        out = self.fc2(out)\n        out = F.log_softmax(out, dim=1)\n        return out\n\n\n# 捕获并可视化前20张图像\ndef log_images(loader, num_images=16):\n    images_logged = 0\n    logged_images = []\n    for images, labels in loader:\n        # images: batch of images, labels: batch of labels\n        for i in range(images.shape[0]):\n            if images_logged < num_images:\n                # 使用swanlab.Image将图像转换为wandb可视化格式\n                logged_images.append(swanlab.Image(images[i], caption=f\"Label: {labels[i]}\"))\n                images_logged += 1\n            else:\n                break\n        if images_logged >= num_images:\n            break\n    swanlab.log({\"MNIST-Preview\": logged_images})\n    \n\ndef train(model, device, train_dataloader, optimizer, criterion, epoch, num_epochs):\n    model.train()\n    # 1. 循环调用train_dataloader，每次取出1个batch_size的图像和标签\n    for iter, (inputs, labels) in enumerate(train_dataloader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        # 2. 传入到resnet18模型中得到预测结果\n        outputs = model(inputs)\n        # 3. 将结果和标签传入损失函数中计算交叉熵损失\n        loss = criterion(outputs, labels)\n        # 4. 根据损失计算反向传播\n        loss.backward()\n        # 5. 优化器执行模型参数更新\n        optimizer.step()\n        print('Epoch [{}/{}], Iteration [{}/{}], Loss: {:.4f}'.format(epoch, num_epochs, iter + 1, len(train_dataloader),\n                                                                      loss.item()))\n        # 6. 每20次迭代，用SwanLab记录一下loss的变化\n        if iter % 20 == 0:\n            swanlab.log({\"train/loss\": loss.item()})\n\ndef test(model, device, val_dataloader, epoch):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        # 1. 循环调用val_dataloader，每次取出1个batch_size的图像和标签\n        for inputs, labels in val_dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            # 2. 传入到resnet18模型中得到预测结果\n            outputs = model(inputs)\n            # 3. 获得预测的数字\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            # 4. 计算与标签一致的预测结果的数量\n            correct += (predicted == labels).sum().item()\n    \n        # 5. 得到最终的测试准确率\n        accuracy = correct / total\n        # 6. 用SwanLab记录一下准确率的变化\n        swanlab.log({\"val/accuracy\": accuracy}, step=epoch)\n    \n\nif __name__ == \"__main__\":\n\n    #检测是否支持mps\n    try:\n        use_mps = torch.backends.mps.is_available()\n    except AttributeError:\n        use_mps = False\n\n    #检测是否支持cuda\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    elif use_mps:\n        device = \"mps\"\n    else:\n        device = \"cpu\"\n\n    # 初始化swanlab\n    run = swanlab.init(\n        project=\"MNIST-example\",\n        experiment_name=\"PlainCNN\",\n        config={\n            \"model\": \"ResNet18\",\n            \"optim\": \"Adam\",\n            \"lr\": 1e-4,\n            \"batch_size\": 256,\n            \"num_epochs\": 10,\n            \"device\": device,\n        },\n    )\n\n    # 设置MNIST训练集和验证集\n    dataset = MNIST(os.getcwd(), train=True, download=True, transform=ToTensor())\n    train_dataset, val_dataset = utils.data.random_split(dataset, [55000, 5000])\n\n    train_dataloader = utils.data.DataLoader(train_dataset, batch_size=run.config.batch_size, shuffle=True)\n    val_dataloader = utils.data.DataLoader(val_dataset, batch_size=8, shuffle=False)\n    \n    # （可选）看一下数据集的前16张图像\n    log_images(train_dataloader, 16)\n\n    # 初始化模型\n    model = ConvNet()\n    model.to(torch.device(device))\n\n    # 打印模型\n    print(model)\n\n    # 定义损失函数和优化器\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=run.config.lr)\n\n    # 开始训练和测试循环\n    for epoch in range(1, run.config.num_epochs+1):\n        swanlab.log({\"train/epoch\": epoch}, step=epoch)\n        train(model, device, train_dataloader, optimizer, criterion, epoch, run.config.num_epochs)\n        if epoch % 2 == 0: \n            test(model, device, val_dataloader, epoch)\n\n    # 保存模型\n    # 如果不存在checkpoint文件夹，则自动创建一个\n    if not os.path.exists(\"checkpoint\"):\n        os.makedirs(\"checkpoint\")\n    torch.save(model.state_dict(), 'checkpoint/latest_checkpoint.pth')\n```",
    "163": "一级标题：MNIST手写体识别\n二级标题：效果演示\n内容：\n![mnist](/assets/example-mnist.jpg)",
    "164": "一级标题：Qwen2命名实体识别\n二级标题：无\n内容：\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/Qwen2-NER-fintune/runs/9gdyrkna1rxjjmz0nks2c/chart)\n\n[Qwen2](https://modelscope.cn/models/qwen/Qwen2-1.5B-Instruct/summary)是通义千问团队最近开源的大语言模型，由阿里云通义实验室研发。\n\n以Qwen2作为基座大模型，通过**指令微调**的方式做高精度的命名实体识别（NER），是学习入门**LLM微调**、建立大模型认知的非常好的任务。\n\n![](./ner/01.png)\n\n> 使用LoRA方法训练，1.5B模型对显存要求不高，10GB左右就可以跑。\n\n在本文中，我们会使用 [Qwen2-1.5b-Instruct](https://modelscope.cn/models/qwen/Qwen2-1.5B-Instruct/summary) 模型在 [中文NER](https://huggingface.co/datasets/qgyd2021/chinese_ner_sft) 数据集上做指令微调训练，同时使用[SwanLab](https://swanlab.cn)监控训练过程、评估模型效果。\n\n- 代码：完整代码直接看本文第5节 或 [Github](https://github.com/Zeyi-Lin/LLM-Finetune)\n- 实验日志过程：[Qwen2-1.5B-NER-Fintune - SwanLab](https://swanlab.cn/@ZeyiLin/Qwen2-NER-fintune/runs/9gdyrkna1rxjjmz0nks2c/chart)\n- 模型：[Modelscope](https://modelscope.cn/models/qwen/Qwen2-1.5B-Instruct/summary)\n- 数据集：[chinese_ner_sft](https://huggingface.co/datasets/qgyd2021/chinese_ner_sft)\n- SwanLab：[https://swanlab.cn](https://swanlab.cn)",
    "165": "一级标题：Qwen2命名实体识别\n二级标题：知识点1：什么是指令微调？\n内容：\n大模型指令微调（Instruction Tuning）是一种针对大型预训练语言模型的微调技术，**其核心目的是增强模型理解和执行特定指令的能力，使模型能够根据用户提供的自然语言指令准确、恰当地生成相应的输出或执行相关任务。**\n\n指令微调特别关注于提升模型在**遵循指令**方面的一致性和准确性，从而拓宽模型在各种应用场景中的泛化能力和实用性。\n\n在实际应用中，我的理解是，指令微调更多**把LLM看作一个更智能、更强大的传统NLP模型（比如Bert）**，来实现更高精度的NLP任务。所以这类任务的应用场景覆盖了以往NLP模型的场景，甚至很多团队拿它来**标注互联网数据**。",
    "166": "一级标题：Qwen2命名实体识别\n二级标题：知识点2：什么是命名实体识别？\n内容：\n命名实体识别 (NER) 是一种NLP技术，主要用于识别和分类文本中提到的重要信息（关键词）。这些实体可以是人名、地名、机构名、日期、时间、货币值等等。 NER 的目标是将文本中的非结构化信息转换为结构化信息，以便计算机能够更容易地理解和处理。\n\n![](./ner/02.png)\n\nNER 也是一项非常实用的技术，包括在互联网数据标注、搜索引擎、推荐系统、知识图谱、医疗保健等诸多领域有广泛应用。",
    "167": "一级标题：Qwen2命名实体识别\n二级标题：1.环境安装\n内容：\n本案例基于`Python>=3.8`，请在您的计算机上安装好Python，并且有一张英伟达显卡（显存要求并不高，大概10GB左右就可以跑）。\n\n我们需要安装以下这几个Python库，在这之前，请确保你的环境内已安装好了**pytorch**以及**CUDA**：\n\n```txt\nswanlab\nmodelscope\ntransformers\ndatasets\npeft\naccelerate\npandas\n```\n\n一键安装命令：\n\n```bash \npip install swanlab modelscope transformers datasets peft pandas accelerate\n```\n\n> 本案例测试于modelscope==1.14.0、transformers==4.41.2、datasets==2.18.0、peft==0.11.1、accelerate==0.30.1、swanlab==0.3.11",
    "168": "一级标题：Qwen2命名实体识别\n二级标题：2.准备数据集\n内容：\n本案例使用的是HuggingFace上的[chinese_ner_sft](https://huggingface.co/datasets/qgyd2021/chinese_ner_sft)数据集，该数据集主要被用于训练命名实体识别模型。\n\n![alt text](./ner/03.png)\n\nchinese_ner_sft由不同来源、不同类型的几十万条数据组成，应该是我见过收录最齐全的中文NER数据集。\n\n这次训练我们不需要用到它的全部数据，只取其中的CCFBDCI数据集（中文命名实体识别算法鲁棒性评测数据集）进行训练，该数据集包含LOC（地点）、GPE（地理）、ORG（组织）和PER（人名）四种实体类型标注，每条数据的例子如下：\n\n```json\n{\n  \"text\": \"今天亚太经合组织第十二届部长级会议在这里开幕，中国外交部部长唐家璇、外经贸部部长石广生出席了会议。\",\n  \"entities\": [\n    {\n        \"start_idx\": 23,\n        \"end_idx\": 25,\n        \"entity_text\": \"中国\",\n        \"entity_label\": \"GPE\",\n        \"entity_names\": [\"地缘政治实体\", \"政治实体\", \"地理实体\", \"社会实体\"]},\n        {\n            \"start_idx\": 25,\n            \"end_idx\": 28,\n            \"entity_text\": \"外交部\",\n            \"entity_label\": \"ORG\",\n            \"entity_names\": [\"组织\", \"团体\", \"机构\"]\n        },\n        {\n            \"start_idx\": 30,\n            \"end_idx\": 33,\n            \"entity_text\": \"唐家璇\",\n            \"entity_label\": \"PER\",\n            \"entity_names\": [\"人名\", \"姓名\"]\n        }, \n        ...\n    ],\n\"data_source\": \"CCFBDCI\"\n}\n```\n\n其中`text`是输入的文本，`entities`是文本抽取出的实体。我们的目标是希望微调后的大模型能够根据由`text`组成的提示词，预测出一个json格式的实体信息：\n\n```txt\n输入：今天亚太经合组织第十二届部长级会议在这里开幕，中国外交部部长唐家璇、外经贸部部长石广生出席了会议。\n\n大模型输出：{\"entity_text\":\"中国\", \"entity_label\":\"组织\"}{\"entity_text\":\"唐家璇\", \"entity_label\":\"人名\"}...\n```\n\n---\n\n现在我们将数据集下载到本地目录。下载方式是前往[chinese_ner_sft - huggingface](https://huggingface.co/datasets/qgyd2021/chinese_ner_sft/tree/main/data)下载`ccfbdci.jsonl`到项目根目录下即可：\n\n![alt text](./ner/04.png)",
    "169": "一级标题：Qwen2命名实体识别\n二级标题：3. 加载模型\n内容：\n这里我们使用modelscope下载Qwen2-1.5B-Instruct模型（modelscope在国内，所以直接用下面的代码自动下载即可，不用担心速度和稳定性问题），然后把它加载到Transformers中进行训练：\n\n```python\nfrom modelscope import snapshot_download, AutoTokenizer\nfrom transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n\nmodel_id = \"qwen/Qwen2-1.5B-Instruct\"    \nmodel_dir = \"./qwen/Qwen2-1___5B-Instruct\"\n\n# 在modelscope上下载Qwen模型到本地目录下\nmodel_dir = snapshot_download(model_id, cache_dir=\"./\", revision=\"master\")\n\n# Transformers加载模型权重\ntokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\nmodel.enable_input_require_grads()  # 开启梯度检查点时，要执行该方法\n```",
    "170": "一级标题：Qwen2命名实体识别\n二级标题：4. 配置训练可视化工具\n内容：\n我们使用SwanLab来监控整个训练过程，并评估最终的模型效果。\n\n这里直接使用SwanLab和Transformers的集成来实现：\n\n```python\nfrom swanlab.integration.huggingface import SwanLabCallback\n\nswanlab_callback = SwanLabCallback(...)\n\ntrainer = Trainer(\n    ...\n    callbacks=[swanlab_callback],\n)\n\n```\n\n如果你是第一次使用SwanLab，那么还需要去[https://swanlab.cn](https://swanlab.cn)上注册一个账号，在**用户设置**页面复制你的API Key，然后在训练开始时粘贴进去即可：\n\n![](./ner/05.png)",
    "171": "一级标题：Qwen2命名实体识别\n二级标题：5. 完整代码\n内容：\n开始训练时的目录结构：\n\n```txt\n|--- train.py\n|--- train.jsonl\n|--- test.jsonl\n```\n\ntrain.py:\n\n```python\nimport json\nimport pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom modelscope import snapshot_download, AutoTokenizer\nfrom swanlab.integration.huggingface import SwanLabCallback\nfrom peft import LoraConfig, TaskType, get_peft_model\nfrom transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\nimport os\nimport swanlab\n\n\ndef dataset_jsonl_transfer(origin_path, new_path):\n    \"\"\"\n    将原始数据集转换为大模型微调所需数据格式的新数据集\n    \"\"\"\n    messages = []\n\n    # 读取旧的JSONL文件\n    with open(origin_path, \"r\") as file:\n        for line in file:\n            # 解析每一行的json数据\n            data = json.loads(line)\n            input_text = data[\"text\"]\n            entities = data[\"entities\"]\n            match_names = [\"地点\", \"人名\", \"地理实体\", \"组织\"]\n            \n            entity_sentence = \"\"\n            for entity in entities:\n                entity_json = dict(entity)\n                entity_text = entity_json[\"entity_text\"]\n                entity_names = entity_json[\"entity_names\"]\n                for name in entity_names:\n                    if name in match_names:\n                        entity_label = name\n                        break\n                \n                entity_sentence += f\"\"\"{{\"entity_text\": \"{entity_text}\", \"entity_label\": \"{entity_label}\"}}\"\"\"\n            \n            if entity_sentence == \"\":\n                entity_sentence = \"没有找到任何实体\"\n            \n            message = {\n                \"instruction\": \"\"\"你是一个文本实体识别领域的专家，你需要从给定的句子中提取 地点; 人名; 地理实体; 组织 实体. 以 json 格式输出, 如 {\"entity_text\": \"南京\", \"entity_label\": \"地理实体\"} 注意: 1. 输出的每一行都必须是正确的 json 字符串. 2. 找不到任何实体时, 输出\"没有找到任何实体\". \"\"\",\n                \"input\": f\"文本:{input_text}\",\n                \"output\": entity_sentence,\n            }\n            \n            messages.append(message)\n\n    # 保存重构后的JSONL文件\n    with open(new_path, \"w\", encoding=\"utf-8\") as file:\n        for message in messages:\n            file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n            \n            \ndef process_func(example):\n    \"\"\"\n    将数据集进行预处理\n    \"\"\"\n\n    MAX_LENGTH = 384 \n    input_ids, attention_mask, labels = [], [], []\n    system_prompt = \"\"\"你是一个文本实体识别领域的专家，你需要从给定的句子中提取 地点; 人名; 地理实体; 组织 实体. 以 json 格式输出, 如 {\"entity_text\": \"南京\", \"entity_label\": \"地理实体\"} 注意: 1. 输出的每一行都必须是正确的 json 字符串. 2. 找不到任何实体时, 输出\"没有找到任何实体\".\"\"\"\n    \n    instruction = tokenizer(\n        f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{example['input']}<|im_end|>\\n<|im_start|>assistant\\n\",\n        add_special_tokens=False,\n    )\n    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n    attention_mask = (\n        instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n    )\n    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n        input_ids = input_ids[:MAX_LENGTH]\n        attention_mask = attention_mask[:MAX_LENGTH]\n        labels = labels[:MAX_LENGTH]\n    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}   \n\n\ndef predict(messages, model, tokenizer):\n    device = \"cuda\"\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\n    generated_ids = model.generate(\n        model_inputs.input_ids,\n        max_new_tokens=512\n    )\n    generated_ids = [\n        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n    \n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    \n    print(response)\n     \n    return response\n\n\nmodel_id = \"qwen/Qwen2-1.5B-Instruct\"    \nmodel_dir = \"./qwen/Qwen2-1___5B-Instruct\"\n\n# 在modelscope上下载Qwen模型到本地目录下\nmodel_dir = snapshot_download(model_id, cache_dir=\"./\", revision=\"master\")\n\n# Transformers加载模型权重\ntokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\nmodel.enable_input_require_grads()  # 开启梯度检查点时，要执行该方法\n\n# 加载、处理数据集和测试集\ntrain_dataset_path = \"ccfbdci.jsonl\"\ntrain_jsonl_new_path = \"ccf_train.jsonl\"\n\nif not os.path.exists(train_jsonl_new_path):\n    dataset_jsonl_transfer(train_dataset_path, train_jsonl_new_path)\n\n# 得到训练集\ntotal_df = pd.read_json(train_jsonl_new_path, lines=True)\ntrain_df = total_df[int(len(total_df) * 0.1):]\ntrain_ds = Dataset.from_pandas(train_df)\ntrain_dataset = train_ds.map(process_func, remove_columns=train_ds.column_names)\n\n\nconfig = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    inference_mode=False,  # 训练模式\n    r=8,  # Lora 秩\n    lora_alpha=32,  # Lora alaph，具体作用参见 Lora 原理\n    lora_dropout=0.1,  # Dropout 比例\n)\n\nmodel = get_peft_model(model, config)\n\nargs = TrainingArguments(\n    output_dir=\"./output/Qwen2-NER\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,\n    logging_steps=10,\n    num_train_epochs=2,\n    save_steps=100,\n    learning_rate=1e-4,\n    save_on_each_node=True,\n    gradient_checkpointing=True,\n    report_to=\"none\",\n)\n\nswanlab_callback = SwanLabCallback(\n    project=\"Qwen2-NER-fintune\",\n    experiment_name=\"Qwen2-1.5B-Instruct\",\n    description=\"使用通义千问Qwen2-1.5B-Instruct模型在NER数据集上微调，实现关键实体识别任务。\",\n    config={\n        \"model\": model_id,\n        \"model_dir\": model_dir,\n        \"dataset\": \"qgyd2021/chinese_ner_sft\",\n    },\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n\n# 用测试集的随机20条，测试模型\n# 得到测试集\ntest_df = total_df[:int(len(total_df) * 0.1)].sample(n=20)\n\ntest_text_list = []\nfor index, row in test_df.iterrows():\n    instruction = row['instruction']\n    input_value = row['input']\n    \n    messages = [\n        {\"role\": \"system\", \"content\": f\"{instruction}\"},\n        {\"role\": \"user\", \"content\": f\"{input_value}\"}\n    ]\n\n    response = predict(messages, model, tokenizer)\n    messages.append({\"role\": \"assistant\", \"content\": f\"{response}\"})\n    result_text = f\"{messages[0]}\\n\\n{messages[1]}\\n\\n{messages[2]}\"\n    test_text_list.append(swanlab.Text(result_text, caption=response))\n    \nswanlab.log({\"Prediction\": test_text_list})\nswanlab.finish()\n```\n\n看到下面的进度条即代表训练开始：\n\n![alt text](./ner/06.png)",
    "172": "一级标题：Qwen2命名实体识别\n二级标题：5.训练结果演示\n内容：\n在[SwanLab](https://swanlab.cn/@ZeyiLin/Qwen2-NER-fintune/runs/9gdyrkna1rxjjmz0nks2c/chart)上查看最终的训练结果：\n\n可以看到在2个epoch之后，微调后的qwen2的loss降低到了不错的水平——当然对于大模型来说，真正的效果评估还得看主观效果。\n\n![alt text](./ner/07.png)\n\n可以看到在一些测试样例上，微调后的qwen2能够给出准确的实体抽取结果：\n\n![alt text](./ner/08.png)\n\n![alt text](./ner/09.png)\n\n至此，你已经完成了qwen2指令微调的训练！",
    "173": "一级标题：Qwen2命名实体识别\n二级标题：6. 推理训练好的模型\n内容：\n训好的模型默认被保存在`./output/Qwen2-NER`文件夹下。\n\n推理模型的代码如下：\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\ndef predict(messages, model, tokenizer):\n    device = \"cuda\"\n\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\n    generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512)\n    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    return response\n\n\n# 加载原下载路径的tokenizer和model\ntokenizer = AutoTokenizer.from_pretrained(\"./qwen/Qwen2-1___5B-Instruct/\", use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"./qwen/Qwen2-1___5B-Instruct/\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n\n# 加载训练好的Lora模型，将下面的[checkpoint-XXX]替换为实际的checkpoint文件名名称\nmodel = PeftModel.from_pretrained(model, model_id=\"./output/Qwen2-NER/checkpoint-1700\")\n\ninput_text = \"西安电子科技大学的陈志明爱上了隔壁西北工业大学苏春红，他们约定好毕业后去中国的苏州定居。\"\ntest_texts = {\n    \"instruction\": \"\"\"你是一个文本实体识别领域的专家，你需要从给定的句子中提取 地点; 人名; 地理实体; 组织 实体. 以 json 格式输出, 如; {\"entity_text\": \"南京\", \"entity_label\": \"地理实体\"} 注意: 1. 输出的每一行都必须是正确的 json 字符串. 2. 找不到任何实体时, 输出\"没有找到任何实体\". \"\"\",\n    \"input\": f\"文本:{input_text}\"\n}\n\ninstruction = test_texts['instruction']\ninput_value = test_texts['input']\n\nmessages = [\n    {\"role\": \"system\", \"content\": f\"{instruction}\"},\n    {\"role\": \"user\", \"content\": f\"{input_value}\"}\n]\n\nresponse = predict(messages, model, tokenizer)\nprint(response)\n```\n\n输出结果为：\n\n```json\n{\"entity_text\": \"西安电子科技大学\", \"entity_label\": \"组织\"}\n{\"entity_text\": \"陈志明\", \"entity_label\": \"人名\"}\n{\"entity_text\": \"西北工业大学\", \"entity_label\": \"组织\"}\n{\"entity_text\": \"苏春红\", \"entity_label\": \"人名\"}\n{\"entity_text\": \"中国\", \"entity_label\": \"地理实体\"}\n{\"entity_text\": \"苏州\", \"entity_label\": \"地理实体\"}\n```",
    "174": "一级标题：Qwen2命名实体识别\n二级标题：相关链接\n内容：\n- 代码：完整代码直接看本文第5节 或 [Github](https://github.com/Zeyi-Lin/LLM-Finetune)\n- 实验日志过程：[Qwen2-1.5B-NER-Fintune - SwanLab](https://swanlab.cn/@ZeyiLin/Qwen2-NER-fintune/runs/9gdyrkna1rxjjmz0nks2c/chart)\n- 模型：[Modelscope](https://modelscope.cn/models/qwen/Qwen2-1.5B-Instruct/summary)\n- 数据集：[chinese_ner_sft](https://huggingface.co/datasets/qgyd2021/chinese_ner_sft)\n- SwanLab：[https://swanlab.cn](https://swanlab.cn)",
    "175": "一级标题：openMind大模型微调教程\n二级标题：无\n内容：",
    "176": "一级标题：openMind大模型微调教程\n二级标题：简介\n内容：\n魔乐社区（[Modelers.cn](https://modelers.cn)）是一个为人工智能开发者及爱好者打造的社区，提供工具链、数据集、模型和应用等AI领域生产要素的托管及展示服务和支撑系统。目前，魔乐社区已支持openMind Library。该工具通过简单的API接口，帮助开发者完成模型预训练、微调、推理等流程。同时，openMind Library原生兼容PyTorch 和 MindSpore 等主流框架，原生支持昇腾NPU处理器。openMind Library可以和PEFT、DeepSpeed等三方库配合使用，来提升模型微调效率。\n\n友情链接：\n\n* [魔乐社区](https://modelers.cn/docs/zh/openmind-library/0.9.1/overview.html)\n* [Huggingface](https://huggingface.co)\n* [SwanLab](https://swanlab.cn)\n\n---",
    "177": "一级标题：openMind大模型微调教程\n二级标题：1、基本概念\n内容：\n1、[openMind Library](https://modelers.cn/docs/zh/openmind-library/0.9.1/overview.html)--->[Huggingface Transformers](https://huggingface.co/docs/transformers/index)\n\nopenMind Library类似于transformers的大模型封装工具，其中就有AutoModelForSequenceClassification、AutoModelForCausalLM等等模型加载工具以及像TrainingArguments参数配置工具等等，原理基本一样，不过对NPU适配更友好些。\n![openmind vs transformers](/zh/examples/openMind/openmind_transformers.png)\n\n2、[魔乐社区](https://modelers.cn/)--->[HuggingFace](https://huggingface.co/)\n\n魔乐社区类似于huggingface这种模型托管社区，里面除了torch的模型还有使用MindSpore实现的模型。transformers可以直接从huggingface获取模型或者数据集，openMind也是一样的，可以从魔乐社区获取模型和数据集。\n![魔乐社区 vs huggingface](/zh/examples/openMind/mole.png)\n\n---",
    "178": "一级标题：openMind大模型微调教程\n二级标题：2、微调代码\n内容：\n如果了解了上述的对应机制，那么就可以跑一个简单的微调代码了，该代码参考了[魔乐社区的教程文档](https://modelers.cn/docs/zh/openmind-library/0.9.1/overview.html)，稍作调整，可以对比NVIDIA显卡的结果。\n\n### 概述\n\nopenMind Library是一个深度学习开发套件，通过简单易用的API支持模型预训练、微调、推理等流程。openMind Library通过一套接口兼容PyTorch和MindSpore等主流框架，同时原生支持昇腾NPU处理器，同时openMind Library可以和PEFT、DeepSpeed等三方库配合使用，来加速模型微调效率。\n\n### 环境配置\n\n#### 直接安装openMind环境\n\n如果是昇腾AI卡系列的话，配置环境前需要先安装驱动等设备，具体可以参考[软件安装-CANN商用版8.0.RC3开发文档-昇腾社区](https://www.hiascend.com/document/detail/zh/canncommercial/80RC3/softwareinst/instg/instg_0000.html?Mode=PmIns&OS=Ubuntu&Software=cannToolKit)。\n\n**驱动安装&验证**\n\n首先得确定有NPU卡和NPU相关驱动，驱动是8.0.RC3.beta1，如果没安装可以参考上面软件安装的链接查看。\n\n安装好后的验证方法是运行下面的命令，该命令作用与nvidia-smi类似，这里是查看NPU的状态和性能\n\n```bash\nnpu-smi info\n```\n\n可以看到如下信息的话就表示驱动已经安装完成了，左侧是安装成功后运行代码后的结果，右侧是每一部分的含义\n\n![npu-smi info](/zh/examples/openMind/npu-info.png)\n\n然后安装好驱动了之后就可以配置环境了，本次微调代码使用pytorch框架，openMind中自带了基于pytorch框架的各类函数，因此正常安装openMind就行。\n\n安装命令如下：\n\n```bash\n \n# 下载PyTorch安装包\nwget https://download.pytorch.org/whl/cpu/torch-2.4.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\n# 下载torch_npu插件包\nwget https://gitee.com/ascend/pytorch/releases/download/v6.0.rc3-pytorch2.4.0/torch_npu-2.4.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\n# 安装命令\npip3 install torch-2.4.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\npip3 install torch_npu-2.4.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\n# 安装openMind Library\npip install openmind[pt]\n# 安装SwanLab\npip install swanlab\n```\n\n> 注意以下几点：\n>\n> 1、可以使用镜像源来安装环境，不然会很浪费时间，可以使用清华源：\n>\n> ```bash\n> pip install -i https://pypi.tuna.tsinghua.edu.cn/simple name\n> ```\n>\n> 2、魔乐社区中有两个框架的分类，如果是pytorch就只能选择pytorch框架，同理如果是mindspore就只能选择mindspore框架\n> ![魔乐社区模型](/zh/examples/openMind/models.png)\n> 3、配置环境的时候，按照openmind官方文档说可以同时存在两个框架，使用的时候分别设置就行，但是实际使用的时候只能存在一个框架，一旦设置了两个框架，使用的时候无论如何设置都会报错说openmind不知道使用哪个框架，所以最好在环境里只安装一个\n>\n> ```bash\n> >>>import openmind\n> Traceback (most recent call last):\n>   File \"<stdin>\", line 1, in <module>\n>   File \"/home/miniconda3/envs/openmind-pt-cp39/lib/python3.9/site-packages/openmind/__init__.py\", line 20, in <module>\n>     from .utils import is_ms_available, is_torch_available\n>   File \"/home/miniconda3/envs/openmind-pt-cp39/lib/python3.9/site-packages/openmind/utils/__init__.py\", line 14, in <module>\n>     from .import_utils import (\n>   File \"/home/miniconda3/envs/openmind-pt-cp39/lib/python3.9/site-packages/openmind/utils/import_utils.py\", line 69, in <module>\n>     CURRENT_FRAMEWORK = get_framework()\n>   File \"/home/miniconda3/envs/openmind-pt-cp39/lib/python3.9/site-packages/openmind/utils/import_utils.py\", line 66, in get_framework\n>     raise RuntimeError(replace_invalid_characters(error_msg))\n> RuntimeError: Multiple frameworks detected, including: pt, ms.\n> ```\n\n#### docker环境安装（推荐）\n\nopenMind官方库也提供了模型的docker环境。\n\n推荐通过点击模型测试部分（下图红框）找到docker的链接，通过docker来拉起拉起环境。下面介绍docker环境的搭建教程。\n\n![bert模型环境](/zh/examples/openMind/bert.png)\n\n首先得确定有NPU卡和NPU相关驱动，驱动是**8.0.RC3.beta1**，如果没安装可以参考[CANN官方安装教程](https://www.hiascend.com/document/detail/zh/canncommercial/80RC3/softwareinst/instg/instg_0000.html?Mode=PmIns&OS=Ubuntu&Software=cannToolKit)\n\n完成安装后检测方法是运行\n\n```bash\nnpu-smi info\n```\n\n可以看到如下信息的话就表示驱动已经安装完成了。\n\n![npu-smi](/zh/examples/openMind/a_mask.png)\n\n接下来使用如下命令创建一个装好openmind环境的容器，这样可以省去大量安装环境的时间：\n\n```bash\ndocker run \\\n    --name openmind \\\n    --device /dev/davinci0 \\    # 指定NPU 0号设备\n    --device /dev/davinci_manager \\\n    --device /dev/devmm_svm \\\n    --device /dev/hisi_hdc \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/ \\\n    -v /etc/ascend_install.info:/etc/ascend_install.info \\\n    -v /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info \\\n    -tid registry.modelers.cn/base_image/openmind:openeuler-python3.10-cann8.0.rc3.beta1-pytorch2.1.0-openmind0.9.1 bash\n```\n\n这将在后台开启一个名为openmind容器。使用如下命令可进入到容器当中\n\n```bash\n docker exec -it openmind bash\n```\n\n出现如下界面即表示进入到容器当中\n\n![indocker](/zh/examples/openMind/indocker.png)\n\n最后在docker中运行如下命令安装swanlab即可完成环境安装。\n\n```bash\n# 安装swanlab命令\npip install swanlab\n```\n\n### 数据集处理\n\nOmDataset.load_dataset()方法目前支持下载的数据集格式如下：\n\n* parquet\n* json或者jsonl\n* tar.gz\n* csv\n* 下载python脚本加载魔乐社区数据集\n* 下载python脚本加载三方站点数据集\n\n```python\nfrom openmind import OmDataset\nfrom openmind import AutoTokenizer\n \n### 准备数据集\ndataset = OmDataset.load_dataset(\"AI_Connect/glue\", \"cola\")\n \n### 结果\n\"\"\"\nDatasetDict({\n    train: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 8551\n    })\n    validation: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 1043\n    })\n    test: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 1063\n    })\n})\n\"\"\"\n \n### 加载分词器\ntokenizer = AutoTokenizer.from_pretrained(\"PyTorch-NPU/bert_base_cased\")\n \n### 处理数据集\ndef tokenize_function(examples):\n    return tokenizer(examples[\"sentence\"],truncation=True,padding=\"max_length\",max_length=512)\n \n### 训练数据封装\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n \n# 训练数据+验证数据，验证发生在每个epoch之后\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\nsmall_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42)\n```\n\n### 加载模型\n\n和transformers使用差不多，分别加载模型和分词器\n\n```python\nfrom openmind import AutoTokenizer\nfrom openmind import AutoModelForSequenceClassification  ## 做分类任务\n \n \n### 加载分词器\ntokenizer = AutoTokenizer.from_pretrained(\"PyTorch-NPU/bert_base_cased\")\n \n### 加载模型\nmodel = AutoModelForSequenceClassification.from_pretrained(\"PyTorch-NPU/bert_base_cased\", num_labels=2)  # 二分类任务\n```\n\n### 训练参数配置\n\n创建一个TrainingArguments类，其中包含可以调整的所有超参数以及不同的训练选项。\n\n```python\nfrom openmind import TrainingArguments\n \n### 参数初始化\n# 指定保存训练检查点的路径\ntraining_args = TrainingArguments(logging_steps=1,\n                                  output_dir=\"test_trainer\",\n                                  evaluation_strategy=\"epoch\",\n                                  half_precision_backend=\"auto\",  # auto:自动选择合适的混合精度训练后端；apex：英伟达的 ；cpu_amp：在CPU上运行\n                                  per_device_train_batch_size=4,\n                                  optim=\"adamw_torch\",\n                                  learning_rate=2e-5)\n```\n\n### 评估参数设置\n\nTrainer在训练过程中不会自动评估模型性能，需要向Trainer传递一个函数来计算和展示指标。\n\n```python\nimport numpy as np\nfrom openmind import metrics\n \n### 配置评估参数\nmetric = metrics.Accuracy()\n \n \ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    return metric.compute(preds=preds, labels=labels)\n```\n\n### 可视化工具配置\n\nswanlab支持记录openMind Library。能够在线/离线查看训练日志。SwanLab支持openMind Library通过callback调用，调用代码可参考后文。\n\n![SwanLab可视化工具](/zh/examples/openMind/modelers&swanlab%20V2.png)\n关于SwanLab的使用方法可以参考[SwanLab官方文档-快速开始](https://docs.swanlab.cn/guide_cloud/general/quick-start.html)\n\n> 如果提示登录swanlab，可以在[官网完成注册](https://swanlab.cn)后，使用[获取API KEY](https://swanlab.cn/settings)找到对应的登陆密钥并粘贴，这样将能够使用**云上看版**随时查看训练过程与结果。\n\n```python\nfrom openmind import Trainer\nfrom swanlab.integration.transformers import SwanLabCallback\n \n### 使用swanlab监测\nswanlab_config = {\n    \"dataset\": \"glue\",\n    \"fp16_backend\":\"auto\",\n    \"datacollator\":\"transformer\"\n}\nswanlab_callback = SwanLabCallback(\n    project=\"new_qwen2.5-7B-finetune\",\n    experiment_name=\"跑的官方例子的微调\",\n    description=\"这个是使用transformers的datacollator封装函数\",\n    workspace=None,\n    config=swanlab_config,\n)\n \n### 创建训练器并且启动训练\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[swanlab_callback],\n)\n \ntrainer.train()\n \n### 保存模型\noutput_dir=\"./output\"\nfinal_save_path = join(output_dir)\ntrainer.save_model(final_save_path)\n```\n\n### 全过程代码\n\n```python\nfrom openmind import OmDataset\nfrom openmind import AutoTokenizer\nfrom openmind import AutoModelForSequenceClassification\nfrom openmind import TrainingArguments\nfrom openmind import metrics\nimport numpy as np\nfrom openmind import Trainer\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom os.path import join\n \n \n### 准备数据集\ndataset = OmDataset.load_dataset(\"AI_Connect/glue\", \"cola\")\n \n### 加载分词器\ntokenizer = AutoTokenizer.from_pretrained(\"PyTorch-NPU/bert_base_cased\")\n \n \n### 处理数据集\ndef tokenize_function(examples):\n    # 填充\n    return tokenizer(examples[\"sentence\"],truncation=True,padding=\"max_length\",max_length=512)\n \n \ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n \n# 减少数据量\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\nsmall_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42)\n \n \n### 加载模型\nmodel = AutoModelForSequenceClassification.from_pretrained(\"PyTorch-NPU/bert_base_cased\", num_labels=2)\n \n### 参数初始化\n# 指定保存训练检查点的路径\ntraining_args = TrainingArguments(logging_steps=1,\n                                  output_dir=\"test_trainer\",\n                                  evaluation_strategy=\"epoch\",\n                                  half_precision_backend=\"auto\",  # auto:自动选择合适的混合精度训练后端；apex：英伟达的 ；cpu_amp：在CPU上运行\n                                  per_device_train_batch_size=4,\n                                  optim=\"adamw_torch\",\n                                  learning_rate=2e-5)\n \n### 配置评估参数\nmetric = metrics.Accuracy()\n \n \ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    return metric.compute(preds=preds, labels=labels)\n \n \n### 使用swanlab监测\nswanlab_config = {\n    \"dataset\": \"glue\",\n    \"fp16_backend\":\"auto\",\n    \"datacollator\":\"transformer\"\n}\nswanlab_callback = SwanLabCallback(\n    project=\"new_qwen2.5-7B-finetune\",\n    experiment_name=\"跑的官方例子的微调\",\n    description=\"这个是使用transformers的datacollator封装函数\",\n    workspace=None,\n    config=swanlab_config,\n)\n### 创建训练器并且启动训练\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[swanlab_callback],\n)\n \ntrainer.train()\n \n### 保存模型\noutput_dir=\"./output\"\nfinal_save_path = join(output_dir)\ntrainer.save_model(final_save_path)\n```\n\n---\n\n这里使用HF Transformers实现同样的训练过程，使用NVIDIA-A100卡来跑了一次做个对比，A100对应的代码如下：\n\n```python\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TrainingArguments\nimport evaluate\nimport numpy as np\nfrom transformers import Trainer\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom os.path import join\nimport os\n \n# 设置只使用第一个GPU\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # 使用第一块 GPU\n \n### 加载数据集\ndataset = load_dataset(\"nyu-mll/glue\",\"cola\")\n \n### 加载分词器\ntokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n \n### 处理数据集\ndef tokenize_function(examples):\n    # 填充\n    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True)\n \ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n \n# 减少数据量\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\nsmall_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42)\n \n### 加载模型\nmodel = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=2)\n \n### 参数初始化\n# 指定保存训练检查点的路径\ntraining_args = TrainingArguments(logging_steps=1,\n                                  output_dir=\"test_trainer\",\n                                  evaluation_strategy=\"epoch\",\n                                  half_precision_backend=\"auto\",  \n                                  per_device_train_batch_size=4,\n                                  optim=\"adamw_torch\",\n                                  learning_rate=2e-5)\n \n### 配置评估参数\nmetric = evaluate.load(\"accuracy\")\n \ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    # 添加评估数据\n    metric.add_batch(predictions=preds, references=labels)  # 使用add_batch方法添加批次数据\n    # 计算准确度\n    return metric.compute()\n \n### 使用swanlab监测\nswanlab_config = {\n    \"dataset\": \"glue\"\n}\nswanlab_callback = SwanLabCallback(\n    project=\"new_qwen2.5-7B-finetune\",\n    experiment_name=\"跑的官方例子的微调\",\n    description=\"用例子跑的，模型用的是bert，做文本分类任务\",\n    workspace=None,\n    config=swanlab_config,\n)\n### 创建训练器并且启动训练\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[swanlab_callback],\n)\n \ntrainer.train()\n \n### 保存模型\noutput_dir=\"./output/A100\"\nfinal_save_path = join(output_dir)\ntrainer.save_model(final_save_path)\n```",
    "179": "一级标题：openMind大模型微调教程\n二级标题：3、结果展示\n内容：\n下面是Ascend NPU与A100实验对比：\n\n首先是实验时间，此次实验epoch=3，\n\n![时间对比](/zh/examples/openMind/time.png)\n\n看样子昇腾卡比A100稍微快点\n\n然后是显存消耗，其中两个监测NPU/GPU状态的代码如下：\n\n```bash\n# NPU：\nwatch -n 1 npu-smi info\n \n# GPU：\nnvtop\n```\n\n![显存对比](/zh/examples/openMind/xiancun.png)\n\n显存消耗差不多\n\n最后是loss等参数的变化\n\n![loss对比](/zh/examples/openMind/loss.png)\n\n感觉A100上运行的结果震荡比较明显，昇腾卡震荡比较少。",
    "180": "一级标题：从零预训练一个自己的大模型\n二级标题：无\n内容：\n[![](/assets/badge1.svg)](https://swanlab.cn/@ShaohonChen/WikiLLM/overview)\n\n大语言模型（Large Language Model，简称LLM），指使用大量文本数据训练的深度学习模型，可以生成自然语言文本或理解语言文本的含义。\n\n![llm](/assets/examples/pretrain_llm/llm.png)\n\n虽然网上有大量关于transformer理论、大语言模型微调的教程。但是少有关于预训练的解释。本文则从如何自己实战预训练一个大语言模型的角度，使用wiki数据集进行一个简单的从零预训练工作，并附上使用swanlab launch白嫖显卡的方法\n\n* 本教程完整代码：[GitHub](https://github.com/ShaohonChen/transformers_from_scratch)\n\n* 实验记录：[SwanLab](https://swanlab.cn/@ShaohonChen/WikiLLM/overview)\n\n* 数据集下载：[百度网盘（j8ee）](https://pan.baidu.com/s/1p5F52bRlnpSY7F78q0hz7A?pwd=j8ee)，[huggingface](https://huggingface.co/datasets/fjcanyue/wikipedia-zh-cn)",
    "181": "一级标题：从零预训练一个自己的大模型\n二级标题：安装环境\n内容：\n首先，项目推荐使用python3.10。需要安装的python包如下：\n\n```txt\nswanlab\ntransformers\ndatasets\naccelerate\n```\n\n使用如下命令一键安装：\n\n```bash\npip install swanlab transformers datasets accelerate modelscope\n```",
    "182": "一级标题：从零预训练一个自己的大模型\n二级标题：下载数据集\n内容：\n本教程使用的是中文wiki数据，理论上预训练数据集种类越丰富、数据量越大越好，后续会增加别的数据集。\n\n![dataset](/assets/examples/pretrain_llm/dataset.png)\n\nhuggingface链接：[wikipedia-zh-cn](https://huggingface.co/datasets/fjcanyue/wikipedia-zh-cn)\n\n百度网盘下载地址：[百度网盘（j8ee）](https://pan.baidu.com/s/1p5F52bRlnpSY7F78q0hz7A?pwd=j8ee)\n\n下载`wikipedia-zh-cn-20240820.json`文件后放到项目目录下`./WIKI_CN/`文件夹中\n\n该数据集文件约1.99G大，共有1.44M条数据。虽然数据集中包含文章标题，但是实际上在预训练阶段用不上。正文片段参考：\n\n```txt\n数学是研究数量、结构以及空间等概念及其变化的一门学科，属于形式科学的一种。数学利用抽象化和逻辑推理，从计数、计算、量度、对物体形状及运动的观察发展而成。数学家们拓展这些概念...\n```\n\n使用[🤗Huggingface Datasets](https://huggingface.co/docs/datasets/index)加载数据集的代码如下：\n\n```python\nfrom datasets import load_dataset\n\nds = load_dataset(\"fjcanyue/wikipedia-zh-cn\")\n```\n\n如果使用百度网盘下载的json文件，可以通过如下代码加载\n\n```python\nraw_datasets = datasets.load_dataset(\n    \"json\", data_files=\"data/wikipedia-zh-cn-20240820.json\"\n)\n\nraw_datasets = raw_datasets[\"train\"].train_test_split(test_size=0.1, seed=2333)\nprint(\"dataset info\")\nprint(raw_datasets)\n```",
    "183": "一级标题：从零预训练一个自己的大模型\n二级标题：构建自己的大语言模型\n内容：\n本教程使用[🤗huggingface transformers](https://huggingface.co/docs/transformers/index)构建自己的大模型。\n\n因为目标是训练一个中文大模型。因此我们参考[通义千问2](https://qwen.readthedocs.io/zh-cn/latest/run_locally/mlx-lm.html)的tokenize和模型架构，仅仅做一些简单的更改让模型更小更好训练。\n\n因为国内无法直接访问到huggingface，推荐使用modelscope先把模型配置文件和checkpoint下载到本地，运行如下代码\n\n```python\nimport modelscope\n\nmodelscope.AutoConfig.from_pretrained(\"Qwen/Qwen2-0.5B\").save_pretrained(\n    \"Qwen2-0.5B\"\n)\nmodelscope.AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\").save_pretrained(\n    \"Qwen2-0.5B\"\n)\n```\n\n配置参数，并修改模型注意力头数量、模型层数和中间层大小，把模型控制到大概120M参数左右（跟GPT2接近）。\n\n```python\nimport transformers\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"./Qwen2-0.5B\")   # 这里使用qwen2的tokenzier\nconfig = transformers.AutoConfig.from_pretrained(\n        \"./Qwen2-0.5B\",\n        vocab_size=len(tokenizer),\n        hidden_size=512,\n        intermediate_size=2048,\n        num_attention_heads=8,\n        num_hidden_layers=12,\n        n_ctx=context_length,\n        bos_token_id=tokenizer.bos_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n    )\nprint(\"Model Config:\")\nprint(config)\n```\n\n使用transformers库初始化模型\n\n```python\nmodel = transformers.Qwen2ForCausalLM(config)\nmodel_size = sum(t.numel() for t in model.parameters())\nprint(f\"Model Size: {model_size/1000**2:.1f}M parameters\")\n```",
    "184": "一级标题：从零预训练一个自己的大模型\n二级标题：设置训练参数\n内容：\n设置预训练超参数：\n\n```python\nargs = transformers.TrainingArguments(\n    output_dir=\"checkpoints\",\n    per_device_train_batch_size=24,  # 每个GPU的训练batch数\n    per_device_eval_batch_size=24,  # 每个GPU的测试batch数\n    eval_strategy=\"steps\",\n    eval_steps=5_000,\n    logging_steps=500,\n    gradient_accumulation_steps=12,  # 梯度累计总数\n    num_train_epochs=2, # 训练epoch数\n    weight_decay=0.1,\n    warmup_steps=1_000,\n    optim=\"adamw_torch\",  # 优化器使用adamw\n    lr_scheduler_type=\"cosine\",  # 学习率衰减策略\n    learning_rate=5e-4,  # 基础学习率，\n    save_steps=5_000,\n    save_total_limit=10,\n    bf16=True,  # 开启bf16训练, 对于Amper架构以下的显卡建议替换为fp16=True\n)\nprint(\"Train Args:\")\nprint(args)\n```",
    "185": "一级标题：从零预训练一个自己的大模型\n二级标题：初始化训练+使用swanlab进行记录\n内容：\n使用transformers自带的train开始训练，并且引入swanlab作为可视化日志记录\n\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\ntrainer = transformers.Trainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=args,\n    data_collator=data_collator,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    callbacks=[SwanLabCallback()],\n)\ntrainer.train()\n```\n\n如果是第一次使用SwanLab，需要登陆SwanLab官网[https://swanlab.cn/](https://swanlab.cn/)，注册，并且在如下位置找到和复制自己的key。\n\n![findkey](/assets/examples/pretrain_llm/findkey.png)\n\n接下来在命令行中输入\n\n```sh\nswanlab login\n```\n\n会看到提示输入key\n\n![login](/assets/examples/pretrain_llm/login.png)\n\n按照提示将key粘贴进去（注意key是不会显示到终端当中的）就可以完成配置，完成效果如下：\n\n![login2](/assets/examples/pretrain_llm/login2.png)",
    "186": "一级标题：从零预训练一个自己的大模型\n二级标题：完整代码\n内容：\n项目目录结构：\n\n```txt\n|---data\\\n|------wikipedia-zh-cn-20240820.json    # 数据集放在data文件夹中\n|--- pretrain.py\n```\n\n`pretrain.py`代码如下：\n\n```python\nimport datasets\nimport transformers\nimport swanlab\nfrom swanlab.integration.transformers import SwanLabCallback\nimport modelscope\n\ndef main():\n    # using swanlab to save log\n    swanlab.init(\"WikiLLM\")\n\n    # load dataset\n    raw_datasets = datasets.load_dataset(\n        \"json\", data_files=\"/data/WIKI_CN/wikipedia-zh-cn-20240820.json\"\n    )\n\n    raw_datasets = raw_datasets[\"train\"].train_test_split(test_size=0.1, seed=2333)\n    print(\"dataset info\")\n    print(raw_datasets)\n\n    # load tokenizers\n    # 因为国内无法直接访问HuggingFace，因此使用魔搭将模型的配置文件和Tokenizer下载下来\n    modelscope.AutoConfig.from_pretrained(\"Qwen/Qwen2-0.5B\").save_pretrained(\n        \"Qwen2-0.5B\"\n    )\n    modelscope.AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\").save_pretrained(\n        \"Qwen2-0.5B\"\n    )\n    context_length = 512  # use a small context length\n    # tokenizer = transformers.AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\")\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        \"./Qwen2-0.5B\"\n    )  # download from local\n\n    # preprocess dataset\n    def tokenize(element):\n        outputs = tokenizer(\n            element[\"text\"],\n            truncation=True,\n            max_length=context_length,\n            return_overflowing_tokens=True,\n            return_length=True,\n        )\n        input_batch = []\n        for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n            if length == context_length:\n                input_batch.append(input_ids)\n        return {\"input_ids\": input_batch}\n\n    tokenized_datasets = raw_datasets.map(\n        tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n    )\n    print(\"tokenize dataset info\")\n    print(tokenized_datasets)\n    tokenizer.pad_token = tokenizer.eos_token\n    data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\n    # prepare a model from scratch\n    config = transformers.AutoConfig.from_pretrained(\n        \"./Qwen2-0.5B\",\n        vocab_size=len(tokenizer),\n        hidden_size=512,\n        intermediate_size=2048,\n        num_attention_heads=8,\n        num_hidden_layers=12,\n        n_ctx=context_length,\n        bos_token_id=tokenizer.bos_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n    )\n    model = transformers.Qwen2ForCausalLM(config)\n    model_size = sum(t.numel() for t in model.parameters())\n    print(\"Model Config:\")\n    print(config)\n    print(f\"Model Size: {model_size/1000**2:.1f}M parameters\")\n\n    # train\n    args = transformers.TrainingArguments(\n        output_dir=\"WikiLLM\",\n        per_device_train_batch_size=32,  # 每个GPU的训练batch数\n        per_device_eval_batch_size=32,  # 每个GPU的测试batch数\n        eval_strategy=\"steps\",\n        eval_steps=5_00,\n        logging_steps=50,\n        gradient_accumulation_steps=8,  # 梯度累计总数\n        num_train_epochs=2,  # 训练epoch数\n        weight_decay=0.1,\n        warmup_steps=2_00,\n        optim=\"adamw_torch\",  # 优化器使用adamw\n        lr_scheduler_type=\"cosine\",  # 学习率衰减策略\n        learning_rate=5e-4,  # 基础学习率，\n        save_steps=5_00,\n        save_total_limit=10,\n        bf16=True,  # 开启bf16训练, 对于Amper架构以下的显卡建议替换为fp16=True\n    )\n    print(\"Train Args:\")\n    print(args)\n    # enjoy training\n    trainer = transformers.Trainer(\n        model=model,\n        tokenizer=tokenizer,\n        args=args,\n        data_collator=data_collator,\n        train_dataset=tokenized_datasets[\"train\"],\n        eval_dataset=tokenized_datasets[\"test\"],\n        callbacks=[SwanLabCallback()],\n    )\n    trainer.train()\n\n    # save model\n    model.save_pretrained(\"./WikiLLM/Weight\")  # 保存模型的路径\n\n    # generate\n    pipe = transformers.pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n    print(\"GENERATE:\", pipe(\"人工智能\", num_return_sequences=1)[0][\"generated_text\"])\n    prompts = [\"牛顿\", \"北京市\", \"亚洲历史\"]\n    examples = []\n    for i in range(3):\n        # 根据提示词生成数据\n        text = pipe(prompts[i], num_return_sequences=1)[0][\"generated_text\"]\n        text = swanlab.Text(text)\n        examples.append(text)\n    swanlab.log({\"Generate\": examples})\n\n\nif __name__ == \"__main__\":\n    main()\n\n```",
    "187": "一级标题：从零预训练一个自己的大模型\n二级标题：训练结果演示\n内容：\n运行如下命令\n\n```\npython pretrain.py\n```\n\n可以看到如下训练日志。由于训练时间较长，推荐使用tmux将训练任务hold住\n\n![terminal](/assets/examples/pretrain_llm/terminal.png)\n\n可以在[SwanLab](https://swanlab.cn)中查看最终的训练结果：\n\n![log](/assets/examples/pretrain_llm/log.png)\n\n<!-- 并且能够看到一些最终生成的案例：\n\n![sample]() -->",
    "188": "一级标题：从零预训练一个自己的大模型\n二级标题：使用训练好的模型进行推理\n内容：\n以“人工智能”为开头生成内容的代码如下：\n\n```python\npipe = transformers.pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\nprint(\"GENERATE:\", pipe(\"人工智能\", num_return_sequences=1)[0][\"generated_text\"])\n```\n\n推理效果如下：\n\n（模型训练ing，可以在[https://swanlab.cn/@ShaohonChen/WikiLLM/overview](https://swanlab.cn/@ShaohonChen/WikiLLM/overview)实时查看训练进展和推理效果）\n<!-- ![result]() -->",
    "189": "一级标题：从零预训练一个自己的大模型\n二级标题：参考链接\n内容：\n* 本教程完整代码:[GitHub](https://github.com/ShaohonChen/transformers_from_scratch)\n\n* 实验记录：[SwanLab](https://swanlab.cn/@ShaohonChen/WikiLLM/overview)\n\n* 数据集下载：[百度网盘（j8ee）](https://pan.baidu.com/s/1p5F52bRlnpSY7F78q0hz7A?pwd=j8ee)，[huggingface](https://huggingface.co/datasets/fjcanyue/wikipedia-zh-cn)",
    "190": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：无\n内容：\n[![](https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg)](https://swanlab.cn/@ZeyiLin/qwen3-sft-medical/runs/agps0dkifth5l1xytcdyk/chart)\n\n![09-01](./qwen3/01.png)\n\n[Qwen3](https://www.modelscope.cn/models?name=qwen3&page=1)是阿里通义实验室最近开源的大语言模型，发布时便登顶了开源LLM榜单第一名。同时，Qwen系列模型也超越LLaMA，成为了HuggingFace上最受欢迎的开源LLM。\n\n![09-02](./qwen3/02.png)\n\n可以说，不论是进行研究学习，还是应用落地，Qwen已经逐渐成为开发者的最优选项之一。\n\n那么，以Qwen3作为基座大模型，通过**全参数微调**的方式，实现垂直专业领域聊天，甚至**支持DeepSeek R1 / QwQ式的带推理过程的对话**，是学习**LLM微调**的入门任务。\n\n在本文中，我们会使用 [Qwen3-1.7b](https://www.modelscope.cn/models/Qwen/Qwen3-1.7B) 模型在 [delicate_medical_r1_data](https://modelscope.cn/datasets/krisfu/delicate_medical_r1_data) 数据集上做全参数微调训练，实现让微调后的Qwen3支持对医学问题进行DeepSeek R1式的推理回复。训练中用到了transformers、datasets等工具，同时使用[SwanLab](https://swanlab.cn)监控训练过程、评估模型效果。\n\n> 全参数微调需要大约32GB显存，如果你的显存大小不足，可以使用Qwen3-0.6b，或Lora微调。\n\n- **代码**：[Github](https://github.com/Zeyi-Lin/Qwen3-Medical-SFT)，或直接看本文第5节\n\n- **实验日志过程**：[qwen3-1.7B-linear - SwanLab](https://swanlab.cn/@ZeyiLin/qwen3-sft-medical/runs/agps0dkifth5l1xytcdyk/chart)，或 [SwanLab基线社区](https://swanlab.cn/benchmarks) 搜索“qwen3-sft-medical”\n\n- **模型**：[Modelscope](https://modelscope.cn/models/Qwen/Qwen3-1.7B)\n\n- **数据集**：[delicate_medical_r1_data](https://modelscope.cn/datasets/krisfu/delicate_medical_r1_data)\n\n- **SwanLab**：[https://swanlab.cn](https://swanlab.cn)",
    "191": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：知识点：什么是全参数微调？\n内容：\n大模型全参数微调是指对预训练大模型的**所有参数**进行更新和优化，区别于部分参数微调和LoRA微调。\n\n这种方法通过将**整个模型权重**（包括底层词嵌入、中间特征提取层和顶层任务适配层）在下游任务数据上进行梯度反向传播，使模型整体适应新任务的需求。**相比仅微调部分参数**，全参数微调能更充分地利用预训练模型的泛化能力，并针对特定任务进行深度适配，**通常在数据差异较大或任务复杂度较高的场景下表现更优。**\n\n![09-03](./qwen3/03.png)\n\n不过，全参数微调往往需要更高的计算资源和存储开销，且存在**过拟合风险**（尤其在小数据集上）。实际应用中常结合学习率调整、参数分组优化或正则化技术来缓解这些问题。\n\n全参数微调多用于对模型表现性能要求较高的场景，例如专业领域知识问答或高精度文本生成。\n\n更多微调技术可参考：https://zhuanlan.zhihu.com/p/682082440\n\n下面是实战正片：",
    "192": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：1. 环境安装\n内容：\n本案例基于**Python>=3.8**，请在您的计算机上安装好Python；\n\n另外，您的计算机上至少要有一张英伟达/昇腾显卡（显存要求大概**32GB**左右可以跑）。\n\n我们需要安装以下这几个Python库，在这之前，请确保你的环境内已安装了pytorch以及CUDA：\n\n```\nswanlab\nmodelscope==1.22.0\ntransformers>=4.50.0\ndatasets==3.2.0\naccelerate\npandas\naddict\n```\n\n一键安装命令：\n\n```bash\npip install swanlab modelscope==1.22.0 \"transformers>=4.50.0\" datasets==3.2.0 accelerate pandas addict\n```\n\n> 本案例测试于modelscope==1.22.0、transformers==4.51.3、datasets==3.2.0、peft==0.11.1、accelerate==1.6.0、swanlab==0.5.7",
    "193": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：2. 准备数据集\n内容：\n本案例使用的是 [delicate_medical_r1_data](https://modelscope.cn/datasets/krisfu/delicate_medical_r1_data) 数据集，该数据集主要被用于医学对话模型。\n\n该数据集由2000多条数据组成，每条数据包含Instruction、question、think、answer、metrics六列：\n\n![09-04](./qwen3/04.png)\n\n这里我们只取`question`、`think`、`answer`这三列：\n\n- `question`：用户提出的问题，即模型的输入\n- `think`：模型的思考过程。大家如果用过DeepSeek R1的话，回复中最开始的思考过程就是这个。\n- `answer`：模型思考完成后，回复的内容。\n\n我们的训练任务，便是希望微调后的大模型，能够根据`question`，给用户一个`think`+`answer`的组合回复，并且think和answer直接在网页展示上是有区分的。\n\n理清需求后，我们设计这样一个数据集样例：\n\n```json\n{\n\"question\": \"我父亲刚刚被诊断为活动性出血，医生说需要立即处理，我们该怎么做？\", \n\"think\": \"嗯，用户的问题是关于病人出现活动性出血时应采取哪些一般处理措施，...\",\n\"answer\": \"首先，您父亲需要卧床休息，活动性出血期间暂时不要进食。为了...\",\n}\n```\n\n在训练代码执行时，会将`think`和`answer`按下面这样的格式组合成一条完整回复：\n\n```\n<think>\n嗯，用户的问题是关于病人出现活动性出血时应采取哪些一般处理措施，...\n</think>\n\n首先，您父亲需要卧床休息，活动性出血期间暂时不要进食。为了...\n```\n\n---\n\n接下来我们来下载数据集，并进行必要的格式转换。\n\n这个流程非常简单，执行下面的代码即可：\n\n```python\nfrom modelscope.msdatasets import MsDataset\nimport json\nimport random\n\nrandom.seed(42)\n\nds = MsDataset.load('krisfu/delicate_medical_r1_data', subset_name='default', split='train')\ndata_list = list(ds)\nrandom.shuffle(data_list)\n\nsplit_idx = int(len(data_list) * 0.9)\n\ntrain_data = data_list[:split_idx]\nval_data = data_list[split_idx:]\n\nwith open('train.jsonl', 'w', encoding='utf-8') as f:\n    for item in train_data:\n        json.dump(item, f, ensure_ascii=False)\n        f.write('\\n')\n\nwith open('val.jsonl', 'w', encoding='utf-8') as f:\n    for item in val_data:\n        json.dump(item, f, ensure_ascii=False)\n        f.write('\\n')\n\nprint(f\"The dataset has been split successfully.\")\nprint(f\"Train Set Size：{len(train_data)}\")\nprint(f\"Val Set Size：{len(val_data)}\")\n```\n\n完成后，你的代码目录下会出现训练集`train.jsonl`和验证集`val.jsonl`文件。\n\n至此，数据集部分完成。",
    "194": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：3. 加载模型\n内容：\n这里我们使用modelscope下载Qwen3-1.7B模型（modelscope在国内，所以下载不用担心速度和稳定性问题），然后把它加载到Transformers中进行训练：\n\n```python\nfrom modelscope import snapshot_download, AutoTokenizer\nfrom transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n\n# 在modelscope上下载Qwen模型到本地目录下\nmodel_dir = snapshot_download(\"Qwen/Qwen3-1.7B\", cache_dir=\"./\", revision=\"master\")\n\n# Transformers加载模型权重\ntokenizer = AutoTokenizer.from_pretrained(\"./Qwen/Qwen3-1.7B\", use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"./Qwen/Qwen3-1.7B\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n```",
    "195": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：4. 配置训练可视化工具\n内容：\n我们使用SwanLab来监控整个训练过程，并评估最终的模型效果。\n\nSwanLab 是一款开源、轻量的 AI 模型训练跟踪与可视化工具，面向人工智能与深度学习开发者，提供了一个跟踪、记录、比较、和协作实验的平台，常被称为\"中国版 Weights & Biases + Tensorboard\"。SwanLab同时支持云端和离线使用，并适配了从PyTorch、Transformers、Lightning再到LLaMA Factory、veRL等40+ AI训练框架。\n\n![09-05](./qwen3/05.png)\n![09-06](./qwen3/06.png)\n\n这里直接使用SwanLab和Transformers的集成来实现，更多用法可以参考[官方文档](https://link.zhihu.com/?target=https%3A//docs.swanlab.cn/zh/guide_cloud/integration/integration-huggingface-transformers.html)：\n\n```python\nfrom transformers import TrainingArguments, Trainer\n\nargs = TrainingArguments(\n    ...,\n    report_to=\"swanlab\",\n    run_name=\"qwen3-1.7B\",\n)\n\ntrainer = Trainer(..., args=args)\n```\n\n如果你是第一次使用SwanLab，那么还需要去[https://swanlab.cn](https://link.zhihu.com/?target=https%3A//swanlab.cn/)上注册一个账号，在用户设置页面复制你的API Key，然后在训练开始时，选择【2】，然后粘贴进去即可：\n\n![09-07](./qwen3/07.png)",
    "196": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：5. 完整代码\n内容：\n开始训练时的目录结构：\n\n```\n|--- train.py\n|--- train.jsonl\n|--- val.jsonl\n```\n\ntrain.py：\n\n```python\nimport json\nimport pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom modelscope import snapshot_download, AutoTokenizer\nfrom transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\nimport os\nimport swanlab\n\nos.environ[\"SWANLAB_PROJECT\"]=\"qwen3-sft-medical\"\nPROMPT = \"你是一个医学专家，你需要根据用户的问题，给出带有思考的回答。\"\nMAX_LENGTH = 2048\n\nswanlab.config.update({\n    \"model\": \"Qwen/Qwen3-1.7B\",\n    \"prompt\": PROMPT,\n    \"data_max_length\": MAX_LENGTH,\n    })\n\ndef dataset_jsonl_transfer(origin_path, new_path):\n    \"\"\"\n    将原始数据集转换为大模型微调所需数据格式的新数据集\n    \"\"\"\n    messages = []\n\n    # 读取旧的JSONL文件\n    with open(origin_path, \"r\") as file:\n        for line in file:\n            # 解析每一行的json数据\n            data = json.loads(line)\n            input = data[\"question\"]\n            output = f\"<think>{data[\"think\"]}</think> \\n {data[\"answer\"]}\"\n            message = {\n                \"instruction\": PROMPT,\n                \"input\": f\"{input}\",\n                \"output\": output,\n            }\n            messages.append(message)\n\n    # 保存重构后的JSONL文件\n    with open(new_path, \"w\", encoding=\"utf-8\") as file:\n        for message in messages:\n            file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n\ndef process_func(example):\n    \"\"\"\n    将数据集进行预处理\n    \"\"\" \n    input_ids, attention_mask, labels = [], [], []\n    instruction = tokenizer(\n        f\"<|im_start|>system\\n{PROMPT}<|im_end|>\\n<|im_start|>user\\n{example['input']}<|im_end|>\\n<|im_start|>assistant\\n\",\n        add_special_tokens=False,\n    )\n    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n    attention_mask = (\n        instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n    )\n    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n        input_ids = input_ids[:MAX_LENGTH]\n        attention_mask = attention_mask[:MAX_LENGTH]\n        labels = labels[:MAX_LENGTH]\n    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}   \n\ndef predict(messages, model, tokenizer):\n    device = \"cuda\"\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\n    generated_ids = model.generate(\n        model_inputs.input_ids,\n        max_new_tokens=MAX_LENGTH,\n    )\n    generated_ids = [\n        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    return response\n\n# 在modelscope上下载Qwen模型到本地目录下\nmodel_dir = snapshot_download(\"Qwen/Qwen3-1.7B\", cache_dir=\"/root/autodl-tmp/\", revision=\"master\")\n\n# Transformers加载模型权重\ntokenizer = AutoTokenizer.from_pretrained(\"/root/autodl-tmp/Qwen/Qwen3-1.7B\", use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"/root/autodl-tmp/Qwen/Qwen3-1.7B\", device_map=\"auto\", torch_dtype=torch.bfloat16)\nmodel.enable_input_require_grads()  # 开启梯度检查点时，要执行该方法\n\n# 加载、处理数据集和测试集\ntrain_dataset_path = \"train.jsonl\"\ntest_dataset_path = \"val.jsonl\"\n\ntrain_jsonl_new_path = \"train_format.jsonl\"\ntest_jsonl_new_path = \"val_format.jsonl\"\n\nif not os.path.exists(train_jsonl_new_path):\n    dataset_jsonl_transfer(train_dataset_path, train_jsonl_new_path)\nif not os.path.exists(test_jsonl_new_path):\n    dataset_jsonl_transfer(test_dataset_path, test_jsonl_new_path)\n\n# 得到训练集\ntrain_df = pd.read_json(train_jsonl_new_path, lines=True)\ntrain_ds = Dataset.from_pandas(train_df)\ntrain_dataset = train_ds.map(process_func, remove_columns=train_ds.column_names)\n\n# 得到验证集\neval_df = pd.read_json(test_jsonl_new_path, lines=True)\neval_ds = Dataset.from_pandas(eval_df)\neval_dataset = eval_ds.map(process_func, remove_columns=eval_ds.column_names)\n\nargs = TrainingArguments(\n    output_dir=\"/root/autodl-tmp/output/Qwen3-1.7B\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=4,\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    logging_steps=10,\n    num_train_epochs=2,\n    save_steps=400,\n    learning_rate=1e-4,\n    save_on_each_node=True,\n    gradient_checkpointing=True,\n    report_to=\"swanlab\",\n    run_name=\"qwen3-1.7B\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n)\n\ntrainer.train()\n\n# 用测试集的前3条，主观看模型\ntest_df = pd.read_json(test_jsonl_new_path, lines=True)[:3]\n\ntest_text_list = []\n\nfor index, row in test_df.iterrows():\n    instruction = row['instruction']\n    input_value = row['input']\n\n    messages = [\n        {\"role\": \"system\", \"content\": f\"{instruction}\"},\n        {\"role\": \"user\", \"content\": f\"{input_value}\"}\n    ]\n\n    response = predict(messages, model, tokenizer)\n\n    response_text = f\"\"\"\n    Question: {input_value}\n\n    LLM:{response}\n    \"\"\"\n    \n    test_text_list.append(swanlab.Text(response_text))\n    print(response_text)\n\nswanlab.log({\"Prediction\": test_text_list})\n\nswanlab.finish()\n```\n\n看到下面的进度条即代表训练开始：\n\n![09-08](./qwen3/08.png)",
    "197": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：6. 训练结果演示\n内容：\n在SwanLab上查看最终的训练结果：\n\n![09-09](./qwen3/09.png)\n\n可以看到核心要关注的指标有train loss和eval loss，同时还有在训练完成时记录的3条LLM生成测试结果。\n\n让我们分析一下吧！使用**「创建折线图」**功能，把train loss和eval loss放到一张图上：\n\n![09-10](./qwen3/10.png)\n\n\n蓝色的是train loss，绿色的式eval loss，可以看到发生了经典的**过拟合**现象：train loss会在epoch的交替阶段，阶梯式下降；而val loss在第1轮epoch是下降的，但是第二轮反而上升了，符合机器学习中经典过拟合的定义。\n\n这也说明，在2000条数据这个量级，全参微调做1个epoch就可以了，往后反而效果越来越差。\n\n---\n\n那我们接下来看看模型生成的效果：\n\n![09-11](./qwen3/11.png)\n\n![09-12](./qwen3/12.png)\n\n可以看到，微调后的模型明确地有了思考过程，并且在思考过程的前后**增加了think标签**。\n\n回答的质量也挺不错，不过我对医学不太了解，也希望如果有学医的同学可以一起看看！\n\n一个example：\n\n```\nQuestion: 医生，我最近胃部不适，听说有几种抗溃疡药物可以治疗，您能详细介绍一下这些药物的分类、作用机制以及它们是如何影响胃黏膜的保护与损伤平衡的吗？\n\nLLM:<think>嗯，用户问的是抗溃疡药物的分类、作用机制，以及它们如何影响胃黏膜的保护和损伤平衡。首先，我需要回忆一下抗溃疡药物的主要分类。记得以前学过，抗溃疡药物通常分为几大类，比如抑酸药、胃黏膜保护剂、促胃动力药和抗幽门螺杆菌药物。可能还有其他类型，比如吸附剂之类的，但可能用户的问题里没有提到，所以可能不需要展开。\n\n接下来，每个类别的具体药物例子。比如抑酸药，常见的有H2受体拮抗剂和质子泵抑制剂。比如雷尼替丁、奥美拉唑这些。作用机制是抑制胃酸分泌，特别是H2受体拮抗剂通过阻断组胺引起的胃酸分泌，而质子泵抑制剂直接作用于胃酸分泌的最后一步，抑制质子泵。这部分需要详细说明。\n\n胃黏膜保护剂的话，比如硫糖铝、铋剂，它们的作用是形成保护层，或者促进黏液分泌，比如硫糖铝可能通过黏附在黏膜表面形成保护膜，而铋剂可能促进黏液分泌，同时可能有中和胃酸的作用？或者可能主要是中和作用？需要确认。另外，胶体果胶铋可能也是例子。\n\n促胃动力药比如多潘立酮、西沙必利，作用是增强胃蠕动，减少胃酸反流，这样胃排空快，可能减少溃疡形成。但用户的问题里提到的是促进胃排空，所以这部分需要说明。\n\n抗幽门螺杆菌的药物通常包括抗生素，比如阿莫西林、克拉霉素，但抗幽门螺杆菌药物可能还有三联或四联疗法，比如加上PPI和铋剂。需要提到这些药物的作用机制是抑制幽门螺杆菌的生长，比如抗生素杀灭细菌，而PPI可能同时抑制胃酸分泌，但如果是抗幽门螺杆菌药物的话，可能是指专门针对该病的药物，比如可能还有铋剂或者其他药物？\n\n不过用户的问题里提到抗幽门螺杆菌药物，可能需要明确是单独针对幽门螺杆菌，还是包括抗生素和PPI。可能需要指出，抗幽门螺杆菌药物通常包括抗生素和PPI，而PPI本身是抑酸药。所以可能需要说明这些药物如何通过抑制胃酸分泌和杀灭细菌来减少溃疡。\n\n然后，关于作用机制如何影响胃黏膜的保护和损伤平衡。比如，抑酸药减少胃酸，从而减少对黏膜的侵蚀，同时保护剂形成保护层，促进黏液分泌，增强黏膜屏障。促胃动力药通过增加胃蠕动，减少胃酸反流，从而保护黏膜。而抗幽门螺杆菌药物通过减少胃酸分泌和杀灭细菌，减少损伤因素。\n\n可能需要检查是否有遗漏的药物分类，比如吸附剂如活性炭，或者中和剂如氢氧化铝，但用户的问题可能只关注主要的四类。另外，是否需要提到铋剂的其他作用，比如促进黏液分泌和前列腺素合成？比如胶体果胶铋可能含有前列腺素，增强黏液分泌。\n\n在解释机制时，要确保每个药物的作用点和如何影响保护和损伤。例如，抑酸药直接减少胃酸，而保护剂形成物理屏障。促胃动力药通过改变胃排空模式，减少反流，从而减少损伤。抗幽门螺杆菌药物通过减少胃酸和杀灭病原体，两者都减少损伤。\n\n可能需要确认每个药物的正确分类和例子是否正确，比如西沙必利是否属于促胃动力药，或者是否属于H2受体拮抗剂？西沙必利其实是属于促胃动力药，因为它的作用是增强胃排空。而奥美拉唑是PPI，属于抑酸药。需要确保这些分类正确。\n\n另外，用户提到自己胃部不适，可能需要建议就医，但答案里已经包含这点，所以没问题。\n\n总结下来，结构应该是先分类，再每个类别下的药物例子和作用机制，然后说明这些机制如何影响保护和损伤平衡。需要确保每个部分都清晰，逻辑连贯，并且覆盖用户的问题点。\n\n</think>\n\n当然可以。抗溃疡药物主要分为四类：抑酸药、胃黏膜保护剂、促胃动力药和抗幽门螺杆菌药物。抑酸药如H₂受体拮抗剂和质子泵抑制剂，它们的作用是减少胃酸的分泌，从而减轻对胃黏膜的损伤。胃黏膜保护剂如硫糖铝和胶体果胶铋，它们能帮助形成一层保护膜，覆盖在胃黏膜表面，减少胃酸和胃蛋白酶对黏膜的侵蚀。促胃动力药如多潘立酮和西沙必利，通过增强胃部的蠕动，减少胃内食物的滞留时间，从而降低溃疡的风险。抗幽门螺杆菌药物则通过抑制幽门螺杆菌的生长，减少其对胃黏膜的伤害。这些药物的共同目标是通过不同的机制来平衡胃黏膜的保护因素和损伤因素，帮助溃疡愈合。如果您有具体的症状或担心某种药物，可以告诉我，我会为您推荐合适的治疗方案。\n\n```\n\n至此，你已经完成了qwen3微调！",
    "198": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：7. 推理训练好的模型\n内容：\n训好的模型默认被保存在`./output/Qwen3`文件夹下。\n\n推理模型的代码如下：\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndef predict(messages, model, tokenizer):\n    device = \"cuda\"\n\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\n    generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=2048)\n    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    return response\n\n# 加载原下载路径的tokenizer和model\ntokenizer = AutoTokenizer.from_pretrained(\"./output/Qwen3-1.7B/checkpoint-1000\", use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"./output/Qwen3-1.7B/checkpoint-1000\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n\ntest_texts = {\n    'instruction': \"你是一个医学专家，你需要根据用户的问题，给出带有思考的回答。\",\n    'input': \"医生，我最近被诊断为糖尿病，听说碳水化合物的选择很重要，我应该选择什么样的碳水化合物呢？\"\n}\n\ninstruction = test_texts['instruction']\ninput_value = test_texts['input']\n\nmessages = [\n    {\"role\": \"system\", \"content\": f\"{instruction}\"},\n    {\"role\": \"user\", \"content\": f\"{input_value}\"}\n]\n\nresponse = predict(messages, model, tokenizer)\nprint(response)\n```",
    "199": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：相关链接\n内容：\n- 代码：完整代码直接看本文第5节 或 [Github](https://github.com/Zeyi-Lin/Qwen3-Medical-SFT)\n- 实验日志过程：[qwen3-1.7B-linear - SwanLab](https://swanlab.cn/@ZeyiLin/qwen3-sft-medical/runs/agps0dkifth5l1xytcdyk/chart)，或 [SwanLab基线社区](https://swanlab.cn/benchmarks) 搜索“qwen3-sft-medical”\n- 模型：[Modelscope](https://modelscope.cn/models/Qwen/Qwen3-1.7B)\n- 数据集：[delicate_medical_r1_data](https://modelscope.cn/datasets/krisfu/delicate_medical_r1_data)\n- SwanLab：[https://swanlab.cn](https://swanlab.cn)",
    "200": "一级标题：Qwen3-\"VL\"——超小中文多模态模型的“拼接微调”之路1（附代码和SwanLab记录）\n二级标题：无\n内容：\n* 作者：情感机器实验室——陈少宏\n\n* 邮箱：<shaohon_chen@115lab.club>\n\n* GitHub：[https://github.com/ShaohonChen/Qwen3-SmVL](https://github.com/ShaohonChen/Qwen3-SmVL)\n* SwanLab：[https://swanlab.cn/@ShaohonChen/Qwen3-SmVL/overview](https://swanlab.cn/@ShaohonChen/Qwen3-SmVL/overview)\n* 数据集：[https://huggingface.co/datasets/HuggingFaceM4/the_cauldron](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron)",
    "201": "一级标题：Qwen3-\"VL\"——超小中文多模态模型的“拼接微调”之路1（附代码和SwanLab记录）\n二级标题：摘要\n内容：\n最近Huggingface团队发布了超小多模态模型SmolVLM2，可以做到端侧1GB显存推理。在怀着惊喜试用后发现，虽然模型有极其强大的视觉文本理解能力，但是模型却无法理解中文。这对一个“四六级压线过”的笔者来说十分不友好。刚好前段时间做SwanLab硬件检测适配时有一台未到期的沐曦曦云C500服务器，因此萌生了使用**沐曦GPU芯片**微调、把当前中文小模型扛把子Qwen3与SmolVLM2直接微调拼接的想法。\n\n本教程将介绍一种模型拼接的思路，将SmolVLM2的视觉模块（0.09B）与Qwen3最小的模型（0.6B）进行对齐微调，最终使得Qwen模型具备一定的视觉理解能力。由于笔者时间有限且考虑到文章篇幅的原因，因此该系列预计将以系列的方式放出。篇幅规划如下：\n\n* **第一篇**：如何构建和微调一个拼接模型（**本篇博客**）\n* **第二篇**：模型测评、数据集优化、回答人类对齐\n* **第三篇**：微调技巧介绍、视觉位置编码改动与模型结构优化\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/PPAP.png\" alt=\"PPAP\" width=\"400\" />\n  <figcaption>I have a Qwen, I have a SmolVLM...</figcaption>\n  </figure>\n</div>\n\n<div style=\"background-color:#fff3cd; color:black; padding:10px; border-radius:4px; border:1px solid #fbe5b0; width: 90%; max-width: 100%; margin: auto;\">\n  ⚠️关于算力的注意：本教程涉及VLM微调训练，对算力要求较高，需要40G及以上的GPU显存才能运行本教程的训练代码。\n</div>",
    "202": "一级标题：Qwen3-\"VL\"——超小中文多模态模型的“拼接微调”之路1（附代码和SwanLab记录）\n二级标题：目录\n内容：\n[[toc]]",
    "203": "一级标题：Qwen3-\"VL\"——超小中文多模态模型的“拼接微调”之路1（附代码和SwanLab记录）\n二级标题：SmolVLM2的背景知识\n内容：\n首先，我们先回顾一下SmolVLM2模型的构建方案，SmolVLM2模型的整体包括三大块：视觉模型层，特征映射层和大语言模型层，见下图：\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/smolvlm2.png\" alt=\"smolvlm2\" width=\"400\" />\n  <figcaption>SmolVLM2的架构图</figcaption>\n  </figure>\n</div>\n\n这个设计是现在比较常见的VLM方案。核心设计思想就是让视觉模型的输出特征与经过embedding的文本特征直接拼接后输入到语言模型（LLM）当中，没有交叉注意力等模块。相比于早期LLaVA等架构，这种最大的优点就是可以最大程度复用已有的语言模型。以Qwen2.5-VL为例，其3B、7B、72B模型大小指的只是LLM部分，并没有包含Vision模块，实际上3B模型的参数量接近4B，视觉模块大概0.4B左右，三个不同大小的VLM使用的是统一的视觉模型。对于一些较大的VLM来说，构建视觉模型时绝大多数的训练都集中在特征映射模块和视觉模块，只在最后阶段为了最终效果进行整体微调时才会调整语言模块。保证了VLM的语言能力。\n\n下面简述一下各个模块的细节：\n\n* 视觉模型层：SmolVLM2-256M版本用的是Google的SigLip模型，一个基于ViT的视觉模型，选用的是最小的SigLip-93M的版本，HF论文里没具体写是直接用的SigLip的参数还是他们从零构建的（有注意到的读者可以评论留言下）。在SmolVLM2代码中对应的是`SmolVLMVisionTransformer`类\n\n* 特征映射层：就是一个简单的MLP，不过SmolVLM2中为了降低图像分辨率还做了一个Pixel shuffle来降低图像分辨率，进一步减少视觉的Token占用，减少了文本长度。HF团队在论文里提到对于参数量较小的VLM来说使用Pixel shuffle还能提升性能。但可训练参数其实就是一个单层的神经网络，这个模块的核心作用就是做特征对齐，将视觉特征从768维（SigLip的维度）映射到576维（SmolLLM2的维度）\n\n* 大语言模型：SmolVLM2-256M模型使用的文本模型是SmolLM-135M版本。可能是由于模型较小，HF团队在论文中说到训练时仅采用两阶段训练：大规模图文训练+针对视频任务的专门微调。为了保障模型的文本能力HF团队在训练数据中参杂了大概14%的纯文本微调数据。不过考虑到视觉模块本身参数量（93M）大小接近于文本模型（135M），因此笔者推测相比于冻结文本模型，数据平衡在这之中会起到更关键的作用。\n\nHF团队在原文中还提到了许多影像小模型VLM性能的trick，感兴趣的读者可以进一步参考SmolVLM2的论文",
    "204": "一级标题：Qwen3-\"VL\"——超小中文多模态模型的“拼接微调”之路1（附代码和SwanLab记录）\n二级标题：模型拼接和微调思路简介\n内容：\n正所谓顶级食材（模型）只需要最简单的烹饪。模型拼接的思路非常简单直接，基本就三步：\n\n1. 调整SmolVLM2的“上下文控制格式”，使得其与Qwen3兼容。\n\n2. 将模型的文本部分直接从SmolLM2换成Qwen3-0.6B，包括其文本tokenizer和词嵌入、文本模型、以及模型最后输出的语言模型头（LM Head）。\n\n3. 需要重新初始化特征映射层的MLP，从768->576的单层神经网络改成768->1024的单层神经网络即可。\n\n整体架构和对图文对前后处理依旧保持SmolVLM2的流程不变，具体改动见下图：\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/concatation.png\" alt=\"concatation\" width=\"400\" />\n  <figcaption>将Qwen3-0.6B替换SmolVLM2的语言模型部分</figcaption>\n  </figure>\n</div>\n\n笔者接下来详细介绍下为了实现“拼接”，具体改动的地方，供之后有类似的任务的读者参考。",
    "205": "一级标题：Qwen3-\"VL\"——超小中文多模态模型的“拼接微调”之路1（附代码和SwanLab记录）\n二级标题：模型拼接实现和关键代码讲解\n内容：\n### 第一处改动：SmolVLM2的Tokenizers部分\n\n首先需要改动的就是需要改动的是SmolVLM2的Tokenizers部分，这里面主要是涉及两个问题：\n\n* 第一个问题是要将SmolVLM2用于指示图像位置的特殊令牌（Special Token）加入到Qwen3的Tokenizer当中，这么做的目的是防止SmolVLM2的图像Token`<image>`被切分为`<`、`image`、`>`三块。幸运的是，Qwen3本身在Tokenizers中预留了未来用于多模态的特殊特殊令牌`<|image_pad|>`。因此读者直接使用了`<|image_pad|>`代替了`<image>`。用于在文本中预留图像特征的插入点。\n\n* 第二个问题是：SmolVLM2的chat_template和Qwen3的chat_template差别极大。chat_template的作用是通过格式化文本让模型清楚知道不同Token所代表的背景信息。用最近比较流行的话来说就是“上下文工程”（Context Engineering）。\n\n这里我列举了一下Qwen3、SmolVLM2、Qwen2.5-VL在聊天场景下的上下文，供读者参考。\n\n**Qwen3聊天上下文格式**\n\n以给一张图片，问题是“你的名字是什么?”，模型回答是“我的名字是Qwen”为例子。模型的上下文如下：\n\n```txt\n<|im_start|>user\n你的名字是什么?<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n我的名字是Qwen<|im_end|>\n\n```\n\n注意Qwen3上下文是没有预留图像位置的，但相比于一般的LLM和VLM多了一个用于插入模型思考过程的`<think><\\think>`，以及包含额外的函数调用控制文本。为了便于读者理解，读者在在下面举了一个函数调用的例子。这些函数调用上下文用于控制模型调用外部函数、API或者MCP接口和接收其返回的信息。\n\n考虑到篇幅限制，本文就不粘贴带函数调用、推理、思考等一系列上下文的信息了（笔者打印了下发现实在太长了）。感兴趣的读者可以在Qwen3的官方文处了解详细设计\n\n* [Qwen3函数调用案例](https://qwen.readthedocs.io/zh-cn/latest/framework/function_call.html#the-example-case)\n\n可以说正是这些复杂的上下文信息让模型有可能实现推理、调用函数等多样化的能力。包括多模态理解任务也需要先对上下文进行设计。\n\n**SmdwadwdoVLM2聊天上下文格式：**\n\n以给一张图片，问题是“How many dog in there.”，模型回答是“There are Three dogs.”为例子。三种不同模型的上下文如下：\n\n```txt\n<|im_start|>User:<fake_token_around_image><row_1_col_1><image>...<image><fake_token_around_image><row_1_col_2><image>...<image><fake_token_around_image><row_1_col_3><image>...<image>...<fake_token_around_image><row_4_col_4><image>...<image>\n\n<fake_token_around_image><global-img><image>...<image><fake_token_around_image>How many dog in there.<end_of_utterance>\nAssistant: There are Three dogs.<end_of_utterance>\nAssistant:\n```\n\n看起来非常乱，是因为有大量的`<image>`占位符。`<image>...<image>`之间是许多的`<image>`，笔者为了文章观感删掉了大量的占位符。注意模型的回车、空格均为上下文的一部分，在进行推理时需要严格遵守缩进关系。\n\n但是我们仍能找到熟悉的内容，如`User:`，`Assistant:`等用于提示模型用户的输入与模型应当输出的位置。这些关键词和Qwen类似。\n\n读者注意到了除了`<fake_token_around_image>`，`<image>`等用于指示图像的词，还出现了<row_1_col_1>这种位置指示符，这是因为SmolVLM2为了防止降采样对图像分辨率影响，专门使用了`image splitting`技术，简单来说就是将全局图和高清的局部图共同输入到模型当中（见下图`image splitting`模块），感兴趣的读者可在文末找到HF的技术报告了解详细技术。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/image-split.png\" alt=\"image-split\" width=\"400\" />\n  <figcaption>SmolVLM2的完整推理流程，可以看到在图像输入前使用`image splitting`进行了预切分</figcaption>\n  </figure>\n</div>\n\n**本博文的拼接模型Qwen3-SmVL模型**\n\n相比于Qwen3，SmolVLM2少了很多上下控制的\n\n为了尽可能保存或者说预留Qwen3的思考、函数调用等能力，笔者最终选择将SmolVLM2对于图像特征的排列插入到Qwen3的上下文格式当中。最终上下文格式如下：\n\n```txt\n<|im_start|>user\n<vision_start><row_1_col_1><|image_pad|>（图像插入的地方）<|image_pad|><vision_start>\n（用户提问的地方）\n<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n（模型回答的地方）<|im_end|>\n<|endoftext|>\n```\n\n可以看到读者尽量保持了与Qwen3的风格和复用特殊令牌。这样能够使得后续拼接的Qwen3-0.6B模型不至于受到上下文差异过大带来的性能损耗。实际上在设计微调上下文时应尽量与模型先前训练的任务接近，以减少微调带来的性能损失。\n\ntransformers实现模型上下文格式控制的代码并非python语言，而是一种前端文本格式控制的语言Jinja。这个语言的变量作用域设计简直可以说是有魔法在里面。配合上Qwen3功能丰富且复杂的上下文策略，让笔者花了2个小时用于修改chat_teamplate。这里笔者不赘述如何修改chat_template，感兴趣的读者可以去文末代码链接寻找`chat_template.jinja`文件，笔者专门将chat_template模版拿出来，并且做了格式化方便读者阅读。未来有时间了笔者专门写一篇模型上下文控制与jinja语言的博客。\n\n### 第二处改动：替换SmolVLM2的SmolLM2模型为Qwen3-0.6B\n\n替换模型这块没什么复杂的，主要是需要处理Transformers比较复杂的嵌套逻辑。Tranformers通常建议模型将预训练模型backbone和下游任务分开来。改动逻辑图如下：\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/change_model.png\" alt=\"change_model\" width=\"400\" />\n  <figcaption>替换smolvlm2的文本模块和语言模型头</figcaption>\n  </figure>\n</div>\n\n以Qwen3为例，预训练Backbone模型为`Qwen3Model`，仅仅包含embedding层、各个Decoder层，最后输出的是所有输入token的hidden state。负责下游任务的Qwen3提供了包括：用于因果语言序列生成的`Qwen3ForCausalLM`，也就是大家常用的语言生成。负责句子分类`Qwen3ForSequenceClassification`，使用最后一个生成的token输入到一个单层MLP做序列级分类，做句子情绪分类等可以用这个下游模型；`Qwen3ForTokenClassification`用于做Token级分类，比如语言实体抽取任务可以使用这个下游模型。`Qwen3ForQuestionAnswering`则是专门做抽取式问答任务的模型，核心思想是输入（问题，参考文本）让模型从参考文本中找到与问题最相关的一段，这类任务由于RAG系统的出现没那么流行了，未来笔者专门出一个系列的教程阐述除了因果语言序列生成以外的任务则怎么微调。\n\n**关键代码如下**\n\n```python\nfrom transformers import (\n    AutoProcessor,\n    AutoModelForImageTextToText,\n    AutoTokenizer,\n    AutoModelForCausalLM\n)\n\n# 替换text模型和head\nsmolvlm2_02B_model = AutoModelForImageTextToText.from_pretrained(\n    \"model/SmolVLM2-256M-Video-Instruct\",\n    torch_dtype=torch.bfloat16,\n    _attn_implementation=\"eager\",\n).to(device)\n\nqwen3_06b_model = AutoModelForCausalLM.from_pretrained(\n    \"model/Qwen3-0.6B\", torch_dtype=torch.bfloat16\n).to(device)\n\nsmolvlm2_02B_model.model.text_model = qwen3_06b_model.model\nsmolvlm2_02B_model.lm_head = qwen3_06b_model.lm_head\n...\n```\n\n接下来比较复杂的是替换所有的关键变量，比如模型内用于在文本序列中为图像特征预留的占位符`image_token_id`，用于指示停止生成的`eos_token_id`，和计算loss值会用到的`vocab_size`，Qwen的词表大小为151936，远远大过SmolVLM2的词表49280。具体代码如下：\n\n```python\n...\n# 替换词表大小\nsmolvlm2_02B_model.vocab_size = qwen3_06b_model.vocab_size\nsmolvlm2_02B_model.model.vocab_size = qwen3_06b_model.vocab_size\nsmolvlm2_02B_model.config.vocab_size = qwen3_06b_model.vocab_size\nsmolvlm2_02B_model.config.text_config.vocab_size = qwen3_06b_model.vocab_size\nsmolvlm2_02B_model.model.config.vocab_siz = qwen3_06b_model.vocab_size\nsmolvlm2_02B_model.model.config.text_config.vocab_size = qwen3_06b_model.vocab_size\n# 替换图像token\nsmolvlm2_02B_model.image_token_id = 151655\nsmolvlm2_02B_model.model.image_token_id = 151655\nsmolvlm2_02B_model.config.image_token_id = 151655\nsmolvlm2_02B_model.model.config.image_token_id = 151655\n# 替换模型生成停止符\nsmolvlm2_02B_model.generation_config.eos_token_id = 151645\n···\n```\n\n上面的代码可以看到在替换各个变量时需要将嵌套模型的变量一起替换掉，笔者之前训练时就因为仅仅替换了`SmolVLMForConditionalGeneration`而忘记替换`SmolVLMModel`中的`image_token_id`，导致语言模型接收不到图像特征，最后表现出来就是loss下降的极快且低，grad_norm看起来也学到位了，一推理效果特别差，附上错误训练的损失图：\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/fail_train.png\" alt=\"fail_train\" width=\"800\" />\n  <figcaption>SwanLab记录训练结果展示：蓝色为错误训练的完整微调loss图，可以看到损失下降很快，然而实际推理会发现模型并没有图像理解能力。冻结语言模型头（红色）后发现grad_norm为零且loss不收敛，正确的应该是黄色</figcaption>\n  </figure>\n</div>\n\n笔者最早没发现改动错误，先做完整微调（蓝色曲线）后发现损失下降很快达到了0.1以下，结果实际一推理发现模型完全没有图像理解能力，就补了一个冻结语言模型只微调视觉模型的实验（红色曲线），结果发现损失完全没下降，才定位到了视觉特征传入有问题。后续修复后正确的损失下降过程见黄色图像。\n\n### 第三处改动：构建和替换特征映射层\n\n这个相对较简单，只需要重新构建一个维度对齐的`SmolVLMConnector`即可。Qwen3的hidden_dim是1024，SigLip的hidden_dim是768，因此构建一个768➡️1024映射的`SmolVLMConnector`即可。代码如下：\n\n```python\n···\n# 构建配置并且创建连接器\n@dataclass\nclass VisionConfig:\n    hidden_size: int = 768\n\n@dataclass\nclass TextConfig:\n    hidden_size: int = 1024\n\n@dataclass\nclass ConnectConfig:\n    scale_factor: int = 4\n    vision_config: VisionConfig = VisionConfig()\n    text_config: TextConfig = TextConfig()\n\nnew_connector_config = ConnectConfig()\n\n# 替换 SigLit 到 LLM 的 connector 层\nnew_connector = SmolVLMConnector(new_connector_config).to(device).to(torch.bfloat16)\nsmolvlm2_02B_model.model.connector = new_connector\n···\n```",
    "206": "一级标题：Qwen3-\"VL\"——超小中文多模态模型的“拼接微调”之路1（附代码和SwanLab记录）\n二级标题：微调数据集构建\n内容：\n笔者最初计划寻找中文多模态数据集，但发现相关的资料比较少。因此决定先用英文的多模态数据集凑合一下。之后再考虑通过数据合成的方式将部分数据翻译为中文。关于数据合成和配比的问题将在之后的博客讨论。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/the_cauldron.png\" alt=\"the_cauldron\" width=\"400\" />\n  <figcaption>the_cauldron数据集logo</figcaption>\n  </figure>\n</div>\n\n这里为了方便本项目直接使用HuggingFace团队整合的多模态数据集the Cauldron数据集，Cauldron翻译成中文类似于煮东西的“釜”，不知道HF团队是不是玩“炼丹”的梗。这个数据集整合了50个视觉微调任务数据集的训练集，用于微调Huggingface发布的多模态模型Idefics2模型。这50多个数据集都被处理成了一致的格式（见下图），共有1,880,992条数据，完整下载约169G，非常方便使用。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/data_show.png\" alt=\"data_show\" width=\"800\" />\n  <figcaption>数据集样本展示</figcaption>\n  </figure>\n</div>\n\n不过可惜数据集的文本都是英文内容，且绝大多数数据集的回复非常短，只有一个词，这也给后面模型训练带来了麻烦。本篇博客暂时不讨论关于数据构建和配比的问题，后续有时间了专门做相关的实验。本博客先以为Qwen3模型带来视觉能力为核心目标。\n\n数据集的下载链接如下，国内推荐用modelscope下载：\n\n* [HuggingFace Hub](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron)\n* [ModelScope](https://modelscope.cn/datasets/AI-ModelScope/the_cauldron)\n\n笔者在实际测试时发现\"mimic_cgd\"，\"localized_narratives\"，\"okvqa\"，\"ocrvqa\"，\"clevr_math\"这几个子数据集加载有点异常，建议使用此数据集训练的读者手动处理下，社区也有用户反馈这几个数据可以在原始来源处额外下载，未来笔者将会补全这几个数据集重新上传一次完整版的the Cauldron数据集。",
    "207": "一级标题：Qwen3-\"VL\"——超小中文多模态模型的“拼接微调”之路1（附代码和SwanLab记录）\n二级标题：微调方法与代码实现\n内容：\n### 冻结模型参数微调\n\n整体微调方法采用了CLM模型通常的Teacher Forcing的学习方法，损失就是标准的交叉熵损失。考虑到此次本教程的目标是先确保模型具备中文多模态能力（优化模型性能等之后撰写其他博客），因此为了实验效率，在对齐微调阶段**采用冻结视觉模型与文本模型，仅微调特征映射器和语言模型头**的方法。\n\n冻结模型参数的核心代码如下：\n\n```python\ndef freeze_model(qwen_smvl):\n    for _, param in qwen_smvl.model.text_model.named_parameters():\n        param.requires_grad = False\n    for _, param in qwen_smvl.model.vision_model.named_parameters():\n        param.requires_grad = False\n    return qwen_smvl\n```\n\n冻结后训练参数、模型总参数、与占比如下：\n\n```txt\ntrainable params: 12.00M || all params: 662.87M || trainable%: 1.81\n```\n\n### 文本长度，损失掩码和截断策略\n\n**文本长度**\n\n由于视觉特征需要占据大量的文本长度，笔者简单测试了下the_cauldron图像占0.8K到1.3K左右的token。而数据集中大多数文本token数在200-500左右，极少情况会有3-4K的情况。因此笔者统一采用2K的文本长度，超出部分截断处理。\n\n这里有一个不同于文本微调的细节要注意，文本截断长度不能小于图像token，否则会导致模型在进行特征拼接时报错（当然图像特征如果被截断了，这条训练数据也就没意义了）。因此对于显存不足64G的同学如果需要适当缩短文本长度（不建议低于1.5K），最好连同图像分辨率也缩小些。在后面的博客我们会专门增加对减少图片token占用的研究。\n\n同样由于文本长度受限，且图像特征没法截断，我们也没使用“packing dataset”的方法提升模型的训练效率。\n\n考虑到部分数据集存在多张图片的情况，考虑到本次训练仅采用2k的文本长度（与之对比HF在训练SmolVLM-256M版本采用的是8K的文本长度，2.2B版使用了16K的文本长度）。针对单条数据中存在多张图片的情况仅仅选用第一张。\n\n**损失掩码**\n\n在采用Teacher Forcing的学习方法时，文本微调中损失掩码有两种策略：\n\n* 对包含“用户问题”和“模型回复”的完整文本进行微调优化\n* 仅对“模型回复”部分进行微调优化\n\n这两种策略的对比如下图：\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/mask.png\" alt=\"mask\" width=\"800\" />\n  <figcaption>两种微调掩码策略的差异，通常建议选择“仅微调模型回答部分”以增强泛化性</figcaption>\n  </figure>\n</div>\n\n通常来说使用“仅微调模型回复部分”的策略模型更容易泛化（这点与HF在SmolVLM2的论文提到的trick）。然而笔者为了提高训练效率选择了完整文本微调。可以在后续博客中增加消融实验做进一步对比。\n\n值得注意的是，在进行完整文本微调时，需要单独屏蔽Image Token以防止对图像占位token计算损失，影响模型表现。\n\n**关键代码如下：**\n\n```python\ndef data_collate_fix2k(examples, processor, device, max_length=2048):\n    batch_text = []\n    batch_image = []\n    for example in examples:\n        images = example[\"images\"][:1]  # 只允许一张图，不然显存压力太大\n        batch_image.append(images)\n        image_num = len(images)\n        chat_texts = example[\"texts\"][0]\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [{\"type\": \"image\"}] * image_num\n                + [{\"type\": \"text\", \"text\": chat_texts[\"user\"]}],\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": [{\"type\": \"text\", \"text\": chat_texts[\"assistant\"]}],\n            },\n        ]\n        text = processor.apply_chat_template(\n            messages, enable_thinking=False, add_generation_prompt=False\n        )\n\n        batch_text.append(text)\n\n    batch = processor(\n        text=batch_text,\n        images=batch_image,\n        max_length=max_length,\n        return_tensors=\"pt\",\n        padding=\"max_length\",\n        truncation=True,\n    )\n    labels = batch[\"input_ids\"].clone()\n    labels[labels == processor.tokenizer.pad_token_id] = -100\n    labels[labels == processor.image_token_id] = -100\n    batch[\"labels\"] = labels\n    return batch.to(device, dtype=torch.bfloat16)\n```\n\n### 微调超参数设置\n\n**学习率**\n\n由于仅仅针对特征映射层（connector）进行训练，且conntector由于要对齐Qwen3的维度因此参数为随机初始化（理论上可以采用一些独特的初始化策略提升性能，但考虑到模型较小因此笔者没关注初始化策略）。因此学习率设置为lora中较为流行的1e-4学习率策略。\n\n为了保障有效收敛，学习率衰减基本是必备的trick，采用的是社区比较流行的cosine学习率衰减，衰减至0。warm up为整体步长的10%（在超过1000k step的情况下固定为50）。\n\n**batch size**\n\nBatch size通常来说越大越好，然而由于VLM模型的文本长度太大，因此采用每卡1 batch和4梯度累加（grad accelerate），在8卡训练中等效32 Batch size。\n\n**训练参数设置代码**\n\n```python\ntraining_args = TrainingArguments(\n    seed=42,\n    data_seed=42,\n    max_steps=200,\n    # num_train_epochs=1,  # 训练1个epoch 约1k steps\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,\n    dataloader_pin_memory=False,\n    warmup_ratio=0.1,\n    learning_rate=1e-4,\n    lr_scheduler_type=\"cosine\",\n    weight_decay=0.01,\n    logging_steps=5,\n    eval_strategy=\"steps\",\n    eval_steps=0.125,\n    save_strategy=\"steps\",\n    save_steps=0.125,\n    save_total_limit=8,\n    optim=\"adamw_torch\",\n    bf16=True,\n    output_dir=f\"./model/freeze_except_connector_cocovqa\",\n    overwrite_output_dir=False,\n    report_to=\"swanlab\",\n    run_name=\"freeze_except_connector_cocovqa\",\n    remove_unused_columns=False,\n    gradient_checkpointing=False,\n)\n```\n\n### 训练环境\n\n微调代码基于沐曦的C500国产通用计算GPU实现，显存为64G。沐曦的AI芯片基本完全兼容pytorch和huggingface transformers场景，并且在做多模态训练时相比较其他国产AI芯片罕见的没有兼容性问题。读者在尝试本项目代码时可以采用Nvidia显存40G以上的显卡运行本教程。\n\n**笔者个人感觉沐曦的GPU整体适配效果还是非常好的，没遇到适配性的问题。体验上和用NV的GPU做训练没什么区别**。笔者自己也用过好几款国产GPU，沐曦的体验肯定是名列前茅的，包括代码中有指定flash attention在沐曦GPU上都能成功迁移，这点非常值得给沐曦团队点个赞。希望国产GPU生态能越发展越好，造福广大炼丹师；）。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/muxi-gpu.jpg\" alt=\"muxi-gpu\" width=\"400\" />\n  <figcaption>沐曦国产GPU，笔者用的云端服务器没见过真机，因此找了张网图</figcaption>\n  </figure>\n</div>\n\n训练环境的话除了安装GPU对应的驱动和pytorch外，本教程需要额外安装Huggingface全家桶，如下：\n\n```txt\ntorch   # 推荐版本>=6.0\ntorchvision\ntransformers>=4.53.0\naccelerate\ndatasets\nnum2words   # SmolVLM2需要\n```\n\n额外补充一句，如果采用沐曦GPU训练的话，需要在沐曦官方文档处寻找[沐曦版torch](https://developer.metax-tech.com/softnova/index)的安装方式进行下载。其他HF环境和NV基本一样。附赠一个沐曦查看GPU的命令：\n\n```bash\nmx-smi\n```\n\n效果如下：\n\n```bash\n=================== MetaX System Management Interface Log ===================\nTimestamp                                         : Sat Jul 12 14:58:51 2025\n\nAttached GPUs                                     : 8\n+---------------------------------------------------------------------------------+\n| MX-SMI 2.1.12                       Kernel Mode Driver Version: 2.12.13         |\n| MACA Version: 2.29.0.19             BIOS Version: 1.22.3.0                      |\n|------------------------------------+---------------------+----------------------+\n| GPU         NAME                   | Bus-id              | GPU-Util             |\n| Temp        Pwr:Usage/Cap          | Memory-Usage        |                      |\n|====================================+=====================+======================|\n| 0           MetaX C500             | 0000:0e:00.0        | 0%                   |\n| 36C         69W / 350W             | 5680/65536 MiB      |                      |\n+------------------------------------+---------------------+----------------------+\n| 1           MetaX C500             | 0000:0f:00.0        | 0%                   |\n| 38C         70W / 350W             | 4986/65536 MiB      |                      |\n+------------------------------------+---------------------+----------------------+\n| 2           MetaX C500             | 0000:10:00.0        | 0%                   |\n| 37C         69W / 350W             | 4986/65536 MiB      |                      |\n+------------------------------------+---------------------+----------------------+\n| 3           MetaX C500             | 0000:12:00.0        | 1%                   |\n| 37C         71W / 350W             | 4986/65536 MiB      |                      |\n+------------------------------------+---------------------+----------------------+\n| 4           MetaX C500             | 0000:35:00.0        | 0%                   |\n| 37C         70W / 350W             | 4986/65536 MiB      |                      |\n+------------------------------------+---------------------+----------------------+\n| 5           MetaX C500             | 0000:36:00.0        | 1%                   |\n| 36C         68W / 350W             | 4986/65536 MiB      |                      |\n+------------------------------------+---------------------+----------------------+\n| 6           MetaX C500             | 0000:37:00.0        | 0%                   |\n| 39C         73W / 350W             | 4986/65536 MiB      |                      |\n+------------------------------------+---------------------+----------------------+\n| 7           MetaX C500             | 0000:38:00.0        | 0%                   |\n| 38C         71W / 350W             | 4986/65536 MiB      |                      |\n+------------------------------------+---------------------+----------------------+\n\n+---------------------------------------------------------------------------------+\n| Process:                                                                        |\n|  GPU                    PID         Process Name                 GPU Memory     |\n|                                                                  Usage(MiB)     |\n|=================================================================================|\n|  0                  3496691         python3.10                   4066           |\n|  0                  3496692         python3.10                   102            |\n|  0                  3496693         python3.10                   102            |\n|  0                  3496694         python3.10                   102            |\n|  0                  3496695         python3.10                   102            |\n|  0                  3496696         python3.10                   102            |\n|  0                  3496697         python3.10                   102            |\n|  0                  3496698         python3.10                   170            |\n|  1                  3496692         python3.10                   4154           |\n|  2                  3496693         python3.10                   4154           |\n|  3                  3496694         python3.10                   4154           |\n|  4                  3496695         python3.10                   4154           |\n|  5                  3496696         python3.10                   4154           |\n|  6                  3496697         python3.10                   4154           |\n|  7                  3496698         python3.10                   4154           |\n+---------------------------------------------------------------------------------+\n```\n\n### 训练代码实现\n\n在构建训练代码时，笔者使用HuggingFace Transfomers框架的Trainer类来完成训练代码。Trainer类实现的训练逻辑基本能完成大部分微调任务。这里唯一需要提到的是笔者使用了Qwen3-0.6B而非通常此类任务该使用的Qwen3-0.6B-Base模型，Qwen3-0.6B相比于Qwen3-0.6B-Base模型经过了指令遵从微调、对齐等，能实现聊天问答功能。\n\n通常来说对经过微调的模型进行持续训练会一定程度带来性能损失，然而此次微调时笔者冻结了LLM参数，因此需要选用经过微调的模型来实现多模态问答能力。\n\n笔者在训练过程中使用的是bfloat16精度，相比于float16来说bfloat16增加了尾数位数，训练过程中精度会更高些。\n\n在前期进行方案验证阶段笔者采用的是cocoqa数据集，并且进行200steps的微调训练。在确定方案可行后笔者计划使用完整数据集进行微调训练，然而考虑到训练数据量仅仅只有整个模型的12M，因此笔者按参数量与训练Token的比值为1:10采样数据集，即总共从数据集中采样出60K条数据用于实际训练（文本长度按照2k计算，实际上有padding部分因此实际参与token数小于120M）。笔者认为参与训练的数量是足以令模型收敛的，后续实验也证明了模型确实能达到我们所期望的效果。\n\n**训练关键代码实现**\n\n代码比较长是因为增加了断点续训的能力\n\n```python",
    "208": "# 开启训练",
    "209": "last_checkpoint = None  # load last checkpoint if available\nif (\n    os.path.isdir(training_args.output_dir)\n    and not training_args.overwrite_output_dir\n):\n    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n        raise ValueError(\n            f\"Output directory ({training_args.output_dir}) already exists\"\n        )\n    print(\n        f\"Checkpoint detected, resuming training at {last_checkpoint}.\"\n    )\n# Init Trainer\ntrainer = Trainer(\n    model=qwen_smvl,\n    args=training_args,\n    train_dataset=raw_data[\"train\"],\n    eval_dataset=raw_data[\"test\"],\n    data_collator=collate_fn,\n)\ntrainer.train(resume_from_checkpoint=last_checkpoint)\nqwen_smvl.save_pretrained(training_args.output_dir)\n```\n\n完整代码见[代码及数据集链接汇总](#代码及数据集链接汇总)\n\n或者直接由[完整项目GitHub地址](https://github.com/ShaohonChen/Qwen3-SmVL)",
    "210": "一级标题：Qwen3-\"VL\"——超小中文多模态模型的“拼接微调”之路1（附代码和SwanLab记录）\n二级标题：微调训练&结果展示\n内容：\n### 环境安装与微调代码执行\n\n**代码准备与环境安装**\n\n可以在[GitHub仓库地址](https://github.com/ShaohonChen/Qwen3-SmVL)处找到实验的完整代码。使用git clone后使用如下命令安装环境\n\n```bash\npip install -r requirements.txt\n```\n\n**数据集和模型下载**\n\n笔者附上自动下载脚本，注意该脚本使用[魔塔社区](https://modelscope.cn/)完成模型与数据集的下载\n\n```bash\nbash download_resource.sh\n```\n\n### 小批量微调训练\n\n为了进行快速验证，笔者首先使用cocoqa数据集并且进行了200steps的训练，所有参数与前文所述一致。通过\n\n运行实验命令如下，推荐使用8卡进行训练，在8张沐曦GPU卡上预计需要使用20min\n\n```bash\n# 单GPU训练\nCUDA_VISIBLE_DEVICES=0 python train.py ./cocoqa_train.yaml\n# 8GPU训练\naccelerate --num_process 8 train.py ./cocoqa_train.yaml\n```\n\n注意，本项目使用SwanLab进行训练日志记录与分析，如果未登陆SwanLab需要使用`swanlab login`进行登陆。运行后看到如下结果即代表实验成功开启：\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/run.png\" alt=\"run\" width=\"800\" />\n  <figcaption>成功训练后可以看到SwanLab链接</figcaption>\n  </figure>\n</div>\n\n下面是笔者完成小批量微调训练的训练损失、测试损失结果图\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/cocoqa_swanlab.png\" alt=\"cocoqa_swanlab\" width=\"800\" />\n  <figcaption>SwanLab训练可视化分析结果，可以看到最后训练损失和测试损失都收敛在0.65左右</figcaption>\n  </figure>\n</div>\n\n模型在完成训练后会自动使用一张狗狗图片配合问题“图中有什么动物？”让模型根据图片进行推理，推理结果如下：\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/bad_case.png\" alt=\"bad_case\" width=\"800\" />\n  <figcaption>SwanLab记录了模型训练好后的推理结果，可以看到模型能正常理解和回复中文</figcaption>\n  </figure>\n</div>\n\n当时看到模型对着三只狗的图片回答“兔子”时笔者一时认为炼丹失败了，当然如果实际炼丹失败后模型是不会输出动物类型的，而是输出一些乱码或者告诉用户并没有看到图片。识别错误的原因实际上是由于训练步数过少导致的。后续加大训练步数与数据量后模型能正常识别出狗狗并且能准确的说出有三只狗。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/dog.png\" alt=\"dog\" width=\"250\" />\n  <figcaption>附上三只眼神忧伤的狗子，难道长得很像兔子吗？</figcaption>\n  </figure>\n</div>\n\nPS: 作者公开了在[SwanLab上的训练结果](https://swanlab.cn/@ShaohonChen/Qwen3-SmVL/overview)，感兴趣的读者可以自己查看，SwanLab也支持Clone作者的训练日志，大家可以在自己训练时clone笔者的项目去做对照。\n\n### 完整微调训练结果展示\n\n运行实验命令如下，推荐使用8卡进行训练，在8片沐曦C500芯片上预计需要使用1.5h\n\n```bash\n# 单GPU训练\nCUDA_VISIBLE_DEVICES=0 python train.py ./full_train.yaml\n# 8GPU训练\naccelerate --num_process 8 train.py ./full_train.yaml\n```\n\n下图展示了使用完整微调数据对比于小批量训练，可以看到全量数据微调时loss变得更为抖动，这是由于数据类型的丰富给模型的学习带来了一定的挑战。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/fulldata_swanlab.png\" alt=\"fulldata_swanlab\" width=\"800\" />\n  <figcaption>红色为完整训练loss，黄色为小批量训练结果</figcaption>\n  </figure>\n</div>\n\n进一步对比完整训练和小批量训练的训练和测试损失，可以看到完整训练的模型训练损失达到了0.61，远低于仅仅使用cocoqa模型的效果，评估损失也远低于前者，维持在0.58左右。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/evalloss.png\" alt=\"evalloss\" width=\"800\" />\n  <figcaption>红色为完整训练loss，黄色为小批量训练结果</figcaption>\n  </figure>\n</div>\n\n这里值得一提的是，由于我们选用的测试集比较小（仅有64条数据），因此训练损失和测试损失的差距并不能直接理解为过拟合的证据。实际上在大模型训练上，如果数据集足够大的情况下，通常可以认为训练损失等同于评估损失。\n\n此外，模型通过分析1k步之后的训练损失、平均梯度范数（Grad Norm）变化。此时训练任务已过半，且学习率开始快速衰减。如下图，可以看到学习率快速衰减的情况下模型损失并没有明显的进一步下降，这说明模型已经实现了充分训练。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/1kstep.png\" alt=\"1kstep\" width=\"800\" />\n  <figcaption>1k step之后模型的训练损失变化</figcaption>\n  </figure>\n</div>\n\n在训练效率方面，可以看到我们仍没有充分榨干沐曦GPU的性能，当然这也是由于多模态任务的网络本身架构上比较复杂，其中包含许多对图像、文本的拼接工作，这也导致了GPU性能没法完全利用。\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/mx-gpu-use.png\" alt=\"mx-gpu-use\" width=\"800\" />\n  <figcaption>SwanLab对沐曦C500训效率自动记录</figcaption>\n  </figure>\n</div>\n\n同样在完成训练后使用狗狗图进行了测试，这次模型能理解图片、中文以及给出正确的回复。更为关键的是模型完全保留了Qwen3-0.6B原有的全部能力，包括函数调用、推理等。在此基础上，仅仅增加了0.09B参数量的情况下为模型带来了图像理解能力！\n\n<div align=\"center\">\n  <figure>\n  <img src=\"./qwen3_smolvlm_muxi/good_case.png\" alt=\"good_case\" width=\"800\" />\n  <figcaption>同样的图片与问题，更大的数据量和更充足的数据使得模型能够正确给出回复</figcaption>\n  </figure>\n</div>\n\n### 模型推理与效果分析\n\n等笔者下完数据集后未来补一下测试环节 ; ）\n\n可以关注[swanlab教程集合](https://docs.swanlab.cn/examples/qwen3_smolvlm_muxi.html)获取最新更新教程！",
    "211": "一级标题：Qwen3-\"VL\"——超小中文多模态模型的“拼接微调”之路1（附代码和SwanLab记录）\n二级标题：代码及数据集链接汇总\n内容：\n微调用The Cauldron数据集下载链接：\n\n* HuggingFace Hub: [https://huggingface.co/datasets/HuggingFaceM4/the_cauldron](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron)\n* ModelScope: [https://modelscope.cn/datasets/AI-ModelScope/the_cauldron](https://modelscope.cn/datasets/AI-ModelScope/the_cauldron)\n\nQwen3-0.6B模型下载：\n\n* HuggingFace Hub: [https://huggingface.co/Qwen/Qwen3-0.6B](https://huggingface.co/Qwen/Qwen3-0.6B)\n* ModelScope: [https://modelscope.cn/Qwen/Qwen3-0.6B](https://modelscope.cn/Qwen/Qwen3-0.6B)\n\n本实验完整代码GitHub链接：\n\n* 完整项目GitHub地址：[https://github.com/ShaohonChen/Qwen3-SmVL](https://github.com/ShaohonChen/Qwen3-SmVL)\n\n本实验SwanLab日志：\n\n* SwanLab训练过程查看：[https://swanlab.cn/@ShaohonChen/Qwen3-SmVL/overview](https://swanlab.cn/@ShaohonChen/Qwen3-SmVL/overview)",
    "212": "一级标题：Qwen3-\"VL\"——超小中文多模态模型的“拼接微调”之路1（附代码和SwanLab记录）\n二级标题：参考资料\n内容：\n* Huggingface SmolVLM2技术报告：[https://arxiv.org/pdf/2504.05299](https://arxiv.org/pdf/2504.05299)",
    "213": "一级标题：Qwen1.5微调案例\n二级标题：无\n内容：\n:::info\n文本分类，大语言模型，大模型微调\n:::\n\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/Qwen-fintune/runs/zy0st4z16sh4bndyehtks/chart)\n\n\n[实验过程](https://swanlab.cn/@ZeyiLin/Qwen-fintune/runs/zy0st4z16sh4bndyehtks/chart) | [Qwen2微调教程](https://zhuanlan.zhihu.com/p/702491999)",
    "214": "一级标题：Qwen1.5微调案例\n二级标题：概述\n内容：\n[Qwen1.5](https://modelscope.cn/models/qwen/Qwen1.5-7B-Chat/summary)是通义千问团队的开源大语言模型，由阿里云通义实验室研发。以Qwen-1.5作为基座大模型，通过任务微调的方式实现高准确率的文本分类，是学习**大语言模型微调**的入门任务。\n\n![](/assets/example-qwen-1.png)\n\n微调是一种通过在由（输入，输出）对组成的数据集上进一步训练LLMs的过程。这个过程有助于让LLM在特定的下游任务上表现的更为主出色。\n\n\n\n在这个任务中我们会使用[Qwen-1.5-7b](https://modelscope.cn/models/qwen/Qwen1.5-7B-Chat/summary)模型在[zh_cls_fudan_news](https://modelscope.cn/datasets/swift/zh_cls_fudan-news)数据集上进行指令微调任务，同时使用SwanLab进行监控和可视化。",
    "215": "一级标题：Qwen1.5微调案例\n二级标题：环境安装\n内容：\n本案例基于`Python>=3.10`，请在您的计算机上安装好Python。  \n\n环境依赖:  \n```txt\nswanlab\nmodelscope\ntransformers\ndatasets\npeft\naccelerat\npandas\n```\n\n\n一键安装命令：\n\n```bash \npip install swanlab modelscope transformers datasets peft pandas\n```\n\n> 本案例测试于modelscope==1.14.0、transformers==4.41.2、datasets==2.18.0、peft==0.11.1、accelerate==0.30.1、swanlab==0.3.8",
    "216": "一级标题：Qwen1.5微调案例\n二级标题：数据集介绍\n内容：\n本案例使用的是[zh_cls_fudan-news](https://modelscope.cn/datasets/swift/zh_cls_fudan-news)数据集，该数据集主要被用于训练文本分类模型。\n\nzh_cls_fudan-news由几千条数据，每条数据包含text、category、output三列：\n- text 是训练语料，内容是书籍或新闻的文本内容\n- category 是text的多个备选类型组成的列表\n- output 则是text唯一真实的类型\n\n![](/assets/example-qwen-2.png)\n\n数据集例子如下：\n```\n\"\"\"\n[PROMPT]Text: 第四届全国大企业足球赛复赛结束新华社郑州５月３日电（实习生田兆运）上海大隆机器厂队昨天在洛阳进行的第四届牡丹杯全国大企业足球赛复赛中，以５：４力克成都冶金实验厂队，进入前四名。沪蓉之战，双方势均力敌，９０分钟不分胜负。最后，双方互射点球，沪队才以一球优势取胜。复赛的其它３场比赛，青海山川机床铸造厂队３：０击败东道主洛阳矿山机器厂队，青岛铸造机械厂队３：１战胜石家庄第一印染厂队，武汉肉联厂队１：０险胜天津市第二冶金机械厂队。在今天进行的决定九至十二名的两场比赛中，包钢无缝钢管厂队和河南平顶山矿务局一矿队分别击败河南平顶山锦纶帘子布厂队和江苏盐城无线电总厂队。４日将进行两场半决赛，由青海山川机床铸造厂队和青岛铸造机械厂队分别与武汉肉联厂队和上海大隆机器厂队交锋。本届比赛将于６日结束。（完）\nCategory: Sports, Politics\nOutput:[OUTPUT]Sports\n\"\"\"\n\n```\n\n我们的训练任务，便是希望微调后的大模型能够根据Text和Category组成的提示词，预测出正确的Output。",
    "217": "一级标题：Qwen1.5微调案例\n二级标题：准备工作\n内容：\n在开始训练之前，请先确保环境已安装完成，并保证你有一张 **显存>=16GB** 的GPU。\n\n然后，将数据集下载到本地目录下。下载方式是前往[zh_cls_fudan-news - 魔搭社区](https://modelscope.cn/datasets/swift/zh_cls_fudan-news/files) ，将`train.jsonl`和`test.jsonl`下载到本地根目录下即可：\n\n![](/assets/example-qwen-3.png)",
    "218": "一级标题：Qwen1.5微调案例\n二级标题：完整代码\n内容：\n开始训练时的目录结构：\n\n```txt\n|--- train.py\n|--- train.jsonl\n|--- test.jsonl\n```\n\ntrain.py:\n\n```python\nimport json\nimport pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom modelscope import snapshot_download, AutoTokenizer\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom peft import LoraConfig, TaskType, get_peft_model\nfrom transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\nimport os\nimport swanlab\n\n\ndef dataset_jsonl_transfer(origin_path, new_path):\n    \"\"\"\n    将原始数据集转换为大模型微调所需数据格式的新数据集\n    \"\"\"\n    messages = []\n\n    # 读取旧的JSONL文件\n    with open(origin_path, \"r\") as file:\n        for line in file:\n            # 解析每一行的json数据\n            data = json.loads(line)\n            context = data[\"text\"]\n            catagory = data[\"category\"]\n            label = data[\"output\"]\n            message = {\n                \"instruction\": \"你是一个文本分类领域的专家，你会接收到一段文本和几个潜在的分类选项，请输出文本内容的正确类型\",\n                \"input\": f\"文本:{context},类型选型:{catagory}\",\n                \"output\": label,\n            }\n            messages.append(message)\n\n    # 保存重构后的JSONL文件\n    with open(new_path, \"w\", encoding=\"utf-8\") as file:\n        for message in messages:\n            file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n            \n            \ndef process_func(example):\n    \"\"\"\n    将数据集进行预处理\n    \"\"\"\n    MAX_LENGTH = 384 \n    input_ids, attention_mask, labels = [], [], []\n    instruction = tokenizer(\n        f\"<|im_start|>system\\n你是一个文本分类领域的专家，你会接收到一段文本和几个潜在的分类选项，请输出文本内容的正确类型<|im_end|>\\n<|im_start|>user\\n{example['input']}<|im_end|>\\n<|im_start|>assistant\\n\",\n        add_special_tokens=False,\n    )\n    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n    attention_mask = (\n        instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n    )\n    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n        input_ids = input_ids[:MAX_LENGTH]\n        attention_mask = attention_mask[:MAX_LENGTH]\n        labels = labels[:MAX_LENGTH]\n    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}   \n\n\ndef predict(messages, model, tokenizer):\n    device = \"cuda\"\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\n    generated_ids = model.generate(\n        model_inputs.input_ids,\n        max_new_tokens=512\n    )\n    generated_ids = [\n        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n    \n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    \n    print(response)\n     \n    return response\n    \n# 在modelscope上下载Qwen1.5-7B模型到本地目录下\nmodel_dir = snapshot_download(\"qwen/Qwen1.5-7B-Chat\", cache_dir=\"./\", revision=\"master\")\n\n# Transformers加载模型权重\ntokenizer = AutoTokenizer.from_pretrained(\"./qwen/Qwen1___5-7B-Chat/\", use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"./qwen/Qwen1___5-7B-Chat/\", device_map=\"auto\", torch_dtype=torch.bfloat16)\nmodel.enable_input_require_grads()  # 开启梯度检查点时，要执行该方法\n\n# 加载、处理数据集和测试集\ntrain_dataset_path = \"train.jsonl\"\ntest_dataset_path = \"test.jsonl\"\n\ntrain_jsonl_new_path = \"new_train.jsonl\"\ntest_jsonl_new_path = \"new_test.jsonl\"\n\nif not os.path.exists(train_jsonl_new_path):\n    dataset_jsonl_transfer(train_dataset_path, train_jsonl_new_path)\nif not os.path.exists(test_jsonl_new_path):\n    dataset_jsonl_transfer(test_dataset_path, test_jsonl_new_path)\n\n# 得到训练集\ntrain_df = pd.read_json(train_jsonl_new_path, lines=True)\ntrain_ds = Dataset.from_pandas(train_df)\ntrain_dataset = train_ds.map(process_func, remove_columns=train_ds.column_names)\n\nconfig = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    inference_mode=False,  # 训练模式\n    r=8,  # Lora 秩\n    lora_alpha=32,  # Lora alaph，具体作用参见 Lora 原理\n    lora_dropout=0.1,  # Dropout 比例\n)\n\nmodel = get_peft_model(model, config)\n\nargs = TrainingArguments(\n    output_dir=\"./output/Qwen1.5\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    logging_steps=10,\n    num_train_epochs=2,\n    save_steps=100,\n    learning_rate=1e-4,\n    save_on_each_node=True,\n    gradient_checkpointing=True,\n    report_to=\"none\",\n)\n\nswanlab_callback = SwanLabCallback(project=\"Qwen-fintune\", experiment_name=\"Qwen1.5-7B-Chat\")\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n\n# 用测试集的前10条，测试模型\ntest_df = pd.read_json(test_jsonl_new_path, lines=True)[:10]\n\ntest_text_list = []\nfor index, row in test_df.iterrows():\n    instruction = row['instruction']\n    input_value = row['input']\n    \n    messages = [\n        {\"role\": \"system\", \"content\": f\"{instruction}\"},\n        {\"role\": \"user\", \"content\": f\"{input_value}\"}\n    ]\n\n    response = predict(messages, model, tokenizer)\n    messages.append({\"role\": \"assistant\", \"content\": f\"{response}\"})\n    result_text = f\"{messages[0]}\\n\\n{messages[1]}\\n\\n{messages[2]}\"\n    test_text_list.append(swanlab.Text(result_text, caption=response))\n    \nswanlab.log({\"Prediction\": test_text_list})\nswanlab.finish()\n```",
    "219": "一级标题：Qwen1.5微调案例\n二级标题：效果演示\n内容：\n![](/assets/example-qwen-4.png)",
    "220": "一级标题：Qwen2.5复现DeepSeek-R1-ZERO\n二级标题：无\n内容：\n[![](/assets/badge1.svg)](https://swanlab.cn/@LiXinYu/Try_r1/overview)",
    "221": "一级标题：Qwen2.5复现DeepSeek-R1-ZERO\n二级标题：简介\n内容：\n本文旨在对deepseek-r1-zero进行复现实验，简单介绍了从r1原理到代码实现，再到结果观测的整个过程。通过SwanLab监控实验过程，确保实验的每个阶段都能精确跟踪与调试。通过这一系列的实验步骤，能够掌握GRPO的实现方法。\n\n![](./grpo/r1-zero-ds-qwen.jpg)\n\n---",
    "222": "一级标题：Qwen2.5复现DeepSeek-R1-ZERO\n二级标题：链接资料\n内容：\n本次实验参考了优秀开源项目[philschmid/deep-learning-pytorch-huggingface](https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/mini-deepseek-r1-aha-grpo.ipynb)，该项目作者是google-deepmind工程师Philipp Schmid，Countdown用于R1训练的idea就是这个项目发起的。\n\n> 模型地址：Qwen2.5-3B-Instruct:[huggingface社区](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct)|[魔搭社区](https://modelscope.cn/models/Qwen/Qwen2.5-3B-Instruct)\n>\n> 数据集地址：Countdown-Tasks-3to4:[huggingface地址](https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4)|[魔搭社区地址](https://modelscope.cn/datasets/zouxuhong/Countdown-Tasks-3to4)\n>\n> 可视化工具SwanLab项目地址：[SwanLab结果可视化](https://swanlab.cn/@LiXinYu/Try_r1/overview)\n\n---",
    "223": "一级标题：Qwen2.5复现DeepSeek-R1-ZERO\n二级标题：DeepSeek-R1原理\n内容：\n论文标题：DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\n\n论文链接：[论文链接](https://arxiv.org/pdf/2501.12948?)\n\n代码地址：[github链接](https://github.com/deepseek-ai/DeepSeek-R1)\n\n**下面是论文里从DeepSeek-V3到DeepSeek-R1的流程图表示**\n\n本次教程仅考虑从DeepSeek-V3--->DeepSeek-R1-Zero的复现过程，基于Qwen2.5-3B-Instruct模型实现。\n\n![](./grpo/deepseek-r1-process.png)\n\n\n**GRPO原理：**\n\n`群体相对策略优化 (GRPO，Group Relative Policy Optimization) `是一种强化学习 (RL) 算法，专门用于增强大型语言模型 (LLM) 中的推理能力。与严重依赖外部评估模型（价值函数）指导学习的传统 RL 方法不同，GRPO 通过评估彼此相关的响应组来优化模型。这种方法可以提高训练效率，使 GRPO 成为需要复杂问题解决和长链思维的推理任务的理想选择。\n\n> GRPO 的本质思路：通过在同一个问题上生成多条回答，把它们彼此之间做“相对比较”，来代替传统 PPO 中的“价值模型”\n\n`传统的强化学习算法（如Proximal Policy Optimization，PPO）`在应用于LLMs的推理任务时面临着重大挑战：\n\n1、依赖批评者模型：\nPPO需要一个独立的批评者模型来评估每个回答的价值，这使内存和计算需求增加了一倍。\n训练批评者模型非常复杂且容易出错，尤其是在需要对主观或细微差别进行评价的任务中。\n\n\n2、高昂的计算成本：\n强化学习流程通常需要大量计算资源来迭代评估和优化回答。\n将这些方法扩展到更大的LLMs会进一步加剧成本。\n\n3、可扩展性问题：\n绝对奖励评估难以应对多样化任务，使得跨推理领域的泛化变得困难。\n---\n`GRPO如何应对这些挑战：`\n\n1、无批评者优化： GRPO通过比较组内回答，消除了对批评者模型的需求，显著降低了计算开销。\n\n2、相对评估： GRPO不依赖外部评价者，而是利用组内动态来评估每个回答在同一批次中的相对表现。\n\n3、高效训练： 通过专注于组内优势，GRPO简化了奖励估计流程，使其对大型模型的训练更快且更具可扩展性。\n\n下图是PPO与GRPO的对比，GRPO放弃了价值模型，从分组得分中估计，显著减少了训练资源\n\n![grpo](./grpo/grpo.png)\n\n> 看到一位作者的看法，把GRPO比作老师给学生上课，老师让一组学生解决一个问题。\n> 老师没有单独为每个学生打分，而是让学生在组内比较彼此的答案。表现更好的学生会得到鼓励，而其他人则从错误中学习。随着时间的推移，整个组会逐渐提高，变得更准确和一致。GRPO 将这一原理应用于训练AI模型，使其能够高效地学习。\n\n---",
    "224": "一级标题：Qwen2.5复现DeepSeek-R1-ZERO\n二级标题：实验代码\n内容：\n### 1、环境搭建\n\n> 环境设置如下：\n> \n> pip install transformers==4.48.1 \n> \n> pip install peft==0.14.0\n> \n> conda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 -c pytorch\n> \n> pip install datasets\n> \n> pip install accelerate\n> \n> pip install trl\n> \n> pip install -U swanlab\n> \n> pip install deepspeed\n\n### 2、数据预处理\n\n本次实验使用一个490k条数据的[Countdown数据集](https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4)来进行实验，内容如下图所示：\n\n![数据集内容](./grpo/data-countdown.png)\n\n该数据集仅有两项，一个是target结果数据，一个是nums组合数据，我们的目的是为了让模型思考如何从nums经过+、-、*、/计算得到target，为了让模型更好的激活思考能力，我们需要对其设置提示词模板，最重要让模型回答成如下模样：\n\n```text\n<think>:\n让我们来思考下,……\n</think>\n\n<answer>\n……\n</answer>\n```\n同时，由于每个模型都有对应的训练格式模板，比如Qwen的模板在其权重文件中的tokenizer_config.json文件里，具体[例子](https://modelscope.cn/models/Qwen/Qwen2.5-3B-Instruct/file/view/master?fileName=tokenizer_config.json&status=1)如下：\n\n```json\n\"chat_template\": \"{%- if tools %}\\n    {{- '<|im_start|>system\\\\n' }}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- messages[0]['content'] }}\\n    {%- else %}\\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\\n    {%- endif %}\\n    {{- \\\"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {{- \\\"\\\\n\\\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\\\"name\\\\\\\": <function-name>, \\\\\\\"arguments\\\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\\\" }}\\n{%- else %}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- '<|im_start|>system\\\\n' + messages[0]['content'] + '<|im_end|>\\\\n' }}\\n    {%- else %}\\n        {{- '<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \\\"user\\\") or (message.role == \\\"system\\\" and not loop.first) or (message.role == \\\"assistant\\\" and not message.tool_calls) %}\\n        {{- '<|im_start|>' + message.role + '\\\\n' + message.content + '<|im_end|>' + '\\\\n' }}\\n    {%- elif message.role == \\\"assistant\\\" %}\\n        {{- '<|im_start|>' + message.role }}\\n        {%- if message.content %}\\n            {{- '\\\\n' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- '\\\\n<tool_call>\\\\n{\\\"name\\\": \\\"' }}\\n            {{- tool_call.name }}\\n            {{- '\\\", \\\"arguments\\\": ' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- '}\\\\n</tool_call>' }}\\n        {%- endfor %}\\n        {{- '<|im_end|>\\\\n' }}\\n    {%- elif message.role == \\\"tool\\\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \\\"tool\\\") %}\\n            {{- '<|im_start|>user' }}\\n        {%- endif %}\\n        {{- '\\\\n<tool_response>\\\\n' }}\\n        {{- message.content }}\\n        {{- '\\\\n</tool_response>' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \\\"tool\\\") %}\\n            {{- '<|im_end|>\\\\n' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|im_start|>assistant\\\\n' }}\\n{%- endif %}\\n\",\n```\n\n这是一个Jinja2 模板。Jinja2 是一个流行的模板引擎，常用于 Python Web 应用中，但它也可以在其他环境中使用。举一个例子：\n\n```text\n<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\n使用给定的数字 [10, 3, 6]，创建一个等于 7 的方程。你可以使用基本算术运算（+、-、*、/）一次或多次，但每个数字只能使用一次。在 <think> </think> 标签中展示你的思考过程，并在 <answer> </answer> 标签中返回最终方程，例如 <answer> (1 + 2) / 3 </answer>。在 <think> 标签中逐步思考。<|im_end|>\n<|im_start|>assistant\n让我们逐步解决这个问题。\n<think>\n```\n\n当然也可以利用tokenizer.apply_chat_template自动根据模型的格式模板进行内容整理，具体如下述代码所示，将数据集转换为R1 Countdown提示词格式：\n\n\n```python\n### 模仿R1的prompt格式来处理数据集，使得GRPO的时候的数据集是可以有思考过程\ndef generate_r1_prompt(question:str,target:str):\n    \"\"\"\n    激活qwen模型的思考过程\n    :param question:数据集的question，给qwen让他自己思考去\n    :param target:数据集的ans\n    :return:\n    \"\"\"\n    r1_prefix = [\n        {\n            \"role\":\"user\",\n            \"content\":f\"现在有一个数学问题，内容是：{question},答案是{target}，你需要根据问题思考其推理过程，使得最终能够得到正确答案，在<think>和</think>标签中展示你的思考过程，并在<answer>和</answer>标签中返回最终答案，比如<answer>19</answer>。在<think>标签后逐步思考。\"\n        },\n        {\n            \"role\":\"assistant\",\n            \"content\":\"让我们逐步解决这个问题。\\n<think>\"\n        }\n    ]\n    # apply_chat_template是应用qwen模型文件中tokenizer_config.json文件中chat_template提示词模板来生成回答。\n    return {\"prompt\": tokenizer.apply_chat_template(r1_prefix, tokenize=False, continue_final_message=True),\n            \"question\":question,\n            \"target\": target}\n            \n### 将数据集转换为R1 Countdown提示词格式，在这里我们会把prompt转换为Qwen2的提示词模版，让它以更熟悉的方式来接收提示词，并且我们把让我们逐步解决这个问题。\\n<think>作为模型输出的开头，让它接着续写。用 Python字典的方式返回样本，这样trl会在调用奖励函数的时候，帮我们把键名设为为对应的参数；另外，trl会把模型的多个输出设为completions。\ndef train_dataset_process(train_data_path:str):\n    dataset = read_jsonl_to_dataset(train_data_path)\n    dataset = dataset.map(lambda x: generate_r1_prompt(x[\"sni_text\"], x[\"ans\"]))\n\n    train_test_split = dataset.train_test_split(test_size=0.1)\n\n    train_dataset = train_test_split[\"train\"]\n    test_dataset = train_test_split[\"test\"]\n\n    return {\n        \"train_dataset\":train_dataset,\n        \"test_dataset\":test_dataset\n    }\n    \n```\n\n> **❗注意：** generate_r1_prompt中最终需要return包含数据提问，以及数据集对应的答案answer，map方法会帮我们把实际的question和answer填入到prompt里\n\n### 3、设置奖励函数\n\n在强化学习中，奖励函数是指导智能体（agent）在环境中如何行动的核心信号。奖励提供了对智能体行为的即时反馈，用于评估某个动作在某一状态下的好坏，从而影响其未来的决策。通过不断地试错和调整，智能体学习到在不同状态下选择能获得高奖励的行为策略。奖励的主要功能是引导智能体朝着最大化长期回报的目标去优化策略。正向奖励（正数）鼓励行为，负向奖励（负数）抑制行为。奖励用于更新智能体的策略或值函数，策略的优化通常基于累计奖励（Return），即智能体从当前状态到未来一段时间内获得的总奖励。\n\n本次实验我们仅对输出格式format以及最终答案answer设置奖励函数，训练过程会不断修正格式输出以及答案输出。\n\n**format奖励函数**\n\n```python\n### 格式奖励函数\ndef format_reward_func(completions, **kwargs):\n    \"\"\"\n    格式奖励函数，检查模型输出格式是否匹配: <think>...</think><answer>...</answer>\n\n    参数:\n        completions (list[str]): 生成的输出\n    返回:\n        list[float]: 奖励分数\n    \"\"\"\n    # 初始化奖励列表\n    rewards = []\n    # 遍历生成的输出\n    for completion in completions:\n        try:\n            # 在生成的输出前添加<think>标签，便于后续正则表达式匹配\n            completion = \"<think>\" + completion\n\n            if random.random() < 0.1:  # 1% 的概率将生成输出写入文件\n                # 创建生成输出目录（如果不存在）\n                os.makedirs(\"completion_samples\", exist_ok=True)\n                log_file = os.path.join(\"completion_samples\", \"completion_samples.txt\")\n                with open(log_file, \"a\") as f:\n                    f.write(f\"\\n\\n==============\\n\")\n                    f.write(completion)  # 写入生成的输出\n\n            # 定义正则表达式模式，用于匹配 <think> 和 <answer> 标签\n            regex = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\n<answer>([\\s\\S]*?)<\\/answer>$\"\n            match = re.search(regex, completion, re.DOTALL)  # 使用正则表达式进行匹配\n\n            if match is None or len(match.groups()) != 2:\n                rewards.append(0.0)  # 如果格式不正确，奖励为 0\n            else:\n                rewards.append(1.0)  # 如果格式正确，奖励为 1\n        except Exception:\n            rewards.append(0.0)  # 如果发生异常，奖励为 0\n\n    return rewards\n```\n\n**answer奖励函数**\n\n```python\n### 答案奖励函数\ndef equation_reward_func(completions, target, nums, **kwargs):\n    \"\"\"\n    方程奖励函数，检查计算结果是否正确，数字是否符合使用要求（每个数字只用一次，只使用所提供的数字）\n\n    参数:\n        completions (list[str]): 生成的输出\n        target (list[str]): 预期的答案\n        nums (list[str]): 可用的数字\n\n    返回:\n        list[float]: 奖励分数\n    \"\"\"\n    # 初始化奖励列表\n    rewards = []\n    # 遍历生成的输出、预期的答案和可用的数字\n    for completion, gt, numbers in zip(completions, target, nums):\n        try:\n            # 在生成的输出前添加 <think> 标签，便于后续正则表达式匹配\n            completion = \"<think>\" + completion\n            # 定义正则表达式模式，用于匹配 <answer> 标签\n            match = re.search(r\"<answer>(.*?)<\\/answer>\", completion)\n            if match is None:\n                rewards.append(0.0)  # 如果没有匹配到 <answer> 标签，奖励为 0\n                continue\n            equation = match.group(1).strip()  # 提取 <answer> 标签中的内容\n            # 提取方程中的所有数字\n            used_numbers = [int(n) for n in re.findall(r\"\\d+\", equation)]\n\n            # 检查所有数字是否被使用且只使用一次\n            if sorted(used_numbers) != sorted(numbers):\n                rewards.append(0.0)\n                continue\n\n            # 定义允许的字符模式，只允许数字、运算符、括号和空白字符\n            allowed_pattern = r\"^[\\d+\\-*/().\\s]+$\"\n            if not re.match(allowed_pattern, equation):\n                rewards.append(0.0)  # 如果方程包含不允许的字符，奖励为 0\n                continue\n\n            # 计算方程的结果\n            result = eval(equation, {\"__builtins__\": None}, {})\n            # 检查方程是否正确且与预期答案匹配（误差小于 1e-5）\n            if abs(float(result) - float(gt)) < 1e-5:\n                rewards.append(1.0)  # 如果正确，奖励为 1\n\n                # 10% 的概率将成功的样本写入文件\n                if random.random() < 0.10:\n                    # 创建生成输出目录（如果不存在）\n                    os.makedirs(\"completion_samples\", exist_ok=True)\n                    log_file = os.path.join(\n                        \"completion_samples\", \"success_completion_samples.txt\"\n                    )\n                    with open(log_file, \"a\") as f:\n                        f.write(f\"\\n\\n==============\\n\")\n                        f.write(completion)  # 写入生成的输出\n            else:\n                rewards.append(0.0)  # 如果不正确，奖励为 0\n        except Exception:\n            rewards.append(0.0)  # 如果评估失败，奖励为 0\n\n    return rewards\n```\n\n> **补充：** 也可以设置思考长度以及语言一致性奖励函数来提高模型性能\n\n### 4、设置模型参数\n\n```python\n# 模型参数设置\nmodel_config = ModelConfig(\n    model_name_or_path=model_path,\n    torch_dtype=\"bfloat16\",\n    # attn_implementation=\"flash_attention_2\",\n    use_peft=True,\n    load_in_4bit=True\n)\n```\n\n `attn_implementation`:使用 flash_attention_2 可以优化显存使用和加速计算，尤其是在处理大规模模型时。若启用，它会减少内存占用并加速训练过程，尤其在使用多GPU时效果显著。未启用时，可能会牺牲性能和显存效率，影响训练速度。\n\n### 5、设置训练参数\n\n```python\n# 训练参数\ntraining_args = GRPOConfig(\n    output_dir=\"/root/test/outputs\",\n    learning_rate=5e-7,\n    lr_scheduler_type=\"cosine\",\n    logging_steps=2,\n    max_steps=200,\n    per_device_train_batch_size=1,\n    gradient_checkpointing=False,\n    gradient_accumulation_steps=8,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    bf16=True,\n    save_steps=50,\n    # GRPO参数设置\n    max_prompt_length=256,\n    max_completion_length=1024,\n    num_generations=2,\n    beta=0.001,\n    # vllm加速\n    use_vllm=False\n    # vllm_device=\"npu:7\"\n    vllm_device=\"cuda:1\"\n    vllm_gpu_memory_utilization=0.8\n)\n```\n\n其中vLLM 是一个用于加速推理的库，能在 GPU 上优化内存使用和计算性能。启用 use_vllm=True 后，它可以在推理阶段通过高效的内存管理和多设备并行来加速计算，特别是在处理大型语言模型时。它还能通过 vllm_device 参数指定加速设备，例如 cuda:1，提升训练和推理速度，减少显存占用。\n这里之所以是false是因为我申请的服务器只有两块卡，使用vllm的时候一块卡训练，一块卡用来推理，而vllm一般在多块卡的时候，比如5、6块卡以上的时候才能体现出加速效果，而本次实验使用的是4090，只有24GB显存，很容易炸显存，如果卡比较多的话推荐vllm。\n\n\n⚠️**注意：**\n> 我们使用的是trl的库来使用GRPO，目前有个小bug，就是gradient_checkpointing和vllm要同时true或者同时false，否则就会报错，而这两个参数都有降低显存占用，提高训练推理速度的功能，因此如何设置可以交给各位炼丹师自行选择。\n\n### 6、可视化训练工具参数\n\n```python",
    "225": "一级标题：Qwen2.5复现DeepSeek-R1-ZERO\n二级标题：swanlab参数配置\n内容：\nswanlab_callback = SwanLabCallback(\n    workspace=None, # 项目不公开\n    project=\"DeepSeek-R1-zero\",  # 项目名称\n    experiment_name=\"4090-grpo\",  # 实验名称\n)\n```\n\n### 7、训练并保存模型\n\n```python\n# 训练器配置\ntrainer = GRPOTrainer(\n    model=model_config.model_name_or_path,\n    reward_funcs=[format_reward_func,answer_reward_func],\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    peft_config=get_peft_config(model_config),\n    callbacks=[swanlab_callback]\n)\n\ntrainer.train()\ntrainer.save_model(training_args.output_dir)\n```\n\n\n### 全过程代码\n\n为了便于管理和配置分布式训练环境、强化学习（RL）训练的超参数，以及定义主训练函数 main，我们建议采用 YAML 格式的脚本文件来系统化地记录和维护这些关键参数，同时使用 Python 文件来实现 main 函数。\n\n```\nroot/project/\n├── data/\n│   └── zouxuhong___countdown-tasks-3to4/\n├── models/\n│   └── Qwen/\n│       └── Qwen2___5-3B-Instruct/\n├── config/\n│   ├── 2rtx4090.yaml\n│   └── grpo-qwen-2.5-3b-deepseek-r1-zero-countdown.yaml\n├── train_r1_grpo.py\n└── train_r1_grpo.sh\n```\n\n\n**1、Accelerate 配置文件，用于分布式训练（两张卡）。新建deepspeed_zero3.yaml，填入以下内容并保存**\n\n一般来说，这个文件内容不需要修改，如果有定制需求，请不要使用这个文件，运行`accelerate config`自行设定。　\n\n```yaml\ncompute_environment: LOCAL_MACHINE\ndebug: false\ndeepspeed_config:\n  gradient_accumulation_steps: 8\n  gradient_clipping: 1.0\n  offload_optimizer_device: cpu\n  offload_param_device: cpu\n  zero3_init_flag: false\n  zero3_save_16bit_model: true\n  zero_stage: 3\ndistributed_type: DEEPSPEED\ndowncast_bf16: 'no'\nenable_cpu_affinity: false\nmachine_rank: 0\nmain_training_function: main\nmixed_precision: bf16\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n```\n\n⚠️**注意：**\n\n由于本次实验资源有限，因此训练优化器还有模型参数部分会转移到CPU上进行计算，以减少显存压力，修改的参数是offload_optimizer_device和offload_param_device\n\n1. offload_optimizer_device: cpu\n\n    作用：将优化器状态（如动量、梯度等）卸载到 CPU 上。\n\n    具体内容：\n\n        优化器状态通常占用大量显存，尤其是在使用 Adam 优化器时。\n\n        将这些状态卸载到 CPU 上可以显著减少 GPU 显存占用，从而支持更大的模型或更大的批量大小。\n\n2. offload_param_device: cpu\n\n    作用：将模型参数卸载到 CPU 上。\n\n    具体内容：\n\n       模型参数是训练过程中占用显存的主要部分。\n\n       将这些参数卸载到 CPU 上可以进一步减少 GPU 显存占用，但会增加 CPU 和 GPU 之间的数据传输开销。\n\n**2、设定训练的超参数。新建grpo-qwen-2.5-3b-deepseek-r1-zero-countdown.yaml填入以下内容，并根据实际情况修改**\n\n```yaml\n# Model arguments\nmodel_name_or_path: /root/epfs/ascend_r1_turtorial/models/Qwen/Qwen2___5-3B-Instruct\nmodel_revision: main\ntorch_dtype: bfloat16\n# attn_implementation: flash_attention_2\nbf16: true\ntf32: false\noutput_dir: /root/epfs/ascend_r1_turtorial/output\n\n# Dataset arguments\ndataset_id_or_path: /root/epfs/zouxuhong___countdown-tasks-3to4\n\n# Lora Arguments\n# No LoRA is used here\n\n# Training arguments\nmax_steps: 450\nper_device_train_batch_size: 1\ngradient_accumulation_steps: 8\ngradient_checkpointing: false\ngradient_checkpointing_kwargs:\n  use_reentrant: false\nlearning_rate: 5.0e-7 # 1.0e-6 as in the deepseek math paper 5-e7 from https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05\nlr_scheduler_type: cosine\nwarmup_ratio: 0.03\n# GRPO specific parameters\nbeta: 0.001 # 0.04 as in the deepseek math paper 0.001 from https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05\nmax_prompt_length: 256\nmax_completion_length: 1024\nnum_generations: 2\nuse_vllm: false\n# vllm_device: \"npu:7\"\nvllm_device: \"cuda:1\"\nvllm_gpu_memory_utilization: 0.8\n\n# Logging arguments\nlogging_strategy: steps\nlogging_steps: 1\nsave_strategy: \"steps\"\nsave_steps: 100\nsave_total_limit: 1\nseed: 2025\n\n# Swanlab 训练流程记录参数\nswanlab: true # 是否开启 Swanlab \nworkspace: none\nproject: Try_r1\nexperiment_name: qingyun-4090-jupyter\n```\n\n\n**3、设置训练函数**\n\n```python\nimport logging\nimport os\nimport random\nimport re\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import List\n\nfrom datasets import load_dataset\nfrom swanlab.integration.transformers import SwanLabCallback\nimport torch\nfrom transformers import AutoTokenizer\nfrom transformers.trainer_utils import get_last_checkpoint\nfrom trl import GRPOConfig, GRPOTrainer, ModelConfig, TrlParser",
    "226": "# 自定义参数类",
    "227": "@dataclass\nclass DatasetArguments:\n    \"\"\"数据集参数的数据类\"\"\"\n\n    # 数据集 ID 或路径\n    dataset_id_or_path: str = \"Jiayi-Pan/Countdown-Tasks-3to4\"\n    # 数据集拆分\n    dataset_splits: str = \"train\"\n    # 分词器名称或路径\n    tokenizer_name_or_path: str = None\n\n@dataclass\nclass SwanlabArguments:\n    \"\"\"SwanLab参数的数据类\"\"\"\n\n    # 是否使用 SwanLab\n    swanlab: bool\n    # SwanLab 用户名\n    workspace: str\n    # SwanLab 的项目名\n    project: str\n    # SwanLab 的实验名\n    experiment_name: str",
    "228": "# 设置日志记录",
    "229": "# 配置日志记录器\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\nhandler = logging.StreamHandler()\nhandler.setFormatter(\n    logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n)  # 设置日志格式\n\nlogger.addHandler(handler)",
    "230": "# 定义奖励函数",
    "231": "def format_reward_func(completions, **kwargs):\n    \"\"\"\n    格式奖励函数，检查模型输出格式是否匹配: <think>...</think><answer>...</answer>\n\n    参数:\n        completions (list[str]): 生成的输出\n    返回:\n        list[float]: 奖励分数\n    \"\"\"\n    # 初始化奖励列表\n    rewards = []\n    # 遍历生成的输出\n    for completion in completions:\n        try:\n            # 在生成的输出前添加<think>标签，便于后续正则表达式匹配\n            completion = \"<think>\" + completion\n\n            if random.random() < 0.1:  # 1% 的概率将生成输出写入文件\n                # 创建生成输出目录（如果不存在）\n                os.makedirs(\"completion_samples\", exist_ok=True)\n                log_file = os.path.join(\"completion_samples\", \"completion_samples.txt\")\n                with open(log_file, \"a\") as f:\n                    f.write(f\"\\n\\n==============\\n\")\n                    f.write(completion)  # 写入生成的输出\n\n            # 定义正则表达式模式，用于匹配 <think> 和 <answer> 标签\n            regex = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\n<answer>([\\s\\S]*?)<\\/answer>$\"\n            match = re.search(regex, completion, re.DOTALL)  # 使用正则表达式进行匹配\n\n            if match is None or len(match.groups()) != 2:\n                rewards.append(0.0)  # 如果格式不正确，奖励为 0\n            else:\n                rewards.append(1.0)  # 如果格式正确，奖励为 1\n        except Exception:\n            rewards.append(0.0)  # 如果发生异常，奖励为 0\n\n    return rewards\n\ndef equation_reward_func(completions, target, nums, **kwargs):\n    \"\"\"\n    方程奖励函数，检查计算结果是否正确，数字是否符合使用要求（每个数字只用一次，只使用所提供的数字）\n\n    参数:\n        completions (list[str]): 生成的输出\n        target (list[str]): 预期的答案\n        nums (list[str]): 可用的数字\n\n    返回:\n        list[float]: 奖励分数\n    \"\"\"\n    # 初始化奖励列表\n    rewards = []\n    # 遍历生成的输出、预期的答案和可用的数字\n    for completion, gt, numbers in zip(completions, target, nums):\n        try:\n            # 在生成的输出前添加 <think> 标签，便于后续正则表达式匹配\n            completion = \"<think>\" + completion\n            # 定义正则表达式模式，用于匹配 <answer> 标签\n            match = re.search(r\"<answer>(.*?)<\\/answer>\", completion)\n            if match is None:\n                rewards.append(0.0)  # 如果没有匹配到 <answer> 标签，奖励为 0\n                continue\n            equation = match.group(1).strip()  # 提取 <answer> 标签中的内容\n            # 提取方程中的所有数字\n            used_numbers = [int(n) for n in re.findall(r\"\\d+\", equation)]\n\n            # 检查所有数字是否被使用且只使用一次\n            if sorted(used_numbers) != sorted(numbers):\n                rewards.append(0.0)\n                continue\n\n            # 定义允许的字符模式，只允许数字、运算符、括号和空白字符\n            allowed_pattern = r\"^[\\d+\\-*/().\\s]+$\"\n            if not re.match(allowed_pattern, equation):\n                rewards.append(0.0)  # 如果方程包含不允许的字符，奖励为 0\n                continue\n\n            # 计算方程的结果\n            result = eval(equation, {\"__builtins__\": None}, {})\n            # 检查方程是否正确且与预期答案匹配（误差小于 1e-5）\n            if abs(float(result) - float(gt)) < 1e-5:\n                rewards.append(1.0)  # 如果正确，奖励为 1\n\n                # 10% 的概率将成功的样本写入文件\n                if random.random() < 0.10:\n                    # 创建生成输出目录（如果不存在）\n                    os.makedirs(\"completion_samples\", exist_ok=True)\n                    log_file = os.path.join(\n                        \"completion_samples\", \"success_completion_samples.txt\"\n                    )\n                    with open(log_file, \"a\") as f:\n                        f.write(f\"\\n\\n==============\\n\")\n                        f.write(completion)  # 写入生成的输出\n            else:\n                rewards.append(0.0)  # 如果不正确，奖励为 0\n        except Exception:\n            rewards.append(0.0)  # 如果评估失败，奖励为 0\n\n    return rewards",
    "232": "# 断点续训处理",
    "233": "def get_checkpoint(training_args: GRPOConfig):\n    \"\"\"\n    获取最后一个检查点\n\n    参数:\n        training_args (GRPOConfig): 训练参数\n    返回:\n        str: 最后一个检查点的路径，如果没有检查点，则返回 None\n    \"\"\"\n    last_checkpoint = None\n    if os.path.isdir(training_args.output_dir):  # 如果输出目录存在\n        # 获取最后一个检查点\n        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n    return last_checkpoint",
    "234": "# 基于trl实现GRPO训练过程",
    "235": "def grpo_function(\n    model_args: ModelConfig,\n    dataset_args: DatasetArguments,\n    training_args: GRPOConfig,\n    callbacks: List,\n):\n    # 记录模型参数\n    logger.info(f\"Model parameters {model_args}\")\n    # 记录训练/评估参数\n    logger.info(f\"Training/evaluation parameters {training_args}\")",
    "236": "# 处理数据",
    "237": "# 加载分词器\n    tokenizer = AutoTokenizer.from_pretrained(\n        (\n            # 如果有指定分词器，则使用指定的分词器，否则使用模型名称\n            dataset_args.tokenizer_name_or_path\n            if dataset_args.tokenizer_name_or_path\n            else model_args.model_name_or_path\n        ),\n        revision=model_args.model_revision,  # 使用指定的模型版本\n        trust_remote_code=model_args.trust_remote_code,  # 允许使用远程代码\n    )\n    # 如果分词器没有填充标记，则使用结束标记作为填充标记\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n\n    # 加载数据集\n    dataset = load_dataset(\n        dataset_args.dataset_id_or_path, split=dataset_args.dataset_splits\n    )\n    # 随机选择 50K 个样本，看你喜好定数字，但是数据集有 409K 个样本\n    dataset = dataset.shuffle(seed=training_args.seed).select(range(50000))\n\n    def generate_r1_prompt(numbers, target):\n        \"\"\"\n        生成 R1 Countdown 游戏提示词\n\n        参数:\n            numbers (list[int]): 数字列表\n            target (int): 目标值\n        返回:\n            dict: 生成的一个数据样本\n        \"\"\"\n        # 定义提示词前缀\n        r1_prefix = [\n            {\n                \"role\": \"user\",\n                \"content\": f\"使用给定的数字 {numbers}，创建一个等于 {target} 的方程。你可以使用基本算术运算（+、-、*、/）一次或多次，但每个数字只能使用一次。在 <think> </think> 标签中展示你的思考过程，并在 <answer> </answer> 标签中返回最终方程，例如 <answer> (1 + 2) / 3 </answer>。在 <think> 标签中逐步思考。\",\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"让我们逐步解决这个问题。\\n<think>\",  # 结尾使用 `<think>` 促使模型开始思考\n            },\n        ]\n\n        return {\n            \"prompt\": tokenizer.apply_chat_template(\n                r1_prefix, tokenize=False, continue_final_message=True\n            ),  # 提示词，continue_final_message=True 表示将提示词中的最后一个消息继续到最终的输出中\n            \"target\": target,\n            \"nums\": numbers,\n        }\n\n    # 将数据集转换为 R1 Countdown 游戏提示词\n    dataset = dataset.map(lambda x: generate_r1_prompt(x[\"nums\"], x[\"target\"]))\n    # 将数据集拆分为训练集和测试集，拆分比例为 9:1\n    train_test_split = dataset.train_test_split(test_size=0.1)\n    train_dataset = train_test_split[\"train\"]  # 获取训练集\n    test_dataset = train_test_split[\"test\"]  # 获取测试集\n\n    # 参考自 huggingface/open-r1, 把attn_implementation（是否使用flash_attention）等参数传入模型初始化参数\n    logger.info(\"*** Initializing model kwargs ***\")\n    torch_dtype = (\n        model_args.torch_dtype if model_args.torch_dtype in [\"auto\", None] else getattr(torch, model_args.torch_dtype)\n    )\n    model_kwargs = dict(\n        revision=model_args.model_revision,\n        trust_remote_code=model_args.trust_remote_code,\n        attn_implementation=model_args.attn_implementation,\n        torch_dtype=torch_dtype,\n        use_cache=False if training_args.gradient_checkpointing else True,\n    )\n    training_args.model_init_kwargs = model_kwargs",
    "238": "# 设置 GRPOTrainer",
    "239": "trainer = GRPOTrainer(\n        model=model_args.model_name_or_path,  # 模型名称或路径\n        # 奖励函数列表，用于计算奖励分数\n        reward_funcs=[\n            format_reward_func,  # 格式奖励函数\n            equation_reward_func,  # 方程奖励函数\n        ],\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n        callbacks=callbacks,\n    )\n\n    last_checkpoint = get_checkpoint(training_args)  # 检查最后一个检查点\n    # 如果检测到检查点且指定从检查点恢复训练，则记录信息\n    if last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n        logger.info(f\"Checkpoint detected, resuming training at {last_checkpoint}.\")\n\n    logger.info(\n        f'*** Starting training {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")} for {training_args.num_train_epochs} epochs***'\n    )",
    "240": "# 训练模型",
    "241": "train_result = trainer.train(resume_from_checkpoint=last_checkpoint)",
    "242": "# 保存训练结果",
    "243": "# 记录和保存指标\n    metrics = train_result.metrics\n    metrics[\"train_samples\"] = len(train_dataset)\n    trainer.log_metrics(\"train\", metrics)\n    trainer.save_metrics(\"train\", metrics)\n    trainer.save_state()\n\n    logger.info(\"*** Training complete ***\")\n\n    # 保存模型和分词器\n    logger.info(\"*** Save model ***\")\n    trainer.model.config.use_cache = True\n    trainer.save_model(training_args.output_dir)\n    logger.info(f\"Model saved to {training_args.output_dir}\")\n    training_args.distributed_state.wait_for_everyone()  # 等待所有进程加载\n    tokenizer.save_pretrained(training_args.output_dir)\n    logger.info(f\"Tokenizer saved to {training_args.output_dir}\")\n\n    logger.info(\"*** Training complete! ***\")\n\ndef main():\n    \"\"\"主函数，用于执行主训练循环\"\"\"\n    # 解析命令行参数和配置文件\n    parser = TrlParser((ModelConfig, DatasetArguments, GRPOConfig, SwanlabArguments))\n    model_args, dataset_args, training_args, swanlab_args = (\n        parser.parse_args_and_config()\n    )\n\n    # 如果使用 SwanLab，则创建 SwanLab 回调对象，用于训练信息记录\n    if swanlab_args.swanlab:\n        swanlab_callback = SwanLabCallback(\n            project=swanlab_args.project,\n            experiment_name=swanlab_args.experiment_name,\n        )\n        callbacks = [swanlab_callback]\n    else:\n        callbacks = None\n\n    # 运行主训练循环\n    grpo_function(model_args, dataset_args, training_args, callbacks=callbacks)\n\nif __name__ == \"__main__\":\n    main()\n```\n\n**4、设置分布式训练脚本**\n\n```python\naccelerate launch \\\n    --num_processes 2 \\\n    --config_file config/2rtx4090.yaml \\\n    train_r1_grpo.py \\\n    --config config/grpo-qwen-2.5-3b-deepseek-r1-zero-countdown.yaml\n```\n\n**5、启动训练**\n\n在命令行输入下面的内容：\n\n```bash\nbash train_r1_grpo.sh\n```",
    "244": "一级标题：Qwen2.5复现DeepSeek-R1-ZERO\n二级标题：训练后模型部署和推理\n内容：\n保存下来的仅仅是模型的权重信息以及配置文件等，是不能直接使用的，需要与原模型进行合并操作，代码如下：\n\n```python\nfrom peft import PeftModel\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nimport os\nimport shutil\n\n# 保证原始模型的各个文件不遗漏保存到merge_path中\ndef copy_files_not_in_B(A_path, B_path):\n    if not os.path.exists(A_path):\n        raise FileNotFoundError(f\"The directory {A_path} does not exist.\")\n    if not os.path.exists(B_path):\n        os.makedirs(B_path)\n\n    # 获取路径A中所有非权重文件\n    files_in_A = os.listdir(A_path)\n    files_in_A = set([file for file in files_in_A if not (\".bin\" in file or \"safetensors\" in file)])\n\n    files_in_B = set(os.listdir(B_path))\n\n    # 找到所有A中存在但B中不存在的文件\n    files_to_copy = files_in_A - files_in_B\n\n    # 将文件或文件夹复制到B路径下\n    for file in files_to_copy:\n        src_path = os.path.join(A_path, file)\n        dst_path = os.path.join(B_path, file)\n\n        if os.path.isdir(src_path):\n            # 复制目录及其内容\n            shutil.copytree(src_path, dst_path)\n        else:\n            # 复制文件\n            shutil.copy2(src_path, dst_path)\n\ndef merge_lora_to_base_model(adapter_name_or_path,save_path,model_name_or_path=\"Qwen/Qwen2-0.5B\"):\n    # 如果文件夹不存在，就创建\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,trust_remote_code=True,)\n\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name_or_path,\n        trust_remote_code=True,\n        low_cpu_mem_usage=True,\n        torch_dtype=torch.float16,\n        device_map=\"auto\"\n    )\n    # 加载保存的 Adapter\n    model = PeftModel.from_pretrained(model, adapter_name_or_path, device_map=\"auto\",trust_remote_code=True)\n    # 将 Adapter 合并到基础模型中\n    merged_model = model.merge_and_unload()  # PEFT 的方法将 Adapter 权重合并到基础模型\n    # 保存合并后的模型\n    tokenizer.save_pretrained(save_path)\n    merged_model.save_pretrained(save_path, safe_serialization=False)\n    copy_files_not_in_B(model_name_or_path, save_path)\n    print(f\"合并后的模型已保存至: {save_path}\")\n\n\nif __name__ == '__main__':\n    adapter_name_or_path=\"你的生成的模型的文件夹\"\n    save_path = \"保存模型的地址\"\n    merge_lora_to_base_model(adapter_name_or_path=adapter_name_or_path,save_path=save_path)\n```\n\n运行上述代码后，会得到最终合并后的模型，我们用该模型进行推理测试，测试代码如下：\n\n```python\nfrom transformers import AutoModelForCausalLM,AutoTokenizer\nimport torch\n\nMODEL_NAME_OR_PATH = \"output/qwen-grpo\"\nPROMPT=\"\"\"使用给定的数字 [80, 9, 18]，创建一个等于 53 的方程。你可以使用基本算术运算（+、-、*、/）一次或多次，\n          但每个数字只能使用一次。在 <think> </think> 标签中展示你的思考过程，并在 <answer> </answer> 标签中返回最终方程，\n          例如 <answer> (1 + 2) / 3 </answer>。在 <think> 标签中逐步思考。让我们逐步解决这个问题。\\n<think>\"\"\"\n\n\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME_OR_PATH,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": PROMPT}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512,\n    top_p=0.95,\n    temperature=0.7,\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\nprint(response)\n```\n\n---",
    "245": "一级标题：Qwen2.5复现DeepSeek-R1-ZERO\n二级标题：实验结果演示\n内容：\n由于训练时间较长，推荐使用tmux将训练任务hold住。可以在[SwanLab](https://swanlab.cn/@LiXinYu/Try_r1/runs/iunfsosyp8ryfanbjcv7g/chart)中查看。\n\n![swanlab观测结果](./grpo/swanlab-results.png)\n\n---",
    "246": "一级标题：Qwen2.5复现DeepSeek-R1-ZERO\n二级标题：参考链接：\n内容：\n* https://github.com/philschmid/deep-learning-pytorch-huggingface\n* https://github.com/Jiayi-Pan/TinyZero\n* https://github.com/datawhalechina/unlock-deepseek\n* https://arxiv.org/pdf/2501.12948?\n* https://github.com/deepseek-ai/DeepSeek-R1\n* https://arxiv.org/pdf/2402.03300\n* https://zhuanlan.zhihu.com/p/21952581194\n* https://github.com/huggingface/open-r1?tab=readme-ov-file#grpo\n* https://zhuanlan.zhihu.com/p/21062322587\n* https://cloud.tencent.com/developer/article/2495699",
    "247": "一级标题：Qwen2-VL多模态大模型微调实战\n二级标题：无\n内容：\n:::info\n多模态，大语言模型，大模型微调\n:::\n\n\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/Qwen2-VL-finetune/runs/pkgest5xhdn3ukpdy6kv5/chart)\n\n[训练过程](https://swanlab.cn/@ZeyiLin/Qwen2-VL-finetune/runs/pkgest5xhdn3ukpdy6kv5/chart) | [知乎教程](https://zhuanlan.zhihu.com/p/7144893529)\n\nQwen2-VL是阿里通义实验室推出的多模态大模型。本文我们将简要介绍基于 transformers、peft 等框架，使用 Qwen2-VL-2B-Instruct 模型在COCO2014图像描述 上进行Lora微调训练，同时使用 SwanLab 监控训练过程与评估模型效果。\n\n![](./qwen_vl_coco/01.png)\n\nLora 是一种高效微调方法，深入了解其原理可参见博客：[知乎|深入浅出 Lora](https://zhuanlan.zhihu.com/p/650197598)。\n\n- 训练过程：[Qwen2-VL-finetune](https://swanlab.cn/@ZeyiLin/Qwen2-VL-finetune/runs/pkgest5xhdn3ukpdy6kv5/chart)\n- Github：[代码仓库](https://github.com/Zeyi-Lin/LLM-Finetune/tree/main/qwen2_vl)、[self-llm](https://github.com/datawhalechina/self-llm)\n- 数据集：[coco_2014_caption](https://modelscope.cn/datasets/modelscope/coco_2014_caption/summary)\n- 模型：[Qwen2-VL-2B-Instruct](https://modelscope.cn/models/Qwen/Qwen2-VL-2B-Instruct)\n\nOCR微调版：[Qwen2-VL-Latex-OCR](https://zhuanlan.zhihu.com/p/10705293665)\n\n\n---",
    "248": "一级标题：Qwen2-VL多模态大模型微调实战\n二级标题：1. 环境配置\n内容：\n环境配置分为三步：\n\n1. 确保你的电脑上至少有一张英伟达显卡，并已安装好了CUDA环境。\n2. 安装Python（版本>=3.8）以及能够调用CUDA加速的PyTorch。\n3. 安装Qwen2-VL微调相关的第三方库，可以使用以下命令：\n\n```bash\npython -m pip install --upgrade pip\n# 更换 pypi 源加速库的安装\npip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n\npip install modelscope==1.18.0\npip install transformers==4.46.2\npip install sentencepiece==0.2.0\npip install accelerate==1.1.1\npip install datasets==2.18.0\npip install peft==0.13.2\npip install swanlab==0.3.25\npip install qwen-vl-utils==0.0.8\n```",
    "249": "一级标题：Qwen2-VL多模态大模型微调实战\n二级标题：2. 准备数据集\n内容：\n本节使用的是 [coco_2014_caption](https://modelscope.cn/datasets/modelscope/coco_2014_caption/summary) 数据集（中的500张图），该数据集主要用于多模态（Image-to-Text）任务。\n\n> 数据集介绍：COCO 2014 Caption数据集是Microsoft Common Objects in Context (COCO)数据集的一部分，主要用于图像描述任务。该数据集包含了大约40万张图像，每张图像都有至少1个人工生成的英文描述语句。这些描述语句旨在帮助计算机理解图像内容，并为图像自动生成描述提供训练数据。\n\n![](./qwen_vl_coco/02.png)\n\n在本节的任务中，我们主要使用其中的前500张图像，并对它进行处理和格式调整，目标是组合成如下格式的json文件：\n\n```json\n[\n{\n    \"id\": \"identity_1\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"COCO Yes: <|vision_start|>图像文件路径<|vision_end|>\"\n      },\n      {\n        \"from\": \"assistant\",\n        \"value\": \"A snow skier assessing the mountain before starting to sky\"\n      }\n    ]\n},\n...\n]\n```\n\n其中，\"from\"是角色（user代表人类，assistant代表模型），\"value\"是聊天的内容，其中`<|vision_start|>`和`<|vision_end|>`是Qwen2-VL模型识别图像的标记，中间可以放图像的文件路径，也可以是URL。\n\n**数据集下载与处理方式**\n\n1. **我们需要做四件事情：**\n    - 通过Modelscope下载coco_2014_caption数据集\n    - 加载数据集，将图像保存到本地\n    - 将图像路径和描述文本转换为一个csv文件\n    - 将csv文件转换为json文件\n\n2. **使用下面的代码完成从数据下载到生成csv的过程：**\n\ndata2csv.py：\n\n```python\n# 导入所需的库\nfrom modelscope.msdatasets import MsDataset\nimport os\nimport pandas as pd\n\nMAX_DATA_NUMBER = 500\n\n# 检查目录是否已存在\nif not os.path.exists('coco_2014_caption'):\n    # 从modelscope下载COCO 2014图像描述数据集\n    ds =  MsDataset.load('modelscope/coco_2014_caption', subset_name='coco_2014_caption', split='train')\n    print(len(ds))\n    # 设置处理的图片数量上限\n    total = min(MAX_DATA_NUMBER, len(ds))\n\n    # 创建保存图片的目录\n    os.makedirs('coco_2014_caption', exist_ok=True)\n\n    # 初始化存储图片路径和描述的列表\n    image_paths = []\n    captions = []\n\n    for i in range(total):\n        # 获取每个样本的信息\n        item = ds[i]\n        image_id = item['image_id']\n        caption = item['caption']\n        image = item['image']\n        \n        # 保存图片并记录路径\n        image_path = os.path.abspath(f'coco_2014_caption/{image_id}.jpg')\n        image.save(image_path)\n        \n        # 将路径和描述添加到列表中\n        image_paths.append(image_path)\n        captions.append(caption)\n        \n        # 每处理50张图片打印一次进度\n        if (i + 1) % 50 == 0:\n            print(f'Processing {i+1}/{total} images ({(i+1)/total*100:.1f}%)')\n\n    # 将图片路径和描述保存为CSV文件\n    df = pd.DataFrame({\n        'image_path': image_paths,\n        'caption': captions\n    })\n    \n    # 将数据保存为CSV文件\n    df.to_csv('./coco-2024-dataset.csv', index=False)\n    \n    print(f'数据处理完成，共处理了{total}张图片')\n\nelse:\n    print('coco_2014_caption目录已存在,跳过数据处理步骤')\n```\n\n\n**3. 在同一目录下，用以下代码，将csv文件转换为json文件：**\n\ncsv2json.py：\n\n```python\nimport pandas as pd\nimport json\n\n# 载入CSV文件\ndf = pd.read_csv('./coco-2024-dataset.csv')\nconversations = []\n\n# 添加对话数据\nfor i in range(len(df)):\n    conversations.append({\n        \"id\": f\"identity_{i+1}\",\n        \"conversations\": [\n            {\n                \"from\": \"user\",\n                \"value\": f\"COCO Yes: <|vision_start|>{df.iloc[i]['image_path']}<|vision_end|>\"\n            },\n            {\n                \"from\": \"assistant\", \n                \"value\": df.iloc[i]['caption']\n            }\n        ]\n    })\n\n# 保存为Json\nwith open('data_vl.json', 'w', encoding='utf-8') as f:\n    json.dump(conversations, f, ensure_ascii=False, indent=2)\n```\n\n此时目录下会多出两个文件：\n- coco-2024-dataset.csv\n- data_vl.json\n\n至此，我们完成了数据集的准备。",
    "250": "一级标题：Qwen2-VL多模态大模型微调实战\n二级标题：3. 模型下载与加载\n内容：\n这里我们使用modelscope下载Qwen2-VL-2B-Instruct模型，然后把它加载到Transformers中进行训练：\n\n```python\nfrom modelscope import snapshot_download, AutoTokenizer\nfrom transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq, Qwen2VLForConditionalGeneration, AutoProcessor\nimport torch\n\n# 在modelscope上下载Qwen2-VL模型到本地目录下\nmodel_dir = snapshot_download(\"Qwen/Qwen2-VL-2B-Instruct\", cache_dir=\"./\", revision=\"master\")\n\n# 使用Transformers加载模型权重\ntokenizer = AutoTokenizer.from_pretrained(\"./Qwen/Qwen2-VL-2B-Instruct/\", use_fast=False, trust_remote_code=True)\n# 特别的，Qwen2-VL-2B-Instruct模型需要使用Qwen2VLForConditionalGeneration来加载\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\"./Qwen/Qwen2-VL-2B-Instruct/\", device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True,)\nmodel.enable_input_require_grads()  # 开启梯度检查点时，要执行该方法\n```\n\n模型大小为 4.5GB，下载模型大概需要 5 分钟。",
    "251": "一级标题：Qwen2-VL多模态大模型微调实战\n二级标题：4. 集成SwanLab\n内容：\n[SwanLab](https://github.com/swanhubx/swanlab) 是一个开源的模型训练记录工具。SwanLab面向AI研究者，提供了训练可视化、自动日志记录、超参数记录、实验对比、多人协同等功能。在SwanLab上，研究者能基于直观的可视化图表发现训练问题，对比多个实验找到研究灵感，并通过在线链接的分享与基于组织的多人协同训练，打破团队沟通的壁垒。\n\nSwanLab与Transformers已经做好了集成，用法是在Trainer的`callbacks`参数中添加`SwanLabCallback`实例，就可以自动记录超参数和训练指标，简化代码如下：\n\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom transformers import Trainer\n\nswanlab_callback = SwanLabCallback()\n\ntrainer = Trainer(\n    ...\n    callbacks=[swanlab_callback],\n)\n```\n\n首次使用SwanLab，需要先在[官网](https://swanlab.cn)注册一个账号，然后在用户设置页面复制你的API Key，然后在训练开始提示登录时粘贴即可，后续无需再次登录：\n\n![](./qwen_vl_coco/04.png)\n\n\n更多用法可参考[快速开始](https://docs.swanlab.cn/zh/guide_cloud/general/quick-start.html)、[Transformers集成](https://docs.swanlab.cn/zh/guide_cloud/integration/integration-huggingface-transformers.html)。",
    "252": "一级标题：Qwen2-VL多模态大模型微调实战\n二级标题：5. 开始微调\n内容：\n查看可视化训练过程：<a href=\"https://swanlab.cn/@ZeyiLin/Qwen2-VL-finetune/runs/53vm3y7sp5h5fzlmlc5up/chart\" target=\"_blank\">Qwen2-VL-finetune</a>\n\n\n**本节代码做了以下几件事：**\n1. 下载并加载Qwen2-VL-2B-Instruct模型\n2. 加载数据集，取前496条数据参与训练，4条数据进行主观评测\n3. 配置Lora，参数为r=64, lora_alpha=16, lora_dropout=0.05\n4. 使用SwanLab记录训练过程，包括超参数、指标和最终的模型输出结果\n5. 训练2个epoch\n\n开始执行代码时的目录结构应该是：\n```\n|———— train.py\n|———— coco_2014_caption\n|———— coco-2024-dataset.csv\n|———— data_vl.json\n|———— data2csv.py\n|———— csv2json.py\n```\n\n\n**完整代码如下**\n\ntrain.py：\n\n```python\nimport torch\nfrom datasets import Dataset\nfrom modelscope import snapshot_download, AutoTokenizer\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom qwen_vl_utils import process_vision_info\nfrom peft import LoraConfig, TaskType, get_peft_model, PeftModel\nfrom transformers import (\n    TrainingArguments,\n    Trainer,\n    DataCollatorForSeq2Seq,\n    Qwen2VLForConditionalGeneration,\n    AutoProcessor,\n)\nimport swanlab\nimport json\n\n\ndef process_func(example):\n    \"\"\"\n    将数据集进行预处理\n    \"\"\"\n    MAX_LENGTH = 8192\n    input_ids, attention_mask, labels = [], [], []\n    conversation = example[\"conversations\"]\n    input_content = conversation[0][\"value\"]\n    output_content = conversation[1][\"value\"]\n    file_path = input_content.split(\"<|vision_start|>\")[1].split(\"<|vision_end|>\")[0]  # 获取图像路径\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image\",\n                    \"image\": f\"{file_path}\",\n                    \"resized_height\": 280,\n                    \"resized_width\": 280,\n                },\n                {\"type\": \"text\", \"text\": \"COCO Yes:\"},\n            ],\n        }\n    ]\n    text = processor.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )  # 获取文本\n    image_inputs, video_inputs = process_vision_info(messages)  # 获取数据数据（预处理过）\n    inputs = processor(\n        text=[text],\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\",\n    )\n    inputs = {key: value.tolist() for key, value in inputs.items()} #tensor -> list,为了方便拼接\n    instruction = inputs\n\n    response = tokenizer(f\"{output_content}\", add_special_tokens=False)\n\n\n    input_ids = (\n            instruction[\"input_ids\"][0] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n    )\n\n    attention_mask = instruction[\"attention_mask\"][0] + response[\"attention_mask\"] + [1]\n    labels = (\n            [-100] * len(instruction[\"input_ids\"][0])\n            + response[\"input_ids\"]\n            + [tokenizer.pad_token_id]\n    )\n    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n        input_ids = input_ids[:MAX_LENGTH]\n        attention_mask = attention_mask[:MAX_LENGTH]\n        labels = labels[:MAX_LENGTH]\n\n    input_ids = torch.tensor(input_ids)\n    attention_mask = torch.tensor(attention_mask)\n    labels = torch.tensor(labels)\n    inputs['pixel_values'] = torch.tensor(inputs['pixel_values'])\n    inputs['image_grid_thw'] = torch.tensor(inputs['image_grid_thw']).squeeze(0)  #由（1,h,w)变换为（h,w）\n    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels,\n            \"pixel_values\": inputs['pixel_values'], \"image_grid_thw\": inputs['image_grid_thw']}\n\n\ndef predict(messages, model):\n    # 准备推理\n    text = processor.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n    image_inputs, video_inputs = process_vision_info(messages)\n    inputs = processor(\n        text=[text],\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\",\n    )\n    inputs = inputs.to(\"cuda\")\n\n    # 生成输出\n    generated_ids = model.generate(**inputs, max_new_tokens=128)\n    generated_ids_trimmed = [\n        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n    ]\n    output_text = processor.batch_decode(\n        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n    )\n    \n    return output_text[0]\n\n\n# 在modelscope上下载Qwen2-VL模型到本地目录下\nmodel_dir = snapshot_download(\"Qwen/Qwen2-VL-2B-Instruct\", cache_dir=\"./\", revision=\"master\")\n\n# 使用Transformers加载模型权重\ntokenizer = AutoTokenizer.from_pretrained(\"./Qwen/Qwen2-VL-2B-Instruct/\", use_fast=False, trust_remote_code=True)\nprocessor = AutoProcessor.from_pretrained(\"./Qwen/Qwen2-VL-2B-Instruct\")\n\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\"./Qwen/Qwen2-VL-2B-Instruct/\", device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True,)\nmodel.enable_input_require_grads()  # 开启梯度检查点时，要执行该方法\n\n# 处理数据集：读取json文件\n# 拆分成训练集和测试集，保存为data_vl_train.json和data_vl_test.json\ntrain_json_path = \"data_vl.json\"\nwith open(train_json_path, 'r') as f:\n    data = json.load(f)\n    train_data = data[:-4]\n    test_data = data[-4:]\n\nwith open(\"data_vl_train.json\", \"w\") as f:\n    json.dump(train_data, f)\n\nwith open(\"data_vl_test.json\", \"w\") as f:\n    json.dump(test_data, f)\n\ntrain_ds = Dataset.from_json(\"data_vl_train.json\")\ntrain_dataset = train_ds.map(process_func)\n\n# 配置LoRA\nconfig = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    inference_mode=False,  # 训练模式\n    r=64,  # Lora 秩\n    lora_alpha=16,  # Lora alaph，具体作用参见 Lora 原理\n    lora_dropout=0.05,  # Dropout 比例\n    bias=\"none\",\n)\n\n# 获取LoRA模型\npeft_model = get_peft_model(model, config)\n\n# 配置训练参数\nargs = TrainingArguments(\n    output_dir=\"./output/Qwen2-VL-2B\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    logging_steps=10,\n    logging_first_step=5,\n    num_train_epochs=2,\n    save_steps=100,\n    learning_rate=1e-4,\n    save_on_each_node=True,\n    gradient_checkpointing=True,\n    report_to=\"none\",\n)\n        \n# 设置SwanLab回调\nswanlab_callback = SwanLabCallback(\n    project=\"Qwen2-VL-finetune\",\n    experiment_name=\"qwen2-vl-coco2014\",\n    config={\n        \"model\": \"https://modelscope.cn/models/Qwen/Qwen2-VL-2B-Instruct\",\n        \"dataset\": \"https://modelscope.cn/datasets/modelscope/coco_2014_caption/quickstart\",\n        \"github\": \"https://github.com/datawhalechina/self-llm\",\n        \"prompt\": \"COCO Yes: \",\n        \"train_data_number\": len(train_data),\n        \"lora_rank\": 64,\n        \"lora_alpha\": 16,\n        \"lora_dropout\": 0.1,\n    },\n)\n\n# 配置Trainer\ntrainer = Trainer(\n    model=peft_model,\n    args=args,\n    train_dataset=train_dataset,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n    callbacks=[swanlab_callback],\n)\n\n# 开启模型训练\ntrainer.train()\n\n# ====================测试模式===================\n# 配置测试参数\nval_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    inference_mode=True,  # 训练模式\n    r=64,  # Lora 秩\n    lora_alpha=16,  # Lora alaph，具体作用参见 Lora 原理\n    lora_dropout=0.05,  # Dropout 比例\n    bias=\"none\",\n)\n\n# 获取测试模型\nval_peft_model = PeftModel.from_pretrained(model, model_id=\"./output/Qwen2-VL-2B/checkpoint-62\", config=val_config)\n\n# 读取测试数据\nwith open(\"data_vl_test.json\", \"r\") as f:\n    test_dataset = json.load(f)\n\ntest_image_list = []\nfor item in test_dataset:\n    input_image_prompt = item[\"conversations\"][0][\"value\"]\n    # 去掉前后的<|vision_start|>和<|vision_end|>\n    origin_image_path = input_image_prompt.split(\"<|vision_start|>\")[1].split(\"<|vision_end|>\")[0]\n    \n    messages = [{\n        \"role\": \"user\", \n        \"content\": [\n            {\n            \"type\": \"image\", \n            \"image\": origin_image_path\n            },\n            {\n            \"type\": \"text\",\n            \"text\": \"COCO Yes:\"\n            }\n        ]}]\n    \n    response = predict(messages, val_peft_model)\n    messages.append({\"role\": \"assistant\", \"content\": f\"{response}\"})\n    print(messages[-1])\n\n    test_image_list.append(swanlab.Image(origin_image_path, caption=response))\n\nswanlab.log({\"Prediction\": test_image_list})\n\n# 在Jupyter Notebook中运行时要停止SwanLab记录，需要调用swanlab.finish()\nswanlab.finish()\n```\n\n看到下面的进度条即代表训练开始：\n\n![](./qwen_vl_coco/05.png)",
    "253": "一级标题：Qwen2-VL多模态大模型微调实战\n二级标题：6. 训练结果演示\n内容：\n详细训练过程请看这里：[qwen2-vl-coco2014](https://swanlab.cn/@ZeyiLin/Qwen2-VL-finetune/runs/pkgest5xhdn3ukpdy6kv5/chart)\n\n![](./qwen_vl_coco/06.png)\n\n\n从SwanLab图表中我们可以看到，lr的下降策略是线性下降，loss随epoch呈现下降趋势，而grad_norm则在上升。这种形态往往反映了模型有过拟合的风险，训练不要超过2个epoch。\n\n在`Prediction`图表中记录着模型最终的输出结果，可以看到模型在回答的风格上是用的COCO数据集的简短英文风格进行的描述：\n\n![](./qwen_vl_coco/07.png)\n\n\n\n而同样的图像，没有被微调的模型输出结果如下：\n\n```\n1-没有微调：The image depicts a cozy living room with a rocking chair in the center, a bookshelf filled with books, and a table with a vase and a few other items. The walls are decorated with wallpaper, and there are curtains on the windows. The room appears to be well-lit, with sunlight streaming in from the windows.\n1-微调后：A living room with a rocking chair, a bookshelf, and a table with a vase and a bowl.\n\n2-没有微调：It looks like a family gathering or a party in a living room. There are several people sitting around a dining table, eating pizza. The room has a cozy and warm atmosphere.\n2-微调后：A group of people sitting around a dining table eating pizza.\n```\n\n可以明显看到微调后风格的变化。",
    "254": "一级标题：Qwen2-VL多模态大模型微调实战\n二级标题：7. 推理LoRA微调后的模型\n内容：\n加载lora微调后的模型，并进行推理：\n\n```python\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nfrom peft import PeftModel, LoraConfig, TaskType\n\nconfig = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    inference_mode=True,\n    r=64,  # Lora 秩\n    lora_alpha=16,  # Lora alaph，具体作用参见 Lora 原理\n    lora_dropout=0.05,  # Dropout 比例\n    bias=\"none\",\n)\n\n# default: Load the model on the available device(s)\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"./Qwen/Qwen2-VL-2B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n)\nmodel = PeftModel.from_pretrained(model, model_id=\"./output/Qwen2-VL-2B/checkpoint-62\", config=config)\nprocessor = AutoProcessor.from_pretrained(\"./Qwen/Qwen2-VL-2B-Instruct\")\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"测试图像路径\",\n            },\n            {\"type\": \"text\", \"text\": \"COCO Yes:\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```",
    "255": "一级标题：Qwen2-VL多模态大模型微调实战\n二级标题：补充\n内容：\n### 详细硬件配置和参数说明\n\n使用4张A100 40GB显卡，batch size为4，gradient accumulation steps为4，训练2个epoch的用时为1分钟57秒。\n\n![](./qwen_vl_coco/08.png)\n\n![](./qwen_vl_coco/09.png)\n\n\n### 注意\n\n- 在微调脚本中，`val_peft_model`加载的是一共固定的checkpoint文件，如果你添加了数据或超参数，请根据实际情况修改checkpoint文件路径。",
    "256": "一级标题：Stable Diffusion文生图微调\n二级标题：无\n内容：\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/SD-Naruto/runs/21flglg1lbnqo67a6f1kr/chart)\n\n[知乎教程](https://zhuanlan.zhihu.com/p/703921817) | [在线Demo](https://swanlab.cn/@ZeyiLin/SD-Naruto/runs/21flglg1lbnqo67a6f1kr/chart)\n\n[Stable Diffusion 1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main)（SD1.5）是由Stability AI在2022年8月22日开源的文生图模型，是SD最经典也是社区最活跃的模型之一。\n\n以SD1.5作为预训练模型，在火影忍者数据集上微调一个火影风格的文生图模型（非Lora方式），是学习**SD训练**的入门任务。\n\n![alt text](./images/stable_diffusion/01.png)\n\n\n> 显存要求 22GB左右\n\n在本文中，我们会使用[SD-1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5)模型在[火影忍者](https://huggingface.co/datasets/lambdalabs/naruto-blip-captions)数据集上做训练，同时使用[SwanLab](https://swanlab.cn)监控训练过程、评估模型效果。\n\n- 代码：[Github](https://github.com/Zeyi-Lin/Stable-Diffusion-Example)\n- 实验日志过程：[SD-naruto - SwanLab](https://swanlab.cn/@ZeyiLin/SD-Naruto/runs/21flglg1lbnqo67a6f1kr)\n- 模型：[runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)\n- 数据集：[lambdalabs/naruto-blip-captions](https://huggingface.co/datasets/lambdalabs/naruto-blip-captions)\n- SwanLab：[https://swanlab.cn](https://swanlab.cn)",
    "257": "一级标题：Stable Diffusion文生图微调\n二级标题：1.环境安装\n内容：\n本案例基于**Python>=3.8**，请在您的计算机上安装好Python；\n\n另外，您的计算机上至少要有一张英伟达显卡（显存大约要求22GB左右）。\n\n我们需要安装以下这几个Python库，在这之前，请确保你的环境内已安装了pytorch以及CUDA：\n\n```txt\nswanlab\ndiffusers\ndatasets\naccelerate\ntorchvision\ntransformers\n```\n\n一键安装命令：\n\n```bash\npip install swanlab diffusers datasets accelerate torchvision transformers\n```\n\n> 本文的代码测试于diffusers==0.29.0、accelerate==0.30.1、datasets==2.18.0、transformers==4.41.2、swanlab==0.3.11，更多库版本可查看[SwanLab记录的Python环境](https://swanlab.cn/@ZeyiLin/SD-Naruto/runs/21flglg1lbnqo67a6f1kr/environment/requirements)。",
    "258": "一级标题：Stable Diffusion文生图微调\n二级标题：2.准备数据集\n内容：\n本案例是用的是[火影忍者](https://huggingface.co/datasets/lambdalabs/naruto-blip-captions)数据集，该数据集主要被用于训练文生图模型。\n\n该数据集由1200条（图像、描述）对组成，左边是火影人物的图像，右边是对它的描述：\n\n![alt text](./images/stable_diffusion/02.png)\n\n\n我们的训练任务，便是希望训练后的SD模型能够输入提示词，生成火影风格的图像：\n\n![alt text](./images/stable_diffusion/03.png)\n\n\n---\n\n数据集的大小大约700MB左右；数据集的下载方式有两种：\n\n1. 如果你的网络与HuggingFace连接是通畅的，那么直接运行我下面提供的代码即可，它会直接通过HF的`datasets`库进行下载。\n2. 如果网络存在问题，我也把它放到[百度网盘](https://pan.baidu.com/s/1Yu5HjXnHxK0Wgymc8G-g5g?pwd=gtk8)（提取码: gtk8），下载`naruto-blip-captions.zip`到本地解压后，运行到与训练脚本同一目录下。",
    "259": "一级标题：Stable Diffusion文生图微调\n二级标题：3.准备模型\n内容：\n这里我们使用HuggingFace上Runway发布的[stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)模型。\n\n![alt text](./images/stable_diffusion/04.png)\n\n\n模型的下载方式同样有两种：\n\n1. 如果你的网络与HuggingFace连接是通畅的，那么直接运行我下面提供的代码即可，它会直接通过HF的`transformers`库进行下载。\n2. 如果网络存在问题，我也把它放到[百度网盘](https://pan.baidu.com/s/1Yu5HjXnHxK0Wgymc8G-g5g?pwd=gtk8)（提取码: gtk8），下载`stable-diffusion-v1-5.zip`到本地解压后，运行到与训练脚本同一目录下。",
    "260": "一级标题：Stable Diffusion文生图微调\n二级标题：4. 配置训练可视化工具\n内容：\n我们使用[SwanLab](https://swanlab.cn)来监控整个训练过程，并评估最终的模型效果。\n\n如果你是第一次使用SwanLab，那么还需要去https://swanlab.cn上注册一个账号，在**用户设置**页面复制你的API Key，然后在训练开始时粘贴进去即可：\n\n![alt text](./images/stable_diffusion/05.png)",
    "261": "一级标题：Stable Diffusion文生图微调\n二级标题：5.开始训练\n内容：\n由于训练的代码比较长，所以我把它放到了[Github](https://github.com/Zeyi-Lin/Stable-Diffusion-Example/tree/main)里，请Clone里面的代码：\n\n```bash\ngit clone https://github.com/Zeyi-Lin/Stable-Diffusion-Example.git\n```\n\n如果你与HuggingFace的网络连接通畅，那么直接运行训练：\n\n```bash\npython train_sd1-5_naruto.py \\\n  --use_ema \\\n  --resolution=512 --center_crop --random_flip \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --seed=42 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --output_dir=\"sd-naruto-model\"\n```\n\n上面这些参数的含义如下：\n\n- `--use_ema`: 使用指数移动平均 (EMA) 技术，该技术可以提高模型的泛化能力，在训练过程中使用模型参数的移动平均值进行预测，而不是直接使用当前模型参数。\n- `--resolution=512`: 设置训练图像的分辨率为 512 像素。\n- `--center_crop`: 对图像进行中心裁剪，将图像的中心部分作为训练样本，忽略图像边缘的部分。\n- `--random_flip`: 在训练过程中对图像进行随机翻转，增加训练数据的多样性。\n- `--train_batch_size=1`: 设置训练批次大小为 1，即每次训练只使用一张图像。\n- `--gradient_accumulation_steps=4`: 梯度累积步数为 4，即每进行 4 次训练才进行一次参数更新。\n- `--gradient_checkpointing`: 使用梯度检查点技术，可以减少内存使用量，加快训练速度。\n- `--max_train_steps=15000`: 设置最大训练步数为 15000 步。\n- `--learning_rate=1e-05`: 设置学习率为 1e-05。\n- `--max_grad_norm=1`: 设置梯度范数的最大值为 1，防止梯度爆炸。\n- `--seed=42`: 设置随机种子为 42，确保每次训练的随机性一致。\n- `--lr_scheduler=\"constant\"`: 使用常数学习率调度器，即在整个训练过程中保持学习率不变。\n- `--lr_warmup_steps=0`: 设置学习率预热步数为 0，即不进行预热。\n- `--output_dir=\"sd-naruto-model\"`: 设置模型输出目录为 \"sd-naruto-model\"。\n\n---\n\n如果你的模型或数据集用的是**上面的网盘下载的**，那么你需要做下面的两件事：\n\n**第一步**：将数据集和模型文件夹放到训练脚本同一目录下，文件结构如下：\n\n```txt\n|--- sd_config.py\n|--- train_sd1-5_naruto.py\n|--- stable-diffusion-v1-5\n|--- naruto-blip-captions\n```\n\n`stable-diffusion-v1-5`是下载好的模型文件夹，`naruto-blip-captions`是下载好的数据集文件夹。\n\n**第二步**：修改`sd_config.py`的代码，将`pretrained_model_name_or_path`和`dataset_name`的default值分别改为下面这样：\n\n```python\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=\"./stable-diffusion-v1-5\",\n    )\n    parser.add_argument(\n        \"--dataset_name\",\n        type=str,\n        default=\"./naruto-blip-captions\",\n    )\n```\n\n然后运行启动命令即可。\n\n---\n\n看到下面的进度条即代表训练开始：\n\n![alt text](./images/stable_diffusion/05.png)",
    "262": "一级标题：Stable Diffusion文生图微调\n二级标题：6. 训练结果演示\n内容：\n我们在[SwanLab](https://swanlab.cn/@ZeyiLin/SD-Naruto/runs/21flglg1lbnqo67a6f1kr/chart)上查看最终的训练结果：\n\n![alt text](./images/stable_diffusion/06.png)\n\n\n可以看到SD训练的特点是loss一直在震荡，随着epoch的增加，loss在最初下降后，后续的变化其实并不大：\n\n![alt text](./images/stable_diffusion/07.png)\n\n\n我们来看看主观生成的图像，第一个epoch的图像长这样：\n\n![alt text](./images/stable_diffusion/08.png)\n\n\n可以看到詹姆斯还是非常的“原生态”，迈克尔杰克逊生成的也怪怪的。。。\n\n再看一下中间的状态：\n\n![alt text](./images/stable_diffusion/09.png)\n\n\n![alt text](./images/stable_diffusion/10.png)\n\n\n经过比较长时间的训练后，效果就好了不少。\n\n> 比较有意思的是，比尔盖茨生成出来的形象总是感觉非常邪恶。。。\n\n![alt text](./images/stable_diffusion/11.png)\n\n至此，你已经完成了SD模型在火影忍者数据集上的训练。",
    "263": "一级标题：Stable Diffusion文生图微调\n二级标题：7. 模型推理\n内容：\n训练好的模型会放到`sd-naruto-model`文件夹下，推理代码如下：\n\n```python\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"./sd-naruto-model\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"Lebron James with a hat\"\nimage = pipe(prompt).images[0]  \n    \nimage.save(\"result.png\")\n```",
    "264": "一级标题：Stable Diffusion文生图微调\n二级标题：相关链接\n内容：\n- 代码：[Github](https://github.com/Zeyi-Lin/Stable-Diffusion-Example)\n- 实验日志过程：[SD-naruto - SwanLab](https://swanlab.cn/@ZeyiLin/SD-Naruto/runs/21flglg1lbnqo67a6f1kr/chart)\n- 模型：[runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)\n- 数据集：[lambdalabs/naruto-blip-captions](https://huggingface.co/datasets/lambdalabs/naruto-blip-captions)\n- SwanLab：[https://swanlab.cn](https://swanlab.cn)",
    "265": "一级标题：UNet 医学影像分割\n二级标题：无\n内容：\n:::info\n计算机视觉，医学影像，图像分割\n:::\n\n\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/Unet-Medical-Segmentation/runs/67konj7kdqhnfdmusy2u6/chart)\n\n[训练过程](https://swanlab.cn/@ZeyiLin/Unet-Medical-Segmentation/runs/67konj7kdqhnfdmusy2u6/chart)\n\nUNet是一种基于卷积神经网络（CNN）的医学影像分割模型，由Ronneberger等人于2015年提出。本文我们将简要介绍基于PyTorch框架，使用UNet模型在脑瘤医学影像分割数据集上进行训练，同时通过SwanLab监控训练过程，实现对病灶区域或器官结构的智能定位。\n\n![](./unet-medical-segmentation/train_image.png)\n\n\n- 代码：完整代码直接看本文第5节 或 [Github](https://github.com/Zeyi-Lin/UNet-Medical)\n- 实验日志过程：[Unet-Medical-Segmentation - SwanLab](https://swanlab.cn/@ZeyiLin/Unet-Medical-Segmentation/runs/67konj7kdqhnfdmusy2u6/chart)\n- 模型：UNet（Pytorch代码直接写）\n- 数据集：[brain-tumor-image-dataset-semantic-segmentation - Kagggle](https://www.kaggle.com/datasets/pkdarabi/brain-tumor-image-dataset-semantic-segmentation)\n- SwanLab：[https://swanlab.cn](https://swanlab.cn)\n- 论文：[《U-Net: Convolutional Networks for Biomedical Image Segmentation》](https://arxiv.org/abs/1505.04597)\n\n---",
    "266": "一级标题：UNet 医学影像分割\n二级标题：1. 环境配置\n内容：\n环境配置分为三步：\n\n1. 确保你的电脑上至少有一张英伟达显卡，并已安装好了CUDA环境。\n2. 安装Python（版本>=3.8）以及能够调用CUDA加速的PyTorch。\n3. 安装UNet微调相关的第三方库，可以使用以下命令：\n\n```bash\ngit clone https://github.com/Zeyi-Lin/UNet-Medical.git\ncd UNet-Medical\npip install -r requirements.txt\n```",
    "267": "一级标题：UNet 医学影像分割\n二级标题：2. 准备数据集\n内容：\n本节使用的是 [脑瘤图像分割](https://www.kaggle.com/datasets/pkdarabi/brain-tumor-image-dataset-semantic-segmentation) 数据集，该数据集主要用于医学影像分割任务。\n\n> ​​数据集介绍​​：Brain Tumor Segmentation Dataset 是专用于医学图像语义分割的数据集，旨在精准识别脑肿瘤区域。该数据集包含两类标注（肿瘤/非肿瘤），通过像素级分类实现肿瘤区域的细粒度分割，适用于训练和评估医学影像分割模型，为脑肿瘤诊断提供自动化分析支持。\n\n![](./unet-medical-segmentation/kaggle.png)\n\n在本节的任务中，我们主要是将数据集下载下来并解压，以供后续的训练。\n\n**下载数据集并解压：**\n\n```bash\npython download.py\nunzip dataset/Brain_Tumor_Image_DataSet.zip -d dataset/\n```\n\n完成上述步骤后，你应该可以根目录下看到这样的文件夹：\n\n![](./unet-medical-segmentation/dir.png)\n\n文件夹中包含训练集、验证集和测试集，里面有图像文件（`jpg`格式）和标注文件（`json`格式）。至此，我们完成了数据集的准备。\n\n下面是一些细节的代码展示，然后你想马上训练起来，可以直接跳到第五节。",
    "268": "一级标题：UNet 医学影像分割\n二级标题：3. 模型代码\n内容：\n这里我们使用PyTorch来写UNet模型（在`net.py`中）。代码展示如下：\n\n```python\nimport torch\nimport torch.nn as nn\n\n# 定义U-Net模型的下采样块\nclass DownBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, dropout_prob=0, max_pooling=True):\n        super(DownBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(2) if max_pooling else None\n        self.dropout = nn.Dropout(dropout_prob) if dropout_prob > 0 else None\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        if self.dropout:\n            x = self.dropout(x)\n        skip = x\n        if self.maxpool:\n            x = self.maxpool(x)\n        return x, skip\n\n# 定义U-Net模型的上采样块\nclass UpBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(UpBlock, self).__init__()\n        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n        self.conv1 = nn.Conv2d(out_channels * 2, out_channels, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x, skip):\n        x = self.up(x)\n        x = torch.cat([x, skip], dim=1)\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        return x\n\n# 定义完整的U-Net模型\nclass UNet(nn.Module):\n    def __init__(self, n_channels=3, n_classes=1, n_filters=32):\n        super(UNet, self).__init__()\n        \n        # 编码器路径\n        self.down1 = DownBlock(n_channels, n_filters)\n        self.down2 = DownBlock(n_filters, n_filters * 2)\n        self.down3 = DownBlock(n_filters * 2, n_filters * 4)\n        self.down4 = DownBlock(n_filters * 4, n_filters * 8)\n        self.down5 = DownBlock(n_filters * 8, n_filters * 16)\n        \n        # 瓶颈层 - 移除最后的maxpooling\n        self.bottleneck = DownBlock(n_filters * 16, n_filters * 32, dropout_prob=0.4, max_pooling=False)\n        \n        # 解码器路径\n        self.up1 = UpBlock(n_filters * 32, n_filters * 16)\n        self.up2 = UpBlock(n_filters * 16, n_filters * 8)\n        self.up3 = UpBlock(n_filters * 8, n_filters * 4)\n        self.up4 = UpBlock(n_filters * 4, n_filters * 2)\n        self.up5 = UpBlock(n_filters * 2, n_filters)\n        \n        # 输出层\n        self.outc = nn.Conv2d(n_filters, n_classes, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # 编码器路径\n        x1, skip1 = self.down1(x)      # 128\n        x2, skip2 = self.down2(x1)     # 64\n        x3, skip3 = self.down3(x2)     # 32\n        x4, skip4 = self.down4(x3)     # 16\n        x5, skip5 = self.down5(x4)     # 8\n        \n        # 瓶颈层\n        x6, skip6 = self.bottleneck(x5)  # 8 (无下采样)\n        \n        # 解码器路径\n        x = self.up1(x6, skip5)    # 16\n        x = self.up2(x, skip4)     # 32\n        x = self.up3(x, skip3)     # 64\n        x = self.up4(x, skip2)     # 128\n        x = self.up5(x, skip1)     # 256\n        \n        x = self.outc(x)\n        x = self.sigmoid(x)\n        return x\n```\n\n该模型保存为`pth`文件，大约需要124MB。",
    "269": "一级标题：UNet 医学影像分割\n二级标题：4. 使用SwanLab跟踪实验\n内容：\n[SwanLab](https://github.com/swanhubx/swanlab) 是一个开源的模型训练记录工具。SwanLab面向AI研究者，提供了训练可视化、自动日志记录、超参数记录、实验对比、多人协同等功能。在SwanLab上，研究者能基于直观的可视化图表发现训练问题，对比多个实验找到研究灵感，并通过在线链接的分享与基于组织的多人协同训练，打破团队沟通的壁垒。\n\n<video controls src=\"../guide_cloud/general/what_is_swanlab/demo.mp4\"></video>\n\n在本次训练中，我们设置swanlab的项目为`Unet-Medical-Segmentation`，实验名称为`bs32-epoch40`，并设置超参数如下：\n\n```python\nswanlab.init(\n    project=\"Unet-Medical-Segmentation\",\n    experiment_name=\"bs32-epoch40\",\n    config={\n        \"batch_size\": 32,\n        \"learning_rate\": 1e-4,\n        \"num_epochs\": 40,\n        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    },\n)\n```\n\n可以看到，这次训练的batch_size为32，学习率为1e-4，训练40个epoch。\n\n首次使用SwanLab，需要先在[官网](https://swanlab.cn)注册一个账号，然后在用户设置页面复制你的API Key，然后在训练开始提示登录时粘贴即可，后续无需再次登录：\n\n![](./qwen_vl_coco/04.png)",
    "270": "一级标题：UNet 医学影像分割\n二级标题：5. 开始训练\n内容：\n查看可视化训练过程：<a href=\"https://swanlab.cn/@ZeyiLin/Unet-Medical-Segmentation/runs/67konj7kdqhnfdmusy2u6/chart\" target=\"_blank\">Unet-Medical-Segmentation</a>\n\n**本节代码做了以下几件事：**\n1. 加载UNet模型\n2. 加载数据集，分为训练集、验证集和测试集，数据处理为Resize为 (256, 256)和 Normalization \n3. 使用SwanLab记录训练过程，包括超参数、指标和最终的模型输出结果\n4. 训练40个epoch\n5. 生成最后的预测图像\n\n开始执行代码时的目录结构应该是：\n\n```\n|———— dataset/\n|———————— train/\n|———————— val/\n|———————— test/\n|———— readme_files/\n|———— train.py\n|———— data.py\n|———— net.py\n|———— download.py\n|———— requirements.txt\n```\n\n**完整代码如下**\n\ntrain.py：\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom pycocotools.coco import COCO\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nimport random\nimport swanlab\nfrom net import UNet\nfrom data import COCOSegmentationDataset\n\n\n# 数据路径设置\ntrain_dir = './dataset/train'\nval_dir = './dataset/valid'\ntest_dir = './dataset/test'\n\ntrain_annotation_file = './dataset/train/_annotations.coco.json'\ntest_annotation_file = './dataset/test/_annotations.coco.json'\nval_annotation_file = './dataset/valid/_annotations.coco.json'\n\n# 加载COCO数据集\ntrain_coco = COCO(train_annotation_file)\nval_coco = COCO(val_annotation_file)\ntest_coco = COCO(test_annotation_file)\n\n# 定义损失函数\ndef dice_loss(pred, target, smooth=1e-6):\n    pred_flat = pred.view(-1)\n    target_flat = target.view(-1)\n    intersection = (pred_flat * target_flat).sum()\n    return 1 - ((2. * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth))\n\ndef combined_loss(pred, target):\n    dice = dice_loss(pred, target)\n    bce = nn.BCELoss()(pred, target)\n    return 0.6 * dice + 0.4 * bce\n\n# 训练函数\ndef train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n    best_val_loss = float('inf')\n    patience = 8\n    patience_counter = 0\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        train_acc = 0\n        \n        for images, masks in train_loader:\n            images, masks = images.to(device), masks.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            \n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            train_acc += (outputs.round() == masks).float().mean().item()\n\n        train_loss /= len(train_loader)\n        train_acc /= len(train_loader)\n        \n        # 验证\n        model.eval()\n        val_loss = 0\n        val_acc = 0\n        \n        with torch.no_grad():\n            for images, masks in val_loader:\n                images, masks = images.to(device), masks.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, masks)\n                \n                val_loss += loss.item()\n                val_acc += (outputs.round() == masks).float().mean().item()\n        \n        val_loss /= len(val_loader)\n        val_acc /= len(val_loader)\n        \n        swanlab.log(\n            {\n                \"train/loss\": train_loss,\n                \"train/acc\": train_acc,\n                \"train/epoch\": epoch+1,\n                \"val/loss\": val_loss,\n                \"val/acc\": val_acc,\n            },\n            step=epoch+1)\n        \n        print(f'Epoch {epoch+1}/{num_epochs}:')\n        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n        \n        # 早停\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), 'best_model.pth')\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\"Early stopping triggered\")\n                break\n\ndef main():\n    swanlab.init(\n        project=\"Unet-Medical-Segmentation\",\n        experiment_name=\"bs32-epoch40\",\n        config={\n            \"batch_size\": 32,\n            \"learning_rate\": 1e-4,\n            \"num_epochs\": 40,\n            \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n        },\n    )\n    \n    # 设置设备\n    device = torch.device(swanlab.config[\"device\"])\n    \n    # 数据预处理\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Resize((256, 256)),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    # 创建数据集\n    train_dataset = COCOSegmentationDataset(train_coco, train_dir, transform=transform)\n    val_dataset = COCOSegmentationDataset(val_coco, val_dir, transform=transform)\n    test_dataset = COCOSegmentationDataset(test_coco, test_dir, transform=transform)\n    \n    # 创建数据加载器\n    BATCH_SIZE = swanlab.config[\"batch_size\"]\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n    \n    # 初始化模型\n    model = UNet(n_filters=32).to(device)\n    \n    # 设置优化器和学习率\n    optimizer = optim.Adam(model.parameters(), lr=swanlab.config[\"learning_rate\"])\n    \n    # 训练模型\n    train_model(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        criterion=combined_loss,\n        optimizer=optimizer,\n        num_epochs=swanlab.config[\"num_epochs\"],\n        device=device,\n    )\n    \n    # 在测试集上评估\n    model.eval()\n    test_loss = 0\n    test_acc = 0\n    \n    with torch.no_grad():\n        for images, masks in test_loader:\n            images, masks = images.to(device), masks.to(device)\n            outputs = model(images)\n            loss = combined_loss(outputs, masks)\n            test_loss += loss.item()\n            test_acc += (outputs.round() == masks).float().mean().item()\n    \n    test_loss /= len(test_loader)\n    test_acc /= len(test_loader)\n    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n    swanlab.log({\"test/loss\": test_loss, \"test/acc\": test_acc})\n    \n    # 可视化预测结果\n    visualize_predictions(model, test_loader, device, num_samples=10)\n    \n\ndef visualize_predictions(model, test_loader, device, num_samples=5, threshold=0.5):\n    model.eval()\n    with torch.no_grad():\n        # 获取一个批次的数据\n        images, masks = next(iter(test_loader))\n        images, masks = images.to(device), masks.to(device)\n        predictions = model(images)\n        \n        # 将预测结果转换为二值掩码\n        binary_predictions = (predictions > threshold).float()\n        \n        # 选择前3个样本\n        indices = random.sample(range(len(images)), min(num_samples, len(images)))\n        indices = indices[:8]\n        \n        # 创建一个大图\n        plt.figure(figsize=(15, 8))  # 调整图像大小以适应新增的行\n        plt.suptitle(f'Epoch {swanlab.config[\"num_epochs\"]} Predictions (Random 6 samples)')\n        \n        for i, idx in enumerate(indices):\n            # 原始图像\n            plt.subplot(4, 8, i*4 + 1)  # 4行而不是3行\n            img = images[idx].cpu().numpy().transpose(1, 2, 0)\n            img = (img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]).clip(0, 1)\n            plt.imshow(img)\n            plt.title('Original Image')\n            plt.axis('off')\n            \n            # 真实掩码\n            plt.subplot(4, 8, i*4 + 2)\n            plt.imshow(masks[idx].cpu().squeeze(), cmap='gray')\n            plt.title('True Mask')\n            plt.axis('off')\n            \n            # 预测掩码\n            plt.subplot(4, 8, i*4 + 3)\n            plt.imshow(binary_predictions[idx].cpu().squeeze(), cmap='gray')\n            plt.title('Predicted Mask')\n            plt.axis('off')\n\n            # 新增：预测掩码叠加在原图上\n            plt.subplot(4, 8, i*4 + 4)\n            plt.imshow(img)  # 先显示原图\n            # 添加红色半透明掩码\n            plt.imshow(binary_predictions[idx].cpu().squeeze(), \n                      cmap='Reds', alpha=0.3)  # alpha控制透明度\n            plt.title('Overlay')\n            plt.axis('off')\n        \n        # 记录图像到SwanLab\n        swanlab.log({\"predictions\": swanlab.Image(plt)})\n\nif __name__ == '__main__':\n    main()\n```\n\n\n**运行训练**\n\n```bash\npython train.py\n```\n\n看到下面的输出即代表训练开始：\n\n![](./unet-medical-segmentation/console.png)",
    "271": "一级标题：UNet 医学影像分割\n二级标题：6. 训练结果演示\n内容：\n详细训练过程请看这里：<a href=\"https://swanlab.cn/@ZeyiLin/Unet-Medical-Segmentation/runs/67konj7kdqhnfdmusy2u6/chart\" target=\"_blank\">Unet-Medical-Segmentation</a>\n\n![](./unet-medical-segmentation/swanlab.png)\n\n\n从SwanLab图表中我们可以看到，train loss和val loss随epoch呈现下降趋势，而train acc和val acc随epoch呈现上升趋势。最终的test acc可以达到 97.93%。\n\n在`prediction`图表中记录着模型最终的测试集图像预测结果，可以看到模型分割的结果还是相对不错的：\n\n![](./unet-medical-segmentation/results.png)\n\n![](./unet-medical-segmentation/results2.png)\n\n当然，这教程主要的目标是帮助大家入门医学影像分割训练，所以没有使用更加复杂的模型结构和数据增强策略，感兴趣的同学可以基于本文的代码进行改变和实验，欢迎在[SwanLab基线社区](https://swanlab.cn/benchmarks)上展示你的结果和过程！",
    "272": "一级标题：UNet 医学影像分割\n二级标题：7. 模型推理\n内容：\n加载训练好的模型`best_model.pth`，并进行推理：\n\n```bash\npython predict.py\n```\n\npredict.py代码：\n\n```python\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom net import UNet\nimport numpy as np\nimport os\n\ndef load_model(model_path='best_model.pth', device='cuda'):\n    \"\"\"加载训练好的模型\"\"\"\n    try:\n        # 检查文件是否存在\n        if not os.path.exists(model_path):\n            raise FileNotFoundError(f\"Model file not found at {model_path}\")\n            \n        model = UNet(n_filters=32).to(device)\n        # 添加weights_only=True来避免警告\n        state_dict = torch.load(model_path, map_location=device, weights_only=True)\n        model.load_state_dict(state_dict)\n        model.eval()\n        print(f\"Model loaded successfully from {model_path}\")\n        return model\n    except Exception as e:\n        print(f\"Error loading model: {str(e)}\")\n        raise\n\ndef preprocess_image(image_path):\n    \"\"\"预处理输入图像\"\"\"\n    # 读取原始图像\n    image = Image.open(image_path).convert('RGB')\n    \n    # 保存调整大小后的原始图像用于显示\n    display_image = image.resize((256, 256), Image.Resampling.BILINEAR)\n    \n    # 模型输入的预处理\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Resize((256, 256)),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    image_tensor = transform(image)\n    return image_tensor.unsqueeze(0), display_image\n\ndef predict_mask(model, image_tensor, device='cuda', threshold=0.5):\n    \"\"\"预测分割掩码\"\"\"\n    with torch.no_grad():\n        image_tensor = image_tensor.to(device)\n        prediction = model(image_tensor)\n        prediction = (prediction > threshold).float()\n    return prediction\n\ndef visualize_result(original_image, predicted_mask):\n    \"\"\"可视化预测结果\"\"\"\n    plt.figure(figsize=(12, 6))\n    plt.suptitle('Predictions')\n    \n    # 显示原始图像\n    plt.subplot(131)\n    plt.imshow(original_image)\n    plt.title('Original Image')\n    plt.axis('off')\n    \n    # 显示预测掩码\n    plt.subplot(132)\n    plt.imshow(predicted_mask.squeeze(), cmap='gray')\n    plt.title('Predicted Mask')\n    plt.axis('off')\n    \n    # 显示叠加结果\n    plt.subplot(133)\n    plt.imshow(np.array(original_image))  # 转换为numpy数组\n    plt.imshow(predicted_mask.squeeze(), cmap='Reds', alpha=0.3)\n    plt.title('Overlay')\n    plt.axis('off')\n        \n    plt.tight_layout()\n    plt.savefig('./predictions.png')\n    print(\"Visualization saved as predictions.png\")\n\ndef main():\n    # 设置设备\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    try:\n        # 加载模型\n        model_path = \"./best_model.pth\"  # 确保这个路径是正确的\n        print(f\"Attempting to load model from: {model_path}\")\n        model = load_model(model_path, device)\n        \n        # 处理单张图像\n        image_path = \"dataset/test/27_jpg.rf.b2a2b9811786cc32a23c46c560f04d07.jpg\"\n        if not os.path.exists(image_path):\n            raise FileNotFoundError(f\"Image file not found at {image_path}\")\n            \n        print(f\"Processing image: {image_path}\")\n        image_tensor, original_image = preprocess_image(image_path)\n        \n        # 预测\n        predicted_mask = predict_mask(model, image_tensor, device)\n        \n        # 将预测结果转回CPU并转换为numpy数组\n        predicted_mask = predicted_mask.cpu().numpy()\n        \n        # 可视化结果\n        print(\"Generating visualization...\")\n        visualize_result(original_image, predicted_mask)\n        print(\"Results saved to predictions.png\")\n        \n    except Exception as e:\n        print(f\"Error during prediction: {str(e)}\")\n        raise\n\nif __name__ == '__main__':\n    main()\n```",
    "273": "一级标题：UNet 医学影像分割\n二级标题：补充\n内容：\n### 详细硬件配置和参数说明\n\n我使用了1张英伟达 vGPU-32GB 显卡，训练40个epoch，用时13分钟22秒。\n\n![](./unet-medical-segmentation/hardware.png)\n\n显存占用情况为`6.124GB`，即只要你的显卡显存大于6GB，就可以跑这个任务。如果想要进一步降低显存要求，可以调低batch size。\n\n![](./unet-medical-segmentation/memory.png)\n\n---",
    "274": "一级标题：UNet 医学影像分割\n二级标题：参考\n内容：\n- 代码：完整代码直接看本文第5节 或 [Github](https://github.com/Zeyi-Lin/UNet-Medical)\n- 实验日志过程：[Unet-Medical-Segmentation - SwanLab](https://swanlab.cn/@ZeyiLin/Unet-Medical-Segmentation/runs/67konj7kdqhnfdmusy2u6/chart)\n- 模型：UNet（Pytorch代码直接写）\n- 数据集：[brain-tumor-image-dataset-semantic-segmentation - Kagggle](https://www.kaggle.com/datasets/pkdarabi/brain-tumor-image-dataset-semantic-segmentation)\n- SwanLab：[https://swanlab.cn](https://swanlab.cn)\n- 论文：[《U-Net: Convolutional Networks for Biomedical Image Segmentation》](https://arxiv.org/abs/1505.04597)",
    "275": "一级标题：Yolo目标检测\n二级标题：无\n内容：\n:::info\n目标检测、计算机视觉\n:::\n\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/ultratest/runs/yux7vclmsmmsar9ear7u5/chart)\n\n[在线Demo](https://swanlab.cn/@ZeyiLin/ultratest/runs/yux7vclmsmmsar9ear7u5/chart) | [YOLO猫狗检测教程](https://zhuanlan.zhihu.com/p/702525559)",
    "276": "一级标题：Yolo目标检测\n二级标题：概述\n内容：\nYOLO（You Only Look Once）是一种由Joseph Redmon等人提出的目标检测模型，广泛应用于各种计算机视觉任务。YOLO通过将图像分成网格，并在每个网格内预测边界框和类别概率，能够实现实时的目标检测，在许多任务上表现出色。\n\n在这个任务中，我们将使用YOLO模型在COCO128数据集上进行目标检测任务，同时用SwanLab进行监控和可视化。\n\n![yolo](/assets/example-yolo-1.png)\n\nCOCO128 数据集是一个小型的目标检测数据集，来源于广泛使用的 COCO（Common Objects in Context）数据集。COCO128 数据集包含 128 张图像，是 COCO 数据集的一个子集，主要用于快速测试和调试目标检测模型。",
    "277": "一级标题：Yolo目标检测\n二级标题：环境安装\n内容：\n本案例基于`Python>=3.8`，请在您的计算机上安装好Python。 环境依赖：\n\n```txt\nultralytics\nswanlab\n```\n\n快速安装命令：\n\n```bash\npip install ultralytics swanlab\n```\n\n> 本文的代码测试于ultralytics==8.2.18、swanlab==0.3.6",
    "278": "一级标题：Yolo目标检测\n二级标题：完整代码\n内容：\n```python\nfrom ultralytics import YOLO\nfrom swanlab.integration.ultralytics import add_swanlab_callback\n\ndef main():\n    model = YOLO(\"yolov8n.pt\")\n    add_swanlab_callback(model)\n    model.train(data=\"coco128.yaml\", epochs=5, imgsz=640, batch=64)\n\nif __name__ == \"__main__\":\n    main()\n```",
    "279": "一级标题：Yolo目标检测\n二级标题：演示效果\n内容：\n![yolo-2](/assets/example-yolo-2.png)\n\n![yolo-3](/assets/example-yolo-3.png)",
    "280": "一级标题：为 SwanLab 作出贡献\n二级标题：无\n内容：\n有兴趣为 SwanLab 做出贡献吗？我们欢迎社区的贡献！本指南讨论`swanlab`的开发工作流和内部结构。",
    "281": "一级标题：为 SwanLab 作出贡献\n二级标题：📦 目录\n内容：\n- [标准开发流程](#标准开发流程)\n- [本地调试](#本地调试)\n  - [IDE 与插件](#IDE与插件)\n  - [配置 Python 环境](#配置python环境)\n  - [调试脚本](#调试脚本)\n- [本地测试](#本地测试)\n  - [python 脚本调试](#python-脚本调试)\n  - [单元测试](#单元测试)",
    "282": "一级标题：为 SwanLab 作出贡献\n二级标题：标准开发流程\n内容：\n1. 浏览 GitHub 上的[Issues](https://github.com/SwanHubX/SwanLab/issues)，查看你愿意添加的功能或修复的错误，以及它们是否已被\n   Pull Request。\n\n    - 如果没有，请创建一个[新 Issue](https://github.com/SwanHubX/SwanLab/issues/new/choose)——这将帮助项目跟踪功能请求和错误报告，并确保不重复工作。\n\n2. 如果你是第一次为开源项目贡献代码，请转到 [本项目首页](https://github.com/SwanHubX/SwanLab) 并单击右上角的\"Fork\"\n   按钮。这将创建你用于开发的仓库的个人副本。\n\n    - 将 Fork 的项目克隆到你的计算机，并添加指向`swanlab`项目的远程链接：\n\n   ```bash\n   git clone https://github.com/<your-username>/swanlab.git\n   cd swanlab\n   git remote add upstream https://github.com/swanhubx/swanlab.git\n   ```\n\n3. 开发你的贡献\n\n    - 确保您的 Fork 与主存储库同步：\n\n   ```bash\n   git checkout main\n   git pull upstream main\n   ```\n\n    - 创建一个`git`分支，您将在其中发展您的贡献。为分支使用合理的名称，例如：\n\n   ```bash\n   git checkout -b <username>/<short-dash-seperated-feature-description>\n   ```\n\n    - 当你取得进展时，在本地提交你的改动，例如：\n\n   ```bash\n   git add changed-file.py tests/test-changed-file.py\n   git commit -m \"feat(integrations): Add integration with the `awesomepyml` library\"\n   ```\n\n4. 发起贡献：\n\n    - [Github Pull Request](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests)\n    - 当您的贡献准备就绪后，将您的分支推送到 GitHub：\n\n   ```bash\n   git push origin <username>/<short-dash-seperated-feature-description>\n   ```\n\n    - 分支上传后， `GitHub`将打印一个 URL，用于将您的贡献作为拉取请求提交。在浏览器中打开该 URL，为您的拉取请求编写信息丰富的标题和详细描述，然后提交。\n\n    - 请将相关 Issue（现有 Issue 或您创建的 Issue）链接到您的 PR。请参阅 PR 页面的右栏。或者，在 PR\n      描述中提及“修复问题链接” - GitHub 将自动进行链接。\n\n    - 我们将审查您的贡献并提供反馈。要合并审阅者建议的更改，请将编辑提交到您的分支，然后再次推送到分支（无需重新创建拉取请求，它将自动跟踪对分支的修改），例如：\n\n   ```bash\n   git add tests/test-changed-file.py\n   git commit -m \"test(sdk): Add a test case to address reviewer feedback\"\n   git push origin <username>/<short-dash-seperated-feature-description>\n   ```\n\n    - 一旦您的拉取请求被审阅者批准，它将被合并到存储库的主分支中。",
    "283": "一级标题：为 SwanLab 作出贡献\n二级标题：本地调试\n内容：\n### IDE 与插件\n\n1. **使用 VSCode 作为你的开发 IDE**\n\n   SwanLab 仓库已经配好了[VSCode](https://code.visualstudio.com/)的环境、插件与调试脚本（位于`.vscode`\n   文件夹中），使用 VSCode 开发 SwanLab 会有最好的体验。\n\n2. **安装 VSCode 插件（可选）**\n\n   用 VSCode 打开项目，进入 [扩展] ，在搜索框输入“@recommended”，会出现一系列推荐插件，推荐全部安装这些插件。\n\n   ![vscode-recommend](/assets/guide_cloud/community/contributing-code/vscode_recommend.png)\n\n### 配置 Python 环境\n\nSwanLab 项目环境需要`python>=3.8`的支持。\n\n必须性的 python 依赖集中记录在项目根目录下的 `requirements.txt`。\n\n同样在项目根目录启动终端，运行以下命令安装依赖：\n\n```Bash\n# swanlab所依赖的包\npip install -r requirements.txt\npip install -r requirements-media.txt\n```\n\n编译、开发、单元测试等工作需要使用以下命令额外安装依赖：\n\n```Bash\n# 编译、单元测试等功能需要使用的包\npip install -r requirements-dev.txt\n```\n\n### 调试脚本\n\n1. **VSCode 调试脚本**\n\n在 VSCode-运行和调试 中，项目配置好了一系列调试脚本：\n\n![img](/assets/guide_cloud/community/contributing-code/debug.png)\n\n- **开启一个实验**：运行`test/create_experiment.py`脚本\n\n- **运行当前文件**：使用配置好的 Python 环境运行你选中的文件\n\n- **测试当前文件**：使用 debug 模式测试你选中的文件\n\n- **进行所有单元测试**：运行`test/unit`中的脚本对 swanlab 基础功能进行完整单元测试\n\n- **(跳过云)进行所有单元测试**：运行`test/unit`中的脚本对 swanlab 基础功能进行完整单元测试，但是跳过云测试\n\n- **构建项目**：打包项目为 whl 文件（pip 安装包格式）\n\nPs: 如果你不想使用 VSCode 进行开发，可以前往`.vscode/launch.json`，查看每个调试项对应的命令，了解其配置。",
    "284": "一级标题：为 SwanLab 作出贡献\n二级标题：本地测试\n内容：\n进行测试的前提是你已经安装完毕所有的所需依赖。\n\n### python 脚本调试\n\n在完成你的改动后，可以将你用于测试的 python 脚本放到根目录或`test`文件夹下，然后通过[VSCode 脚本](#调试脚本)中的\"\n运行当前文件\"来运行你的 Python 测试脚本, 这样你的脚本运行将使用到已改动后的 swanlab。\n\n### 单元测试\n\n可以通过[VSCode 脚本](#调试脚本)或者在项目根目录下运行以下命令进行单元测试：\n\n```Bash\nexport PYTHONPATH=. && pytest test/unit\n```\n\n由于 swanlab 涉及与云端的交互，而云端部分是闭源的，所以如果你是第一次贡献代码，最简单的方式是只进行本地测试。\n针对这种情况，请在本地根目录下创建`.env`文件，并填写如下环境变量配置：\n\n```dotenv\nSWANLAB_RUNTIME=test-no-cloud\n```\n\n这样就可以跳过云端测试，只进行本地的部分功能测试。 如果想进行完整的测试，请在`.env`中补充如下信息：\n\n```dotenv\nSWANLAB_RUNTIME=test\nSWANLAB_API_KEY=<你的API KEY>\nSWANLAB_API_HOST=https://swanlab.cn/api\nSWANLAB_WEB_HOST=https://swanlab.cn\n```\n\n*注意：在进行云端版测试时会在您的云端账号下生成一些无用的测试实验数据，需要手动删除*\n\n配置完后即可运行完整测试",
    "285": "一级标题：为SwanLab官方文档做贡献\n二级标题：无\n内容：\n为项目贡献的方式不仅仅是贡献代码，包括维护文档、在issue和群中答疑、提交bug等都是为swanlab项目贡献的方式！\n\n我们在[github仓库](https://github.com/SwanHubX/SwanLab-Docs)中托管了SwanLab的[官方文档](https://docs.swanlab.cn)，基于[vitepress](https://vitepress.dev/zh/guide/getting-started)。\n\n### 如何为文档做贡献\n\n很简单！只需要克隆项目、增添或修改Markdown文件、提交他们，再创建一个PR就可以。\n\n### 环境安装\n\n1. 克隆本仓库\n\n```bash\ngit clone https://github.com/SwanHubX/SwanLab-Docs\n```\n\n2. 安装依赖环境\n\n需要提前安装nodejs和npm，详细方法请查询[node官方教程](https://nodejs.org/en/download/package-manager)\n\n使用如下命令安装其他依赖项目\n\n```bash\nnpm add -D vitepress\n```\n\n### 本地运行文档\n\n如果进行本地开发或者预览文档，可在项目根目录运行：\n\n```bash\nnpm run docs:dev\n```\n\n如果要进行完整的编译打包，使用如下命令：\n\n```bash\nnpm run docs:build\nnpm run docs:preview\n```",
    "286": "一级标题：关于我们\n二级标题：无\n内容：\n情感机器（北京）科技有限公司 是一家专注于人工智能和机器学习底层工具研发的高科技企业。公司致力于为AI开发者提供基础开发工具和建立开源开放的技术社区。  \n\n![](/assets/emotion-machine.png)\n\n使命：打造AI工具链，赋能全球AI开发者生态。\n\n---\n\n**公司**：情感机器（北京）科技有限公司  \n**工作地点**：北京市朝阳区关庄路2号院1号楼中关村科技服务大厦2层B205-1室  \n**联系我们**：contact@swanlab.cn  \n**交流群**：[微信](/guide_cloud/community/online-support.md)",
    "287": "一级标题：Github README徽章\n二级标题：无\n内容：\n如果你喜欢在工作与学习中使用 SwanLab，欢迎将 SwanLab 徽章添加到您的README中：",
    "288": "一级标题：Github README徽章\n二级标题：徽章\n内容：\n[![](https://raw.githubusercontent.com/SwanHubX/assets/main/badge2.svg)](https://swanlab.cn)\n\n[![](https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg)](https://swanlab.cn)\n\n\n复制下面的代码到您的README.md文件中（二选一）：\n\n```markdown\n[![](https://raw.githubusercontent.com/SwanHubX/assets/main/badge2.svg)](your experiment url)\n[![](https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg)](your experiment url)\n```",
    "289": "一级标题：Github README徽章\n二级标题：更多设计素材\n内容：\n- [Iconfont](https://www.iconfont.cn/search/index?searchType=icon&q=swanlab)\n- [Github](https://github.com/SwanHubX/assets)",
    "290": "一级标题：在线支持\n二级标题：无\n内容：",
    "291": "一级标题：在线支持\n二级标题：👋 欢迎与我们交流\n内容：\n| 微信公众号 | 微信交流群 |\n| --- | ---  |\n| <div align=\"center\"><img src=\"/assets/wechat_public_account.jpg\" width=300></div> | <div align=\"center\"><img src=\"/assets/wechat-QR-Code.png\" width=300></div> |\n\n| 飞书群 |\n| --- |\n| <div align=\"center\"><img src=\"/assets/feishu-QR-Code.png\" width=300></div> |",
    "292": "一级标题：在线支持\n二级标题：📧 用Github或邮件联系我们\n内容：\n- **GitHub Issues**：[链接](https://github.com/SwanHubX/SwanLab/issues)，反馈使用SwanLab时遇到的错误和问题\n- **电子邮件支持**：<contact@swanlab.cn>",
    "293": "一级标题：在论文中引用SwanLab\n二级标题：无\n内容：\n如果您发现 SwanLab 对您的研究之旅有帮助，请考虑以下列格式引用：\n\n```bibtex\n@software{Zeyilin_SwanLab_2023,\n  author = {Zeyi Lin, Shaohong Chen, Kang Li, Qiushan Jiang, Zirui Cai,  Kaifang Ji and {The SwanLab team}},\n  doi = {10.5281/zenodo.11100550},\n  license = {Apache-2.0},\n  title = {{SwanLab}},\n  url = {https://github.com/swanhubx/swanlab},\n  year = {2023}\n}\n```",
    "294": "一级标题：FAQ\n二级标题：无\n内容：",
    "295": "一级标题：FAQ\n二级标题：登录时，API Key为什么输入不进去？\n内容：\n见此回答：[链接](https://www.zhihu.com/question/720308649/answer/25076837539)",
    "296": "一级标题：FAQ\n二级标题：如何从一个脚本启动多个实验？\n内容：\n在多次创建实验之间增加`swanlab.finish()`即可。\n\n执行了`swanlab.finish()`之后，再次执行`swanlab.init()`就会创建新的实验；  \n如果不执行`swanlab.finish()`的情况下，再次执行`swanlab.init()`，将无视此次执行。",
    "297": "一级标题：FAQ\n二级标题：如何将数据上传到私有化部署的SwanLab?\n内容：\n有两种方法可以做到这一点：\n\n::: code-group\n\n```python [方法一]\nswanlab.login(api_key='你的API Key', host='你的私有化部署IP地址')\n```\n\n```bash [方法二]\nswanlab login --host 你的私有化部署IP地址 --api-key 你的API Key\n```\n\n完成登录后，就可以将数据指定上传到私有化部署的SwanLab了。\n\n:::",
    "298": "一级标题：FAQ\n二级标题：如何在训练时关闭swanlab记录（Debug调试）？\n内容：\n将`swanlab.init`的`mode`参数设置为disabled，就可以不创建实验以及不写入数据。\n\n```python\nswanlab.init(mode='disabled')\n```",
    "299": "一级标题：FAQ\n二级标题：在同一台机器上，有多个人都在使用SwanLab，应该如何配置？\n内容：\n`swanlab.login`登录完成之后，会在该机器上生成一个配置文件记录登录信息，以便下次不用重复登录。但如果有多人使用这一台机器的话，则需要小心日志传递到对方账号上。\n\n**推荐的配置方式有两种：**\n\n**方式一(推荐)**：在代码开头加上`swanlab.login(api_key='你的API Key')`，这样不会将登录配置文件写入到本地，[文档](/api/py-login)\n\n**方式二**：在运行代码前，设置环境变量`SWANLAB_API_KEY=\"你的API Key\"`",
    "300": "一级标题：FAQ\n二级标题：本地的训练已经结束，但SwanLab UI上仍然在运行中，要怎么改变状态？\n内容：\n点击实验名旁边的终止按钮，会将实验状态从“进行中”转为“中断”，并停止接收数据的上传。\n\n![stop](/assets/stop.png)",
    "301": "一级标题：FAQ\n二级标题：如何查看折线图的局部细节？\n内容：\n放大折线图，长按鼠标划过目标的区域，即可放大查看该区域。\n\n![details](/assets/faq-chart-details.png)",
    "302": "一级标题：FAQ\n二级标题：内部指标名\n内容：\n指标名称是指`swanlab.log()`传入字典的key部分。有一部分key在内部被SwanLab用于传递系统硬件指标，所以不太建议使用。\n\n内部指标包括：\n\n- `__swanlab__.xxx`",
    "303": "一级标题：FAQ\n二级标题：实验状态规则\n内容：\n实验一共分为三种状态：完成、运行中与中断。\n\n- **完成**：训练进程自然结束，或手动执行了`swanlab.finish()`。\n- **运行中**：训练进程正在运行，且没有执行`swanlab.finish()`。\n- **中断**：训练进程因为Bug、机器关闭、`Ctrl+C`等异常中断。\n\n有些用户会遇到这样的情况：为什么我的训练进程好像还在进行中，但是SwanLab图表上显示中断？\n\n这是因为SwanLab判定中断有一条隐藏规则，如果训练进程在15分钟以内没有任何日志上传（包含自动收集的系统指标），则判定为中断，这是为了避免训练进程被意外Kill后，无法触达SwanLab SDK中的状态上传逻辑，导致实验永远处于“运行中”状态。\n\n所以如果你的机器出现了网络问题，且时间大于15分钟，就会导致实验状态显示为“中断”。",
    "304": "一级标题：FAQ\n二级标题：命令行记录与截断\n内容：\nSwanLab会记录`swanlab.init()`之后进程中的标准输出流，可以在实验的「日志」选项卡查看。如果一行的命令行输出过长，会被截断，目前的默认限制是`1024`个字符，最大限制是`4096`个字符。\n\n如果你想修改限制，可以使用下面的代码进行修改：\n\n```python\nimport swanlab\n\n# 创建新的设置对象，修改max_log_length参数\nnew_settings = swanlab.Settings(\n    max_log_length=4096,\n)\n\n# 更新全局设置\nswanlab.merge_settings(new_settings)\n\nswanlab.init()\n...\n```",
    "305": "一级标题：FAQ\n二级标题：如何开启实验平滑\n内容：\n找到实验页面的右上角，点击「设置」按钮：\n\n![](./faq/smooth_setting.png)\n\n在右侧拉出的菜单中，找到「平滑」选项，拉动滑动条即可开启平滑：\n\n![](./faq/smooth_button.png)",
    "306": "一级标题：FAQ\n二级标题：如何修改实验“中断”状态\n内容：\n在实验页面，点击状态标签：\n\n![](./faq/exp_header_crash.png)\n\n在弹窗中，选择你想要的状态：\n\n![](./faq/exp_windows_finish.png)",
    "307": "一级标题：FAQ\n二级标题：如何开启断点续训？\n内容：\n参考文档：[resume](/guide_cloud/experiment_track/resume-experiment.md)",
    "308": "一级标题：用配置文件创建实验\n二级标题：无\n内容：\n本节将介绍如何使用json、yaml格式的配置文件来创建SwanLab实验。",
    "309": "一级标题：用配置文件创建实验\n二级标题：swanlab.config载入配置文件\n内容：\n`swanlab.init`的`config`参数支持传入json或yaml格式的配置文件路径，并将配置文件解析为字典以进行实验创建。\n\n### 使用json文件\n\n下面是一个json格式的配置文件示例：\n\n```json\n{\n    \"epochs\": 20,\n    \"learning-rate\": 0.001,\n}\n```\n\n将配置文件的路径传入config参数，它会把配置文件解析为字典：\n\n```python\nswanlab.init(config=\"swanlab-init-config.json\")\n# 等价于swanlab.init(config={\"epochs\": 20, \"learning-rate\": 0.001})\n```\n\n### 使用yaml文件\n\n下面是一个yaml格式的配置文件示例：\n\n```yaml\nepochs: 20\nlearning-rate: 0.001\n```\n\n将配置文件的路径传入`config`参数，它会把配置文件解析为字典：\n```python\nswanlab.init(config=\"swanlab-init-config.yaml\")\n# 等价于swanlab.init(config={\"epochs\": 20, \"learning-rate\": 0.001})\n```",
    "310": "一级标题：用配置文件创建实验\n二级标题：swanlab.init载入配置文件\n内容：\n`swanlab.init`的`load`参数支持传入json或yaml格式的配置文件路径，并解析配置文件以进行实验创建。\n\n### 使用json文件\n\n下面是一个json格式的配置文件示例：\n\n```json\n{\n    \"project\": \"cat-dog-classification\",\n    \"experiment_name\": \"Resnet50\",\n    \"description\": \"我的第一个人工智能实验\",\n    \"config\":{\n        \"epochs\": 20,\n        \"learning-rate\": 0.001}\n}\n```\n\n将配置文件的路径传入`load`参数，它会解析配置文件以初始化实验：\n\n```python\nswanlab.init(load=\"swanlab-config.json\")\n# 等价于\n# swanlab.init(\n#     project=\"cat-dog-classification\",\n#     experiment_name=\"Resnet50\",\n#     description=\"我的第一个人工智能实验\",\n#     config={\n#         \"epochs\": 20,\n#         \"learning-rate\": 0.001}\n# )\n```\n\n### 使用yaml文件\n\n下面是一个json格式的配置文件示例：\n\n```yaml\nproject: cat-dog-classification\nexperiment_name: Resnet50\ndescription: 我的第一个人工智能实验\nconfig:\n  epochs: 20\n  learning-rate: 0.001\n```\n\n将配置文件的路径传入`load`参数，它会解析配置文件以初始化实验：\n\n```python\nswanlab.init(load=\"swanlab-config.yaml\")\n# 等价于\n# swanlab.init(\n#     project=\"cat-dog-classification\",\n#     experiment_name=\"Resnet50\",\n#     description=\"我的第一个人工智能实验\",\n#     config={\n#         \"epochs\": 20,\n#         \"learning-rate\": 0.001}\n# )\n```",
    "311": "一级标题：用配置文件创建实验\n二级标题：常见问题\n内容：\n### 1. 配置文件命名是固定的吗？\n\n配置文件的命名是自由的，但推荐使用`swanlab-init`和`swanlab-init-config`这两个配置名。\n\n### 2. 配置文件和脚本内的参数之间是什么关系？\n\n脚本内参数的优先级大于配置文件，即脚本内参数会覆盖配置文件参数。\n\n比如，下面有一段yaml配置文件和示例代码片段：\n\n```yaml\nproject: cat-dog-classification\nexperiment_name: Resnet50\ndescription: 我的第一个人工智能实验\nconfig:\n  epochs: 20\n  learning-rate: 0.001\n```\n\n```python\nswanlab.init(\n    experiment_name=\"resnet101\"，\n    config={\"epochs\": 30},\n    load=\"swanlab-init.yaml\"\n)\n```\n\n最终`experiment_name`为resnet101，`config`为{\"epochs\":30}。",
    "312": "一级标题：创建一个实验\n二级标题：无\n内容：\n使用 **SwanLab Python SDK** 跟踪人工智能实验，然后你可以在 在线交互式仪表板 中查看结果。  \n\n![](./create-experiment/overview.jpg)\n\n本节将介绍如何创建一个SwanLab实验。",
    "313": "一级标题：创建一个实验\n二级标题：如何创建一个SwanLab实验?\n内容：\n创建一个SwanLab实验分为3步：\n1. 初始化SwanLab\n2. 传递一个超参数字典\n3. 在你的训练循环中记录指标\n\n### 1. 初始化SwanLab\n\n`swanlab.init()`的作用是初始化一个SwanLab实验，它将启动后台进程以同步和记录数据。  \n下面的代码片段展示了如何创建一个名为 **cat-dog-classification** 的新SwanLab项目。并为其添加了：\n\n1. **project**：项目名。\n1. **experiment_name**：实验名。实验名为当前实验的标识，以帮助您识别此实验。  \n2. **description**：描述。描述是对实验的详细介绍。\n\n```python\n# 导入SwanLab Python库\nimport swanlab\n\n# 1. 开启一个SwanLab实验\nrun = swanlab.init(\n    project=\"cat-dog-classification\",\n    experiment_name=\"Resnet50\",\n    description=\"我的第一个人工智能实验\",\n)\n```\n\n当你初始化SwanLab时，`swanlab.init()`将返回一个对象。  \n此外，SwanLab会创建一个本地目录（默认名称为“swanlog”），所有日志和文件都保存在其中，并异步传输到 SwanLab 服务器。（该目录也可以被`swanlab watch -l [logdir]`命令打开本地实验看板。）\n\n::: info\n**注意**：如果调用 `swanlab.init` 时该项目已存在，则实验会添加到预先存在的项目中。  \n例如，如果您已经有一个名为`\"cat-dog-classification\"`的项目，那么新的实验会添加到该项目中。\n:::\n\n<br>\n\n### 2. 传递超参数字典\n\n传递超参数字典，例如学习率或模型类型。  \n你在`config`中传入的字典将被保存并用于后续的实验对比与结果查询。\n\n```python\n# 2. 传递一个超参数字典\nswanlab.config={\"epochs\": 20, \"learning_rate\": 1e-4, \"batch_size\": 32, \"model_type\": \"CNN\"}\n```\n\n有关如何配置实验的更多信息，请参阅[设置实验配置](/guide_cloud/experiment_track/set-experiment-config.md)。\n\n<br>\n\n### 3. 在训练循环中记录指标\n在每轮for循环（epoch）中计算准确率与损失值指标，并用`swanlab.log()`将它们记录到SwanLab中。  \n在默认情况下，当您调用`swanlab.log`时，它会创建一个新的step添加到对应指标的历史数据中，规则是新的step=旧的最大step数+1。  \n下面的代码示例展示了如何用`swanlab.log()`记录指标：  \n\n```python\n# 省略了如何设置模型与如何设置数据集的细节\n\n# 设置模型和数据集\nmodel, dataloader = get_model(), get_data()\n\n# 训练循环\nfor epoch in range(swanlab.config.epochs):\n    for batch in dataloader:\n        loss, acc = model.train_step()\n        # 3. 在你的训练循环中记录指标，用于在仪表盘中进行可视化\n        swanlab.log({\"acc\": acc, \"loss\": loss})\n```\n\n<br>\n\n### 完整代码\n\n包含上述代码片段的完整脚本如下：\n\n```python\n# 导入SwanLab Python库\nimport swanlab\n\n# 1. 开启一个SwanLab实验\nrun = swanlab.init(\n    project=\"cat-dog-classification\",\n    experiment_name=\"Resnet50\",\n    description=\"我的第一个人工智能实验\",\n)\n\n# 2. 传递一个超参数字典\nswanlab.config={\"epochs\": 20, \"learning_rate\": 1e-4, \"batch_size\": 32, \"model_type\": \"CNN\"}\n\n# 省略了如何设置模型与如何设置数据集的细节\n# 设置模型和数据集\nmodel, dataloader = get_model(), get_data()\n\n# 训练循环\nfor epoch in range(swanlab.config.epochs):\n    for batch in dataloader:\n        loss, acc = model.train_step()\n        # 3. 在你的训练循环中记录指标，用于在仪表盘中进行可视化\n        swanlab.log({\"acc\": acc, \"loss\": loss})\n```\n\n<br>\n\n### 可视化你的实验\n\n使用SwanLab仪表盘作为管理和可视化人工智能模型结果的一站式节点。  \n可以可视化丰富的交互式图表，例如折线图、图像图表、音频图表、3D点云图表等。  \n有关如何查看实验更多信息，请参阅[查看实验结果](/guide_cloud/experiment_track/view-result.md)。\n\n![](./create-experiment/show.jpg)",
    "314": "一级标题：创建一个实验\n二级标题：最佳实践\n内容：\n下面介绍一下创建实验时可以参考的写法，一个完整的实验创建可以包含下面这四个参数：  \n- `config`：配置。记录你想要用于复现模型的任何内容，比如超参数、模型名称、数据集等。这些内容将显示在仪表盘的“表格视图”与“实验卡片”页中，也可以作为实验比较、筛选、过滤的依据。\n- `project`：项目。项目是一组可以一起比较的实验，它们将在一个统一的仪表盘中显示。\n- `experiment_name`：实验名。定义实验的名称。您在脚本中设置，可以之后在SwanLab应用上编辑。\n- `description`：描述。对实验的介绍文本，记录不同实验之间的差异和灵感。您在脚本中设置，可以之后在SwanLab应用上编辑。\n\n以下代码片段展示了一个最佳实践案例：\n\n```python\nimport swanlab\n\nconfig = dict(\n    learning_rate=1e-4, optimizer=\"Adam\", architecture=\"Transformer\", dataset_id=\"cats-dogs-2024\"\n)\n\nswanlab.init(\n    project=\"cats-dogs-classification\",\n    experiment_name=\"ViT-Adam-1e-4\",\n    description=\"基于ViT模型和1e-4学习率的Adam优化器的猫狗分类实验。\",\n    config=config,\n)\n```\n\n关于创建SwanLab实验时更多可用参数的信息，请参阅API文档中的[swanlab.init](/api/py-init.md)文档。",
    "315": "一级标题：实验元数据\n二级标题：无\n内容：\n> 获取实验元数据需swanlab>=0.3.25\n\n总有些时候，你想要在代码中获取实验的元数据，比如实验的项目名、ID、实验名、网址等。\n\n获取方式：\n\n```python\nimport swanlab\n\nrun = swanlab.init(\n    project=\"test-project\",\n    experiment=\"test-exp\",\n)\n\n# 打印出所有元数据\nprint(run.public.json())\n\n# 打印出单个元数据\nprint(run.public.project_name)\nprint(run.public.cloud.experiment_url)\n```\n\n`swanlab.init`返回的类`run`会携带`public`属性，替换了之前的`settings`属性，他会返回：\n\n- `project_name`：当前运行的项目名称\n- `version`：当前运行的swanlab版本\n- `run_id`：一个唯一id\n- `swanlog_dir`：swanlab保存文件夹\n- `run_dir`：本次实验的保存文件夹\n- `cloud`：云端环境的相关信息\n    - `available`：是否运行在云端模式，如果不是，下面的属性全部为None\n    - `project_name`：本次运行的项目名称\n    - `project_url`：本次运行在云端项目url\n    - `experiment_name`：本次运行的实验名称\n    - `experiment_url`：本次运行的云端实验url",
    "316": "一级标题：结束一个实验\n二级标题：无\n内容：\n在一般的Python运行环境下，当脚本运行结束时，SwanLab会自动调用`swanlab.finish`来关闭实验，并将运行状态设置为「完成」。这一步无需显式调用。\n\n但在一些特殊情况下，比如**Jupyter Notebook**中，则需要用`swanlab.finish`来显式关闭实验。\n\n使用方式也很简单, 在`init`之后执行`finish`即可：\n\n```python (5)\nimport swanlab\n\nswanlab.init()\n...\nswanlab.finish()\n```",
    "317": "一级标题：结束一个实验\n二级标题：FAQ\n内容：\n### 在运行一次Python脚本中，我可以初始化多次实验吗？\n\n可以，但你需要在多次`init`中间加上`finish`，如：\n\n```python\nswanlab.init()\n···\nswanlab.finish()\n···\nswanlab.init()\n```",
    "318": "一级标题：用 Notebook 跟踪实验\n二级标题：无\n内容：\n将 SwanLab 与 Jupyter 结合使用，无需离开Notebook即可获得交互式可视化效果。\n\n![](./jupyter-notebook/swanlab-love-jupyter.jpg)",
    "319": "一级标题：用 Notebook 跟踪实验\n二级标题：在Notebook中安装SwanLab\n内容：\n```bash\n!pip install swanlab -qqq\n```\nps: `-qqq`是用来控制命令执行时的输出信息量的，可选。",
    "320": "一级标题：用 Notebook 跟踪实验\n二级标题：在Notebok中与SwanLab交互\n内容：\n```python\nimport swanlab\n\nswanlab.init()\n...\n# 在Notebook中，需要显式关闭实验\nswanlab.finish()\n```\n\n在用`swanlab.init`初始化实验时，打印信息的最后会出现一个“Display SwanLab Dashboard”按钮：\n\n![](/assets/jupyter-notebook-1.jpg)\n\n点击该按钮，就会在Notebook中嵌入该实验的SwanLab网页：\n\n![](/assets/jupyter-notebook-2.jpg)\n\n现在，你可以在这个嵌入的网页中直接看到训练过程，以及和它交互。",
    "321": "一级标题：限制与性能\n二级标题：无\n内容：",
    "322": "一级标题：限制与性能\n二级标题：优化指标记录\n内容：\n使用 `swanlab.log` 跟踪记录实验指标，记录后，这些指标会生成图表与显示在表格中。当记录的数据量过多时，可能会使网页的访问变慢。\n\n### 建议1：将不同指标的总数保持在1万以下\n\n记录超过10k个不同的指标名，可能会减慢你仪表盘渲染与表格操作速度。\n\n对于媒体数据，尽量将相关的媒体数据记录到相同的指标名称下：\n\n```python\n# ❌ 不推荐的做法\nfor i, img in enumerate(images):\n    swanlab.log({f\"pred_img_{i}\": swanlab.Image(image)})\n\n# ✅ 推荐的做法\nswanlab.log({\"pred_imgs\": [swanlab.Image(image) for image in images]})\n```\n\n<br>\n\n### 建议2：指标宽度保持在1000万以下\n\n指标宽度在以step为横轴的折线图中，指的是step最小值与最大值之间的范围差。\n\n在指标宽度过大时，会影响实验中所有指标的绘图加载时间，导致访问缓慢。\n\n<br>\n\n### 建议3：限制指标的提交频率\n\n选择适合你正在记录的指标的记录频率。在经验上，指标越宽，记录它的频率就越低。\n\n具体来说，我们建议：\n\n- 标量：每个指标 < `50k` 个记录点\n- 媒体数据：每个指标 < `10k` 个记录点\n\n如果你超出这些准则，SwanLab 将继续接受你记录的数据，但页面加载速度可能会很慢。\n\n推荐的记录方法如下代码所示：\n\n```python\n# 比如有1MB次循环\nfor step in range(1000000):\n    ....\n\n    # 每1k次循环提交一次，有效降低指标的提交频次\n    if step % 1000 == 0:\n        swanlab.log({\"scalar\": step})\n```",
    "323": "一级标题：记录自定义3D图表\n二级标题：无\n内容：\nSwanLab 兼容 [pyecharts](https://pyecharts.org/#/zh-cn/intro) 的 API，可以方便地记录 pyecharts 的图表到 SwanLab，以呈现丰富的数据组织和图表展现形式。\n\n**在线Demo点击下面的标签：**\n\n[![](/assets/visualization_swanlab.svg)](https://swanlab.cn/@ZeyiLin/swanlab-echarts-3d-demo/charts)\n\n<!--@include: @zh/shared/custom-charts-3d.md-->",
    "324": "一级标题：记录自定义图表\n二级标题：无\n内容：\n<!--@include: @zh/shared/custom-charts.md-->",
    "325": "一级标题：记录实验指标\n二级标题：无\n内容：\n使用SwanLab Python库记录训练每一步（step）的指标与媒体数据。\n\nSwanLab用 `swanlab.log()` 在训练循环中收集指标名和数据（key-value），然后同步到云端服务器。\n\n![](./log-experiment-metric/line.png)",
    "326": "一级标题：记录实验指标\n二级标题：记录标量指标\n内容：\n在训练循环中，将指标名和数据组成一个键值对字典，传递给 `swanlab.log()` 完成1次指标的记录：\n\n```python\nfor epoch in range(num_epochs):\n    for data, ground_truth in dataloader:\n        predict = model(data)\n        loss = loss_fn(predict, ground_truth)\n        # 记录指标，指标名为loss\n        swanlab.log({\"loss\": loss})\n```\n\n在 `swanlab.log` 记录时，会根据指标名，将`{指标名: 指标}`字典汇总到一个统一位置存储。\n\n⚠️需要注意的是，`swanlab.log({key: value})`中的value必须是`int` / `float` / `BaseType`这三种类型（如果传入的是`str`类型，会先尝试转为`float`，如果转换失败就会报错），其中`BaseType`类型主要是多媒体数据，详情请看[记录多媒体数据](/guide_cloud/experiment_track/log-media.md)。\n\n在每次记录时，会为该次记录赋予一个 `step`。在默认情况下，`step` 为0开始，并在你每一次在同一个指标名下记录时，`step` 等于该指标名历史记录的最大 `step` + 1，例如：\n\n```python\nimport swanlab\nswanlab.init()\n\n...\n\nswanlab.log({\"loss\": loss, \"acc\": acc})  \n# 此次记录中，loss的step为0, acc的step为0\n\nswanlab.log({\"loss\": loss, \"iter\": iter})  \n# 此次记录中，loss的step为1, iter的step为0, acc的step为0\n\nswanlab.log({\"loss\": loss, \"iter\": iter})  \n# 此次记录中，loss的step为2, iter的step为1, acc的step为0\n```",
    "327": "一级标题：记录实验指标\n二级标题：指标分组\n内容：\n在脚本中可以通过指标名的前缀（以“/”为分隔）进行图表分组，例如 `train/loss` 会被分到名为“train”的分组、`val/loss` 会被分到名为“val”的分组：\n\n```python\n# 分到train组\nswanlab.log({\"train/loss\": loss})\nswanlab.log({\"train/batch_cost\": batch_cost})\n\n# 分到val组\nswanlab.log({\"val/acc\": acc})\n```",
    "328": "一级标题：记录实验指标\n二级标题：指定记录的step\n内容：\n在一些指标的记录频率不一致，但希望它们的step可以对齐时，可以通过设置 `swanlab.log` 的 `step` 参数实现对齐：\n\n```python\nfor iter, (data, ground_truth) in enumerate(train_dataloader):\n    predict = model(data)\n    train_loss = loss_fn(predict, ground_truth)\n    swanlab.log({\"train/loss\": loss}, step=iter)\n\n    # 测试部分\n    if iter % 1000 == 0:\n        acc = val_trainer(model)\n        swanlab.log({\"val/acc\": acc}, step=iter)\n```\n\n需要注意的是，同一个指标名不允许出现2个相同的step的数据，一旦出现，SwanLab将保留先记录的数据，抛弃后记录的数据。",
    "329": "一级标题：记录实验指标\n二级标题：打印指标\n内容：\n也许你希望在训练循环中打印指标，可以通过 `print_to_console` 参数控制是否将指标打印到控制台（以`dict`的形式）：\n\n```python\nswanlab.log({\"acc\": acc}, print_to_console=True)\n```\n\n或者：\n\n```python\nprint(swanlab.log({\"acc\": acc}))\n```",
    "330": "一级标题：记录实验指标\n二级标题：自动记录环境信息\n内容：\nSwanLab在实验期间自动记录以下信息：\n\n- **命令行输出**：标准输出流和标准错误流被自动记录，并显示在实验页面的“日志”选项卡中。\n- **实验环境**：记录包括操作系统、硬件配置、Python解释器路径、运行目录、Python库依赖等在内的数十项的环境信息。\n- **训练时间**：记录训练开始时间和总时长。",
    "331": "一级标题：记录媒体数据\n二级标题：无\n内容：\nSwanLab 支持记录媒体数据（图像、音频、文本、三维点云等）以直观地探索你的实验结果，实现模型的主观评估。",
    "332": "一级标题：记录媒体数据\n二级标题：1.图像\n内容：\n`swanlab.Image` 支持记录多种图像类型，包括 numpy、PIL、Tensor、读取文件等。[API文档](/api/py-Image)。\n\n![](/assets/media-image-1.jpg)\n\n### 1.1 记录 Array 型图像\n\nArray型包括numpy和tensor。直接将 Array 传入 `swanlab.Image`，它将根据类型自动做相应处理：\n\n- 如果是 `numpy.ndarray`：SwanLab 会使用 pillow (PIL) 对其进行读取 。\n- 如果是 `tensor`：SwanLab 会使用 `torchvision` 的 `make_grid`函数做转换，然后使用 pillow 对其进行读取。\n\n示例代码：\n\n```python\nimage = swanlab.Image(image_array, caption=\"左图: 输入, 右图: 输出\")\nswanlab.log({\"examples\": image})\n```\n\n### 1.2 记录 PIL 型图像\n\n直接传入 `swanlab.Image`：\n\n```python\nimage = PIL.Image.fromarray(image_array)\nswanlab.log({\"examples\": image})\n```\n\n### 1.3 记录文件图像\n\n提供文件路径给 `swanlab.Image`：\n\n```python\nimage = swanlab.Image(\"myimage.jpg\")\nswanlab.log({\"example\": image})\n```\n\n### 1.4 记录 Matplotlib\n\n将 `matplotlib.pyplot` 的 `plt` 对象传入 `swanlab.Image`：\n\n```python\nimport matplotlib.pyplot as plt\n\n# 数据\nx = [1, 2, 3, 4, 5]\ny = [2, 3, 5, 7, 11]\n# 创建折线图\nplt.plot(x, y)\n# 添加标题和标签\nplt.title(\"Examples\")\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")\n\nswanlab.log({\"example\": swanlab.Image(plt)})\n```\n\n### 1.5 单步记录多个图像\n\n单步记录多个图像即在一次 `swanlab.log` 中，传递一个由 `swanlab.Image` 类型对象组成的列表。\n\n```python\n# 创建一个空列表\nimage_list = []\nfor i in range(3):\n    random_image = np.random.randint(low=0, high=256, size=(100, 100, 3))\n    image = swanlab.Image(random_image, caption=f\"随机图像{i}\")\n    # 将 swanlab.Image 类型对象添加到列表中\n    image_list.append(image)\n\nswanlab.log({\"examples\": image_list})\n```\n\n关于图像的更多细节，可参考[API文档](/api/py-Image)",
    "333": "一级标题：记录媒体数据\n二级标题：2. 音频\n内容：\n[API文档](/api/py-Audio)\n\n![](/assets/media-audio-1.jpg)\n\n### 2.1 记录 Array 型音频\n\n```python\naudio = swanlab.Audio(np_array, sample_rate=44100, caption=\"white_noise\")\nswanlab.log({\"white_noise\": audio})\n```\n\n### 2.2 记录音频文件\n\n```python\nswanlab.log({\"white_noise\": swanlab.Audio(\"white_noise.wav\")})\n```\n\n### 2.3 单步记录多个音频\n\n```python\nexamples = []\nfor i in range(3):\n    white_noise = np.random.randn(100000)\n    audio = swanlab.Audio(white_noise, caption=\"audio_{i}\")\n    # 列表中添加swanlab.Audio类型对象\n    examples.append(audio)\n\nrun.log({\"examples\": examples})\n```",
    "334": "一级标题：记录媒体数据\n二级标题：3. 文本\n内容：\n[API文档](/api/py-Text)\n\n### 3.1 记录字符串\n\n```python\nswanlab.log({\"text\": swanlab.Text(\"A example text.\")})\n```\n\n### 3.2 单步记录多个文本\n\n```python\n# 创建一个空列表\ntext_list = []\nfor i in range(3):\n    text = swanlab.Text(\"A example text.\", caption=f\"{i}\")\n    text_list.append(text)\n\nswanlab.log({\"examples\": text_list})\n```\n\n![alt text](/assets/log-media-text.png)",
    "335": "一级标题：记录媒体数据\n二级标题：4. 3D点云\n内容：\n![](/zh/api/py-object3d/demo.png)\n\n请参考此文档：[API-Oject3D](/api/py-object3d)",
    "336": "一级标题：记录媒体数据\n二级标题：5. 生物化学分子\n内容：\n![](/assets/molecule.gif)\n\n请参考此文档：[API-Molecule](/api/py-molecule)",
    "337": "一级标题：记录媒体数据\n二级标题：6. 视频\n内容：\n请参考此文档：[API-Video](/api/py-video)",
    "338": "一级标题：记录媒体数据\n二级标题：Q&A\n内容：\n### 1. caption参数有什么作用？\n\n每一个媒体类型都会有1个`caption`参数，它的作用是对该媒体数据的文字描述，比如对于图像：\n\n```python\napple_image = swanlab.Image(data, caption=\"苹果\")\nswanlab.log({\"im\": apple_image})\n```\n<img src=\"/assets/log-media-image.png\" width=400, height=400>\n\n\n### 2. 想要媒体数据和epoch数同步，怎么办？\n\n在用swanlab.log记录媒体数据时，指定`step`参数为epoch数即可。\n\n```python\nfor epoch in epochs:\n    ···\n    swanlab.log({\"im\": sw_image}, step=epoch)\n```",
    "339": "一级标题：恢复实验/断点续训\n二级标题：无\n内容：\n> 恢复中断或已完成的SwanLab实验。\n\n断点续训的意思是，如果你之前有一个状态为完成或中断的实验，需要补充一些实验数据，那么你可以通过`resume`和`id`参数来恢复这个实验，实验将重新变成进行中状态。\n\n:::warning 使用场景\n1. **断点续训：** 之前的训练进程断了，基于checkpoint继续训练时，希望实验图表能和之前的swanlab实验续上，而非创建1个新swanlab实验\n2. **补充图表：** 训练和评估分为了两个进程，但希望评估和训练记录在同一个swanlab实验中\n3. **更新超参：** config中有一些参数填写有误，希望更新config参数\n:::",
    "340": "一级标题：恢复实验/断点续训\n二级标题：基本用法\n内容：\n恢复实验主要依赖两个参数，`resume`和`id`：\n\n```python\nswanlab.init(\n    project=\"<project>\",\n    workspace=\"<workspace>\",\n    resume=True,\n    id=\"<exp_id>\",  # id必须为21位字符串\n)\n```\n\n`resume`参数控制了实验恢复的行为，有以下几种选择：\n\n- `must`：如果项目下存在id对应的实验，则会resume该实验，否则将报错\n- `allow`：如果项目下存在id对应的实验，则会resume该实验，否则将创建一个新的实验。\n- `never`：传递 id 参数将会报错；否则会创建一个新的实验。(即不开启resume的效果)\n- `True`：即`allow`\n- `False`：即`never`\n\n`实验id`是实验的唯一标识，可以在实验的「环境」选项卡或URL中找到，必须为1个21位字符串：\n\n![](./resume-experiment/exp_id.png)\n\n或者打开一个实验，在其URL结构中的`<exp_id>`部分就是实验id：\n\n```\nhttps://swanlab.cn/@<username>/<project>/runs/<exp_id>/...\n```",
    "341": "一级标题：恢复实验/断点续训\n二级标题：代码示例\n内容：\n```python\nimport swanlab\n\nrun = swanlab.init(project=\"resume_test\")\nswanlab.log({\"loss\": 2, \"acc\":0.4})\n# 完成实验\nrun.finish()\n\n# 恢复实验\nrun = swanlab.init(project=\"resume_test\", resume=True, id=run.id)\nswanlab.log({\"loss\": 0.2, \"acc\": 0.9})\n```",
    "342": "一级标题：恢复实验/断点续训\n二级标题：技巧：使用环境变量执行resume\n内容：\n如果你使用的是一些框架训练，不太方便修改`swanlab.init`处的源码，那么可以使用环境变量来执行resume：\n\n```bash\nexport SWANLAB_RESUME=must\nexport SWANLAB_RUN_ID=<exp_id>\n```",
    "343": "一级标题：恢复实验/断点续训\n二级标题：技巧：复制一份实验再resume\n内容：\n如果你担心本次resume的内容可能存在Bug等，保险起见可以将1份实验复制成2份，然后再resume其中一份实验：\n\n1. 找到原实验在本地`swanlog`文件夹下对应的`run`目录（通过环境Tab下的「日志目录」可以查到路径）\n2. 使用`swanlab sync`命令将此实验上传到云端：\n```bash\nswanlab sync <run_dir>\n```\n3. resume新上传的实验",
    "344": "一级标题：邮件/第三方通知\n二级标题：无\n内容：\nSwanLab支持通过邮件或第三方通知的方式，在实验结束/发生错误时发送通知。\n\n![](../../plugin/notification-email/logo.jpg)\n\n- [邮件通知](/plugin/notification-email.md)\n- [飞书通知](/plugin/notification-lark.md)\n- [钉钉通知](/plugin/notification-dingtalk.md)\n- [企业微信通知](/plugin/notification-wxwork.md)\n- [Discord通知](/plugin/notification-discord.md)\n- [Slack通知](/plugin/notification-slack.md)",
    "345": "一级标题：设置实验配置\n二级标题：无\n内容：\n使用 `swanlab.config` 保存你的训练配置，例如：\n- 超参数\n- 输入设置，例如数据集名称或模型类型\n- 实验的任何其他变量\n\n`swanlab.config` 使你可以轻松分析你的实验并在将来复现你的工作。你还可以在SwanLab应用中比较不同实验的配置，并查看不同的训练配置如何影响模型输出。",
    "346": "一级标题：设置实验配置\n二级标题：设置实验配置\n内容：\n`config` 通常在训练脚本的开头定义。当然，不同的人工智能工作流可能会有所不同，因此 `config` 也支持在脚本的不同位置定义，以满足灵活的需求。\n\n以下部分概述了定义实验配置的不同场景。\n\n### 在init中设置\n\n下面的代码片段演示了如何使用Python字典定义 `config`，以及如何在初始化SwanLab实验时将该字典作为参数传递：\n\n```python\nimport swanlab\n\n# 定义一个config字典\nconfig = {\n  \"hidden_layer_sizes\": [64, 128],\n  \"activation\": \"ELU\",\n  \"dropout\": 0.5,\n  \"num_classes\": 10,\n  \"optimizer\": \"Adam\",\n  \"batch_normalization\": True,\n  \"seq_length\": 100,\n}\n\n# 在你初始化SwanLab时传递config字典\nrun = swanlab.init(project=\"config_example\", config=config)\n```\n\n访问 `config` 中的值与在Python中访问其他字典的方式类似：\n\n- 用键名作为索引访问值\n  ```python\n  hidden_layer_sizes = swanlab.config[\"hidden_layer_sizes\"]\n  ```\n- 用 `get()` 方法访问值\n  ```python\n  activation = swanlab.config.get[\"activation\"]\n  ```\n- 用点号访问值\n  ```python\n  dropout = swanlab.config.dropout\n  ```\n\n### 用argparse设置\n\n你可以用 `argparse` 对象设置 `config`。`argparse` 是Python标准库（Python >= 3.2）中的一个非常强大的模块，用于从命令行接口（CLI）解析程序参数。这个模块让开发者能够轻松地编写用户友好的命令行界面。\n\n可以直接传递 `argparse` 对象设置 `config`：\n\n```python\nimport argparse\nimport swanlab\n\n# 初始化Argparse\nparser = argparse.ArgumentParser()\nparser.add_argument('--epochs', default=20)\nparser.add_argument('--learning-rate', default=0.001)\nargs = parser.parse_args()\n\nswanlab.init(config=args)\n```\n\n等同于 `swanlab.init(config={\"epochs\": 20, \"learning-rate\": 0.001})`\n\n### 在脚本的不同位置设置\n\n你可以在整个脚本的不同位置向 `config` 对象添加更多参数。\n\n下面的代码片段展示了如何向 `config` 对象添加新的键值对：\n\n```python\nimport swanlab\n\n# 定义一个config字典\nconfig = {\n  \"hidden_layer_sizes\": [64, 128],\n  \"activation\": \"ELU\",\n  \"dropout\": 0.5,\n  # ... 其他配置项\n}\n\n# 在你初始化SwanLab时传递config字典\nrun = swanlab.init(project=\"config_example\", config=config)\n\n# 在你初始化SwanLab之后，更新config\nswanlab.config[\"dropout\"] = 0.8\nswanlab.config.epochs = 20\nswanlab.config.set[\"batch_size\", 32]\n```\n\n### 用配置文件设置\n\n可以用json和yaml配置文件初始化 `config`，详情请查看[用配置文件创建实验](/guide_cloud/experiment_track/create-experiment-by-configfile)。",
    "347": "一级标题：设置实验Tag\n二级标题：无\n内容：\n实验Tag可以快速标记本次实验所使用的**方法、数据集、模型、超参数、Git仓库等**，以及在未来可以用于分组和过滤实验。\n\n设置好的Tag会在实验名的下方出现：\n\n![](./set-experiment-tag/example.png)",
    "348": "一级标题：设置实验Tag\n二级标题：常规标签\n内容：\n**方法一：编程设置**\n\n你可以使用`swanlab.init`中的`tags`参数，来设置实验的Tag（标签）。\n\n```python\nswanlab.init(\n    tags=[\"tag1\", \"tag2\"],\n)\n```\n\n**方法二：GUI设置**\n\n在网页中，找到实验的顶部区域，点击「添加标签」按钮，即可开始编辑标签：\n\n![](./set-experiment-tag/gui-setting.png)",
    "349": "一级标题：设置实验Tag\n二级标题：Git标签\n内容：\n支持识别标签中的Github、Gitee的仓库链接，呈现一个特殊样式，并可以点击跳转。\n\n```python\nswanlab.init(\n    tags=[\n        \"https://github.com/SwanHubX/SwanLab\",\n        \"https://gitee.com/SwanHubX/SwanLab\",\n    ],\n)\n```\n\n![](./set-experiment-tag/git-tag.png)",
    "350": "一级标题：设置实验Tag\n二级标题：Arxiv标签\n内容：\n支持识别标签中的Arxiv链接，呈现一个特殊样式，并可以点击跳转。\n\n```python\nswanlab.init(\n    tags=[\"https://arxiv.org/abs/1706.03762\"],\n)\n```\n\n![](./set-experiment-tag/arxiv-tag.png)",
    "351": "一级标题：设置实验Tag\n二级标题：AI开源社区标签\n内容：\n支持识别标签中的AI开源社区链接（[HuggingFace](https://huggingface.co/)、[魔搭社区](https://www.modelscope.cn/)、[魔乐社区](https://www.modelers.cn/)），呈现一个特殊样式，并可以点击跳转。\n\n```python\nswanlab.init(\n    tags=[\n        \"https://huggingface.co/deepseek-ai/DeepSeek-R1-0528\",\n        \"https://modelscope.cn/models/deepseek-ai/DeepSeek-R1-0528\",\n        \"https://modelers.cn/models/Modelers_Park/DeepSeek-R1-0528-Qwen3-8B\",\n    ],\n)\n```\n\n![](./set-experiment-tag/ai-community-tag.png)",
    "352": "一级标题：设置实验Tag\n二级标题：算力标签\n内容：\n支持识别标签中的算力卡品牌（nvidia、ascend、apple）。\n\n```python\nswanlab.init(\n    tags=[\"nvidia\", \"ascend\", \"apple\"],\n)\n```\n\n![](./set-experiment-tag/power-tag.png)",
    "353": "一级标题：在内网计算节点访问SwanLab Cloud\n二级标题：无\n内容：\n通常，在算力集群中计算节点无法连接到互联网，外部开发机也必须通过跳板机才能连接到计算节点。如果无法连接到公网那就无法将数据上传到 SwanLab 云端。但是跳板机作为“中间人”角色，可以连接到互联网，那么就可以利用跳板机来实现代理计算节点连接到公网环境。\n\n<img src=\"./ssh-portforwarding/cluster-network.png\" alt=\"cluster-network\" style=\"zoom:30%;\" />\n\n我们可以通过使用SSH代理转发来实现让计算节点也能连接上 [SwanLab Cloud](https://swanlab.cn/)。",
    "354": "一级标题：在内网计算节点访问SwanLab Cloud\n二级标题：开启代理转发网络\n内容：\n> 确保你的计算节点能通过SSH连接上跳板机\n\n在计算节点上执行以下命令连接到跳板机：\n\n```bash\nssh -D {port} {user}@{ip}\n```\n\n- `port` 参数为用于代理转发的端口，例如 `2015`\n- `user` 和 `ip` 参数为跳板机服务器对应的用户名和内网IP地址\n\n例如：`ssh -D 2015 hello@192.168.31.10`\n\n连接到跳板机成功后，即在对应的端口开启了一个SOCKS代理通道，那么可以直接在终端设置环境变量来配置代理，例如：\n\n```bash\nexport http_proxy=socks5://127.0.0.1:{port} https_proxy=socks5://127.0.0.1:{port}\n```\n\n> 注意将对应的 `port` 更换为自己设置的端口，协议为 [socks5](https://en.wikipedia.org/wiki/SOCKS)\n\n配置成功后可以使用以下命令测试是否正确连接到公网:\n\n```bash\ncurl ipinfo.io\n```\n\n配置成功后就可以愉快地使用SwanLab云端版了🥳。\n\n注意SSH连接不能断开，关闭终端会话会导致连接断开，那么可以使用 [tmux](https://github.com/tmux/tmux/wiki) 将SSH连接命令放置在后台。\n\n```bash\ntmux\n# 在tmux中执行SSH连接命令\nssh -D {port} {user}@{ip}\n```\n\n新开终端会话必须重新配置环境变量，当然可以将上述导入环境变量的命令写入 `.bashrc` 文件中实现每次开启新终端会话时自动写入环境变量。例如：\n```bash\necho \"export http_proxy=socks5://127.0.0.1:{port}\" >> ~/.bashrc\necho \"export https_proxy=socks5://127.0.0.1:{port}\" >> ~/.bashrc\n```\n> 注意将 `{port}` 替换为自己设置的端口",
    "355": "一级标题：在内网计算节点访问SwanLab Cloud\n二级标题：实现原理\n内容：\n上述实现借助于 [SSH 动态转发](https://en.wikipedia.org/wiki/Port_forwarding#Dynamic_port_forwarding)功能，SSH 动态端口转发将 SSH 服务器变成 SOCKS 代理服务器，您计算机上的应用程序可以将其用作连接远程服务器的中介。\n\n> **注意：**程序必须支持 SOCKS 类型的代理，您才能使用动态端口转发从该应用程序路由流量。",
    "356": "一级标题：系统硬件监控\n二级标题：无\n内容：\nSwanLab在跟踪实验的过程中，会**自动监控**机器的硬件资源情况，并记录到 **「系统」图表** 当中。当前支持的硬件列表：\n\n| 硬件 | 信息记录 | 资源监控 | 脚本 |\n| --- | --- | --- | --- |\n| 英伟达GPU | ✅ | ✅ | [nvidia.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/gpu/nvidia.py) |\n| 昇腾NPU | ✅ | ✅ | [ascend.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/npu/ascend.py) |\n| 寒武纪MLU | ✅ | ✅ | [cambricon.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/mlu/cambricon.py) |\n| 昆仑芯XPU | ✅ | ✅ | [kunlunxin.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/xpu/kunlunxin.py) |\n| 摩尔线程GPU | ✅ | ✅ | [moorethreads.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/gpu/moorethreads.py) |\n| 沐曦GPU | ✅ | ✅ | [metax.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/gpu/metax.py) |\n| 海光DCU | ✅ | ✅ | [hygon.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/dcu/hygon.py) |\n| CPU | ✅ | ✅ | [cpu.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/cpu.py) |\n| 内存 | ✅ | ✅ | [memory.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/memory.py) |\n| 硬盘 | ✅ | ✅ | [disk.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/disk.py) |\n| 网络 | ✅ | ✅ | [network.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/network.py) |\n\n[[toc]]",
    "357": "一级标题：系统硬件监控\n二级标题：系统监控指标详解\n内容：\nSwanLab 支持在当前实验运行的机器上自动监控硬件资源情况，并为每个指标生成图表，统一展示在 **「系统」图表** 选项卡中。\n\n![](./system-monitor/head.png)\n\n**采集策略与频率**：SwanLab根据当前实验的持续运行时间，自动调整硬件数据采集的频率，以平衡数据粒度与系统性能，采集频率分为以下几档：\n\n| 已采集数据点数 | 采集频率 |\n|   :---:   |   :---:   |\n| 0~10    | 10 秒/次 |\n| 10~50   | 30 秒/次 |\n| 50+     | 60 秒/次 |\n\nSwanLab 采集的硬件资源情况涵盖了GPU、NPU、CPU、系统内存、硬盘IO以及网络情况等多个与训练过程相关的指标。以下详细介绍每个部分的监控内容及其在图表展示中的意义。",
    "358": "一级标题：系统硬件监控\n二级标题：GPU（NVIDIA）\n内容：\n![](./system-monitor/nvidia.png)\n\n> 在多卡机器上，每个GPU的资源情况都会单独记录，最终在图表中展示多条图线。\n\n| 指标 | 描述 |  \n|--------|------------|  \n| GPU Memory Allocated (%) | **GPU 显存使用率**，表示此GPU的显存占用百分比。|\n| GPU Memory Allocated (MB) | **GPU 显存使用率**，表示此GPU的显存占用量，以MB为单位。该指标对应图表的纵坐标上限为所有GPU中的最大总显存。|\n| GPU Utilization (%) | **GPU 利用率**，表示此GPU的计算资源占用百分比。|\n| GPU Temperature (℃) | **GPU 温度**，表示此GPU的温度，以摄氏度为单位。|\n| GPU Power Usage (W) | **GPU 功耗**，表示此GPU的功耗，以瓦特为单位。|\n| GPU Time Spent Accessing Memory (%) | **GPU 内存访问时间**，表示此GPU在执行任务时，花费在访问 GPU 内存（显存）上的时间百分比。|\n\n<br>",
    "359": "一级标题：系统硬件监控\n二级标题：NPU（Ascend）\n内容：\n![](./system-monitor/ascend.png)\n\n> 在多卡机器上，每个NPU的资源情况都会单独记录，最终在图表中展示多条图线。\n\n| 指标 | 描述 |  \n|--------|------------|  \n| NPU Utilization (%) | **NPU 利用率**，表示此NPU的计算资源占用百分比。|\n| NPU Memory Allocated (MB) | **NPU 显存使用率**，表示此GPU的显存占用量，以MB为单位。该指标对应图表的纵坐标上限为所有GPU中的最大总显存。|\n| NPU Memory Allocated (%) | **NPU 显存使用率**，表示此NPU的显存占用百分比。|\n| NPU Temperature (℃) | **NPU 温度**，表示此NPU的温度，以摄氏度为单位。|\n| NPU Power (W) | **NPU 功率**，表示此NPU的功率，以瓦特为单位。|\n\n<br>",
    "360": "一级标题：系统硬件监控\n二级标题：MLU（寒武纪）\n内容：\n![](./system-monitor/cambricon.png)\n\n> 在多卡机器上，每个MLU的资源情况都会单独记录，最终在图表中展示多条图线。\n\n| 指标 | 描述 |  \n|--------|------------|  \n| MLU Utilization (%) | **MLU 利用率**，表示此MLU的计算资源占用百分比。|\n| MLU Memory Allocated (MB) | **MLU 显存使用率**，表示此MLU的显存占用量，以MB为单位。该指标对应图表的纵坐标上限为所有MLU中的最大总显存。|\n| MLU Memory Allocated (%) | **MLU 显存使用率**，表示此MLU的显存占用百分比。|\n| MLU Temperature (℃) | **MLU 温度**，表示此MLU的温度，以摄氏度为单位。|\n| MLU Power (W) | **MLU 功率**，表示此MLU的功率，以瓦特为单位。|\n\n<br>",
    "361": "一级标题：系统硬件监控\n二级标题：XPU（昆仑芯）\n内容：\n![](./system-monitor/kunlunxin.png)\n\n> 在多卡机器上，每个XPU的资源情况都会单独记录，最终在图表中展示多条图线。\n\n| 指标 | 描述 |  \n|--------|------------|  \n| XPU Utilization (%) | **XPU 利用率**，表示此XPU的计算资源占用百分比。|\n| XPU Memory Allocated (MB) | **XPU 显存使用率**，表示此XPU的显存占用量，以MB为单位。该指标对应图表的纵坐标上限为所有XPU中的最大总显存。|\n| XPU Memory Allocated (%) | **XPU 显存使用率**，表示此XPU的显存占用百分比。|\n| XPU Temperature (℃) | **XPU 温度**，表示此XPU的温度，以摄氏度为单位。|\n| XPU Power (W) | **XPU 功率**，表示此XPU的功率，以瓦特为单位。|\n\n<br>",
    "362": "一级标题：系统硬件监控\n二级标题：GPU（摩尔线程）\n内容：\n![](./system-monitor/moorethread.png)\n\n> 在多卡机器上，每个摩尔线程GPU的资源情况都会单独记录，最终在图表中展示多条图线。\n\n| 指标 | 描述 |  \n|--------|------------|  \n| GPU Utilization (%) | **GPU 利用率**，表示此GPU的计算资源占用百分比。|\n| GPU Memory Allocated (MB) | **GPU 显存使用率**，表示此GPU的显存占用量，以MB为单位。该指标对应图表的纵坐标上限为所有GPU中的最大总显存。|\n| GPU Memory Allocated (%) | **GPU 显存使用率**，表示此GPU的显存占用百分比。|\n| GPU Temperature (℃) | **GPU 温度**，表示此GPU的温度，以摄氏度为单位。|\n| GPU Power (W) | **GPU 功率**，表示此GPU的功率，以瓦特为单位。|\n\n<br>",
    "363": "一级标题：系统硬件监控\n二级标题：GPU（沐曦）\n内容：\n![](./system-monitor/metax.png)\n\n> 在多卡机器上，每个沐曦GPU的资源情况都会单独记录，最终在图表中展示多条图线。\n\n| 指标 | 描述 |     \n|--------|------------|  \n| GPU Utilization (%) | **GPU 利用率**，表示此GPU的计算资源占用百分比。|\n| GPU Memory Allocated (MB) | **GPU 显存使用率**，表示此GPU的显存占用量，以MB为单位。该指标对应图表的纵坐标上限为所有GPU中的最大总显存。|\n| GPU Memory Allocated (%) | **GPU 显存使用率**，表示此GPU的显存占用百分比。|\n| GPU Temperature (℃) | **GPU 温度**，表示此GPU的温度，以摄氏度为单位。|\n| GPU Power (W) | **GPU 功率**，表示此GPU的功率，以瓦特为单位。|\n\n<br>",
    "364": "一级标题：系统硬件监控\n二级标题：DCU（海光）\n内容：\n> 在多卡机器上，每个海光DCU的资源情况都会单独记录，最终在图表中展示多条图线。\n\n| 指标 | 描述 |     \n|--------|------------|  \n| GPU Utilization (%) | **GPU 利用率**，表示此GPU的计算资源占用百分比。|\n| GPU Memory Allocated (MB) | **GPU 显存使用率**，表示此GPU的显存占用量，以MB为单位。该指标对应图表的纵坐标上限为所有GPU中的最大总显存。|\n| GPU Memory Allocated (%) | **GPU 显存使用率**，表示此GPU的显存占用百分比。|\n| GPU Temperature (℃) | **GPU 温度**，表示此GPU的温度，以摄氏度为单位。|\n| GPU Power (W) | **GPU 功率**，表示此GPU的功率，以瓦特为单位。|\n\n\n<br>",
    "365": "一级标题：系统硬件监控\n二级标题：CPU\n内容：\n| 指标 | 描述 |  \n|--------|------------|  \n| CPU Utilization (%) | **CPU 利用率**，表示此CPU的计算资源占用百分比。|\n| Process CPU Threads | **CPU 线程数**，表示当前运行的实验所使用的CPU总线程数。|\n\n<br>",
    "366": "一级标题：系统硬件监控\n二级标题：内存\n内容：\n| 指标 | 描述 |  \n|--------|------------|  \n| System Memory Utilization (%) | **系统内存使用率**，表示当前系统的内存占用百分比。|\n| Process Memory In Use (non-swap) (MB) | **进程占用内存**，当前进程实际占用的物理内存量（不包含交换区），直观反映实验运行时的内存消耗。|\n| Process Memory Utilization (MB) | **进程分配内存**，当前进程分配的内存量（包含交换区），不一定是实际使用的内存量。|\n| Process Memory Available （non-swap） (MB) | **进程可用内存**，当前进程可用的物理内存量（不包含交换区），即当前进程可以使用的内存量。|\n\n<br>",
    "367": "一级标题：系统硬件监控\n二级标题：硬盘\n内容：\n| 指标 | 描述 |  \n|--------|------------|  \n| Disk IO Utilization (MB) | **硬盘I/O**，表示硬盘的读写速度，以MB/s为单位。读速率和写速率会在图表中作为两条图线，分开展示。|\n| Disk Utilization (%) | **硬盘使用情况**，表示当前系统盘的使用率，以百分比为单位。|\n\n在Linux平台，取根目录`/`的使用率；若操作系统为Windows，则取系统盘（通常是`C:`）的使用率。\n\n<br>",
    "368": "一级标题：系统硬件监控\n二级标题：网络\n内容：\n| 指标 | 描述 |  \n|--------|------------|  \n| Network Traffic (KB) | **网络I/O**，表示网络的读写速度，以KB/s为单位。接收速率和发送速率会在图表中作为两条图线，分开展示。|\n\n> 表示网络的读写速度，以KB/s为单位。接收速率和发送速率会在图表中作为两条图线，分开展示。",
    "369": "一级标题：查看实验结果\n二级标题：无\n内容：\n使用SwanLab丰富的实验仪表盘，一站式管理和可视化AI模型训练结果。\n\n[[toc]]",
    "370": "一级标题：查看实验结果\n二级标题：云端同步\n内容：\n无论您在哪里训练模型 —— **自己的电脑、实验室的服务器集群、还是云上的实例**，我们都能轻松收集与汇总您的训练数据，并且随时随地访问训练进展，哪怕是在手机上。\n\n您也无需花时间将终端的输出截图或粘贴到Excel，也无需管理来自不同计算机的Tensorboard文件，用SwanLab就能轻松搞定。\n\n![](./view-result/cloud.jpg)",
    "371": "一级标题：查看实验结果\n二级标题：📱 移动端看实验\n内容：\n你一定遇到过，实验正在training，但你不在电脑旁边 —— 也许在运动、在通勤、或者刚刚起床，十分想瞄一眼实验的进展和结果。这个时候，手机+SwanLab，会是绝佳组合。[查看详情](../general/app.md)\n\n![](../general/app/android.png)",
    "372": "一级标题：查看实验结果\n二级标题：表格视图\n内容：\n通过表格视图比较每次训练实验，看看哪些超参数发生了变化。  \n表格视图默认会将数据以`[实验名]-[元信息]-[配置]-[指标]`的顺序排序。\n\n![view-result](/assets/view-result-1.jpg)",
    "373": "一级标题：查看实验结果\n二级标题：图表对比视图\n内容：\n通过**图表对比视图**可以将每个实验的图表进行整合，生成一个多实验对比图表视图。  \n在多实验图表当中，可以清晰地对比不同实验在同一个指标下的变化情况与性能差异。\n\n![chart-comparison](/assets/chart-comparison.jpg)",
    "374": "一级标题：查看实验结果\n二级标题：日志\n内容：\n在实验开始到结束，SwanLab会记录下从`swanlab.init`到实验结束的终端输出，并记录在实验的「日志」选项卡，可以随时查看、复制与下载。我们也支持通过搜索找到关键信息。\n\n![logging](/assets/logging.jpg)",
    "375": "一级标题：查看实验结果\n二级标题：环境\n内容：\n在实验开始后，SwanLab会记录下训练相关的环境参数，包括：\n\n- **基础数据**：运行时间、主机名、操作系统、Python版本、Python解释器、运行目录、命令行、Git仓库URL、Git分支、Git提交、日志文件目录、SwanLab版本\n- **系统硬件**：CPU核心数、内存大小、GPU数量、GPU型号、GPU显存\n- **Python库**：运行环境下的所有Python库\n\n![environment](/assets/environment.jpg)",
    "376": "一级标题：什么是实验跟踪\n二级标题：无\n内容：\n**实验跟踪** 是指在机器学习模型开发过程中，记录每个实验从开始到结束的**超参数、指标、硬件、环境、日志**等数据，并在UI界面进行**组织**和**呈现**的过程。实验跟踪的目的是帮助研究人员更有效地**管理**和**分析**实验结果，以便更好地理解模型性能的变化，进而优化模型开发过程。\n\n::: warning 🤔简单来说\n实验跟踪的作用可以理解为，在进行机器学习实验时，记录下实验的各个关键信息，**为后续模型的进化提供“弹药”**。\n:::\n\n![](./what-is-experiment-track/overview.jpg)\n\n与**实验跟踪**息息相关的，是**可视化**、**可复现性**、**实验比较**以及**团队协作**。\n\n1. **📊 可视化**: 通过UI界面对实验跟踪数据进行可视化，可以让训练师**直观地看到实验每一步**的结果，**分析指标走势**，判断哪些**变化**导致了模型效果的提升，从而**整体性地提升模型迭代效率**。\n\n![](./what-is-experiment-track/visualization.jpg)\n\n<br>\n\n2. **♻️ 可复现性**: 实验从跑通到可用，再到SOTA，往往需要经历**大量试验**，而一些非常好的结果可能出现在中前期。但如果没有实验跟踪和可视化，训练师难以记住这些结果，从而导致大量优秀的实验结果**记不清细节或被遗忘**。而通过SwanLab的实验跟踪和可视化功能，可以帮助训练师随时**回顾**这些结果，大大提高了可复现性与整体效率。\n\n![](./what-is-experiment-track/card.jpg)\n\n<br>\n\n3. **🆚 实验比较**: 训练师可以通过SwanLab**轻松地比较**多组实验结果，分析哪些变化导致了性能提升，从而**快速找到最优的训练策略**。\n\n![](./what-is-experiment-track/table.jpg)\n\n<br>\n\n4. **👥 团队协作**: 通过SwanLab的**实验分享、团队空间、多人协同**实验等功能，无缝地共享训练进展和心得经验，打通团队成员之间的信息孤岛，**提高团队协作效率**。",
    "377": "一级标题：什么是实验跟踪\n二级标题：SwanLab是如何进行实验跟踪的？\n内容：\n**SwanLab**帮助你只需使用几行代码，便可以跟踪机器学习实验，并在交互式仪表板中查看与比较结果。跟踪流程：\n\n1. 创建SwanLab实验。\n2. 将超参数字典（例如学习率或模型类型）存储到您的配置中 (swanlab.config)。\n3. 在训练循环中随时间记录指标 (swanlab.log)，例如准确性acc和损失loss。\n\n下面的伪代码演示了常见的**SwanLab实验跟踪工作流**：\n\n```python\n# 1. 创建1个SwanLab实验\nswanlab.init(project=\"my-project-name\")\n\n# 2. 存储模型的输入或超参数\nswanlab.config.learning_rate = 0.01\n\n# 这里写模型的训练代码\n...\n\n# 3. 记录随时间变化的指标以可视化表现\nswanlab.log({\"loss\": loss})\n```",
    "378": "一级标题：什么是实验跟踪\n二级标题：如何开始？\n内容：\n探索以下资源以了解SwanLab实验跟踪：\n\n- 阅读[快速开始](/guide_cloud/general/quick-start)\n- 探索本章以了解如何：\n  - [创建一个实验](/guide_cloud/experiment_track/create-experiment)\n  - [配置实验](/guide_cloud/experiment_track/set-experiment-config.md)\n  - [记录指标](/guide_cloud/experiment_track/log-experiment-metric.md)\n  - [查看实验结果](/guide_cloud/experiment_track/view-result.md)\n- 在[API文档](/api/api-index)中探索SwanLab Python 库。",
    "379": "一级标题：在手机上使用SwanLab\n二级标题：无\n内容：\n你一定遇到过，实验正在training，但你不在电脑旁边 —— 也许在运动、在通勤、或者刚刚起床，十分想瞄一眼实验的进展和结果。这个时候，**手机+SwanLab**，会是绝佳组合。\n\n下面将介绍如何将SwanLab添加到你的手机主屏幕，以接近APP的体验，快速访问SwanLab。",
    "380": "一级标题：在手机上使用SwanLab\n二级标题：安卓\n内容：\n流程示例图如下所示，以Chrome浏览器为例：\n\n![alt text](/zh/guide_cloud/general/app/android.png)\n\n1. 在你的手机浏览器上，访问[swanlab.cn](https://swanlab.cn)\n2. 点击右上角三个点按钮后，在菜单中点击 **「添加到主屏幕」**\n3. 在弹窗中，选择 **「安装」** 或 **「创建快捷方式」** 均可\n4. 回到桌面，现在你在主屏幕上就可以找到 **SwanLab\"APP\"** 了！",
    "381": "一级标题：在手机上使用SwanLab\n二级标题：iOS\n内容：\n流程示例图如下所示，以Safari浏览器为例：\n\n![alt text](/zh/guide_cloud/general/app/ios.png)\n\n1. 在你的手机浏览器上，访问[swanlab.cn](https://swanlab.cn)\n2. 点击底部中间的分享按钮后，在菜单中点击 **「添加到主屏幕」**\n3. 在弹窗中，编辑应用名称，点击右上角的 **「添加」**\n4. 回到桌面，现在你在主屏幕上就可以找到 **SwanLab\"APP\"** 了！",
    "382": "一级标题：⚡️更新日志\n二级标题：无\n内容：\n::: warning 更新指南\n升级到最新版：`pip install -U swanlab`  \nGithub: https://github.com/SwanHubX/SwanLab\n:::",
    "383": "一级标题：⚡️更新日志\n二级标题：v0.6.8 - 2025.7.29\n内容：\n**🚀新增功能**\n- 侧边栏支持**实验筛选、排序**\n- 表格视图上线**列控制面板**，能够方便地实现列的隐藏与显示\n- **多API Key管理**上线，让你的数据更安全\n- [swanlab sync](/guide_cloud/experiment_track/sync-logfile.md) 提高了对日志文件完整性的兼容，适配训练崩溃等场景\n- 新图表类型-[PR曲线](/api/py-pr_curve.md)、[ROC曲线](/api/py-roc_curve.md)、[混淆矩阵](/api/py-confusion_matrix.md)上线\n- 开放接口新增**获取实验指标**接口\n\n**🤔优化**\n- 增加 日语、俄语 语言支持\n- 实验卡片中的配置表格支持一键折叠/展开\n- 修复了一些问题",
    "384": "一级标题：⚡️更新日志\n二级标题：v0.6.7 - 2025.7.17\n内容：\n**🚀新增功能**\n- 更强大的折线图配置，支持灵活配置线型、颜色、粗细、网格和图例位置\n- 支持`swanlab.Video`数据类型，支持记录与可视化GIF格式文件\n- 全局图表仪表盘支持配置Y轴与最大显示实验数\n- 更强大的文本图表，适配大语言模型训练场景\n\n**🤔优化**\n- 最大实验名提升到250个字符\n- 修复了一些问题",
    "385": "一级标题：⚡️更新日志\n二级标题：v0.6.5 - 2025.7.5\n内容：\n**🚀新增功能**\n- 支持**resume断点续训**\n- 支持小折线图局部放大\n- 支持配置单个折线图平滑\n\n**⚙️优化**\n- 大幅改进了图像图表放大后的交互效果\n\n**🔌集成**\n- 🤗集成[accelerate](https://github.com/huggingface/accelerate)框架，[文档](/guide_cloud/integration/integration-huggingface-accelerate.md)增强分布式训练中的实验记录体验；\n- 集成[ROLL](https://github.com/alibaba/ROLL)框架，[文档](/guide_cloud/integration/integration-roll.md)增强分布式训练中的实验记录体验；\n- 集成[Ray](https://github.com/ray-project/ray)框架，[文档](/guide_cloud/integration/integration-ray.md)增强分布式训练中的实验记录体验；\n\n**🔌插件**\n- 新增`LogdirFileWriter`插件，支持将文件写入到日志文件夹\n\n\n**生态**\n- 阿里云计算巢服务上架：[指引](/guide_cloud/self_host/alibabacloud-computenest.md)",
    "386": "一级标题：⚡️更新日志\n二级标题：v0.6.4 - 2025.6.18\n内容：\n**🚀新增功能**\n- 新增与[AREAL](https://github.com/inclusionAI/AReaL)框架的集成，[PR](https://github.com/inclusionAI/AReaL/pull/98)\n- 支持鼠标Hover到侧边栏实验时，高亮相应曲线\n- 支持跨组对比折线图\n- 启用渐进式图表渲染，提高页面加载速度\n- 支持设置实验名裁剪规则\n\n**⚙️修复**\n- 修复了`local`模式下，日志文件无法正确`sync`和`watch`的问题",
    "387": "一级标题：⚡️更新日志\n二级标题：v0.6.3 - 2025.6.12\n内容：\n**🚀新增功能**\n- 新增`swnalab.echarts.table`，支持创建表格图表\n- 昇腾/沐曦/海光/寒武纪/昆仑芯 硬件监控 增加显存（MB）记录\n- `swanlab sync`支持一次多日志上传\n- 工作区增加`公开/私有`筛选\n- 表格视图增加`最新/最大/最小值`切换模块",
    "388": "一级标题：⚡️更新日志\n二级标题：v0.6.2 - 2025.6.9\n内容：\n**🚀新增功能**\n- 新增`swanlab sync`命令，支持将本地日志同步到SwanLab云端/私有化部署端\n- 支持在本地存储完整的实验日志文件",
    "389": "一级标题：⚡️更新日志\n二级标题：v0.6.1 - 2025.6.5\n内容：\n**🚀新增功能**\n- 鼠标放到表头，可以显示缩略的名称了\n- 表格视图增加「展开子表」功能\n- 硬件监控支持海光DCU\n- 硬件监控支持获取昇腾NPU的功耗信息\n\n**🤔优化**\n- 优化了HuggigngFace accelerate框架的集成\n- 默认不再打印重复step log warning",
    "390": "一级标题：⚡️更新日志\n二级标题：v0.6.0 - 2025.6.1\n内容：\n**🚀新增功能**\n- 支持 **图表自由拖拽**\n- 支持ECharts自定义图表，增加包括柱状图、饼状图、直方图在内的20+图表类型\n- 硬件监控已支持 **沐曦** 显卡\n- 集成 [PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP) 框架",
    "391": "一级标题：⚡️更新日志\n二级标题：v0.5.9 - 2025.5.25\n内容：\n**🚀新增功能**\n-  📒 日志记录支持了标准错误流，EvalScope / PyTorch Lightning等这些框架的日志记录体验大幅提升\n-  💻 硬件监控已支持 **摩尔线程** 显卡\n-  🔐 新增运行命令记录的安全防护功能，API Key将被自动隐藏\n-  ⚙️ 设置新增「默认空间」和「默认可见性」配置，可以指定你的项目默认创建在哪个组织下啦！",
    "392": "一级标题：⚡️更新日志\n二级标题：v0.5.8 - 2025.5.13\n内容：\n**🚀新增功能**\n\n- 新增**实验Tag**功能\n- 新增折线图 **Log Scale** 功能\n- 新增 **实验分组拖拽** 功能\n- 新增实验卡片中**配置**与**指标**表格下载功能\n- 新增[开放接口](/zh/api/py-openapi.md)，支持通过API获取SwanLab数据\n- 大幅优化了指标传输性能，提升上千指标的传输速度\n- 集成`paddlenlp`框架\n\n**🤔优化**\n- 优化了个人主页的一系列交互\n\n**生态**\n- 腾讯云云应用上架：[指引](/zh/guide_cloud/self_host/tencentcloud-app.md)",
    "393": "一级标题：⚡️更新日志\n二级标题：v0.5.6 - 2025.4.23\n内容：\n**🚀新增功能**\n\n- 折线图支持**图表配置**功能，本次更新支持配置图表的X、Y轴范围；主标题；X、Y轴标题\n- 图表搜索支持**正则表达式**\n- SwanLab私有化部署版，已支持离线激活验证\n- 支持**昆仑芯XPU**的环境记录与硬件监控\n- 适配对使用`uv`环境下的pip环境记录\n- 环境记录支持记录**Linux发行版**（如Ubuntu、CentOS、Kylin等）\n\n**🤔优化**\n- 修复了侧边栏一键隐藏实验的一些问题",
    "394": "一级标题：⚡️更新日志\n二级标题：v0.5.5 - 2025.4.7\n内容：\n**🚀新增功能**\n- 新增`swanlab.Molecule`数据类型，支持生物化学分子可视化，为AlphaFold等AI4Science训练任务提供更好的训练体验\n- 实验表格，现在支持记忆你的排序、筛选、列拖拽了！\n- 支持了寒武纪MLU的温度和功率指标记录\n- 新增SWANLAB_PROJ、SWANLAB_WORKSPACE、SWANLAB_EXP_NAME三个环境变量\n- 环境中支持显示寒武纪MLU Logo\n\n**🌍生态**\n- 大模型评估框架[EvalScope](https://github.com/modelscope/evalscope) 已集成SwanLab！：https://github.com/modelscope/evalscope/pull/453\n\n**🤔优化**\n- 优化了网页加载性能",
    "395": "一级标题：⚡️更新日志\n二级标题：v0.5.4 - 2025.3.31\n内容：\n**🚀新增功能**\n- 新增`swanlab.Settings`方法，支持更精细化的实验行为控制，进一步增强开放性\n- 支持了寒武纪MLU的硬件记录和资源监控\n- 昇腾NPU的硬件记录支持记录CANN版本\n- 英伟达GPU的硬件记录支持记录GPU架构和cuda核心数\n- 英伟达GPU的硬件监控支持记录“GPU 访问内存所花费的时间百分比”\n- 「个人主页」支持显示你所在的「组织」\n- 「概览」页支持编辑\"项目描述\"文本\n\n**🤔优化**\n- 修复了sync_wandb的一些问题\n- 修复了Obejct3D类的一些问题\n- 优化「常规」设置样式\n- 大幅优化了打开项目的性能\n\n**🔌插件**\n- 官方插件增加Slack通知、Discord通知，进一步打通海外生态",
    "396": "一级标题：⚡️更新日志\n二级标题：v0.5.3 - 2025.3.20\n内容：\n![swanlab x huggingface](./changelog/hf.png)\n\n**🚀新增功能**\n- SwanLab已正式加入 **🤗HuggingFace生态**！Transformers 4.50.0版本开始 正式将SwanLab集成为实验跟踪工具，在TrainingArguments中加入`report_to=\"swanlab\"`即可开始跟踪训练。\n- 新增了`swanlab.Object3D`，支持记录三维点云，[文档](/api/py-object3d)\n- 硬件监控支持了 GPU显存（MB）、磁盘利用率、网络上下行 的记录\n\n**优化**\n- 修复了一些问题",
    "397": "一级标题：⚡️更新日志\n二级标题：v0.5.0 - 2025.3.12\n内容：\n![logo](../self_host/docker-deploy/swanlab-docker.jpg)\n\n**🎉🎉SwanLab私有化部署（社区版）现已重磅发布！！**[部署文档](/guide_cloud/self_host/docker-deploy.md)\n\n**🚀新增功能**\n- `swanlab.init`新增参数`callbacks`，支持在初始化时注册回调函数，以支持各式各样的自定义插件类\n- 新增`swanlab.register_callback()`，支持在`init`外部注册回调函数，[文档](/api/py-register-callback.html)\n- `swanlab.login()`升级，新增`host`、`web_host`、`save`参数，适配了私有化部署服务的特性，同时支持不将用户登录凭证写入本地，以适应共用服务器场景。[文档](/zh/api/py-login.md)\n- `swanlab login`升级，新增`host`、`web_host`、`api-key`参数，[文档](/zh/api/cli-swanlab-login.md)\n- 新增支持使用`swanlab.sync_mlflow()`将MLFlow项目同步到SwanLab，[文档](/guide_cloud/integration/integration-mlflow.md)\n\n**🤔优化**\n- 我们大幅优化了sdk架构，提升了sdk在大量metric场景下的性能\n- 实验侧边栏可以拉伸了！\n- 实验页面右上角增加了「Git代码」按钮，一键跳转到对应的仓库\n\n**🔌插件**：\n- 新增**通知类插件**，支持在训练结束时使用**邮件、飞书、钉钉、企业微信**进行通知\n- 新增**记录类插件**，支持在训练过程中将元数据、配置、指标写入到**本地CSV文件**",
    "398": "一级标题：⚡️更新日志\n二级标题：v0.4.12 - 2025.3.8\n内容：\n**优化**\n- 修复了一些问题",
    "399": "一级标题：⚡️更新日志\n二级标题：v0.4.11 - 2025.3.5\n内容：\n**优化**\n- 修复了部分版本W&B格式转换报错的问题\n- 修复了一些交互问题",
    "400": "一级标题：⚡️更新日志\n二级标题：v0.4.10 - 2025.3.4\n内容：\n**🚀新增功能**\n- 新增了和[DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio)的集成, [文档](/guide_cloud/integration/integration-diffsynth-studio.md)\n- 新增支持转换 **MLFlow** 实验到 SwanLab，[文档](/guide_cloud/integration/integration-mlflow.md)\n- 新增**项目描述**，支持给你的项目记一些简短的笔记\n\n**优化**\n- 修复了在OpenEuler系统上无法正确记录CPU型号的问题",
    "401": "一级标题：⚡️更新日志\n二级标题：v0.4.9 - 2025.2.28\n内容：\n**🚀新增功能**\n- 新增了`移动实验`功能\n- 对一些集成Callback类增加了`update_config`方法\n- `run`新增`get_url()`和`get_project_url()`方法，支持获取实验和项目的URL\n\n**优化**\n- 修复了在部分Linux系统上CPU品牌获取不到的问题",
    "402": "一级标题：⚡️更新日志\n二级标题：v0.4.8 - 2025.2.16\n内容：\n**🚀新增功能**\n- 新增了和Modelscope Swift的集成，[文档](/guide_cloud/integration/integration-swift.md)\n- 新增了`添加分组`和`移动图表到其他分组`功能\n\n**优化**\n- 修复了sdk的一些问题",
    "403": "一级标题：⚡️更新日志\n二级标题：v0.4.7 - 2025.2.11\n内容：\n**🚀新增功能**\n- `swanlab.log`支持了参数`print_to_console`，开启后可以将`swanlab.log`的`key`、`value`以字典的形式打印到终端\n- `swanlab.init`支持了对`name`、`notes`参数的适配，等价于`experiment_name`和`description`",
    "404": "一级标题：⚡️更新日志\n二级标题：v0.4.6 - 2025.2.3\n内容：\n**🚀新增功能**\n- 新增与LLM强化学习框架[verl](https://github.com/volcengine/verl)的集成，[文档](/guide_cloud/integration/integration-verl.md)\n- `swanlab.log`支持了嵌套字典传入\n\n**优化**\n- 优化了在PyTorch Lightning框架下的分布式训练优化",
    "405": "一级标题：⚡️更新日志\n二级标题：v0.4.5 - 2025.1.22\n内容：\n**🚀新增功能**\n- 新增`swanlab.sync_tensorboardX()`和`swanlab.sync_tensorboard_torch()`：支持使用TensorboardX或PyTorch.utils.tensorboard跟踪实验时，同步指标到SwanLab\n\n**优化**\n- 优化了`sync_wandb()`的代码兼容性",
    "406": "一级标题：⚡️更新日志\n二级标题：v0.4.3 - 2025.1.17\n内容：\n**🚀新增功能**\n- 新增`swanlab.sync_wandb()`：支持使用Weights&Biases跟踪实验时，同步指标到SwanLab，[文档](/guide_cloud/integration/integration-wandb.md)\n- 新增在使用框架集成时，配置项将记录所使用的框架\n\n**优化**\n- 改进了表格视图的交互，增加了行列拖拽、筛选、排序交互\n- 大幅优化了工作区加载的性能\n- 大幅优化了日志渲染的性能\n- 改进了在未登录的计算机上，执行`swanlab.init()`的交互\n- 修复了一些已知问题",
    "407": "一级标题：⚡️更新日志\n二级标题：元旦节更新\n内容：\n**🚀新增功能**\n- 升级了图表平滑，网页刷新后状态将仍然保留\n- 更新了图表大小修改，现在可以通过拖拽图表的右下角来改变大小\n\n**⚙️问题修复**\n- 修复了没有实验时，项目设置不显示删除的bug",
    "408": "一级标题：⚡️更新日志\n二级标题：v0.4.2 - 2024.12.24\n内容：\n**🚀新增功能**\n- 新增密码登录\n- 新增项目设置页\n\n**优化**\n- 修复在一些设备上运行硬件监控会warning的问题",
    "409": "一级标题：⚡️更新日志\n二级标题：v0.4.0 - 2024.12.15\n内容：\n🎉万众期待的硬件监控功能（云端版）已经上线，支持**CPU、NPU、GPU**的系统级信息监控：\n\n- **CPU**：利用率、线程数\n- **内存**：利用率、进程利用率、可用内存\n- **Nvidia GPU**：利用率、显存分配、温度、功耗\n- **Ascend NPU**：利用率、HBM分配、温度\n\n更多信息的监控已经在路上！\n\nby Cunyue",
    "410": "一级标题：⚡️更新日志\n二级标题：v0.3.28 - 2024.12.6\n内容：\n> 🍥公告：硬件监控功能即将推出！\n\n**🚀新增功能**\n- 新增与LightGBM的集成\n- 新增与XGBoost的集成\n\n**优化**\n- 提高了对日志记录时单行长度的限制\n- 改善了部分性能，为0.4.0版本做准备",
    "411": "一级标题：⚡️更新日志\n二级标题：v0.3.27 - 2024.11.26\n内容：\n**🚀新增功能**\n- 新增华为昇腾NPU显卡检测\n- 新增与青云基石智算(Coreshub)的集成",
    "412": "一级标题：⚡️更新日志\n二级标题：新UI上线！\n内容：\n![alt text](/assets/new-homepage.png)\n\n**🚀我们改进了什么**\n- 从用户体验出发，上线全新的官网和UI界面\n- 上线个人/组织主页\n- 增加「黑夜模式」\n- 全面优化的「新手快速开始」，增加了框架集成和案例\n- 优化「图表对比视图」的实验选择逻辑",
    "413": "一级标题：⚡️更新日志\n二级标题：v0.3.25 - 2024.11.11\n内容：\n**🚀新增功能**\n- 🎉[VSCode插件](https://marketplace.visualstudio.com/items?itemName=SwanLab.swanlab&ssr=false#overview)已上线\n- 新增与Keras框架的集成\n- 新增`run.public`方法，支持获取实验的项目名、实验名、链接等信息，[#732](https://github.com/SwanHubX/SwanLab/pull/732)",
    "414": "一级标题：⚡️更新日志\n二级标题：v0.3.22 - 2024.10.18\n内容：\n**🚀新增功能**\n- 🎉基线社区Beta版本已上线：https://swanlab.cn/benchmarks\n- 新增与PaddleYolo的集成，[文档](/guide_cloud/integration/integration-paddleyolo.md)\n\n**修复问题**\n- 修复了在多组并行实验提交时，出现sqlite并行读写报错的问题，[#715](https://github.com/SwanHubX/SwanLab/issues/715)\n- 修复了在CPU品牌记录的兼容性问题",
    "415": "一级标题：⚡️更新日志\n二级标题：v0.3.21 - 2024.9.26\n内容：\n**🚀新增功能**\n- [组织创建](/guide_cloud/general/organization.md)已全面开放，每个组织上限为15人。\n- 实验名现已支持「重名」，并使用新的一套新建实验名体系。",
    "416": "一级标题：⚡️更新日志\n二级标题：v0.3.19 - 2024.9.2\n内容：\n**🚀新增功能**\n- （内测）新增任务式训练`swanlab task`的网盘存储功能\n\n**优化**\n- 【环境】增加对CPU品牌的记录\n\n**问题修复**\n- 修复了在Win命令行下`swanlab login`容易出现误操作引发的问题",
    "417": "一级标题：⚡️更新日志\n二级标题：v0.3.17 - 2024.8.18\n内容：\n1. 完成了对云端图表库以及前端的代码重构，改进了大量交互\n2. 修复了实验表格中侧边栏未加载实验没有正常显示参数的问题\n3. 修复了requests包引起的部分用户网络连接错误的问题\n4. 【环境】增加对NVIDIA驱动版本的记录\n5. 本地看版支持对已占用的端口自动续新端口了",
    "418": "一级标题：⚡️更新日志\n二级标题：v0.3.16 - 2024.7.31\n内容：\n**🚀新增功能**\n- （内测）新增任务式训练`swanlab task`功能\n- 新增与`torchtune`的集成，[文档](/guide_cloud/integration/integration-pytorch-torchtune)\n\n**优化**\n- `swanlab.init`增加参数`public`，可用于设置创建的新项目的可见性，默认为`False`\n- 用`swanlab.init`创建的项目默认可见性改为私有\n- 新增了`swanlab.config`对`dataclass`类型的支持\n\n**问题修复**\n- 修复了在conda-forge环境下import swanlab会提示缺乏依赖库的问题",
    "419": "一级标题：⚡️更新日志\n二级标题：v0.3.14 - 2024.7.20\n内容：\n**问题修复**\n- 修复环境依赖安装问题\n- 修复在Windows系统上存在的一些适配问题",
    "420": "一级标题：⚡️更新日志\n二级标题：v0.3.13 - 2024.6.27\n内容：\n**🚀新增功能**\n- 新增支持修改实验颜色\n\n**⚡️改进**\n- 优化了在Google CoLab、Jupyter Notebook下的一些问题\n- 优化了错误日志收集与打印\n\n**问题修复**\n- 修复了Windows系统下运行的一些问题\n- 修复了在Hydra等框架上的终端打印问题\n- 修复了了在mmengine集成中SwanlabVisBackend的save_dir不能为None的问题",
    "421": "一级标题：⚡️更新日志\n二级标题：v0.3.11 - 2024.6.14\n内容：\n**🚀新增功能**\n- 环境记录增加PID和Python Verbose\n- 支持修改项目可见性\n- 离线看版命令修改为`swanlab watch [LOG PATH]`\n\n**⚡️改进**\n- 优化了Python环境搜索的性能\n- 优化了SwanLab库的架构\n\n**问题修复**\n- 修复了离线看版启动失败的问题",
    "422": "一级标题：⚡️更新日志\n二级标题：v0.3.10 - 2024.6.10\n内容：\n**问题修复**\n- 修复了部分文本上传时会出现编码错误的问题\n- 修复了环境信息没有正确上传的问题",
    "423": "一级标题：⚡️更新日志\n二级标题：v0.3.9 - 2024.6.8\n内容：\n**🚀新增功能**\n- `swanlab logout`：支持在终端退出SwanLab账号\n\n**👥集成**\n- 增加与HuggingFace Accelerate的集成，[文档](/guide_cloud/integration/integration-huggingface-accelerate.md)\n\n**⚡️改进**\n- 改进了媒体文件上传的稳定性\n\n**问题修复**\n- 修复了nvml库的兼容性问题\n- 解决在实验结束时上传大量媒体文件可能引发的409错误\n- 修复了在部分机器上会出现OSError的问题",
    "424": "一级标题：⚡️更新日志\n二级标题：v0.3.8 - 2024.5.31\n内容：\n**⚡️改进**\n- 改进了与ultralytics在ddp场景下的集成\n- swanlab.init时增加最新版本的提示\n\n**问题修复**\n\n- 修复了当log的value为`inf`会导致线程崩溃的问题\n- 修复了训练时间过长时，部分图片上传会失败的问题",
    "425": "一级标题：⚡️更新日志\n二级标题：v0.3.6 - 2024.5.28\n内容：\n**问题修复**\n\n- 修复了部分logging日志无法上传的问题\n- 修复了`swanlab login`无法登陆的问题",
    "426": "一级标题：⚡️更新日志\n二级标题：v0.3.4 - 2024.5.27\n内容：\n**🚀新增功能**\n- `swanlab.init`增加参数`mode`，支持新模式`disabled`\n- 支持批量删除实验\n\n**⚡️改进**\n- 优化ultralytics集成代码\n\n**👥集成**\n- 与Stable Baseline3集成，[指引](/guide_cloud/integration/integration-sb3.md)",
    "427": "一级标题：⚡️更新日志\n二级标题：v0.3.3 - 2024.5.22\n内容：\n**👥集成**\n- 与Weights & Biases集成，支持将wandb项目转换为`SwanLab`项目，[指引](/guide_cloud/integration/integration-wandb.md)\n- 与Ultralytics集成，[指引](/guide_cloud/integration/integration-ultralytics.md)\n- 与fastai集成，[指引](/guide_cloud/integration/integration-fastai.md)",
    "428": "一级标题：⚡️更新日志\n二级标题：v0.3.2 - 2024.5.17\n内容：\n**👥集成**\n- 与Tensorboard集成，支持将`Tensorboard`日志文件转换为`SwanLab`实验，[指引](/guide_cloud/integration/integration-tensorboard.md)\n\n**🚀新增功能**\n- 支持下载折线图为PNG图像\n- SwanLab实验可以被嵌入到在线文档中了（飞书/Notion等支持嵌入网页的在线文档）\n- 表格视图支持导出CSV\n- 表格视图支持仅看指标\n\n**⚡️改进**\n- 优化了折线图与表格视图的数值显示\n\n**⚙️修复问题**\n- 修复了在Windows系统下，`swanlab.config`载入`hydra`配置文件时，config表格的显示Bug\n- 解决SwanLab在jupyter Notebook中的登录问题",
    "429": "一级标题：⚡️更新日志\n二级标题：v0.3.1 - 2024.5.3\n内容：\n**⚡️改进**\n- `swanlog`日志文件夹默认增加一个`.gitignore`\n\n**⚙️修复问题**\n- 修复`swanlab.init`的config不兼容Omegaconfig等类型的问题",
    "430": "一级标题：团队使用SwanLab\n二级标题：无\n内容：",
    "431": "一级标题：团队使用SwanLab\n二级标题：创建组织\n内容：\n在主页的左上方，点击“创建组织”按钮，填写组织名、组织ID等信息，即可完成组织创建。\n\n<div align=\"center\">\n<img src=\"/assets/organization-create.jpg\" width=\"400\">\n</div>",
    "432": "一级标题：团队使用SwanLab\n二级标题：邀请成员\n内容：\n<div align=\"center\">\n<img src=\"./organization/invite.png\">\n</div>\n\n在组织空间下，点击「设置」-「常规」，在「成员」栏下，点击「邀请成员」按钮，将邀请链接分享给要加入组织的成员。\n\n<div align=\"center\">\n<img src=\"./organization/join.png\">\n</div>\n\n成员点击邀请链接，提交申请后，经管理员审核通过，即可完成加入。",
    "433": "一级标题：团队使用SwanLab\n二级标题：将实验上传到组织空间\n内容：\n在默认情况下（即不设置`workspace`参数），你的项目会被上传到个人空间下。  \n想要上传到组织空间下，则将`swanlab.init`的`workspace`参数设置为组织的组织名（不是组织昵称）即可。\n\n```python\nimport swanlab\n\nswanlab.init(\n    workspace=\"[组织名username]\"\n)\n```\n\n如果组织里的多个人想要在一个项目下协作，则只需要将`swanlab.init`的`project`参数设置为同一个即可。",
    "434": "一级标题：🚀快速开始\n二级标题：无\n内容：\n安装 SwanLab 并在几分钟内开始跟踪你的人工智能实验。\n\n![quick-start-1](./quick_start/quick-start.png)",
    "435": "一级标题：🚀快速开始\n二级标题：1. 安装SwanLab\n内容：\n使用 [pip](https://pip.pypa.io/en/stable/) 在Python3环境的计算机上安装swanlab库。\n\n打开命令行，输入：\n\n```bash\npip install swanlab\n```\n\n按下回车，等待片刻完成安装。\n\n> 如果遇到安装速度慢的问题，可以指定国内源安装：  \n> `pip install swanlab -i https://mirrors.cernet.edu.cn/pypi/web/simple`",
    "436": "一级标题：🚀快速开始\n二级标题：2. 登录账号\n内容：\n> 如果你还没有SwanLab账号，请在 [官网](https://swanlab.cn) 免费注册。\n\n打开命令行，输入：\n\n```bash\nswanlab login\n```\n\n当你看到如下提示时：\n\n```bash\nswanlab: Logging into swanlab cloud.\nswanlab: You can find your API key at: https://swanlab.cn/settings\nswanlab: Paste an API key from your profile and hit enter, or press 'CTRL-C' to quit:\n```\n\n在[用户设置](https://swanlab.cn/settings)页面复制您的 **API Key**，粘贴后按下回车（你不会看到粘贴后的API Key，请放心这是正常的），即可完成登录。之后无需再次登录。\n\n::: info\n\n如果你的计算机不太支持`swanlab login`的登录方式，也可以使用python脚本登录：\n\n```python\nimport swanlab\nswanlab.login(api_key=\"你的API Key\", save=True)\n```\n\n:::",
    "437": "一级标题：🚀快速开始\n二级标题：3. 开启一个实验并跟踪超参数\n内容：\n在Python脚本中，我们用`swanlab.init`创建一个SwanLab实验，并向`config`参数传递将一个包含超参数键值对的字典：\n\n```python\nimport swanlab\n\nrun = swanlab.init(\n    # 设置项目\n    project=\"my-project\",\n    # 跟踪超参数与实验元数据\n    config={\n        \"learning_rate\": 0.01,\n        \"epochs\": 10,\n    },\n)\n```\n\n`run`是SwanLab的基本组成部分，你将经常使用它来记录与跟踪实验指标。",
    "438": "一级标题：🚀快速开始\n二级标题：4. 记录实验指标\n内容：\n在Python脚本中，用`swanlab.log`记录实验指标（比如准确率acc和损失值loss）。\n\n用法是将一个包含指标的字典传递给`swanlab.log`：\n\n```python\nswanlab.log({\"accuracy\": acc, \"loss\": loss})\n```",
    "439": "一级标题：🚀快速开始\n二级标题：5. 完整代码，在线查看可视化看板\n内容：\n我们将上面的步骤整合为下面所示的完整代码：\n\n```python (5,25)\nimport swanlab\nimport random\n\n# 初始化SwanLab\nrun = swanlab.init(\n    # 设置项目\n    project=\"my-project\",\n    # 跟踪超参数与实验元数据\n    config={\n        \"learning_rate\": 0.01,\n        \"epochs\": 10,\n    },\n)\n\nprint(f\"学习率为{run.config.learning_rate}\")\n\noffset = random.random() / 5\n\n# 模拟训练过程\nfor epoch in range(2, run.config.epochs):\n    acc = 1 - 2**-epoch - random.random() / epoch - offset\n    loss = 2**-epoch + random.random() / epoch + offset\n    print(f\"epoch={epoch}, accuracy={acc}, loss={loss}\")\n    # 记录指标\n    swanlab.log({\"accuracy\": acc, \"loss\": loss})\n```\n\n运行代码，访问[SwanLab](https://swanlab.cn)，查看在每个训练步骤中，你使用SwanLab记录的指标（准确率和损失值）的改进情况。\n\n![quick-start-1](./quick_start/line-chart.png)",
    "440": "一级标题：🚀快速开始\n二级标题：下一步是什么\n内容：\n1. 查看SwanLab如何[记录多媒体内容](/guide_cloud/experiment_track/log-media)（图片、音频、文本、...）\n1. 查看SwanLab记录[MNIST手写体识别](/examples/mnist.md)的案例\n2. 查看与其他框架的[集成](/guide_cloud/integration/integration-pytorch-lightning.md)\n3. 查看如何通过SwanLab与[团队协作](/guide_cloud/general/organization.md)",
    "441": "一级标题：🚀快速开始\n二级标题：常见问题\n内容：\n### 1. 在哪里可以找到我的API Key？\n\n登陆SwanLab网站后，API Key将显示在[用户设置](https://swanlab.cn/settings)页面上。\n\n### 2. 我可以离线使用SwanLab吗？\n\n可以，具体流程请查看[自托管部分](/guide_cloud/self_host/docker-deploy.md)。",
    "442": "一级标题：欢迎使用SwanLab\n二级标题：无\n内容：\n[官网](https://swanlab.cn) · [框架集成](/guide_cloud/integration/integration-huggingface-transformers.html) · [Github](https://github.com/swanhubx/swanlab) · [快速开始](/guide_cloud/general/quick-start.md) · [同步Wandb](/guide_cloud/integration/integration-wandb.md#_1-同步跟踪) · [基线社区](https://swanlab.cn/benchmarks)\n\n::: warning 🎉 私有化部署版正式上线！\n私有化部署版支持在本地使用到与公有云版体验相当的功能，部署方式见[此文档](/guide_cloud/self_host/docker-deploy.md)\n:::\n\n![alt text](/assets/product-swanlab-1.png)\n\nSwanLab 是一款**开源、轻量**的 AI 模型训练跟踪与可视化工具，提供了一个**跟踪、记录、比较、和协作实验**的平台。\n\nSwanLab 面向人工智能研究者，设计了友好的Python API 和漂亮的UI界面，并提供**训练可视化、自动日志记录、超参数记录、实验对比、多人协同等功能**。在SwanLab上，研究者能基于直观的可视化图表发现训练问题，对比多个实验找到研究灵感，并通过**在线网页**的分享与基于组织的**多人协同训练**，打破团队沟通的壁垒，提高组织训练效率。\n\n借助SwanLab，科研人员可以沉淀自己的每一次训练经验，与合作者无缝地交流和协作，机器学习工程师可以更快地开发可用于生产的模型。",
    "443": "一级标题：欢迎使用SwanLab\n二级标题：📹在线演示\n内容：\n| [ResNet50 猫狗分类][demo-cats-dogs] | [Yolov8-COCO128 目标检测][demo-yolo] |\n| :--------: | :--------: |\n| [![][demo-cats-dogs-image]][demo-cats-dogs] | [![][demo-yolo-image]][demo-yolo] |\n| 跟踪一个简单的 ResNet50 模型在猫狗数据集上训练的图像分类任务。 | 使用 Yolov8 在 COCO128 数据集上进行目标检测任务，跟踪训练超参数和指标。 |\n\n| [Qwen2 指令微调][demo-qwen2-sft] | [LSTM Google 股票预测][demo-google-stock] |\n| :--------: | :--------: |\n| [![][demo-qwen2-sft-image]][demo-qwen2-sft] | [![][demo-google-stock-image]][demo-google-stock] |\n| 跟踪 Qwen2 大语言模型的指令微调训练，完成简单的指令遵循。 | 使用简单的 LSTM 模型在 Google 股价数据集上训练，实现对未来股价的预测。 |\n\n| [ResNeXt101 音频分类][demo-audio-classification] | [Qwen2-VL COCO数据集微调][demo-qwen2-vl] |\n| :--------: | :--------: |\n| [![][demo-audio-classification-image]][demo-audio-classification] | [![][demo-qwen2-vl-image]][demo-qwen2-vl] |\n| 从ResNet到ResNeXt在音频分类任务上的渐进式实验过程 | 基于Qwen2-VL多模态大模型，在COCO2014数据集上进行Lora微调。 |\n\n| [EasyR1 多模态LLM RL训练][demo-easyr1-rl] | [Qwen2.5-0.5B GRPO训练][demo-qwen2-grpo] |\n| :--------: | :--------: |\n| [![][demo-easyr1-rl-image]][demo-easyr1-rl] | [![][demo-qwen2-grpo-image]][demo-qwen2-grpo] |\n| 使用EasyR1框架进行多模态LLM RL训练 | 基于Qwen2.5-0.5B模型在GSM8k数据集上进行GRPO训练 |\n\n视频Demo：\n\n<video controls src=\"./what_is_swanlab/demo.mp4\"></video>",
    "444": "一级标题：欢迎使用SwanLab\n二级标题：SwanLab能做什么？\n内容：\n**1. 📊 实验指标与超参数跟踪**: 极简的代码嵌入您的机器学习 pipeline，跟踪记录训练关键指标\n\n- ☁️ 支持**云端**使用（类似Weights & Biases），随时随地查看训练进展。[手机看实验的方法](https://docs.swanlab.cn/guide_cloud/general/app.html)\n- 🌸 **可视化训练过程**: 通过UI界面对实验跟踪数据进行可视化，可以让训练师直观地看到实验每一步的结果，分析指标走势，判断哪些变化导致了模型效果的提升，从而整体性地提升模型迭代效率。\n- 📝 **超参数记录**、**指标总结**、**表格分析**\n- **支持的元数据类型**：标量指标、图像、音频、文本、视频、3D点云、生物化学分子、Echarts自定义图表...\n\n![swanlab-table](/assets/molecule.gif)\n\n- **支持的图表类型**：折线图、媒体图（图像、音频、文本）、3D点云、生物化学分子、柱状图、散点图、箱线图、热力图、饼状图、雷达图...\n\n![swanlab-echarts](./what_is_swanlab/echarts.png)\n\n- **LLM生成内容可视化组件**：为大语言模型训练场景打造的文本内容可视化图表，支持Markdown渲染\n\n![swanlab-llm-content](/assets/text-chart.gif)\n\n- **后台自动记录**：日志logging、硬件环境、Git 仓库、Python 环境、Python 库列表、项目运行目录\n- **断点续训记录**：支持在训练完成/中断后，补充新的指标数据到同个实验中\n\n\n**2. ⚡️ 全面的框架集成**: PyTorch、🤗HuggingFace Transformers、PyTorch Lightning、🦙LLaMA Factory、MMDetection、Ultralytics、PaddleDetetion、LightGBM、XGBoost、Keras、Tensorboard、Weights&Biases、OpenAI、Swift、XTuner、Stable Baseline3、Hydra 在内的 **40+** 框架\n\n![](/assets/integrations.png)\n\n**3. 💻 硬件监控**: 支持实时记录与监控CPU、GPU（**英伟达Nvidia**、**沐曦MetaX**、**摩尔线程MooreThread**）、NPU（**昇腾Ascend**）、MLU（**寒武纪MLU**）、XPU（**昆仑芯KunlunX**）、内存的系统级硬件指标\n\n**4. 📦 实验管理**: 通过专为训练场景设计的集中式仪表板，通过整体视图速览全局，快速管理多个项目与实验\n\n**5. 🆚 比较结果**: 通过在线表格与对比图表比较不同实验的超参数和结果，挖掘迭代灵感\n\n![](./what_is_swanlab/chart3.png)\n\n**6. 👥 在线协作**: 您可以与团队进行协作式训练，支持将实验实时同步在一个项目下，您可以在线查看团队的训练记录，基于结果发表看法与建议\n\n**7. ✉️ 分享结果**: 复制和发送持久的 URL 来共享每个实验，方便地发送给伙伴，或嵌入到在线笔记中\n\n**8. 💻 支持自托管**: 支持离线环境使用，自托管的社区版同样可以查看仪表盘与管理实验，[使用攻略](#-自托管)\n\n**9. 🔌 插件拓展**: 支持通过插件拓展SwanLab的使用场景，比如 [飞书通知](https://docs.swanlab.cn/plugin/notification-lark.html)、[Slack通知](https://docs.swanlab.cn/plugin/notification-slack.html)、[CSV记录器](https://docs.swanlab.cn/plugin/writer-csv.html)等",
    "445": "一级标题：欢迎使用SwanLab\n二级标题：为什么使用SwanLab？\n内容：\n与软件工程不同，人工智能是一个**实验性学科**，产生灵感、快速试验、验证想法 是AI研究的主旋律。而记录下实验过程和灵感，就像化学家记录实验手稿一样，是每一个AI研究者、研究组织**形成积累、提升加速度**的核心。\n\n先前的实验记录方法，是在计算机前盯着终端打印的输出，复制粘贴日志文件（或TFEvent文件），**粗糙的日志对灵感的涌现造成了障碍，离线的日志文件让研究者之间难以形成合力**。\n\n与之相比，SwanLab提供了一套云端AI实验跟踪方案，面向训练过程，提供了训练可视化、实验跟踪、超参数记录、日志记录、多人协同等功能，研究者能轻松**通过直观的可视化图表找到迭代灵感，并且通过在线链接的分享与基于组织的多人协同训练**，打破团队沟通的壁垒。\n\n> 以往的AI研究的分享和开源更关注结果，而我们更关注过程。<br>\n> 社区用户对SwanLab的产品评价可以归结为**简洁易用、提升效率与迭代迅速**<br>\n> ——泽毅，SwanLab 联合创始人\n\n<img src=\"./what_is_swanlab/carton.png\" width=\"350\">\n\n更重要的是，SwanLab是开源的，由一帮热爱开源的机器学习工程师与社区共同构建，我们提供了完全自托管的版本，可以保证你的数据安全与隐私性。\n\n希望以上信息和这份指南可以帮助你了解这款产品，我们相信 SwanLab 能够帮助到你。",
    "446": "一级标题：欢迎使用SwanLab\n二级标题：从哪里开始\n内容：\n- [快速开始](/guide_cloud/general/quick-start.md): SwanLab入门教程，五分钟玩转实验跟踪！\n- [API文档](/api/api-index.md): 完整的API文档\n- [在线支持](/guide_cloud/community/online-support.md): 加入社区、反馈问题和联系我们\n- [自托管](/guide_cloud/self_host/docker-deploy.md): 自托管（私有化部署）使用方式教程\n- [案例](/examples/mnist.md): 查看SwanLab与各个深度学习任务的案例",
    "447": "一级标题：欢迎使用SwanLab\n二级标题：与熟悉产品的对比\n内容：\n### Tensorboard vs SwanLab\n\n- **☁️支持在线使用**：\n  通过SwanLab可以方便地将训练实验在云端在线同步与保存，便于远程查看训练进展、管理历史项目、分享实验链接、发送实时消息通知、多端看实验等。而Tensorboard是一个离线的实验跟踪工具。\n\n- **👥多人协作**：\n  在进行多人、跨团队的机器学习协作时，通过SwanLab可以轻松管理多人的训练项目、分享实验链接、跨空间交流讨论。而Tensorboard主要为个人设计，难以进行多人协作和分享实验。\n\n- **💻持久、集中的仪表板**：\n  无论你在何处训练模型，无论是在本地计算机上、在实验室集群还是在公有云的GPU实例中，你的结果都会记录到同一个集中式仪表板中。而使用TensorBoard需要花费时间从不同的机器复制和管理 TFEvent文件。\n  \n- **💪更强大的表格**：\n  通过SwanLab表格可以查看、搜索、过滤来自不同实验的结果，可以轻松查看数千个模型版本并找到适合不同任务的最佳性能模型。 TensorBoard 不适用于大型项目。  \n\n\n### W&B vs SwanLab\n\n- Weights and Biases 是一个必须联网使用的闭源MLOps平台\n\n- SwanLab 不仅支持联网使用，也支持开源、免费、自托管的版本",
    "448": "一级标题：欢迎使用SwanLab\n二级标题：训练框架集成\n内容：\n将你最喜欢的框架与 SwanLab 结合使用！  \n下面是我们已集成的框架列表，欢迎提交 [Issue](https://github.com/swanhubx/swanlab/issues) 来反馈你想要集成的框架。\n\n**基础框架**\n- [PyTorch](/guide_cloud/integration/integration-pytorch.html)\n- [MindSpore](/guide_cloud/integration/integration-ascend.html)\n- [Keras](/guide_cloud/integration/integration-keras.html)\n\n**专有/微调框架**\n- [PyTorch Lightning](/guide_cloud/integration/integration-pytorch-lightning.html)\n- [HuggingFace Transformers](/guide_cloud/integration/integration-huggingface-transformers.html)\n- [LLaMA Factory](/guide_cloud/integration/integration-llama-factory.html)\n- [Modelscope Swift](/guide_cloud/integration/integration-swift.html)\n- [DiffSynth-Studio](/guide_cloud/integration/integration-diffsynth-studio.html)\n- [Sentence Transformers](/guide_cloud/integration/integration-sentence-transformers.html)\n- [OpenMind](https://modelers.cn/docs/zh/openmind-library/1.0.0/basic_tutorial/finetune/finetune_pt.html#%E8%AE%AD%E7%BB%83%E7%9B%91%E6%8E%A7)\n- [Torchtune](/guide_cloud/integration/integration-pytorch-torchtune.html)\n- [XTuner](/guide_cloud/integration/integration-xtuner.html)\n- [MMEngine](/guide_cloud/integration/integration-mmengine.html)\n- [FastAI](/guide_cloud/integration/integration-fastai.html)\n- [LightGBM](/guide_cloud/integration/integration-lightgbm.html)\n- [XGBoost](/guide_cloud/integration/integration-xgboost.html)\n\n\n**计算机视觉**\n- [Ultralytics](/guide_cloud/integration/integration-ultralytics.html)\n- [MMDetection](/guide_cloud/integration/integration-mmdetection.html)\n- [MMSegmentation](/guide_cloud/integration/integration-mmsegmentation.html)\n- [PaddleDetection](/guide_cloud/integration/integration-paddledetection.html)\n- [PaddleYOLO](/guide_cloud/integration/integration-paddleyolo.html)\n\n**强化学习**\n- [Stable Baseline3](/guide_cloud/integration/integration-sb3.html)\n- [veRL](/guide_cloud/integration/integration-verl.html)\n- [HuggingFace trl](/guide_cloud/integration/integration-huggingface-trl.html)\n- [EasyR1](/guide_cloud/integration/integration-easyr1.html)\n- [AReaL](/guide_cloud/integration/integration-areal.html)\n- [ROLL](/guide_cloud/integration/integration-roll.html)\n\n**其他框架：**\n- [Tensorboard](/guide_cloud/integration/integration-tensorboard.html)\n- [Weights&Biases](/guide_cloud/integration/integration-wandb.html)\n- [MLFlow](/guide_cloud/integration/integration-mlflow.html)\n- [HuggingFace Accelerate](/guide_cloud/integration/integration-huggingface-accelerate.html)\n- [Hydra](/guide_cloud/integration/integration-hydra.html)\n- [Omegaconf](/guide_cloud/integration/integration-omegaconf.html)\n- [OpenAI](/guide_cloud/integration/integration-openai.html)\n- [ZhipuAI](/guide_cloud/integration/integration-zhipuai.html)\n\n[更多集成](/guide_cloud/integration/integration-pytorch-lightning.html)",
    "449": "一级标题：欢迎使用SwanLab\n二级标题：在线支持\n内容：\n- **[GitHub Issues](https://github.com/SwanHubX/SwanLab/issues)**：反馈使用SwanLab时遇到的错误和问题\n\n- **电子邮件支持**：反馈关于使用SwanLab的问题\n  - 产品: <contact@swanlab.cn>, <zeyi.lin@swanhub.co>(产品经理邮箱)\n\n- **微信群与飞书群**: 见[在线支持](/guide_cloud/community/online-support.md)\n\n- **微信公众号**:\n\n<div align=\"center\">\n<img src=\"/assets/wechat_public_account.jpg\" width=300>\n</div>\n\n\n<!-- link -->\n\n[release-shield]: https://img.shields.io/github/v/release/swanhubx/swanlab?color=369eff&labelColor=black&logo=github&style=flat-square\n[release-link]: https://github.com/swanhubx/swanlab/releases\n\n[license-shield]: https://img.shields.io/badge/license-apache%202.0-white?labelColor=black&style=flat-square\n[license-shield-link]: https://github.com/SwanHubX/SwanLab/blob/main/LICENSE\n\n[last-commit-shield]: https://img.shields.io/github/last-commit/swanhubx/swanlab?color=c4f042&labelColor=black&style=flat-square\n[last-commit-shield-link]: https://github.com/swanhubx/swanlab/commits/main\n\n[pypi-version-shield]: https://img.shields.io/pypi/v/swanlab?color=orange&labelColor=black&style=flat-square\n[pypi-version-shield-link]: https://pypi.org/project/swanlab/\n\n[pypi-downloads-shield]: https://static.pepy.tech/badge/swanlab?labelColor=black&style=flat-square\n[pypi-downloads-shield-link]: https://pepy.tech/project/swanlab\n\n[swanlab-cloud-shield]: https://img.shields.io/badge/Product-SwanLab云端版-636a3f?labelColor=black&style=flat-square\n[swanlab-cloud-shield-link]: https://swanlab.cn/\n\n[wechat-shield]: https://img.shields.io/badge/WeChat-微信-4cb55e?labelColor=black&style=flat-square\n[wechat-shield-link]: https://docs.swanlab.cn/guide_cloud/community/online-support.html\n\n[colab-shield]: https://colab.research.google.com/assets/colab-badge.svg\n[colab-shield-link]: https://colab.research.google.com/drive/1RWsrY_1bS8ECzaHvYtLb_1eBkkdzekR3?usp=sharing\n\n[github-stars-shield]: https://img.shields.io/github/stars/swanhubx/swanlab?labelColor&style=flat-square&color=ffcb47\n[github-stars-link]: https://github.com/swanhubx/swanlab\n\n[github-issues-shield]: https://img.shields.io/github/issues/swanhubx/swanlab?labelColor=black&style=flat-square&color=ff80eb\n[github-issues-shield-link]: https://github.com/swanhubx/swanlab/issues\n\n[github-contributors-shield]: https://img.shields.io/github/contributors/swanhubx/swanlab?color=c4f042&labelColor=black&style=flat-square\n[github-contributors-link]: https://github.com/swanhubx/swanlab/graphs/contributors\n\n[demo-cats-dogs]: https://swanlab.cn/@ZeyiLin/Cats_Dogs_Classification/runs/jzo93k112f15pmx14vtxf/chart\n[demo-cats-dogs-image]: /assets/example-catsdogs.png\n\n[demo-yolo]: https://swanlab.cn/@ZeyiLin/ultratest/runs/yux7vclmsmmsar9ear7u5/chart\n[demo-yolo-image]: /assets/example-yolo.png\n\n[demo-qwen2-sft]: https://swanlab.cn/@ZeyiLin/Qwen2-fintune/runs/cfg5f8dzkp6vouxzaxlx6/chart\n[demo-qwen2-sft-image]: /assets/example-qwen2.png\n\n[demo-google-stock]:https://swanlab.cn/@ZeyiLin/Google-Stock-Prediction/charts\n[demo-google-stock-image]: /assets/example-lstm.png\n\n[demo-audio-classification]:https://swanlab.cn/@ZeyiLin/PyTorch_Audio_Classification/charts\n[demo-audio-classification-image]: /assets/example-audio-classification.png\n\n[demo-qwen2-vl]:https://swanlab.cn/@ZeyiLin/Qwen2-VL-finetune/runs/pkgest5xhdn3ukpdy6kv5/chart\n[demo-qwen2-vl-image]: /assets/example-qwen2-vl.jpg\n\n[demo-easyr1-rl]:https://swanlab.cn/@Kedreamix/easy_r1/runs/wzezd8q36bb6dlza6wtpc/chart\n[demo-easyr1-rl-image]: /assets/example-easyr1-rl.png\n\n[demo-qwen2-grpo]:https://swanlab.cn/@kmno4/Qwen-R1/runs/t0zr3ak5r7188mjbjgdsc/chart\n[demo-qwen2-grpo-image]: /assets/example-qwen2-grpo.png\n\n[tracking-swanlab-shield-link]:https://swanlab.cn\n[tracking-swanlab-shield]: https://raw.githubusercontent.com/SwanHubX/assets/main/badge2.svg\n\n[visualize-swanlab-shield-link]:https://swanlab.cn\n[visualize-swanlab-shield]: https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg\n\n[dockerhub-shield]: https://img.shields.io/docker/v/swanlab/swanlab-next?color=369eff&label=docker&labelColor=black&logoColor=white&style=flat-square\n[dockerhub-link]: https://hub.docker.com/r/swanlab/swanlab-next/tags",
    "450": "一级标题：⚡️集成框架一览\n二级标题：无\n内容：\n将你最喜欢的框架与 SwanLab 结合使用！  \n下面是我们已集成的框架列表，欢迎提交 [Issue](https://github.com/swanhubx/swanlab/issues) 来反馈你想要集成的框架。",
    "451": "一级标题：⚡️集成框架一览\n二级标题：基础框架\n内容：\n- [PyTorch](/guide_cloud/integration/integration-pytorch.html)\n- [MindSpore](/guide_cloud/integration/integration-ascend.html)\n- [Keras](/guide_cloud/integration/integration-keras.html)",
    "452": "一级标题：⚡️集成框架一览\n二级标题：专有/微调框架\n内容：\n- [PyTorch Lightning](/guide_cloud/integration/integration-pytorch-lightning.html)\n- [HuggingFace Transformers](/guide_cloud/integration/integration-huggingface-transformers.html)\n- [LLaMA Factory](/guide_cloud/integration/integration-llama-factory.html)\n- [Modelscope Swift](/guide_cloud/integration/integration-swift.html)\n- [DiffSynth-Studio](/guide_cloud/integration/integration-diffsynth-studio.html)\n- [Sentence Transformers](/guide_cloud/integration/integration-sentence-transformers.html)\n- [PaddleNLP](/guide_cloud/integration/integration-paddlenlp.html)\n- [OpenMind](https://modelers.cn/docs/zh/openmind-library/1.0.0/basic_tutorial/finetune/finetune_pt.html#%E8%AE%AD%E7%BB%83%E7%9B%91%E6%8E%A7)\n- [Torchtune](/guide_cloud/integration/integration-pytorch-torchtune.html)\n- [XTuner](/guide_cloud/integration/integration-xtuner.html)\n- [MMEngine](/guide_cloud/integration/integration-mmengine.html)\n- [FastAI](/guide_cloud/integration/integration-fastai.html)\n- [LightGBM](/guide_cloud/integration/integration-lightgbm.html)\n- [XGBoost](/guide_cloud/integration/integration-xgboost.html)",
    "453": "一级标题：⚡️集成框架一览\n二级标题：评估框架\n内容：\n- [EvalScope](/guide_cloud/integration/integration-evalscope.html)",
    "454": "一级标题：⚡️集成框架一览\n二级标题：计算机视觉\n内容：\n- [Ultralytics](/guide_cloud/integration/integration-ultralytics.html)\n- [MMDetection](/guide_cloud/integration/integration-mmdetection.html)\n- [MMSegmentation](/guide_cloud/integration/integration-mmsegmentation.html)\n- [PaddleDetection](/guide_cloud/integration/integration-paddledetection.html)\n- [PaddleYOLO](/guide_cloud/integration/integration-paddleyolo.html)",
    "455": "一级标题：⚡️集成框架一览\n二级标题：强化学习\n内容：\n- [Stable Baseline3](/guide_cloud/integration/integration-sb3.html)\n- [veRL](/guide_cloud/integration/integration-verl.html)\n- [HuggingFace trl](/guide_cloud/integration/integration-huggingface-trl.html)\n- [EasyR1](/guide_cloud/integration/integration-easyr1.html)\n- [AReaL](/guide_cloud/integration/integration-areal.html)\n- [ROLL](/guide_cloud/integration/integration-roll.html)",
    "456": "一级标题：⚡️集成框架一览\n二级标题：其他框架\n内容：\n- [Tensorboard](/guide_cloud/integration/integration-tensorboard.html)\n- [Weights&Biases](/guide_cloud/integration/integration-wandb.html)\n- [MLFlow](/guide_cloud/integration/integration-mlflow.html)\n- [HuggingFace Accelerate](/guide_cloud/integration/integration-huggingface-accelerate.html)\n- [Ray](/guide_cloud/integration/integration-ray.html)\n- [Hydra](/guide_cloud/integration/integration-hydra.html)\n- [Omegaconf](/guide_cloud/integration/integration-omegaconf.html)\n- [OpenAI](/guide_cloud/integration/integration-openai.html)\n- [ZhipuAI](/guide_cloud/integration/integration-zhipuai.html)",
    "457": "一级标题：将SwanLab集成到你的库\n二级标题：无\n内容：\n本指南提供了如何将SwanLab集成到您的Python库中的最佳实践，以获得强大的实验跟踪、GPU和系统监控、超参数记录等功能。\n\n下面我们将介绍，如果您正在处理的代码库比单个 Python 训练脚本或 Jupyter Notebook 更复杂时，我们整理的最佳实践。\n\n**🪵目录：**\n\n[[toc]]",
    "458": "一级标题：将SwanLab集成到你的库\n二级标题：1. 补充Requirements\n内容：\n在开始之前，请决定是否在您的库的依赖项中要求 SwanLab：\n\n### 1.1 将swanlab作为依赖项\n\n```plaintext\ntorch==2.5.0\n...\nswanlab==0.4.*\n```\n\n### 1.2 将swanlab作为可选安装\n\n有两种设置swanlab成为可选安装的方法。\n\n1. 在代码中使用try-except语句，当用户没有安装swanlab时，抛出错误。\n\n```python\ntry:\n    import swanlab\nexcept ImportError:\n    raise ImportError(\n        \"You are trying to use swanlab which is not currently installed.\"\n        \"Please install it using pip install swanlab\"\n    )\n```\n\n2. 如果你要构建Python包，请将`swanlab`作为可选依赖项添加到`pyproject.toml`文件中：\n\n```toml\n[project]\nname = \"my_awesome_lib\"\nversion = \"0.1.0\"\ndependencies = [\n    \"torch\",\n    \"transformers\"\n]\n\n[project.optional-dependencies]\ndev = [\n    \"swanlab\"\n]\n```",
    "459": "一级标题：将SwanLab集成到你的库\n二级标题：2. 用户登录\n内容：\n您的用户有几种方法可以登录SwanLab：\n\n::: code-group\n\n```bash [命令行]\nswanlab login\n```\n\n```python [Python]\nimport swanlab\nswanlab.login()\n```\n\n```bash [环境变量(Bash)]\nexport SWANLAB_API_KEY=$YOUR_API_KEY\n```\n\n```python [环境变量(Python)]\nimport os\nos.environ[\"SWANLAB_API_KEY\"] = \"zxcv1234...\"\n```\n\n:::\n\n如果用户是第一次使用`swanlab`而没有遵循上述任何步骤，则当您的脚本调用`swanlab.init`时，系统会自动提示他们登录。",
    "460": "一级标题：将SwanLab集成到你的库\n二级标题：3. 启动SwanLab实验\n内容：\n实验是SwanLab的计算单元。通常，你可以为每个实验创建一个`Experiment`对象，并使用`swanlab.init`方法启动实验。\n\n### 3.1 初始化实验\n\n初始化SwanLab，并在您的代码种启动实验：\n\n```python\nswanlab.init()\n```\n\n你可以为这个实验提供项目名、实验名、工作空间等参数：\n\n```python\nswanlab.init(\n    project=\"my_project\",\n    experiment_name=\"my_experiment\",\n    workspace=\"my_workspace\",\n    )\n```\n\n::: warning 最好把 swanlab.init 放在哪里？\n\n您的库应该尽早创建SwanLab实验，因为SwanLab会自动收集控制台中的任何输出，这将使得调试更加容易。\n\n:::\n\n### 3.2 配置三种启动模式\n\n你可以通过`mode`参数来配置SwanLab的启动模式：\n\n::: code-group\n\n```python [云端模式]\nswanlab.init(\n    mode=\"cloud\",  # 默认模式\n    )\n```\n\n```python [本地模式]\nswanlab.init(\n    mode=\"local\",\n    )\n```\n\n```python [禁用模式]\nswanlab.init(\n    mode=\"disabled\",\n    )\n```\n\n:::\n\n- **云端模式**：默认模式。SwanLab会将实验数据上传到一个web服务器（SwanLab官方云或您自行部署的私有云）。\n- **本地模式**：SwanLab不会将实验数据上传到云端，但会记录一个特殊的`swanlog`目录，可以被`dashboard`插件打开进行可视化。\n- **禁用模式**：SwanLab不会收集任何数据，代码执行到`swanlab`相关代码时将不做任何处理。\n\n### 3.3 定义实验超参数/配置\n\n使用swanlab实验配置(config)，您可以在创建SwanLab实验时提供有关您的模型、数据集等的元数据。您可以使用这些信息来比较不同的实验并快速了解主要差异。\n\n您可以记录的典型配置参数包括：\n\n- 模型名称、版本、架构参数等\n- 数据集名称、版本、训练/测试数据数等。\n- 训练参数，例如学习率、批量大小、优化器等。\n\n以下代码片段显示了如何记录配置：\n\n```python\nconfig = {\"learning_rate\": 0.001, ...}\nswanlab.init(..., config=config)\n```\n\n**更新配置**：\n\n使用`swanlab.config.update`方法来更新配置。在定义config字典后获取参数时，用此方法更新config字典非常方便。\n\n例如，你可能希望在实例化模型后，添加模型的参数：\n\n```python\nswanlab.config.update({\"model_params\": \"1.5B\"})\n```",
    "461": "一级标题：将SwanLab集成到你的库\n二级标题：4. 记录数据到SwanLab\n内容：\n创建一个字典，其中key是指标的名称，value是指标的值。将此字典对象传递给`swanlab.log`：\n\n::: code-group\n\n```python [记录一组指标]\nmetrics = {\"loss\": 0.5, \"accuracy\": 0.8}\nswanlab.log(metrics)\n```\n\n```python [循环记录指标]\nfor epoch in range(NUM_EPOCHS):\n    for input, ground_truth in data:\n        prediction = model(input)\n        loss = loss_fn(prediction, ground_truth)\n        metrics = { \"loss\": loss }\n        swanlab.log(metrics)\n```\n\n:::\n\n如果您有很多指标，则可以在指标名称中使用前缀（如 `train/...` 和 `val/...`）。在 UI 中，SwanLab将自动对它们进行分组，来隔离不同门类的图表数据：\n\n```python\nmetrics = {\n    \"train/loss\": 0.5,\n    \"train/accuracy\": 0.8,\n    \"val/loss\": 0.6,\n    \"val/accuracy\": 0.7,\n}\nswanlab.log(metrics)\n```\n\n有关`swanlab.log`的更多信息，请参阅[记录指标](../experiment_track/log-experiment-metric)章节。",
    "462": "一级标题：将SwanLab集成到你的库\n二级标题：5. 高级集成\n内容：\n您还可以在以下集成中查看高级 SwanLab 集成的形态：\n\n- [HuggingFace Transformers](../integration/integration-huggingface-transformers.md)\n- [PyTorch Lightning](../integration/integration-pytorch-lightning.md)",
    "463": "一级标题：AREAL\n二级标题：无\n内容：\n[AReaL](https://github.com/inclusionAI/AReaL)（Ant Reasoning RL）是由蚂蚁研究院强化学习实验室（RL Lab） 开发的一套开源 、完全异步的强化学习训练系统， 适用于大型推理模型。该系统基于开源项目 [RealHF](https://github.com/openpsi-project/ReaLHF) 致力于开源，提供训练细节、数据以及复现结果所需的基础设施，并提供模型本身。\n\n<img src=\"./areal/logo.png\" width=\"200\">\n\nAReaL项目已集成SwanLab，指引可见此文档：[Areal - monitoring-the-training-process](https://inclusionai.github.io/AReaL/tutorial/quickstart.html#monitoring-the-training-process)",
    "464": "一级标题：Argparse\n二级标题：无\n内容：\n`argparse` 是 Python 标准库中的一个模块，用于解析命令行参数和选项。通过 argparse，开发者可以轻松地编写用户友好的命令行接口，定义命令行参数的名称、类型、默认值、帮助信息等。\n\n`argparse` 与swanlab的集成非常简单，直接将创建好的argparse对象传递给swanlab.config，即可记录为超参数：\n\n```python\nimport argparse\nimport swanlab\n\n# 初始化Argparse\nparser = argparse.ArgumentParser()\nparser.add_argument('--epochs', default=20)\nparser.add_argument('--lr', default=0.001)\nargs = parser.parse_args()\n\nswanlab.init(config=args)\n```\n\n运行案例：\n```bash\npython main.py --epochs 100 --lr 1e-4\n```\n\n![alt text](/assets/ig-argparse.png)",
    "465": "一级标题：Ascend NPU & MindSpore\n二级标题：无\n内容：\nSwanLab支持[Ascend系列显卡](https://www.hiascend.com/)的硬件检测和[mindspore](https://www.mindspore.cn/)项目的训练跟踪。（计划20241215硬件监控上线）\n\nSwanLab实验记录Ascend NPU信息截图：\n\n![device](/assets/guide_cloud/integration/ascend/device_mask.png)",
    "466": "一级标题：Ascend NPU & MindSpore\n二级标题：简介\n内容：\n本案例使用实现的IMDB数据集情感分类任务。并使用SwanLab跟踪模型训练进展。",
    "467": "一级标题：Ascend NPU & MindSpore\n二级标题：任务介绍\n内容：\nIMDB情感分类任务是一种自然语言处理任务，旨在分析IMDB（Internet Movie Database）电影评论中的文本内容，以判断评论的情感倾向，通常分为正面（Positive）和负面（Negative）两类。该任务广泛用于研究情感分析技术，尤其是在监督学习和深度学习领域。\n\n数据集中通常包含预处理好的评论文本及其对应的情感标签，每条评论均标注为正面或负面。如下图：\n\n![data_image](/assets/guide_cloud/integration/ascend/data_image.png)\n\nLSTM（Long Short-Term Memory）是一种改进的循环神经网络，专为处理和预测序列数据中的长距离依赖而设计。与传统RNN相比，LSTM通过引入**记忆单元**和**门机制**，能够有效缓解梯度消失和梯度爆炸问题，使其在长序列数据的建模中表现优异。使用LSTM能轻松完成IMDB的语言情感分类任务。关于LSTM的具体原理建议参考[大神博客](https://blog.csdn.net/zhaojc1995/article/details/80572098)\n\n![lstm](/assets/guide_cloud/integration/ascend/lstm.png)\n\n本代码参考[MindSpore官方文档](https://www.mindspore.cn/tutorials/zh-CN/r2.4.1/nlp/sentiment_analysis.html#%E6%95%B0%E6%8D%AE%E9%9B%86%E9%A2%84%E5%A4%84%E7%90%86)，进行整理并简化了部分实现.",
    "468": "一级标题：Ascend NPU & MindSpore\n二级标题：环境安装\n内容：\n### 克隆项目\n\n附上[github项目链接](https://github.com/ShaohonChen/mindspore_imdb_train.git)和下载命令\n\n```bash\ngit clone https://github.com/ShaohonChen/mindspore_imdb_train.git\n```\n\n如果访问不了github可在本博客后文找到[代码章节](#代码章节)\n\n推荐还是用github ;)\n\n### CPU环境安装\n\n可以在CPU环境下安装MindSpore，虽然看起来没有Pytorch那么好用，但实际上文档还是写的很细的，真的很细，看得出华为工程师的严谨orz。配合sheng腾卡使用的话是非常有潜力的框架（MAC死活打不出sheng字）。\n\n官方安装文档[link](https://www.mindspore.cn/install/)\n\n也可以直接使用如下命令安装：\n\n```bash\npip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/2.4.1/MindSpore/unified/x86_64/mindspore-2.4.1-cp311-cp311-linux_x86_64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.tuna.tsinghua.edu.cn/simple\n```\n\n验证安装成功命令：\n\n```bash\npython -c \"import mindspore;mindspore.set_context(device_target='CPU');mindspore.run_check()\"\n```\n\n如果输出如下信息说明MindSpore安装成功了：\n\n```bash\nMindSpore version: 2.4.1\nThe result of multiplication calculation is correct, MindSpore has been installed on platform [CPU] successfully!\n```\n\n### 华为Ascend NPU显卡环境安装\n\n由于华为Ascend环境安装较为复杂，建议参考[MindSpore安装教程和踩坑记录](///)教程完成MindSpore环境安装。下面简述MindSpore安装过程\n\n>本博客写的时间是2024年12月6日，安装的版本是**MindSpore2.4.1**，因为感觉MindSpore变动会比较大特意记录一下时间和版本。\n\n#### 驱动安装&验证\n\n首先得确定有NPU卡和NPU相关驱动，驱动是**8.0.RC3.beta1**，如果没安装可以参考[CANN官方安装教程](https://www.hiascend.com/document/detail/zh/canncommercial/80RC3/softwareinst/instg/instg_0000.html?Mode=PmIns&OS=Ubuntu&Software=cannToolKit)\n\n完成安装后检测方法是运行\n\n```bash\nnpu-smi info\n```\n\n可以看到如下信息的话就表示驱动已经安装完成了。\n\n![npu-smi](/assets/guide_cloud/integration/ascend/a_mask.png)\n\n#### 安装MindSpore\n\n个人比较推荐使用conda安装，这样环境比较好管理，自动安装的依赖项也比较多\n\n首先需要安装前置依赖的包：\n\n```bash\npip install sympy\npip install \"numpy>=1.20.0,<2.0.0\"\npip install /usr/local/Ascend/ascend-toolkit/latest/lib64/te-*-py3-none-any.whl\npip install /usr/local/Ascend/ascend-toolkit/latest/lib64/hccl-*-py3-none-any.whl\n```\n\n如果本地下载比较慢可以使用带国内源版本的命令\n\n```bash\npip install sympy -i https://mirrors.cernet.edu.cn/pypi/web/simple\npip install \"numpy>=1.20.0,<2.0.0\" -i https://mirrors.cernet.edu.cn/pypi/web/simple\npip install /usr/local/Ascend/ascend-toolkit/latest/lib64/te-*-py3-none-any.whl -i https://mirrors.cernet.edu.cn/pypi/web/simple\npip install /usr/local/Ascend/ascend-toolkit/latest/lib64/hccl-*-py3-none-any.whl  -i https://mirrors.cernet.edu.cn/pypi/web/simple\n```\n\nconda安装MindSpore方法如下：\n\n```bash\nconda install mindspore=2.4.1 -c mindspore -c conda-forge\n```\n\n因为某些众所周知的原因，有时候conda源会失效，反应出来就是conda安装mindspore时会进度一直为0%，如下图：\n\n![condainstallfailed](/assets/guide_cloud/integration/ascend/b.png)\n\n可以使用如下方法指定国内源：\n\n```bash\nconda install mindspore=2.4.1 -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/MindSpore/ -c conda-forge\n```\n\npip安装MindSpore命令如下：\n\n```bash\npip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/2.4.1/MindSpore/unified/aarch64/mindspore-2.4.1-cp311-cp311-linux_aarch64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.tuna.tsinghua.edu.cn/simple\n```\n\n安装完成后可以使用如下命令进行测试\n\n```bash\npython -c \"import mindspore;mindspore.set_context(device_target='Ascend');mindspore.run_check()\"\n```\n\n如果这步出现报错可以参考本文后面[环境安装疑难杂症](#环境安装疑难杂症)章节\n\n出现版本号信息和计算验证便意味着安装成功\n\n```bash\nMindSpore version:  2.4.1\nThe result of multiplication calculation is correct, MindSpore has been installed on platform [Ascend] successfully!\n```\n\n也附上官方安装教程链接[mindspore官方安装教程](https://www.mindspore.cn/install)，注意本教程使用的是[Mindspore 2.4.1](https://www.mindspore.cn/versions#2.4.1)，建议环境与本教程保持一致。\n\n此外本教程使用[SwanLab](https://swanlab.cn)进行训练过程跟踪，SwanLab支持对Ascend系列NPU进行硬件识别和跟踪。\n\n### 记得安装SwanLab ;)\n\n安装方法：\n\n```bash\npip install swanlab\n```",
    "469": "一级标题：Ascend NPU & MindSpore\n二级标题：数据集&词编码文件准备\n内容：\n### 数据集准备\n\nLinux使用如下命令完成下载+解压\n\n```bash\nwget -P ./data/ https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\ntar -xzvf data/aclImdb_v1.tar.gz -C data/\n```\n\n如果下载太慢可以使用[华为云提供的国内链接](https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/aclImdb_v1.tar.gz)下载。并且在`./data/`目录下解压。\n\n> 如果解压不了tar.gz推荐安装[7zip解压器](https://www.7-zip.org/)，开源且通用的解压器\n\n### 词编码器准备\n\n使用如下命令下载+解压词编码器文件\n\n```bash\nwget -P ./embedding/ https://nlp.stanford.edu/data/glove.6B.zip\nunzip embedding/glove.6B.zip -d embedding/\n```\n\n如果下载太慢可以使用[华为云提供的国内链接](https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/glove.6B.zip)下载。并且在`./embedding/`目录下解压。",
    "470": "一级标题：Ascend NPU & MindSpore\n二级标题：开始训练\n内容：\n使用如下命令开始训练\n\n```\npython train.py\n```\n\n可是这\n\n> 如果提示登录swanlab，可以参考[如何登录SwanLab](https://docs.swanlab.cn/guide_cloud/general/quick-start.html#_2-%E7%99%BB%E5%BD%95%E8%B4%A6%E5%8F%B7)，这样将能够使用**云上看版**随时查看训练过程与结果。\n\n完成设置便可以在云上实时看到训练进展，我的实验记录可参考[完整实验记录](https://swanlab.cn/@ShaohonChen/Ascend_IMDB_CLS/charts)\n\n![log_img](/assets/guide_cloud/integration/ascend/log_img.png)\n\n并且附上其他脚本与在线实验记录：\n\n| 内容  | 训练命令  | 实验log  |\n|--------|--------|--------|\n| 基线 | `python train.py configs/baseline.json` | [log](https://swanlab.cn/@ShaohonChen/Ascend_IMDB_CLS/runs/qhl47nxl23tc4oycr6pmg/chart) |\n| CPU运行 | `python train.py configs/baseline.json CPU` | [log](https://swanlab.cn/@ShaohonChen/Ascend_IMDB_CLS/runs/s60wuicmwaitxe2v401ry/chart) |\n| 双层LSTM | `python train.py configs/two_layer.json` | [log](https://swanlab.cn/@ShaohonChen/Ascend_IMDB_CLS/runs/ydrgxvnqhjfrimzdj3oh4/chart) |\n| 小batch数 | `python train.py configs/small_batch.json` | [log](https://swanlab.cn/@ShaohonChen/Ascend_IMDB_CLS/runs/uovjgenfzcnxrl9gup900/chart) |\n| 隐藏层加大 | `python train.py configs/large_hs.json` | [log](https://swanlab.cn/@ShaohonChen/Ascend_IMDB_CLS/runs/eki6pa1him482w4jcc7gn/chart) |\n| 学习率加大 | `python train.py configs/large_hs.json` | [log](https://swanlab.cn/@ShaohonChen/Ascend_IMDB_CLS/runs/if3o10o6nf3am87f4ou62/chart) |\n\n相关超参数和最终结果可在[图标视图查看](https://swanlab.cn/@ShaohonChen/Ascend_IMDB_CLS/overview)\n\n![log_table](/assets/guide_cloud/integration/ascend/log_table.png)\n\n> PS: 观察了下日志，发现还是训练量不足，应该增大些训练量（40-50epoch比较合适）",
    "471": "一级标题：Ascend NPU & MindSpore\n二级标题：代码章节\n内容：\n如果访问不了github也提供一段测试代码，不过就是没法使用其他超参数了T_T\n\n```python\n# 读取训练参数+初始化日志记录\nimport os\nimport sys\nimport json\nimport mindspore as ms\nimport swanlab\n\n# ms.set_context(device_target=\"CPU\") # 使用CPU\nms.set_context(device_target=\"Ascend\")  # 使用NPU\n\nargs={  # 超参数\n    \"hidden_size\": 256,\n    \"output_size\": 1,\n    \"num_layers\": 2,\n    \"lr\": 0.001,\n    \"num_epochs\": 10,\n    \"batch_size\": 64,\n    \"report_interval\": 10\n}\n\nexp_name = \"baseline\"\nswanlab.init(project=\"Ascend_IMDB_CLS\", experiment_name=exp_name, config=args)\n\n\n# 构造数据集\nimport mindspore.dataset as ds\n\n\nclass IMDBData:\n    label_map = {\"pos\": 1, \"neg\": 0}\n\n    def __init__(self, path, mode=\"train\"):\n        self.docs, self.labels = [], []\n        for label in self.label_map.keys():\n            doc_dir = os.path.join(path, mode, label)\n            doc_list = os.listdir(doc_dir)\n            for fname in doc_list:\n                with open(os.path.join(doc_dir, fname)) as f:\n                    doc = f.read()\n                    doc = doc.lower().split()\n                    self.docs.append(doc)\n                    self.labels.append([self.label_map[label]])\n\n    def __getitem__(self, idx):\n        return self.docs[idx], self.labels[idx]\n\n    def __len__(self):\n        return len(self.docs)\n\n\nimdb_path = \"data/aclImdb\"\nimdb_train = ds.GeneratorDataset(\n    IMDBData(imdb_path, \"train\"), column_names=[\"text\", \"label\"], shuffle=True\n)\nimdb_test = ds.GeneratorDataset(\n    IMDBData(imdb_path, \"test\"), column_names=[\"text\", \"label\"], shuffle=False\n)\n\n# 构造embedding词表\nimport numpy as np\n\n\ndef load_glove(glove_path):\n    embeddings = []\n    tokens = []\n    with open(os.path.join(glove_path, \"glove.6B.100d.txt\"), encoding=\"utf-8\") as gf:\n        for glove in gf:\n            word, embedding = glove.split(maxsplit=1)\n            tokens.append(word)\n            embeddings.append(np.fromstring(embedding, dtype=np.float32, sep=\" \"))\n    # 添加 <unk>, <pad> 两个特殊占位符对应的embedding\n    embeddings.append(np.random.rand(100))\n    embeddings.append(np.zeros((100,), np.float32))\n\n    vocab = ds.text.Vocab.from_list(\n        tokens, special_tokens=[\"<unk>\", \"<pad>\"], special_first=False\n    )\n    embeddings = np.array(embeddings).astype(np.float32)\n    return vocab, embeddings\n\n\nvocab, embeddings = load_glove(\"./embedding\")\nprint(f\"VOCAB SIZE: {len(vocab.vocab())}\")\n\n# 数据预处理\nimport mindspore as ms\n\nlookup_op = ds.text.Lookup(vocab, unknown_token=\"<unk>\")\npad_op = ds.transforms.PadEnd([500], pad_value=vocab.tokens_to_ids(\"<pad>\"))\ntype_cast_op = ds.transforms.TypeCast(ms.float32)\n\nimdb_train = imdb_train.map(operations=[lookup_op, pad_op], input_columns=[\"text\"])\nimdb_train = imdb_train.map(operations=[type_cast_op], input_columns=[\"label\"])\n\nimdb_test = imdb_test.map(operations=[lookup_op, pad_op], input_columns=[\"text\"])\nimdb_test = imdb_test.map(operations=[type_cast_op], input_columns=[\"label\"])\n\nimdb_train, imdb_valid = imdb_train.split([0.7, 0.3])\n\nprint(f\"TRAIN SET SIZE: {len(imdb_train)}\")\nprint(f\"VALID SET SIZE: {len(imdb_valid)}\")\nprint(f\"TEST SET SIZE: {len(imdb_test)}\")\n\nimdb_train = imdb_train.batch(args[\"batch_size\"], drop_remainder=True)\nimdb_valid = imdb_valid.batch(args[\"batch_size\"], drop_remainder=True)\n\n\n# LSTM分类器实现\nimport math\nimport mindspore as ms\nimport mindspore.nn as nn\nimport mindspore.ops as ops\nfrom mindspore.common.initializer import Uniform, HeUniform\n\n\nclass LSTM_CLS(nn.Cell):\n    def __init__(self, embeddings, hidden_dim, output_dim, n_layers, pad_idx):\n        super().__init__()\n        vocab_size, embedding_dim = embeddings.shape\n        self.embedding = nn.Embedding(\n            vocab_size,\n            embedding_dim,\n            embedding_table=ms.Tensor(embeddings),\n            padding_idx=pad_idx,\n        )\n        self.rnn = nn.LSTM(\n            embedding_dim, hidden_dim, num_layers=n_layers, batch_first=True\n        )\n        weight_init = HeUniform(math.sqrt(5))\n        bias_init = Uniform(1 / math.sqrt(hidden_dim * 2))\n        self.fc = nn.Dense(\n            hidden_dim, output_dim, weight_init=weight_init, bias_init=bias_init\n        )\n\n    def construct(self, inputs):\n        embedded = self.embedding(inputs)\n        _, (hidden, _) = self.rnn(embedded)\n        hidden = hidden[-1, :, :]\n        output = self.fc(hidden)\n        return output\n\n\nmodel = LSTM_CLS(\n    embeddings,\n    args[\"hidden_size\"],\n    args[\"output_size\"],\n    args[\"num_layers\"],\n    vocab.tokens_to_ids(\"<pad>\"),\n)\n\n# 损失函数与优化器\nloss_fn = nn.BCEWithLogitsLoss(reduction=\"mean\")\noptimizer = nn.Adam(model.trainable_params(), learning_rate=args[\"lr\"])\n\n# 训练过程实现\nfrom tqdm import tqdm\nimport time\n\n\ndef forward_fn(data, label):\n    logits = model(data)\n    loss = loss_fn(logits, label)\n    return loss\n\n\ngrad_fn = ms.value_and_grad(forward_fn, None, optimizer.parameters)\n\n\ndef train_step(data, label):\n    loss, grads = grad_fn(data, label)\n    optimizer(grads)\n    return loss\n\n\ndef train_one_epoch(model, train_dataset, epoch=0):\n    model.set_train()\n    total = train_dataset.get_dataset_size()\n    step_total = 0\n    last_time = time.time()\n    for i in train_dataset.create_tuple_iterator():\n        loss = train_step(*i)\n        step_total += 1\n        loss_item = loss.item()\n        if step_total % args[\"report_interval\"] == 1:\n            now_time = time.time()\n            per_batch_time = (now_time - last_time) / args[\"report_interval\"]\n            last_time = now_time\n            swanlab.log(\n                {\n                    \"train/epoch\": epoch,\n                    \"train/step\": step_total,\n                    \"train/loss\": loss_item,\n                    \"train/per_batch_time(s)\": per_batch_time,\n                }\n            )\n            print(\n                f\"[train epoch-{epoch:2d} step-{step_total:4d}/{total:4d}] loss:{loss_item:.4f} use_time:{per_batch_time:10.4f}s\"\n            )\n\n\n# 评估过程实现\ndef binary_accuracy(preds, y):\n    rounded_preds = np.around(ops.sigmoid(preds).asnumpy())\n    correct = (rounded_preds == y).astype(np.float32)\n    acc = correct.sum() / len(correct)\n    return acc\n\n\ndef evaluate(model, test_dataset, criterion, epoch=0, mode=\"eval\"):\n    last_time = time.time()\n    total = test_dataset.get_dataset_size()\n    epoch_loss = 0\n    epoch_acc = 0\n    model.set_train(False)\n    for i in test_dataset.create_tuple_iterator():\n        predictions = model(i[0])\n        loss = criterion(predictions, i[1])\n        epoch_loss += loss.asnumpy()\n        acc = binary_accuracy(predictions, i[1])\n        epoch_acc += acc\n\n    final_loss = float(epoch_loss / total)\n    final_acc = float(epoch_acc / total)\n    use_time = time.time() - last_time\n    swanlab.log(\n        {\n            f\"{mode}/loss\": final_loss,\n            f\"{mode}/acc\": final_acc,\n            f\"{mode}/use_time\": use_time,\n        }\n    )\n    print(\n        f\"[{mode} epoch-{epoch:2d} loss:{final_loss:.4f} acc:{final_acc*100:.2f}% use_time:{use_time:10.4f}s\"\n    )\n\n    return final_loss, final_acc\n\n\n# 开启训练=\nbest_valid_loss = float(\"inf\")\nsave_path = os.path.join(\"output\", exp_name)\nos.makedirs(save_path, exist_ok=True)\nckpt_file_name = os.path.join(save_path, \"sentiment-analysis.ckpt\")\n\n\nfor epoch in range(args[\"num_epochs\"]):\n    train_one_epoch(model, imdb_train, epoch)\n    valid_loss, _ = evaluate(model, imdb_valid, loss_fn, epoch)\n\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        ms.save_checkpoint(model, ckpt_file_name)\n\n\n# 开始测试\nparam_dict = ms.load_checkpoint(ckpt_file_name)\nms.load_param_into_net(model, param_dict)\nimdb_test = imdb_test.batch(64)\ntest_loss, test_acc = evaluate(model, imdb_test, loss_fn, mode=\"test\")\n\n# 开始预测\nscore_map = {1: \"Positive\", 0: \"Negative\"}\n\n\ndef predict_sentiment(model, vocab, sentence):\n    model.set_train(False)\n    tokenized = sentence.lower().split()\n    indexed = vocab.tokens_to_ids(tokenized)\n    tensor = ms.Tensor(indexed, ms.int32)\n    tensor = tensor.expand_dims(0)\n    prediction = model(tensor)\n    return score_map[int(np.round(ops.sigmoid(prediction).asnumpy()))]\n\n\npredict_sentiment(model, vocab, \"This film is great\")\npredict_sentiment(model, vocab, \"This film is terrible\")\n\n```",
    "472": "一级标题：Ascend NPU & MindSpore\n二级标题：疑难杂症\n内容：\n### 可能出现的问题一：MindSpore和CANN版本不对应\n\n务必确保MindSpore版本和驱动一致，否则会出现如下报错：\n\n```bash\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:11.112.000 [mindspore/run_check/_check_version.py:357] MindSpore version 2.3.1 and Ascend AI software package (Ascend Data Center Solution)version 7.5 does not match, the version of software package expect one of ['7.2', '7.3']. Please refer to the match info on: https://www.mindspore.cn/install\n/home/huawei/miniconda3/envs/mindspore231/lib/python3.10/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n  setattr(self, word, getattr(machar, word).flat[0])\n/home/huawei/miniconda3/envs/mindspore231/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n  return self._float_to_str(self.smallest_subnormal)\n/home/huawei/miniconda3/envs/mindspore231/lib/python3.10/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n  setattr(self, word, getattr(machar, word).flat[0])\n/home/huawei/miniconda3/envs/mindspore231/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n  return self._float_to_str(self.smallest_subnormal)\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:13.700.000 [mindspore/run_check/_check_version.py:375] MindSpore version 2.3.1 and \"te\" wheel package version 7.5 does not match. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:13.701.000 [mindspore/run_check/_check_version.py:382] MindSpore version 2.3.1 and \"hccl\" wheel package version 7.5 does not match. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:13.702.000 [mindspore/run_check/_check_version.py:396] Please pay attention to the above warning, countdown: 3\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:14.703.000 [mindspore/run_check/_check_version.py:396] Please pay attention to the above warning, countdown: 2\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:15.704.000 [mindspore/run_check/_check_version.py:396] Please pay attention to the above warning, countdown: 1\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:18.608.000 [mindspore/run_check/_check_version.py:357] MindSpore version 2.3.1 and Ascend AI software package (Ascend Data Center Solution)version 7.5 does not match, the version of software package expect one of ['7.2', '7.3']. Please refer to the match info on: https://www.mindspore.cn/install\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:18.608.000 [mindspore/run_check/_check_version.py:375] MindSpore version 2.3.1 and \"te\" wheel package version 7.5 does not match. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:18.608.000 [mindspore/run_check/_check_version.py:382] MindSpore version 2.3.1 and \"hccl\" wheel package version 7.5 does not match. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:18.608.000 [mindspore/run_check/_check_version.py:396] Please pay attention to the above warning, countdown: 3\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:19.609.000 [mindspore/run_check/_check_version.py:396] Please pay attention to the above warning, countdown: 2\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:20.611.000 [mindspore/run_check/_check_version.py:396] Please pay attention to the above warning, countdown: 1\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:21.614.000 [mindspore/run_check/_check_version.py:357] MindSpore version 2.3.1 and Ascend AI software package (Ascend Data Center Solution)version 7.5 does not match, the version of software package expect one of ['7.2', '7.3']. Please refer to the match info on: https://www.mindspore.cn/install\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:21.614.000 [mindspore/run_check/_check_version.py:375] MindSpore version 2.3.1 and \"te\" wheel package version 7.5 does not match. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:21.614.000 [mindspore/run_check/_check_version.py:382] MindSpore version 2.3.1 and \"hccl\" wheel package version 7.5 does not match. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:21.615.000 [mindspore/run_check/_check_version.py:396] Please pay attention to the above warning, countdown: 3\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:22.616.000 [mindspore/run_check/_check_version.py:396] Please pay attention to the above warning, countdown: 2\n[WARNING] ME(1049852:281473041023008,MainProcess):2024-12-06-12:23:23.617.000 [mindspore/run_check/_check_version.py:396] Please pay attention to the above warning, countdown: 1\nMindSpore version:  2.3.1\nSegmentation fault (core dumped)\n```\n\n解决方法：装对版本即可解决。对于MindSpore2.4.1，安装**8.0.RC3.beta1**驱动\n\n### 可能出现的问题二：少装了前置的包\n\n这里面\n\n```bash\n[ERROR] ME(1051780:281473416683552,MainProcess):2024-12-06-12:39:02.460.00 [mindspore/run_check/_check_version.py:360] CheckFailed: cannot import name 'version' from 'te' (unknown location)\n[ERROR] ME(1051780:281473416683552,MainProcess):2024-12-06-12:39:02.460.00 [mindspore/run_check/_check_version.py:361] MindSpore relies on whl packages of \"te\" and \"hccl\" in the \"latest\" folder of the Ascend AI software package (Ascend Data Center Solution). Please check whether they are installed correctly or not, refer to the match info on: https://www.mindspore.cn/install\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/home/huawei/miniconda3/envs/mindspore241/lib/python3.11/site-packages/mindspore/__init__.py\", line 19, in <module>\n    from mindspore import common, dataset, mindrecord, train, log, amp\n...\nImportError: cannot import name 'util' from 'tbe.tvm.topi.cce' (unknown location)\nFatal Python error: PyThreadState_Get: the function must be called with the GIL held, but the GIL is released (the current Python thread state is NULL)\nPython runtime state: finalizing (tstate=0x00000000008aceb0)\n\nAborted (core dumped)\n```\n\n### 可能出现的问题三：pip安装阶段报错opc-tool 0.1.0 requires attrs, which is not installed\n\n若出现如下报错（之前安装的时候有概率pip会报如下错误）：\n\n```bash\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nauto-tune 0.1.0 requires decorator, which is not installed.\ndataflow 0.0.1 requires jinja2, which is not installed.\nopc-tool 0.1.0 requires attrs, which is not installed.\nopc-tool 0.1.0 requires decorator, which is not installed.\nopc-tool 0.1.0 requires psutil, which is not installed.\nschedule-search 0.0.1 requires absl-py, which is not installed.\nschedule-search 0.0.1 requires decorator, which is not installed.\nte 0.4.0 requires attrs, which is not installed.\nte 0.4.0 requires cloudpickle, which is not installed.\nte 0.4.0 requires decorator, which is not installed.\nte 0.4.0 requires ml-dtypes, which is not installed.\nte 0.4.0 requires psutil, which is not installed.\nte 0.4.0 requires scipy, which is not installed.\nte 0.4.0 requires tornado, which is not installed.\n```\n\n尝试使用如下命令解决：\n\n```bash\npip install attrs cloudpickle decorator jinja2 ml-dtypes psutil scipy tornado absl-py\n```\n\n### 可能出现的问题四：在测试或者实际训练的时候出现KeyError: 'op_debug_dir'\n\n出现如下情况大概率是没有运行环境变量命令。\n\n```bash\nTraceback (most recent call last):\n  File \"/home/huawei/miniconda3/envs/mindspore241/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/home/huawei/miniconda3/envs/mindspore241/lib/python3.11/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/huawei/.local/lib/python3.11/site-packages/te_fusion/parallel_compilation.py\", line 249, in exec_compilation_task\n    check_dict_paras(dict_ops)\n  File \"/home/huawei/.local/lib/python3.11/site-packages/te_fusion/parallel_compilation.py\", line 183, in check_dict_paras\n    if dict_ops['op_debug_dir'] == None or dict_ops['op_debug_dir'] == '':\n       ~~~~~~~~^^^^^^^^^^^^^^^^\nKeyError: 'op_debug_dir'\n```\n\n解决方法：使用如下命令设置环境变量\n\n```bash\n# control log level. 0-DEBUG, 1-INFO, 2-WARNING, 3-ERROR, 4-CRITICAL, default level is WARNING.\nexport GLOG_v=2\n\n# environment variables\nLOCAL_ASCEND=/usr/local/Ascend # 设置为软件包的实际安装路径\n\n# set environmet variables using script provided by CANN, swap \"ascend-toolkit\" with \"nnae\" if you are using CANN-nnae package instead\nsource ${LOCAL_ASCEND}/ascend-toolkit/set_env.sh\n```\n\n使用conda的时候发现似乎每次都要运行一次如上命令。如果想要永久解决这个问题，可以使用如下命令解决：\n\n```bash\nexport LOCAL_ASCEND=/usr/local/Ascend # 设置为软件包的实际安装路径\necho \"source ${LOCAL_ASCEND}/ascend-toolkit/set_env.sh\" >> ~/.bashrc\nsource ~/.bashrc\n```",
    "473": "一级标题：DiffSynth Studio\n二级标题：无\n内容：\n[DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio) 是 [ModelScope](https://modelscope.cn/) 推出的一个开源的扩散模型引擎，专注于图像与视频的风格迁移与生成任务。它通过优化架构设计（如文本编码器、UNet、VAE 等组件），在保持与开源社区模型兼容性的同时，显著提升计算性能，为用户提供高效、灵活的创作工具。\n\nDiffSynth Studio 支持多种扩散模型，包括 Wan-Video、StepVideo、HunyuanVideo、CogVideoX、FLUX、ExVideo、Kolors、Stable Diffusion 3 等。\n\n![](./diffsynth/logo.jpg)\n\n你可以使用DiffSynth Studio快速进行Diffusion模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n[[toc]]",
    "474": "一级标题：DiffSynth Studio\n二级标题：准备工作\n内容：\n**1. 克隆仓库并安装环境**\n\n```bash\ngit clone https://github.com/modelscope/DiffSynth-Studio.git\ncd DiffSynth-Studio\npip install -e .\npip install swanlab\npip install lightning lightning_fabric\n```\n\n**2. 准备数据集**\n\nDiffSynth Studio 的数据集需要按下面的格式进行构建，比如将图像数据存放在`data/dog`目录下：\n\n```bash\ndata/dog/\n└── train\n    ├── 00.jpg\n    ├── 01.jpg\n    ├── 02.jpg\n    ├── 03.jpg\n    ├── 04.jpg\n    └── metadata.csv\n```\n\n`metadata.csv` 文件需要按下面的格式进行构建：\n\n```csv\nfile_name,text\n00.jpg,一只小狗\n01.jpg,一只小狗\n02.jpg,一只小狗\n03.jpg,一只小狗\n04.jpg,一只小狗\n```\n\n> 这里有一份整理好格式的火影忍者数据集，[百度云](https://pan.baidu.com/s/1kPvkTV6gy2xWFRpyXRX0Yw?pwd=2p6h)，供参考与测试\n\n**3. 准备模型**\n\n这里以Kolors模型为例，下载模型权重和VAE权重：\n\n```bash\nmodelscope download --model=Kwai-Kolors/Kolors --local_dir models/kolors/Kolors\nmodelscope download --model=AI-ModelScope/sdxl-vae-fp16-fix --local_dir models/kolors/sdxl-vae-fp16-fix\n```",
    "475": "一级标题：DiffSynth Studio\n二级标题：设置SwanLab参数\n内容：\n在运行训练脚本时，添加`--use_swanlab`，即可将训练过程记录到SwanLab平台。\n\n如果你需要离线记录，可以添加`--swanlab_mode \"local\"`。\n\n```bash\nCUDA_VISIBLE_DEVICES=\"0\" python examples/train/kolors/train_kolors_lora.py \\\n...\n--use_swanlab \\  # [!code ++]\n--swanlab_mode \"cloud\"  # [!code ++]\n```",
    "476": "一级标题：DiffSynth Studio\n二级标题：开启训练\n内容：\n使用下面的命令即可开启训练，并使用SwanLab记录超参数、训练日志、loss曲线等信息：\n\n```bash {11,12}\nCUDA_VISIBLE_DEVICES=\"0\" python examples/train/kolors/train_kolors_lora.py \\\n--pretrained_unet_path models/kolors/Kolors/unet/diffusion_pytorch_model.safetensors \\\n--pretrained_text_encoder_path models/kolors/Kolors/text_encoder \\\n--pretrained_fp16_vae_path models/kolors/sdxl-vae-fp16-fix/diffusion_pytorch_model.safetensors \\\n--dataset_path data/dog \\\n--output_path ./models \\\n--max_epochs 10 \\\n--center_crop \\\n--use_gradient_checkpointing \\\n--precision \"16-mixed\" \\\n--use_swanlab \\\n--swanlab_mode \"cloud\"\n```\n\n![](./diffsynth/ui-1.png)\n\n![](./diffsynth/ui-2.png)",
    "477": "一级标题：DiffSynth Studio\n二级标题：补充\n内容：\n如果你想要自定义SwanLab的项目名、实验名等参数，可以：\n\n**1. 文生图任务**\n\n在`DiffSynth-Studio/diffsynth/trainers/text_to_image.py`文件中，找到`swanlab_logger`变量的位置，修改`project`和`name`参数：\n\n```python {6-7}\nif args.use_swanlab:\n    from swanlab.integration.pytorch_lightning import SwanLabLogger\n    swanlab_config = {\"UPPERFRAMEWORK\": \"DiffSynth-Studio\"}\n    swanlab_config.update(vars(args))\n    swanlab_logger = SwanLabLogger(\n        project=\"diffsynth_studio\", \n        name=\"diffsynth_studio\",\n        config=swanlab_config,\n        mode=args.swanlab_mode,\n        logdir=args.output_path,\n    )\n    logger = [swanlab_logger]\n```\n\n**2. Wan-Video文生视频任务**\n\n在`DiffSynth-Studio/examples/wanvideo/train_wan_t2v.py`文件中，找到`swanlab_logger`变量的位置，修改`project`和`name`参数：\n\n```python {6-7}\nif args.use_swanlab:\n    from swanlab.integration.pytorch_lightning import SwanLabLogger\n    swanlab_config = {\"UPPERFRAMEWORK\": \"DiffSynth-Studio\"}\n    swanlab_config.update(vars(args))\n    swanlab_logger = SwanLabLogger(\n        project=\"wan\", \n        name=\"wan\",\n        config=swanlab_config,\n        mode=args.swanlab_mode,\n        logdir=args.output_path,\n    )\n    logger = [swanlab_logger]\n```",
    "478": "一级标题：EasyR1\n二级标题：无\n内容：\n[EasyR1](https://github.com/hiyouga/EasyR1) 是基于[veRL](https://github.com/volcengine/verl)的一个高效、可扩展、多模态强化学习LLM训练框架。\n\n![](./easyr1/logo.png)\n\nEasyR1 受益于 veRL 的 HybridEngine 和 vLLM 0.7 的 SPMD mode，并适配了 Qwen2.5-VL 模型，在多模态几何题任务 Geometry3k 上通过 30 个 batch 的 GRPO 训练，即可提升 5% 验证集准确率。\n\n> **作者hiyouga**：EasyR1旨在钻研多模态 RL 训练的难点，由团队的算法同学和工程同学共同迭代框架效率、算法表现和扩展性，未来将会支持更多的 RL 算法和多模态模型。\n\n你可以使用EasyR1训练你的多模态RL模型，并使用SwanLab跟踪与可视化训练曲线。",
    "479": "一级标题：EasyR1\n二级标题：1. 准备工作\n内容：\n在执行下面的命令之前，请先确保你的环境中已经安装了Python>=3.9，CUDA和PyTorch。\n\n```bash\ngit clone https://github.com/hiyouga/EasyR1.git\ncd EasyR1\npip install -e .\npip install git+https://github.com/hiyouga/MathRuler.git\npip install swanlab\n```\n\n:::warning 注意\n\nEasyR1的依赖中有flash-attn，直接安装非常慢，请在[flash-attention预编译包](https://github.com/Dao-AILab/flash-attention/releases)中找到对应Python与CUDA版本的包，下载并安装。\n\n:::",
    "480": "一级标题：EasyR1\n二级标题：2. 训练Qwen2.5-7b数学模型\n内容：\n在`EasyR1`目录下，执行下面的命令，即可使用GRPO训练Qwen2.5-7b数学模型，并使用SwanLab进行跟踪与可视化：\n\n```bash\nbash examples/run_qwen2_5_7b_math_swanlab.sh\n```\n\n![](./easyr1/qwen_math.png)\n\n当然，这里我们可以剖析一下，由于EasyR1是原始 veRL 项目的一个干净分叉，所以继承了[veRL与SwanLab的集成](/guide_cloud/integration/integration-verl.md)。所以这里我们来看`run_qwen2_5_7b_math_swanlab.sh`文件：\n\n```sh {10}\nset -x\n\nexport VLLM_ATTENTION_BACKEND=XFORMERS\n\nMODEL_PATH=Qwen/Qwen2.5-7B-Instruct  # replace it with your local file path\n\npython3 -m verl.trainer.main \\\n    config=examples/grpo_example.yaml \\\n    worker.actor.model.model_path=${MODEL_PATH} \\\n    trainer.logger=['console','swanlab'] \\\n    trainer.n_gpus_per_node=4\n```\n\n只需要在`python3 -m verl.trainer.main`参数中加入一行`trainer.logger=['console','swanlab']`，即可使用SwanLab进行跟踪与可视化。",
    "481": "一级标题：EasyR1\n二级标题：3. 训练Qwen2.5-VL-7b多模态模型\n内容：\n在`EasyR1`目录下，执行下面的命令，即可使用GRPO训练Qwen2.5-VL-7b多模态模型，并使用SwanLab进行跟踪与可视化：\n\n```bash\nbash examples/run_qwen2_5_vl_7b_geo_swanlab.sh\n```",
    "482": "一级标题：EasyR1\n二级标题：4. 每轮评估时记录生成文本\n内容：\n如果你希望在每轮评估（val）时将生成的文本记录到SwanLab中，只需在命令行钟增加一行`val_generations_to_log=1`即可：\n\n```bash {6}\npython3 -m verl.trainer.main \\\n    config=examples/grpo_example.yaml \\\n    worker.actor.model.model_path=${MODEL_PATH} \\\n    trainer.logger=['console','swanlab'] \\\n    trainer.n_gpus_per_node=4 \\\n    val_generations_to_log=1\n```",
    "483": "一级标题：EasyR1\n二级标题：写在最后\n内容：\nEasyR1 是 [LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory) 作者 [hiyouga](https://github.com/hiyouga) 的全新开源项目，一个适用于多模态大模型的强化学习框架。感谢 [hiyouga](https://github.com/hiyouga) 为全球开源生态的贡献，SwanLab也将继续与AI开发者同行。",
    "484": "一级标题：EvalScope\n二级标题：无\n内容：\n[EvalScope](https://github.com/modelscope/evalscope) 是 [ModelScope](https://modelscope.cn/) 的官方模型评估和基准测试框架，专为满足各种评估需求而设计。它支持各种模型类型，包括大型语言模型、多模态模型、Embedding模型、Reranker模型和 CLIP 模型。\n\n![evalscope-logo](./evalscope/logo.png)\n\n该框架支持多种评估场景，如端到端的RAG评估、竞技场模式和推理性能测试。它内置了MMLU、CMMLU、C-Eval和GSM8K等基准和指标。与 [ms-swift](https://github.com/modelscope/ms-swift) 训练框架无缝集成，EvalScope实现了单击评估，为模型训练和评估提供全面支持 🚀。\n\n现在，你可以使用 EvalScope 评估LLM性能，同时使用SwanLab方便地跟踪、对比、可视化。\n\n[Demo](https://swanlab.cn/@ShaohonChen/perf_benchmark/overview)",
    "485": "一级标题：EvalScope\n二级标题：1. 准备工作\n内容：\n安装下面的环境：\n\n```bash\npip install evalscope\npip install swanlab\n```\n\n如果你需要扩展evalscope的更多功能，可以按需安装：\n\n```bash\npip install -e '.[opencompass]'   # Install OpenCompass backend\npip install -e '.[vlmeval]'       # Install VLMEvalKit backend\npip install -e '.[rag]'           # Install RAGEval backend\npip install -e '.[perf]'          # Install Perf dependencies\npip install -e '.[app]'           # Install visualization dependencies\npip install -e '.[all]'           # Install all backends (Native, OpenCompass, VLMEvalKit, RAGEval)\n```",
    "486": "一级标题：EvalScope\n二级标题：2. Qwen模型推理性能压测\n内容：\n如果你希望评估`Qwen2.5-0.5B-Instruct`在[openqa格式默认数据集](https://www.modelscope.cn/datasets/AI-ModelScope/HC3-Chinese)上的表现，同时使用`SwanLab`观测性能，可以运行下面的命令：\n\n```bash {5,6}\nexport CUDA_VISIBLE_DEVICES=0\nevalscope perf \\\n --model Qwen/Qwen2.5-0.5B-Instruct \\\n --dataset openqa \\\n --number 20 \\\n --parallel 2 \\\n --limit 5 \\\n --swanlab-api-key '你的API Key' \\\n --name 'qwen2.5-openqa' \\\n --temperature 0.9 \\\n --api local\n```\n\n其中`swanlab-api-key`是你的SwanLab API Key，`name`是实验名。  \n如果你希望设置自定义项目名，可以去往`EvalScope`源码的 `evalscope/perf/benchmark.py` 的 `statistic_benchmark_metric_worker`函数，找到swanlab部分，修改`project`参数。\n\n**可视化效果案例：**\n\n![](./evalscope/show.png)",
    "487": "一级标题：EvalScope\n二级标题：上传到私有化部署版\n内容：\n如果你希望将评估结果上传到私有化部署版，可以先在命令行登录到私有化部署版。比如你的部署地址是`http://localhost:8000`，可以运行：\n\n```bash\nswanlab login --host http://localhost:8000\n```\n\n完成登录后，再运行`evalscope`的命令，就可以将评估结果上传到私有化部署版了。",
    "488": "一级标题：fastai\n二级标题：无\n内容：\n[fastai](https://github.com/fastai/fastai) 是一个基于 PyTorch 的高层次深度学习库，旨在使现代深度学习的应用更加容易和高效。它提供了一个简单的 API，使用户能够快速构建、训练和评估复杂的模型，而无需深入了解底层细节。\n\n你可以使用fastai快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。",
    "489": "一级标题：fastai\n二级标题：1.引入SwanLabCallback\n内容：\n```python\nfrom swanlab.integration.fastai import SwanLabCallback\n```\n**SwanLabCallback**是适配于fastai的日志记录类。  \n\n**SwanLabCallback**可以定义的参数有：\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。\n- 你也可以在外部通过`swanlab.init`创建项目，集成会将实验记录到你在外部创建的项目中。",
    "490": "一级标题：fastai\n二级标题：2.传入训练器\n内容：\n```python\nfrom fastai.vision.all import *\nfrom swanlab.integration.fastai import SwanLabCallback\n\n...\n\n# 定义模型\nlearn = vision_learner(...)\n\n# 添加SwanLabCallback\nlearn.fit_one_cycle(5, cbs=SwanLabCallback)\n```",
    "491": "一级标题：fastai\n二级标题：3.案例-宠物分类\n内容：\n```python (2,16)\nfrom fastai.vision.all import *\nfrom swanlab.integration.fastai import SwanLabCallback\n\n# 加载数据\npath = untar_data(URLs.PETS)\ndls = ImageDataLoaders.from_name_re(\n    path, get_image_files(path / \"images\"), pat=r\"([^/]+)_\\d+.jpg$\", item_tfms=Resize(224)\n)\n\n# 定义模型\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\n\n# 添加SwanLabCallback\nlearn.fit_one_cycle(\n    5,\n    cbs=SwanLabCallback(\n        project=\"fastai-swanlab-integration-test\",\n        experiment_name=\"super-test\",\n        description=\"Test fastai integration with swanlab\",\n        logdir=\"./logs\",\n    ),\n)\n```",
    "492": "一级标题：🤗HuggingFace Accelerate\n二级标题：无\n内容：\nHuggingFace 的 [accelerate](https://huggingface.co/docs/accelerate/index) 是一个简化和优化深度学习模型训练与推理的开源库。\n\n> 🚀在几乎任何设备和分布式配置上启动、训练和使用PyTorch模型的简单方法，支持自动混合精度(包括fp8)，以及易于配置的FSDP和DeepSpeed\n\n它提供了高效的分布式训练和推理的工具，使开发者能够更轻松地在不同硬件设备上部署和加速模型。通过简单的几行代码改动，就可以轻松将现有的训练代码集成进 `torch_xla` 和 `torch.distributed` 这类平台，而无需为复杂的分布式计算架构烦恼，从而提升工作效率和模型性能。\n\n![hf-accelerate-image](./huggingface_accelerate/logo.png)\n\n你可以使用`accelerate`快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n> `accelerate`>=1.8.0 的版本，已官方集成了swanlab  \n> 如果你的版本低于1.8.0，请使用 **SwanLabTracker集成**",
    "493": "一级标题：🤗HuggingFace Accelerate\n二级标题：1. 两行代码完成集成\n内容：\n```python {4,9}\nfrom accelerate import Accelerator\n\n# 告诉 Accelerator 对象使用 swanlab 进行日志记录\naccelerator = Accelerator(log_with=\"swanlab\")\n\n# 初始化您的 swanlab 实验，传递 swanlab 参数和任何配置信息\naccelerator.init_trackers(\n    ...\n    init_kwargs={\"swanlab\": {\"experiment_name\": \"hello_world\"}}\n    )\n```\n\n::: warning 补充信息\n1. swanlab项目名由`accelerator.init_trackers`的`project_name`参数指定\n2. 向`init_kwargs`传递的`swanlab`字典，key-value和`swanlab.init`的参数完全一致（除了project）。\n:::\n\n最小能跑代码：\n\n```python {4,10}\nfrom accelerate import Accelerator\n\n# Tell the Accelerator object to log with swanlab\naccelerator = Accelerator(log_with=\"swanlab\")\n\n# Initialise your swanlab experiment, passing swanlab parameters and any config information\naccelerator.init_trackers(\n    project_name=\"accelerator\",\n    config={\"dropout\": 0.1, \"learning_rate\": 1e-2},\n    init_kwargs={\"swanlab\": {\"experiment_name\": \"hello_world\"}}\n    )\n\nfor i in range(100):\n    # Log to swanlab by calling `accelerator.log`, `step` is optional\n    accelerator.log({\"train_loss\": 1.12, \"valid_loss\": 0.8}, step=i+1)\n\n# Make sure that the swanlab tracker finishes correctly\naccelerator.end_training()\n```",
    "494": "一级标题：🤗HuggingFace Accelerate\n二级标题：2. SwanLabTracker集成\n内容：\n如果你使用的是`accelerate<1.8.0`的版本，则可以使用SwanLabCallback集成。\n\n### 2.1 引入\n\n```bash\nfrom swanlab.integration.accelerate import SwanLabTracke\n```\n\n\n### 2.2 在初始化accelerate时指定日志记录器\n\n```python (1,7,9,12)\nfrom swanlab.integration.accelerate import SwanLabTracker\nfrom accelerate import Accelerator\n\n...\n\n# 创建SwanLab日志记录器\ntracker = SwanLabTracker(\"YOUR_SMART_PROJECT_NAME\")\n# 传入Accelerator\naccelerator = Accelerator(log_with=tracker)\n\n# 初始化所有日志记录器\naccelerator.init_trackers(\"YOUR_SMART_PROJECT_NAME\", config=config)\n\n# training code\n...\n```\n\n- 虽然上面的代码两次设定了项目名，实际上只有第一个项目名设置才起了作用\n\n- 显式调用`init_trackers`来初始化所有日志记录是`accelerate`的机制，第二次设置的项目名是当有多个日志记录器时,初始化内置的日志记录器的情况下才会用到。\n\n### 2.3 完整案例代码\n\n下面是一个使用accelerate进行cifar10分类，并使用SwanLab进行日志跟踪的案例：\n\n```python (10,45,46,47,71,90)\nimport torch\nimport torch.utils\nimport torch.utils.data\nimport torch.utils.data.dataloader\nimport torchvision\nfrom torchvision.models import resnet18, ResNet18_Weights\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nimport swanlab\nfrom swanlab.integration.accelerate import SwanLabTracker\n\n\ndef main():\n    # hyperparameters\n    config = {\n        \"num_epoch\": 5,\n        \"batch_num\": 16,\n        \"learning_rate\": 1e-3,\n    }\n\n    # Download the raw CIFAR-10 data.\n    transform = torchvision.transforms.Compose(\n        [\n            torchvision.transforms.ToTensor(),\n            torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n        ]\n    )\n    train_data = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n    test_data = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n    BATCH_SIZE = config[\"batch_num\"]\n    my_training_dataloader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n    my_testing_dataloader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n\n    # Using resnet18 model, make simple changes to fit the data set\n    my_model = resnet18(weights=ResNet18_Weights.DEFAULT)\n    my_model.conv1 = torch.nn.Conv2d(my_model.conv1.in_channels, my_model.conv1.out_channels, 3, 1, 1)\n    my_model.maxpool = torch.nn.Identity()\n    my_model.fc = torch.nn.Linear(my_model.fc.in_features, 10)\n\n    # Criterion and optimizer\n    criterion = torch.nn.CrossEntropyLoss()\n    my_optimizer = torch.optim.SGD(my_model.parameters(), lr=config[\"learning_rate\"], momentum=0.9)\n\n    # Init accelerate with swanlab tracker\n    tracker = SwanLabTracker(\"CIFAR10_TRAING\")\n    accelerator = Accelerator(log_with=tracker)\n    accelerator.init_trackers(\"CIFAR10_TRAING\", config=config)\n    my_model, my_optimizer, my_training_dataloader, my_testing_dataloader = accelerator.prepare(\n        my_model, my_optimizer, my_training_dataloader, my_testing_dataloader\n    )\n    device = accelerator.device\n    my_model.to(device)\n\n    # Get logger\n    logger = get_logger(__name__)\n\n    # Begin training\n\n    for ep in range(config[\"num_epoch\"]):\n        # train model\n        if accelerator.is_local_main_process:\n            print(f\"begin epoch {ep} training...\")\n        step = 0\n        for stp, data in enumerate(my_training_dataloader):\n            my_optimizer.zero_grad()\n            inputs, targets = data\n            outputs = my_model(inputs)\n            loss = criterion(outputs, targets)\n            accelerator.backward(loss)\n            my_optimizer.step()\n            accelerator.log({\"training_loss\": loss, \"epoch_num\": ep})\n            if accelerator.is_local_main_process:\n                print(f\"train epoch {ep} [{stp}/{len(my_training_dataloader)}] | train loss {loss}\")\n\n        # eval model\n        if accelerator.is_local_main_process:\n            print(f\"begin epoch {ep} evaluating...\")\n        with torch.no_grad():\n            total_acc_num = 0\n            for stp, (inputs, targets) in enumerate(my_testing_dataloader):\n                predictions = my_model(inputs)\n                predictions = torch.argmax(predictions, dim=-1)\n                # Gather all predictions and targets\n                all_predictions, all_targets = accelerator.gather_for_metrics((predictions, targets))\n                acc_num = (all_predictions.long() == all_targets.long()).sum()\n                total_acc_num += acc_num\n                if accelerator.is_local_main_process:\n                    print(f\"eval epoch {ep} [{stp}/{len(my_testing_dataloader)}] | val acc {acc_num/len(all_targets)}\")\n\n            accelerator.log({\"eval acc\": total_acc_num / len(my_testing_dataloader.dataset)})\n\n    accelerator.wait_for_everyone()\n    accelerator.save_model(my_model, \"cifar_cls.pth\")\n\n    accelerator.end_training()\n\n\nif __name__ == \"__main__\":\n    main()\n```",
    "495": "一级标题：🤗HuggingFace Transformers\n二级标题：无\n内容：\n[![](/assets/colab.svg)](https://colab.research.google.com/drive/1iYwrAM4ToCWt5p5hlrrkHlQqBIav_r2E?usp=sharing)\n\nHugging Face 的 [Transformers](https://github.com/huggingface/transformers) 是一个非常流行的开源库，它提供了大量预训练的模型，主要用于自然语言处理（NLP）任务。这个库的目标是使最新的模型能够易于使用，并支持多种框架，如 TensorFlow 和 PyTorch。\n\n![hf-transformers-image](/assets/ig-huggingface-transformers.png)\n\n你可以使用Transformers快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n> `transformers>=4.50.0` 的版本，已官方集成了SwanLab  \n> 如果你的版本低于4.50.0，请使用[SwanLabCallback集成](#_4-swanlabcallback集成)。",
    "496": "一级标题：🤗HuggingFace Transformers\n二级标题：1. 一行代码完成集成\n内容：\n只需要在你的训练代码中，找到`TrainingArguments`部分，添加`report_to=\"swanlab\"`参数，即可完成集成。\n\n```python\nfrom transformers import TrainingArguments, Trainer\n\nargs = TrainingArguments(\n    ...,\n    report_to=\"swanlab\" # [!code ++]\n\n)\n\ntrainer = Trainer(..., args=args)\n```\n\n如果你想要设定一下实验名，以区分每次训练，可以设置`run_name`参数：\n\n```python\nargs = TrainingArguments(\n    ...,\n    report_to=\"swanlab\",\n    run_name=\"great_try_1\", # [!code ++]\n)\n```",
    "497": "一级标题：🤗HuggingFace Transformers\n二级标题：2. 自定义项目/工作空间\n内容：\n默认下，项目名会使用你运行代码的`目录名`，实验名等于`output_dir`。\n\n如果你想自定义项目名或工作空间，可以设置`SWANLAB_PROJECT`和`SWANLAB_WORKSPACE`环境变量：\n\n::: code-group\n\n```python\nimport os  # [!code ++]\n\nos.environ[\"SWANLAB_PROJECT\"]=\"qwen2-sft\"  # [!code ++]\nos.environ[\"SWANLAB_WORKSPACE\"]=\"EmotionMachine\"  # [!code ++]\n\n...\n\nfrom transformers import TrainingArguments, Trainer\n\nargs = TrainingArguments(\n    ...,\n    report_to=\"swanlab\",\n    run_name=\"great_try_1\",\n)\n\ntrainer = Trainer(..., args=args)\n```\n\n```bash [Command Line（Linux/MacOS）]\nexport SWANLAB_PROJECT=\"qwen2-sft\"\nexport SWANLAB_WORKSPACE=\"EmotionMachine\"\n```\n\n```bash [Command Line（Windows）]\nset SWANLAB_PROJECT=\"qwen2-sft\"\nset SWANLAB_WORKSPACE=\"EmotionMachine\"\n```\n\n:::",
    "498": "一级标题：🤗HuggingFace Transformers\n二级标题：3. 案例代码：Bert文本分类\n内容：\n```python\nimport evaluate\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\n\ndataset = load_dataset(\"yelp_review_full\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n\nmetric = evaluate.load(\"accuracy\")\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\n\ntraining_args = TrainingArguments(\n    output_dir=\"test_trainer\",\n    num_train_epochs=3,\n    logging_steps=50,\n    report_to=\"swanlab\", # [!code ++]\n    run_name=\"bert_train\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n```",
    "499": "一级标题：🤗HuggingFace Transformers\n二级标题：4. SwanLabCallback集成\n内容：\n如果你使用的是`Transformers<4.50.0`的版本，或者你希望更灵活地控制SwanLab的行为，则可以使用SwanLabCallback集成。\n\n### 4.1 引入SwanLabCallback\n\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\n```\n\n**SwanLabCallback**是适配于Transformers的日志记录类。\n\n**SwanLabCallback**可以定义的参数有：\n\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。\n- 你也可以在外部通过`swanlab.init`创建项目，集成会将实验记录到你在外部创建的项目中。\n\n### 4.2 传入Trainer\n\n```python (1,7,12)\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom transformers import Trainer, TrainingArguments\n\n...\n\n# 实例化SwanLabCallback\nswanlab_callback = SwanLabCallback(project=\"hf-visualization\")\n\ntrainer = Trainer(\n    ...\n    # 传入callbacks参数\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n```\n\n### 4.3 完整案例代码\n\n```python (4,41,50)\nimport evaluate\nimport numpy as np\nimport swanlab\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom datasets import load_dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\n\ndataset = load_dataset(\"yelp_review_full\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n\nmetric = evaluate.load(\"accuracy\")\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\n\ntraining_args = TrainingArguments(\n    output_dir=\"test_trainer\",\n    # 如果只需要用SwanLab跟踪实验，则将report_to参数设置为”none“\n    report_to=\"none\",\n    num_train_epochs=3,\n    logging_steps=50,\n)\n\n# 实例化SwanLabCallback\nswanlab_callback = SwanLabCallback(experiment_name=\"TransformersTest\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n    # 传入callbacks参数\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n```\n\n### 4.4 GUI效果展示\n\n超参数自动记录：\n\n![ig-hf-transformers-gui-1](./huggingface_transformers/card.jpg)\n\n指标记录：\n\n![ig-hf-transformers-gui-2](./huggingface_transformers/chart.jpg)\n\n\n### 4.5 拓展：增加更多回调\n\n试想一个场景，你希望在每个epoch结束时，让模型推理测试样例，并用swanlab记录推理的结果，那么你可以创建一个继承自`SwanLabCallback`的新类，增加或重构生命周期函数。比如：\n\n```python\nclass NLPSwanLabCallback(SwanLabCallback):    \n    def on_epoch_end(self, args, state, control, **kwargs):\n        test_text_list = [\"example1\", \"example2\"]\n        log_text_list = []\n        for text in test_text_list:\n            result = model(text)\n            log_text_list.append(swanlab.Text(result))\n            \n        swanlab.log({\"Prediction\": test_text_list}, step=state.global_step)\n```\n\n上面是一个在NLP任务下的新回调类，增加了`on_epoch_end`函数，它会在`transformers`训练的每个epoch结束时执行。\n\n查看全部的Transformers生命周期回调函数：[链接](https://github.com/huggingface/transformers/blob/main/src/transformers/trainer_callback.py#L311)",
    "500": "一级标题：🤗HuggingFace Transformers\n二级标题：5. 环境变量\n内容：\n参考：[HuggingFace Docs: transformers.integrations.SwanLabCallback](https://huggingface.co/docs/transformers/main/en/main_classes/callback#transformers.integrations.SwanLabCallback)",
    "501": "一级标题：🤗HuggingFace Trl\n二级标题：无\n内容：\n[TRL](https://github.com/huggingface/trl) (Transformers Reinforcement Learning，用强化学习训练Transformers模型) 是一个领先的Python库，旨在通过监督微调（SFT）、近端策略优化（PPO）和直接偏好优化（DPO）等先进技术，对基础模型进行训练后优化。TRL 建立在 🤗 Transformers 生态系统之上，支持多种模型架构和模态，并且能够在各种硬件配置上进行扩展。\n\n![logo](./huggingface_trl/logo.png)\n\n你可以使用Trl快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n[Demo](https://swanlab.cn/@ZeyiLin/trl-visualization/runs/q1uf2r4wmao7iomc5z1ff/overview)\n\n> `transformers>=4.50.0` 的版本，已官方集成了SwanLab  \n> 如果你的版本低于4.50.0，请使用[SwanLabCallback集成](#_5-使用swanlabcallback)。",
    "502": "一级标题：🤗HuggingFace Trl\n二级标题：1. 一行代码集成\n内容：\n只需要在你的训练代码中，找到HF的`Config`部分（比如`SFTConfig`、`GRPOConfig`等），添加`report_to=\"swanlab\"`参数，即可完成集成。\n\n```python\nfrom trl import SFTConfig, SFTTrainer\n\nargs = SFTConfig(\n    ...,\n    report_to=\"swanlab\" # [!code ++]\n)\n\ntrainer = Trainer(..., args=args)\n```",
    "503": "一级标题：🤗HuggingFace Trl\n二级标题：2. 自定义项目名\n内容：\n默认下，项目名会使用你运行代码的`目录名`。\n\n如果你想自定义项目名，可以设置`SWANLAB_PROJECT`环境变量：\n\n::: code-group\n\n```python\nimport os\nos.environ[\"SWANLAB_PROJECT\"]=\"qwen2-sft\"\n```\n\n```bash [Command Line（Linux/MacOS）]\nexport SWANLAB_PROJECT=\"qwen2-sft\"\n```\n\n```bash [Command Line（Windows）]\nset SWANLAB_PROJECT=\"qwen2-sft\"\n```\n\n:::",
    "504": "一级标题：🤗HuggingFace Trl\n二级标题：3. 案例代码\n内容：\n使用Qwen2.5-0.5B-Instruct模型，使用Capybara数据集进行SFT训练：\n\n```python\nfrom trl import SFTConfig, SFTTrainer\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"trl-lib/Capybara\", split=\"train\")\n\ntraining_args = SFTConfig(\n    output_dir=\"Qwen/Qwen2.5-0.5B-SFT\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    num_train_epochs=1,\n    logging_steps=20,\n    learning_rate=2e-5,\n    report_to=\"swanlab\", # [!code ++]\n    )\n\ntrainer = SFTTrainer(\n    args=training_args,\n    model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n    train_dataset=dataset,\n)\n\ntrainer.train()\n```\n\nDPO、GRPO、PPO等同理，只需要将`report_to=\"swanlab\"`传入对应的`Config`即可。",
    "505": "一级标题：🤗HuggingFace Trl\n二级标题：4. GUI效果展示\n内容：\n**超参数自动记录：**\n\n![ig-hf-trl-gui-1](./huggingface_trl/ig-hf-trl-gui-1.png)\n\n**指标记录：**\n\n![ig-hf-trl-gui-2](./huggingface_trl/ig-hf-trl-gui-2.png)",
    "506": "一级标题：🤗HuggingFace Trl\n二级标题：5.使用SwanLabCallback\n内容：\n如果你使用的是`Transformers<4.50.0`的版本，或者你希望更灵活地控制SwanLab的行为，则可以使用SwanLabCallback集成。\n\n### 5.1 引入SwanLabCallback\n\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\n```\n\n**SwanLabCallback**是适配于Transformers的日志记录类。\n\n**SwanLabCallback**可以定义的参数有：\n\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。\n- 你也可以在外部通过`swanlab.init`创建项目，集成会将实验记录到你在外部创建的项目中。\n\n### 5.2 传入Trainer\n\n```python (1,7,12)\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom trl import SFTConfig, SFTTrainer\n\n...\n\n# 实例化SwanLabCallback\nswanlab_callback = SwanLabCallback(project=\"trl-visualization\")\n\ntrainer = SFTTrainer(\n    ...\n    # 传入callbacks参数\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n```\n\n### 5.3 完整案例代码\n\n使用Qwen2.5-0.5B-Instruct模型，使用Capybara数据集进行SFT训练：\n\n```python (3,7,26)\nfrom trl import SFTConfig, SFTTrainer\nfrom datasets import load_dataset\nfrom swanlab.integration.transformers import SwanLabCallback\n\ndataset = load_dataset(\"trl-lib/Capybara\", split=\"train\")\n\nswanlab_callback = SwanLabCallback(\n    project=\"trl-visualization\",\n    experiment_name=\"Qwen2.5-0.5B-SFT\",\n    description=\"测试使用trl框架sft训练\"\n)\n\ntraining_args = SFTConfig(\n    output_dir=\"Qwen/Qwen2.5-0.5B-SFT\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    num_train_epochs=1,\n    logging_steps=20,\n    learning_rate=2e-5,\n    report_to=\"none\",\n    )\n\ntrainer = SFTTrainer(\n    args=training_args,\n    model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n    train_dataset=dataset,\n)\n\ntrainer.train()\n```\n\nDPO、GRPO、PPO等同理，只需要将`SwanLabCallback`传入对应的`Trainer`即可。",
    "507": "一级标题：🤗HuggingFace Trl\n二级标题：5. 环境变量\n内容：\n参考：[HuggingFace Docs: transformers.integrations.SwanLabCallback](https://huggingface.co/docs/transformers/main/en/main_classes/callback#transformers.integrations.SwanLabCallback)",
    "508": "一级标题：Hydra\n二级标题：无\n内容：\n[hydra](https://hydra.cc/)是一个由Facebook AI Research创建的开源框架，旨在简化Python应用程序中配置的创建、管理和使用过程。Hydra通过使配置内容动态化和可组合，大大简化了处理多个配置集合的复杂性，特别是对于那些具有大量参数和需要在多种环境下运行的应用程序而言。\n\n![hydra-image](/assets/hydra-image.jpg)\n\n你可以继续使用 Hydra 进行配置管理，同时使用SwanLab的强大功能。",
    "509": "一级标题：Hydra\n二级标题：跟踪指标\n内容：\n和常规一样，用`swanlab.init`和`swanlab.log`跟踪你的指标。  \n假设你的hydra配置文件为`configs/defaults.yaml`，则添加几行：\n\n```yaml\nswanlab:\n  project: \"my-project\"\n```\n\n\n在训练脚本中，将配置文件中的`project`传入：\n\n```python\nimport swanlab\nimport hydra\n\n@hydra.main(config_path=\"configs/\", config_name=\"defaults\")\ndef run_experiment(cfg):\n    run = swanlab.init(project=cfg.swanlab.project)\n    ...\n    swanlab.log({\"loss\": loss})\n```",
    "510": "一级标题：Hydra\n二级标题：跟踪超参数\n内容：\nHydra使用[omegaconf](https://omegaconf.readthedocs.io/en/2.1_branch/)作为与配置字典交互的默认方式。\n\n可以直接将OmegaConf的字典传递给`swanlab.config`：\n\n```python\n@hydra.main(config_path=\"configs/\", config_name=\"defaults\")\ndef run_experiment(cfg):\n    run = swanlab.init(project=cfg.swanlab.project,\n                       config=cfg,\n    )\n    ...\n    swanlab.log({\"loss\": loss})\n    model = Model(**swanlab.config.model.configs)\n```\n\n如果传递`cfg`时出现意外的结果，那么可以先转换`omegaconf.DictConfig`为原始类型：\n\n```python\n@hydra.main(config_path=\"configs/\", config_name=\"defaults\")\ndef run_experiment(cfg):\n    run = swanlab.init(project=cfg.swanlab.project,\n                       config=omegaconf.OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True)\n    ...\n    swanlab.log({\"loss\": loss})\n    model = Model(**swanlab.config.model.configs)\n```",
    "511": "一级标题：Keras\n二级标题：无\n内容：\n[Keras](https://keras.io/) 是一个用 Python 编写的高级神经网络 API，最初由 François Chollet 创建，并于 2017 年合并到 TensorFlow 中，但依然可以作为一个独立的框架使用。它是一个开源的深度学习框架，运行在 TensorFlow、Theano 或 Microsoft Cognitive Toolkit (CNTK) 等深度学习后端之上。\n\n![keras-image](/assets/ig-keras-1.png)\n\n你可以使用Keras快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n[在线演示](https://swanlab.cn/@ZeyiLin/keras_mnist/runs/9gzx3m1ga2q2xb6t6ekxb/chart)",
    "512": "一级标题：Keras\n二级标题：1. 引入SwanLabLogger\n内容：\n```python\nfrom swanlab.integration.keras import SwanLabLogger\n```",
    "513": "一级标题：Keras\n二级标题：2. 与model.fit配合\n内容：\n首先初始化SwanLab：\n\n```python\nswanlab.init(\n    project=\"keras_mnist\",\n    experiment_name=\"mnist_example\",\n    description=\"Keras MNIST Example\"\n    )\n```\n\n然后，在`model.fit`的`callbacks`参数中添加`SwanLabLogger`，即可完成集成：\n\n```python\nmodel.fit(..., callbacks=[SwanLabLogger()])\n```",
    "514": "一级标题：Keras\n二级标题：3. 案例-MNIST\n内容：\n```python\nfrom swanlab.integration.keras import SwanLabLogger\nimport tensorflow as tf\nimport swanlab\n\n# Initialize SwanLab\nswanlab.init(\n    project=\"keras_mnist\",\n    experiment_name=\"mnist_example\",\n    description=\"Keras MNIST Example\"\n    )\n\n# Load and preprocess MNIST data\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\nx_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n\n# Build a simple CNN model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Conv2D(64, 3, activation='relu'),\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Train the model with SwanLabLogger\nmodel.fit(\n    x_train, \n    y_train,\n    epochs=5,\n    validation_data=(x_test, y_test),\n    callbacks=[SwanLabLogger()]\n)\n```\n\n效果演示：\n\n![keras-image](/assets/ig-keras-2.png)\n\n[在线演示](https://swanlab.cn/@ZeyiLin/keras_mnist/runs/9gzx3m1ga2q2xb6t6ekxb/chart)",
    "515": "一级标题：LightGBM\n二级标题：无\n内容：\nLightGBM（Light Gradient Boosting Machine）是一种基于决策树算法的分布式梯度提升框架，由微软公司在2017年发布。它以高效、快速和准确著称，广泛应用于分类、回归和排序等机器学习任务。\n\n![lightgbm](/zh/guide_cloud/integration/lightgbm/logo.png)\n\n你可以使用LightGBM快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。",
    "516": "一级标题：LightGBM\n二级标题：1. 引入SwanLabCallback\n内容：\n```python\nfrom swanlab.integration.lightgbm import SwanLabCallback\n```\n\nSwanLabCallback是适配于LightGBM的日志记录类。",
    "517": "一级标题：LightGBM\n二级标题：2. 初始化SwanLab\n内容：\n```python\nswanlab.init(\n    project=\"lightgbm-example\", \n    experiment_name=\"breast-cancer-classification\"\n)\n```",
    "518": "一级标题：LightGBM\n二级标题：3. 传入`lgb.train`\n内容：\n```python\nimport lightgbm as lgb\n\ngbm = lgb.train(\n    ...\n    callbacks=[SwanLabCallback()]\n)\n```",
    "519": "一级标题：LightGBM\n二级标题：4. 完整测试代码\n内容：\n```python\nimport lightgbm as lgb\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport swanlab\nfrom swanlab.integration.lightgbm import SwanLabCallback\n\n# Step 1: 初始化swanlab\nswanlab.init(project=\"lightgbm-example\", experiment_name=\"breast-cancer-classification\")\n\n# Step 2: 加载数据集\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Step 3: 分割数据集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Step 4: 创建LightGBM数据集\ntrain_data = lgb.Dataset(X_train, label=y_train)\ntest_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n\n# Step 5: 设置参数\nparams = {\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'boosting_type': 'gbdt',\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9\n}\n\n# Step 6: 使用swanlab callback训练模型\nnum_round = 100\ngbm = lgb.train(\n    params,\n    train_data,\n    num_round,\n    valid_sets=[test_data],\n    callbacks=[SwanLabCallback()]\n)\n\n# Step 7: 预测\ny_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\ny_pred_binary = [1 if p >= 0.5 else 0 for p in y_pred]\n\n# Step 8: 评估模型\naccuracy = accuracy_score(y_test, y_pred_binary)\nprint(f\"模型准确率: {accuracy:.4f}\")\nswanlab.log({\"accuracy\": accuracy})\n\n# Step 9: 保存模型\ngbm.save_model('lightgbm_model.txt')\n\n# Step 10: 加载模型并预测\nbst_loaded = lgb.Booster(model_file='lightgbm_model.txt')\ny_pred_loaded = bst_loaded.predict(X_test)\ny_pred_binary_loaded = [1 if p >= 0.5 else 0 for p in y_pred_loaded]\n\n# Step 11: 评估加载模型\naccuracy_loaded = accuracy_score(y_test, y_pred_binary_loaded)\nprint(f\"加载模型后的准确率: {accuracy_loaded:.4f}\")\nswanlab.log({\"accuracy_loaded\": accuracy_loaded})\n\n# Step 12: 结束swanlab实验\nswanlab.finish()\n```",
    "520": "一级标题：LLaMA Factory\n二级标题：无\n内容：\n[[toc]]",
    "521": "一级标题：LLaMA Factory\n二级标题：0. 前言\n内容：\n![](/zh/guide_cloud/integration/llama_factory/0.png)\n\n我们非常高兴地宣布**SwanLab**与[LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory)建立合作伙伴关系，致力于为中国训练者提供优质、高效的大模型训练体验。\n\n现在你使用新版本的LLaMA Factory启动训练前，可以在WebUI的「SwanLab configurations」（中文：SwanLab参数设置）卡片中勾选「Use SwanLab」，就可以通过SwanLab强大的训练看板进行这一次大模型微调的跟踪、记录与可视化。\n\n![](/zh/guide_cloud/integration/llama_factory/1.png)\n\nLLaMA Factory 是一个用于微调大语言模型 (LLM) 的开源工具包，它提供了一个统一且高效的框架，支持 100 多个 LLM （包括Qwen、LLaMA、ChatGLM、Mistral等）的微调，涵盖了各种训练方法、数据集和先进算法。\n\n大语言模型的微调是一个上手门槛颇高的工作，LLaMA Factory通过提供用户友好的 Web UI 和命令行界面，结合其统一且高效的框架，大幅降低了大模型从微调到测试评估的上手门槛。\n\n为了提供用户更好的大模型微调过程监控与日志记录体验，我们与LLaMA Factory团队合作开展了两项举措：利用SwanLab增强LLaMA Factory的实验监控能力，以及在SwanLab中记录 LLaMA Factory的专属超参数。\n\n\n> LLaMA Factory：https://github.com/hiyouga/LLaMA-Factory  \n> SwanLab：https://swanlab.cn  \n> SwanLab开源仓库：https://github.com/SwanHubX/SwanLab  \n> 实验过程：https://swanlab.cn/@ZeyiLin/llamafactory/runs/y79f9ri9jr1mkoh24a7g8/chart\n\n我们将以使用LLaMA Factory + SwanLab可视化微调Qwen2.5为案例。",
    "522": "一级标题：LLaMA Factory\n二级标题：1. 安装环境\n内容：\n首先，你需要确保你拥有Python3.8以上环境与Git工具，然后克隆仓库：\n\n```shellscript\ngit clone --depth 1 https://github.com/hiyouga/LLaMA-Factory\n```\n\n安装相关环境：\n\n```shellscript\ncd LLaMA-Factory\npip install -e \".[torch,metrics,swanlab]\"\n```\n\n> 如果你是昇腾NPU用户，可以访问：[华为NPU适配](https://llamafactory.readthedocs.io/zh-cn/latest/advanced/npu.html) 查看昇腾NPU版安装教程。",
    "523": "一级标题：LLaMA Factory\n二级标题：2. 使用LLaMA Board开启训练\n内容：\nLLaMA Board是基于Gradio的可视化微调界面，你可以通过下面的代码启动LLaMA Board：\n\n```shellscript\nllamafactory-cli webui\n```\n\n提示：LLaMA Factory默认的模型/数据集下载源是HuggingFace，如果你所在的网络环境对与HuggingFace下载并不友好，可以在启动LLaMA Board之前，将下载源设置为魔搭社区或魔乐社区：\n\n```shellscript\n# 下载源改为魔搭社区\nexport USE_MODELSCOPE_HUB=1 # Windows 使用 `set USE_MODELSCOPE_HUB=1`\n\n# 下载源改为魔乐社区\nexport USE_OPENMIND_HUB=1 # Windows 使用 `set USE_OPENMIND_HUB=1`\n```\n\n执行 llamafactory-cli webui 之后，你可以在浏览器看到下面的UI界面。本案例选择Qwen2-1.5B-instruct作为模型，alpaca\\_zh\\_demo作为数据集：\n\n![](/zh/guide_cloud/integration/llama_factory/2.png)\n\n在页面的下方，你会看到一个「SwanLab参数设置」的卡片，展开后，你就可以配置SwanLab的项目名、实验名、工作区、API 密钥以及模式等参数。\n\n> 如果你是第一次使用SwanLab，还需要在 swanlab.cn 注册一个账号获取专属的API密钥。\n\n我们勾&#x9009;**「使用SwanLab」：**\n\n![](/zh/guide_cloud/integration/llama_factory/3.png)\n\n现在，点&#x51FB;**「开始」按钮**，就可以开启微调：\n\n![](/zh/guide_cloud/integration/llama_factory/4.png)\n\n在完成载入模型、载入数据集，正式开启微调后，我们可以在命令行界面找到SwanLab部分：\n\n![](/zh/guide_cloud/integration/llama_factory/5.png)\n\n点击箭头对应的实验链接，就可以在**浏览器**中打开SwanLab实验跟踪看板：\n\n![](/zh/guide_cloud/integration/llama_factory/6.png)\n\n在「卡片」栏下的「配置」表中，第一个就会是LLamaFactory，标识了这次训练的使用框架。\n\n![](/zh/guide_cloud/integration/llama_factory/7.png)",
    "524": "一级标题：LLaMA Factory\n二级标题：3. 使用命令行开启训练\n内容：\nLLaMA Factory还支持通过yaml配置文件，在命令行中进行微调。\n\n我们编辑LLaMA Factory项目目录下的 **examples/train\\_lora/qwen2vl\\_lora\\_sft.yaml** 文件，在文件尾部增加：\n\n```yaml\n...\n\n### swanlab\nuse_swanlab: true\nswanlab_project: llamafactory\nswanlab_run_name: Qwen2-VL-7B-Instruct\n```\n\n然后运行：\n\n```shellscript\nllamafactory-cli train examples/train_lora/qwen2vl_lora_sft.yaml\n```\n\n在完成载入模型、载入数据集，正式开启微调后，与LLaMA Board一样，可以在命令行界面找到SwanLab部分，通过实验链接访问SwanLab实验看板。\n\n![](/zh/guide_cloud/integration/llama_factory/8.png)\n\n![](/zh/guide_cloud/integration/llama_factory/9.png)\n\n***\n\n致敬 LLaMA Factory 团队，感谢他们为开源社区提供了这么一个优秀的模型训练工具。随着我们的继续合作，敬请期待SwanLab工具为大模型训练师提供更深入、强大的实验跟踪功能。",
    "525": "一级标题：LLaMA Factory\n二级标题：4.附录：支持的参数\n内容：\n```yaml\n# swanlab\nuse_swanlab: true\nswanlab_project: your_project_name\nswanlab_run_name: your_experiment_name\nswanlab_workspace: your_workspace\nswanlab_mode: your_mode\nswanlab_api_key: your_api_key\n```\n\n> 更多可见：[LLaMA Factory - Github](https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md#%E5%AE%89%E8%A3%85-llama-factory) 中的`SwanLabArguments`类。",
    "526": "一级标题：MLFlow\n二级标题：无\n内容：\n[MLFlow](https://github.com/mlflow/mlflow) 是一个开源的机器学习生命周期管理平台，由 Databricks 创建并维护。它旨在帮助数据科学家和机器学习工程师更高效地管理机器学习项目的整个生命周期，包括实验跟踪、模型管理、模型部署和协作。MLflow 的设计是模块化的，可以与任何机器学习库、框架或工具集成。\n\n![mlflow](./mlflow/logo.png)\n\n:::warning 其他工具的同步教程\n\n- [TensorBoard](/guide_cloud/integration/integration-tensorboard.md)\n- [Weights & Biases](/guide_cloud/integration/integration-wandb.md)\n:::\n\n**你可以用两种方式将MLflow上的项目同步到SwanLab：**\n\n1. **同步跟踪**：如果你现在的项目使用了mlflow进行实验跟踪，你可以使用`swanlab.sync_mlflow()`命令，在运行训练脚本时同步记录指标到SwanLab。\n2. **转换已存在的项目**：如果你想要将mlflow上的项目复制到SwanLab，你可以使用`swanlab convert`，将mlflow上已存在的项目转换成SwanLab项目。\n\n::: info\n在当前版本暂仅支持转换标量图表。\n:::\n\n[[toc]]",
    "527": "一级标题：MLFlow\n二级标题：1. 同步跟踪\n内容：\n### 1.1 添加sync_mlflow命令\n\n在你的代码执行`mlflow.start_run()`之前的任何位置，添加一行`swanlab.sync()`命令，即可在运行训练脚本时同步记录指标到SwanLab。\n\n```python\nimport swanlab\n\nswanlab.sync_mlflow()\n\n...\n\nmlflow.start_run()\n```\n\n在上述这种代码写法中，`mlflow.start_run()`的同时会初始化swanlab，项目名、实验名和配置和`mlflow.start_run()`中的`experiment_name`、`run_name`、`log_param`一致，因此你不需要再手动初始化swanlab。\n\n\n### 1.2 另一种写法\n\n另一种用法是先手动初始化swanlab，再运行mlflow的代码。\n\n```python\nimport swanlab\n\nswanlab.init(...)\nswanlab.sync_mlflow()\n```\n\n在这种写法中，项目名、实验名、配置和`swanlab.init()`中的`project`、`experiment_name`、`config`一致，而后续`mlflow.start_run()`中的`experiment_name`、`run_name`会被忽略，`config`会更新进`swanlab.config`中。\n\n### 1.3 测试代码\n\n```python\nimport mlflow\nimport random\nimport swanlab\n\nswanlab.sync_mlflow()\n\nmlflow.set_experiment(\"mlflow_sync_test\")\n\nwith mlflow.start_run(run_name=\"test_run\"):\n    mlflow.log_param(\"learning_rate\", 0.01)\n    mlflow.log_params({\"batch_size\": 32, \"epochs\": 10})\n    \n    for epoch in range(10):\n        acc = 1 - 2 ** -epoch - random.random() / epoch\n        loss = 2 ** -epoch + random.random() / epoch\n        mlflow.log_metric(\"accuracy\", acc, step=epoch)\n        mlflow.log_metric(\"loss\", loss, step=epoch)\n        \n        mlflow.log_metrics({\n            \"precision\": acc * 0.9,\n            \"recall\": acc * 0.8\n        }, step=epoch)\n```",
    "528": "一级标题：MLFlow\n二级标题：2. 转换已经存在的项目\n内容：\n### 2.1 准备工作\n\n**（必须）mlflow服务的url链接**\n\n首先，需要记下mlflow服务的**url链接**，如`http://127.0.0.1:5000`。\n\n> 如果还没有启动mlflow服务，那么需要使用`mlflow ui`命令启动服务，并记下url链接。\n\n**（可选）实验ID**\n\n如果你只想转换其中的一组实验，那么在下图所示的地方，记下该实验ID。\n\n![](./mlflow/ui-1.png)\n\n### 2.2 方式一：命令行转换\n\n转换命令行：\n\n```bash\nswanlab convert -t mlflow --mlflow-url <MLFLOW_URL> --mlflow-exp <MLFLOW_EXPERIMENT_ID>\n```\n\n支持的参数如下：\n\n- `-t`: 转换类型，可选wandb、tensorboard和mlflow。\n- `-p`: SwanLab项目名。\n- `-w`: SwanLab工作空间名。\n- `--cloud`: (bool) 是否上传模式为\"cloud\"，默认为True\n- `-l`: logdir路径。\n- `--mlflow-url`: mlflow服务的url链接。\n- `--mlflow-exp`: mlflow实验ID。\n\n如果不填写`--mlflow-exp`，则会将指定项目下的全部实验进行转换；如果填写，则只转换指定的实验组。\n\n### 2.3 方式二：代码内转换\n\n```python\nfrom swanlab.converter import MLFLowConverter\n\nmlflow_converter = MLFLowConverter(project=\"mlflow_converter\")\n# mlflow_exp可选\nmlflow_converter.run(tracking_uri=\"http://127.0.0.1:5000\", experiment=\"1\")\n```\n\n效果与命令行转换一致。\n\n`MLFLowConverter`支持的参数：\n\n- `project`: SwanLab项目名。\n- `workspace`: SwanLab工作空间名。\n- `cloud`: (bool) 是否上传模式为\"cloud\"，默认为True。\n- `logdir`: wandb Run（项目下的某一个实验）的id。",
    "529": "一级标题：MMDetection\n二级标题：无\n内容：\n:::info 教程\n[mmdetection如何使用swanlab远程查看训练日志](https://zhuanlan.zhihu.com/p/699058426)\n:::\n\n<div align=\"center\">\n<img src=\"/assets/integration-mmdetection.png\" width=600>\n</div>\n\n[MMdetection](https://github.com/open-mmlab/mmdetection) 是一个由 [OpenMMLab](https://openmmlab.com/) 社区开发的深度学习训练框架，建立在 PyTorch 深度学习框架之上，旨在为研究人员和工程师提供一个高效、灵活、易于扩展的目标检测平台。MMDetection 支持多种主流的目标检测方法，并提供了大量预训练模型和丰富的配置选项，使得在目标检测任务中的应用和开发变得更加便捷。\n\n<div align=\"center\">\n<img src=\"/assets/integration-mmdetection-intro.png\">\n</div>\n\n可以通过修改MMDetection的配置文件来使用SwanLab作为实验记录工具。",
    "530": "一级标题：MMDetection\n二级标题：在配置文件中指定SwanLab作为VisBackend\n内容：\n确保你安装了SwanLab，或者使用`pip install -U swanlab`安装最新版。\n\n将如下内容添加到mmdetection的config文件中, 其中`init_kwargs`中填入的参数字典与`swanlab.init`的规则一致:\n\n```python\n# swanlab visualizer\ncustom_imports = dict(  # 引入SwanLab作为日志记录器，对于部分不支持custom_imports的项目可以直接初始化SwanlabVisBackend并加入vis_backends\n    imports=[\"swanlab.integration.mmengine\"], allow_failed_imports=False\n)\n\nvis_backends = [\n    dict(\n        type=\"SwanlabVisBackend\",\n        init_kwargs={ # swanlab.init 参数\n            \"project\": \"swanlab-mmengine\",\n            \"experiment_name\": \"Your exp\",  # 实验名称\n            \"description\": \"Note whatever you want\",  # 实验的描述信息\n        },\n    ),\n]\n\nvisualizer = dict(\n    type=\"Visualizer\",\n    vis_backends=vis_backends,\n    name=\"visualizer\",\n)\n```\n\n> 有关其他引入方法和更灵活的配置，可以参考[MMEngine接入SwanLab](https://docs.swanlab.cn/zh/guide_cloud/integration/integration-mmengine.html)",
    "531": "一级标题：MMDetection\n二级标题：使用案例：MMDetection训练faster-rcnn\n内容：\n首先克隆[MMDetction](https://github.com/open-mmlab/mmdetection)项目到本地。\n\n然后在faster-rnn对应的config文件（`configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py`）的最后增加下面的代码：\n\n```python\n_base_ = [\n    '../_base_/models/faster-rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n\n# swanlab\ncustom_imports = dict(  # 引入SwanLab作为日志记录器\n    imports=[\"swanlab.integration.mmengine\"], allow_failed_imports=False\n)\nvis_backends = [\n    dict(type=\"LocalVisBackend\"),\n    dict(\n        type=\"SwanlabVisBackend\",\n        init_kwargs={  # swanlab.init 参数\n            \"project\": \"MMDetection\",  # 项目名称\n            \"experiment_name\": \"faster-rcnn\",  # 实验名称\n            \"description\": \"faster-rcnn r50 fpn 1x coco\",  # 实验的描述信息\n        },\n    ),\n]\nvisualizer = dict(\n    type=\"DetLocalVisualizer\", vis_backends=vis_backends, name=\"visualizer\"\n)\n```\n\n**然后开启训练即可**：\n\n```bash\npython tools/train.py configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py\n```\n\n![ig-mmengine-1](/assets/ig-mmengine-1.png)\n\n**在swanlab中远程查看训练日志**：\n\n![ig-mmengine-2](/assets/ig-mmengine-2.png)",
    "532": "一级标题：MMEngine\n二级标题：无\n内容：\n[MMEngine](https://github.com/open-mmlab/mmengine) 是一个由 [OpenMMLab](https://openmmlab.com/) 社区开发的深度学习训练框架，专为深度学习研究和开发而设计。MMEngine 提供了一种高效、灵活且用户友好的方式来构建、训练和测试深度学习模型，尤其是在计算机视觉领域。它的目标是简化研究人员和开发者在深度学习项目中的工作流程，并提高其开发效率。\n\n<div align=\"center\">\n<img src=\"/assets/integration-mmengine.jpeg\" width=440>\n</div>\n\nMMEngine 为 OpenMMLab 算法库实现了下一代训练架构，为 OpenMMLab 中的 30 多个算法库提供了统一的执行基础。其核心组件包括训练引擎、评估引擎和模块管理。\n\nSwanLab将专为MMEngine设计的`SwanlabVisBackend`集成到MMEngine中，可用于记录训练、评估指标、记录实验配置、记录图像等。\n\n::: warning MM生态的其他集成\n\n- [MMPretrain](/zh/guide_cloud/integration/integration-mmpretrain.md)\n- [MMDetection](/zh/guide_cloud/integration/integration-mmdetection.md)\n- [MMSegmentation](/zh/guide_cloud/integration/integration-mmsegmentation.md)\n- [XTuner](/zh/guide_cloud/integration/integration-xtuner.md)\n\n:::",
    "533": "一级标题：MMEngine\n二级标题：MMEngine系列框架兼容性说明\n内容：\n使用mmengine的框架都可以使用如下方法引入SwanLab，比如MM官方框架 [mmdetection](https://docs.swanlab.cn/zh/guide_cloud/integration/integration-mmdetection.html)，[mmsegmentation](https://docs.swanlab.cn/zh/guide_cloud/integration/integration-mmsegmentation.html)等，以及[自己基于mmengine实现的训练框架](https://mmengine.readthedocs.io/zh-cn/latest/get_started/15_minutes.html)。\n\n> 可以在[OpenMMLab官方GitHub账号](https://github.com/open-mmlab)下查看有哪些优秀框架。\n\n部分框架比如[Xtuner](https://github.com/InternLM/xtuner)项目，其没有完全兼容mmengine，需要做一些简单改动，可以前往[SwanLab的Xtuner集成](https://docs.swanlab.cn/zh/guide_cloud/integration/integration-xtuner.html)查看如何在Xtuner中使用SwanLab。\n\nmmengine有两种引入SwanLab进行实验可视化跟踪的方法：",
    "534": "一级标题：MMEngine\n二级标题：使用方法一：训练脚本传入visualizer，开始训练\n内容：\n:::info\n可以参考[mmengine15分钟教程](https://mmengine.readthedocs.io/zh-cn/latest/get_started/15_minutes.html)将自己的训练代码适配mmengine\n:::\n\n确保你安装了SwanLab，或者使用`pip install -U swanlab`安装最新版。\n\n如果你按照官方案例使用了mmengine作为你的训练框架。只需在训练脚本中进行如下改动：\n1. 在初始化`visualizer`时加入SwanlabVisBackend\n2. 初始化`runner`传入`visualizer`即可：\n\n```python (10,20)\nfrom mmengine.visualization import Visualizer\nfrom mmengine.runner import Runner\n\nfrom swanlab.integration.mmengine import SwanlabVisBackend\n...\n# 初始化SwanLab\nswanlab_vis_backend = SwanlabVisBackend(init_kwargs={})# init args can be found in https://docs.swanlab.cn/zh/guide_cloud/integration/integration-mmengine.html\n# 初始化mmegine的Visulizer，并引入SwanLab作为Visual Backend\nvisualizer = Visualizer(\n    vis_backends=swanlab_vis_backend\n)  \n\n# 构建mmengine的Runner\nrunner = Runner(\n    model,\n    work_dir='runs/gan/',\n    train_dataloader=train_dataloader,\n    train_cfg=train_cfg,\n    optim_wrapper=opt_wrapper_dict,\n    visualizer=visualizer,\n)\n\n# 开始训练\nrunner.train()\n```\n\n如果希望像平常使用swanlab那样指定实验名等信息，可以在实例化SwanlabVisBackend时在init_kwargs中指定参数，具体参考[init api](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/sdk.py#L71)，不过不像使用`swanlab.init`那样直接作为参数传入，而是需要构建字典。\n\n下面列举了两者在交互上的不同：\n\n直接使用`swanlab.init`:\n\n```python\nrun = swanlab.init(\n    project=\"cat-dog-classification\",\n    experiment_name=\"Resnet50\",\n    description=\"我的第一个人工智能实验\",\n)\n```\n\n使用`SwanlabVisBackend`，则是以字典的形式传入`init`的参数:\n\n```python\nswanlab_vis_backend = SwanlabVisBackend(\n    init_kwargs={\n        \"project\": \"cat-dog-classification\",\n        \"experiment_name\": \"Resnet50\",\n        \"description\": \"我的第一个人工智能实验\",\n    }\n)\n```",
    "535": "一级标题：MMEngine\n二级标题：使用方法二：config文件引入SwanlabVisBackend\n内容：\n:::info\n此方法对于大多数基于mmengine的训练框架都是适用的\n:::\n\n将如下内容添加到mm系列框架的任意config文件中, 其中`init_kwargs`中填入的参数字典与`swanlab.init`的规则一致:\n\n```python\n# swanlab visualizer\ncustom_imports = dict(  # 引入SwanLab作为日志记录器，对于部分不支持custom_imports的项目可以直接初始化SwanlabVisBackend并加入vis_backends\n    imports=[\"swanlab.integration.mmengine\"], allow_failed_imports=False\n)\n\nvis_backends = [\n    dict(\n        type=\"SwanlabVisBackend\",\n        init_kwargs={ # swanlab.init 参数\n            \"project\": \"swanlab-mmengine\",\n            \"experiment_name\": \"Your exp\",  # 实验名称\n            \"description\": \"Note whatever you want\",  # 实验的描述信息\n        },\n    ),\n]\n\nvisualizer = dict(\n    type=\"Visualizer\",\n    vis_backends=vis_backends,\n    name=\"visualizer\",\n)\n```\n\n可以使用如下代码测试config文件是否能够成功引入SwanLab，将上面的config文件保存为`my_swanlab_config.py`，创建一个`test_config.py`写入如下代码并运行：\n\n```python\nfrom mmengine.config import Config\nimport mmengine\n\nprint(mmengine.__version__)\ncfg = Config.fromfile(\n    \"my_swanlab_config.py\"\n)\n\nfrom mmengine.registry import VISUALIZERS\n\ncustom_vis = VISUALIZERS.build(cfg.visualizer)\nprint(custom_vis)\n\n```\n\n如果看到终端打印出类似如下信息，则表示成功引入了swanlab：\n\n```console\nMMEngine Version: 0.10.4\nSwanLab Version: 0.3.11\n<mmengine.visualization.visualizer.Visualizer object at 0x7f7cf15b1e20>\n```",
    "536": "一级标题：MMEngine\n二级标题：3.案例：MMEngine训练ResNet-50\n内容：\n:::info 参考MMEngine官方15分钟上手教程\n[15 分钟上手 MMENGINE](https://mmengine.readthedocs.io/zh-cn/latest/get_started/15_minutes.html)\n:::\n\n按照[MMEngine官方教程](https://mmengine.readthedocs.io/zh-cn/latest/get_started/installation.html)安装MMEngine。\n\n这里将安装环境的命令抄录下来，强烈建议按照官方文档安装，以环境为python3.11，CUDA12.1为例。\n\n```sh\n# with cuda12.1 or you can find torch version you want at pytorch.org\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\npip install -U openmim\nmim install mmengine\npip install swanlab\n```\n\n使用如下代码构建ResNet-50网络并引入Cifar10数据集开始训练\n\n```python\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.optim import SGD\nfrom torch.utils.data import DataLoader\n\nfrom mmengine.evaluator import BaseMetric\nfrom mmengine.model import BaseModel\nfrom mmengine.runner import Runner\nfrom mmengine.visualization import Visualizer\n\nfrom swanlab.integration.mmengine import SwanlabVisBackend\n\n\nclass MMResNet50(BaseModel):\n    def __init__(self):\n        super().__init__()\n        self.resnet = torchvision.models.resnet50()\n\n    def forward(self, imgs, labels, mode):\n        x = self.resnet(imgs)\n        if mode == \"loss\":\n            return {\"loss\": F.cross_entropy(x, labels)}\n        elif mode == \"predict\":\n            return x, labels\n\n\nclass Accuracy(BaseMetric):\n    def process(self, data_batch, data_samples):\n        score, gt = data_samples\n        self.results.append(\n            {\n                \"batch_size\": len(gt),\n                \"correct\": (score.argmax(dim=1) == gt).sum().cpu(),\n            }\n        )\n\n    def compute_metrics(self, results):\n        total_correct = sum(item[\"correct\"] for item in results)\n        total_size = sum(item[\"batch_size\"] for item in results)\n        return dict(accuracy=100 * total_correct / total_size)\n\n\nnorm_cfg = dict(mean=[0.491, 0.482, 0.447], std=[0.202, 0.199, 0.201])\ntrain_dataloader = DataLoader(\n    batch_size=32,\n    shuffle=True,\n    dataset=torchvision.datasets.CIFAR10(\n        \"data/cifar10\",\n        train=True,\n        download=True,\n        transform=transforms.Compose(\n            [\n                transforms.RandomCrop(32, padding=4),\n                transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                transforms.Normalize(**norm_cfg),\n            ]\n        ),\n    ),\n)\n\nval_dataloader = DataLoader(\n    batch_size=32,\n    shuffle=False,\n    dataset=torchvision.datasets.CIFAR10(\n        \"data/cifar10\",\n        train=False,\n        download=True,\n        transform=transforms.Compose(\n            [transforms.ToTensor(), transforms.Normalize(**norm_cfg)]\n        ),\n    ),\n)\n\nvisualizer = Visualizer(\n    vis_backends=SwanlabVisBackend(init_kwargs={})\n)  # init args can be found in https://docs.swanlab.cn/zh/guide_cloud/integration/integration-mmengine.html\n\nrunner = Runner(\n    model=MMResNet50(),\n    work_dir=\"./work_dir\",\n    train_dataloader=train_dataloader,\n    optim_wrapper=dict(optimizer=dict(type=SGD, lr=0.001, momentum=0.9)),\n    train_cfg=dict(by_epoch=True, max_epochs=5, val_interval=1),\n    val_dataloader=val_dataloader,\n    val_cfg=dict(),\n    val_evaluator=dict(type=Accuracy),\n    visualizer=visualizer,\n)\nrunner.train()\n\n```\n\n可以在[公开训练图表](https://swanlab.cn/@ShaohonChen/cifar10_with_resnet50/runs/f8znz8vj06huv6rm7j5a8/chart)查看到上脚本的训练结果。\n\n<div align=\"center\">\n<img src=\"/assets/integration-mmegine-train.png\" width=600>\n</div>",
    "537": "一级标题：MMPretrain\n二级标题：无\n内容：\n[MMPretrain](https://github.com/open-mmlab/mmpretrain) 是 [OpenMMLab](https://openmmlab.com/) 旗下的一个开源预训练模型库，专注于为计算机视觉任务提供高效、易用的预训练模型。\n\n<div align=\"center\">\n<img src=\"/assets/integration-mmpretrain.jpg\" width=440>\n</div>\n\n基于 PyTorch 构建，MMPretrain 旨在帮助研究人员和开发人员快速应用和评估预训练模型，从而提升下游任务的性能和效率。该库包含了多种预训练模型，如 ResNet、Vision Transformer（ViT）和 Swin Transformer 等，这些模型经过大规模数据集的训练，能够直接用于图像分类、目标检测和分割等任务。此外，MMPretrain 提供了灵活的配置系统和丰富的接口，用户可以方便地进行模型的加载、微调和评估。详细的文档和教程使得用户能够快速上手和应用，适用于学术研究和工业实践中的各种场景。通过使用 MMPretrain，用户可以显著减少模型训练时间，专注于模型优化和应用创新。\n\n可以通过修改MMPretrain的配置文件来使用SwanLab作为实验记录工具。",
    "538": "一级标题：MMPretrain\n二级标题：在配置文件中指定\n内容：\n确保你安装了SwanLab，或者使用`pip install -U swanlab`安装最新版。\n\n将如下内容添加到所使用的mmpretrain的config文件中, 其中`init_kwargs`中填入的参数字典与`swanlab.init`的规则一致:\n\n```python\n...\n# swanlab visualizer\ncustom_imports = dict(  # 引入SwanLab作为日志记录器，对于部分不支持custom_imports的项目可以直接初始化SwanlabVisBackend并加入vis_backends\n    imports=[\"swanlab.integration.mmengine\"], allow_failed_imports=False\n)\n\nvis_backends = [\n    dict(\n        type=\"SwanlabVisBackend\",\n        init_kwargs={ # swanlab.init 参数\n            \"project\": \"swanlab-mmengine\",\n            \"experiment_name\": \"Your exp\",  # 实验名称\n            \"description\": \"Note whatever you want\",  # 实验的描述信息\n        },\n    ),\n]\n\nvisualizer = dict(\n    type=\"Visualizer\",\n    vis_backends=vis_backends,\n    name=\"visualizer\",\n)\n...\n```\n\n> 有关其他引入方法和更灵活的配置，可以参考[MMEngine接入SwanLab](https://docs.swanlab.cn/zh/guide_cloud/integration/integration-mmengine.html)",
    "539": "一级标题：MMSegmentation\n二级标题：无\n内容：\n[MMSegmentation](https://github.com/open-mmlab/mmengine) 是一个由 [OpenMMLab](https://openmmlab.com/) 社区开发的深度学习训练框架，基于 PyTorch 构建，旨在为研究人员和开发人员提供便捷高效的图像分割解决方案。\n\n<div align=\"center\">\n<img src=\"/assets/integration-mmsegmentation.png\" width=440>\n</div>\n\n该工具箱采用模块化设计，提供多种预训练模型如 U-Net、DeepLabV3 和 PSPNet 等，支持语义分割、实例分割和全景分割任务。MMSegmentation 内置强大的数据处理功能和多种分割性能评价指标，如 mIoU 和 Dice 系数，能够全面评估模型性能。其灵活的配置系统允许用户快速进行实验配置和参数调整。\n\n<div align=\"center\">\n<img src=\"/assets/integration-mmsegmentation-demo.gif\">\n</div>\n\nMMSegmentation 提供详细的文档和示例，帮助用户快速上手，并支持分布式训练和模型加速推理。该工具箱广泛应用于医学图像分割、遥感图像分割和自动驾驶等领域。\n\n可以通过修改MMSegmentation的配置文件来使用SwanLab作为实验记录工具。",
    "540": "一级标题：MMSegmentation\n二级标题：在配置文件中指定\n内容：\n确保你安装了SwanLab，或者使用`pip install -U swanlab`安装最新版。\n\n将如下内容添加到所使用的mmsegmentation的config文件中, 其中`init_kwargs`中填入的参数字典与`swanlab.init`的规则一致:\n\n```python\n...\n# swanlab visualizer\ncustom_imports = dict(  # 引入SwanLab作为日志记录器，对于部分不支持custom_imports的项目可以直接初始化SwanlabVisBackend并加入vis_backends\n    imports=[\"swanlab.integration.mmengine\"], allow_failed_imports=False\n)\n\nvis_backends = [\n    dict(\n        type=\"SwanlabVisBackend\",\n        init_kwargs={ # swanlab.init 参数\n            \"project\": \"swanlab-mmengine\",\n            \"experiment_name\": \"Your exp\",  # 实验名称\n            \"description\": \"Note whatever you want\",  # 实验的描述信息\n        },\n    ),\n]\n\nvisualizer = dict(\n    type=\"Visualizer\",\n    vis_backends=vis_backends,\n    name=\"visualizer\",\n)\n...\n```\n\n> 有关其他引入方法和更灵活的配置，可以参考[MMEngine接入SwanLab](https://docs.swanlab.cn/zh/guide_cloud/integration/integration-mmengine.html)",
    "541": "一级标题：Omegaconf\n二级标题：无\n内容：\nOmegaConf 是一个用于处理配置的 Python 库，尤其适用于需要灵活配置和配置合并的场景。\nOmegaConf 与swanlab的集成非常简单，直接将`omegaconf`对象传递给`swanlab.config`，即可记录为超参数：\n\n```python\nfrom omegaconf import OmegaConf\nimport swanlab\n\ncfg = OmegaConf.load(\"config.yaml\")\nswanlab.init(config=cfg,)\n```\n\n如果传递`cfg`时出现意外的结果，那么可以先转换`omegaconf.DictConfig`为原始类型：\n\n```python\nfrom omegaconf import OmegaConf\nimport swanlab\n\ncfg = OmegaConf.load(\"config.yaml\")\nswanlab.init(config=OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True))\n\n```",
    "542": "一级标题：OpenAI\n二级标题：无\n内容：\n[openai](https://github.com/openai/openai-python)是ChatGPT在Python环境下使用的核心库。\n\n![openai](/assets/ig-openai.png)\n\n你可以使用openai获得ChatGPT的回复，同时使用SwanLab自动进行过程记录。",
    "543": "一级标题：OpenAI\n二级标题：1. 引入autolog\n内容：\n```python\nfrom swanlab.integration.openai import autolog\n```\n\n`autolog`是一个为openai适配的过程记录类，能够自动记录你的openai交互的过程。",
    "544": "一级标题：OpenAI\n二级标题：2. 传入参数\n内容：\n```python\nautolog(init={\"project\":\"openai_autologging\", \"experiment_name\":\"chatgpt4.0\"})\n```\n\n这里给`init`传入的参数与`swanlab.init`的参数形式完全一致。",
    "545": "一级标题：OpenAI\n二级标题：3. 自动记录\n内容：\n由于`openai`在1.0.0版本以后，采用了和先前不一样的API设计，所以下面分为两个版本：\n\n### openai>=1.0.0\n\n需要将`client=openai.OpenAI()`替换为`client=autolog.client`。\n\n```python\nfrom swanlab.integration.openai import autolog\n\nautolog(init=dict(experiment_name=\"openai_autologging\"))\nclient = autolog.client\n\n# chat_completion\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-0125\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n        {\"role\": \"user\", \"content\": \"Who won the world series in 2015?\"},\n    ],\n)\n\n# text_completion\nresponse2 = client.completions.create(model=\"gpt-3.5-turbo-instruct\", prompt=\"Write a song for jesus.\")\n```\n\n### openai<=0.28.0\n\n```python\nimport openai\nfrom swanlab.integration.openai import autolog\n\nautolog(init=dict(experiment_name=\"openai_logging123\"))\n\nchat_request_kwargs = dict(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers\"},\n        {\"role\": \"user\", \"content\": \"Where was it played?\"},\n    ],\n)\nresponse = openai.ChatCompletion.create(**chat_request_kwargs)\n```",
    "546": "一级标题：PaddleDetection\n二级标题：无\n内容：\n[PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection) 是百度基于其深度学习框架 PaddlePaddle 开发的一个端到端的目标检测开发工具包。它支持对象检测、实例分割、多对象跟踪和实时多人关键点检测，旨在帮助开发者更高效地进行目标检测模型的开发和训练。\n\n![PaddleDetection](/assets/ig-paddledetection-1.png)\n\n你可以使用PaddleDetection快速进行目标检测模型训练，同时使用SwanLab进行实验跟踪与可视化。",
    "547": "一级标题：PaddleDetection\n二级标题：1. 引入SwanLabCallback\n内容：\n首先在你clone的PaddleDetection项目中，找到`ppdet/engine/callbacks.py`文件，在代码的底部添加如下代码：\n\n```python\nclass SwanLabCallback(Callback):\n    def __init__(self, model):\n        super(SwanLabCallback, self).__init__(model)\n\n        try:\n            import swanlab\n            self.swanlab = swanlab\n        except Exception as e:\n            logger.error('swanlab not found, please install swanlab. '\n                         'Use: `pip install swanlab`.')\n            raise e\n\n        self.swanlab_params = {k[8:]: v for k, v in model.cfg.items() if k.startswith(\"swanlab_\")}\n\n        self._run = None\n        if dist.get_world_size() < 2 or dist.get_rank() == 0:\n            _ = self.run\n            self.run.config.update(self.model.cfg)\n\n        self.best_ap = -1000.\n        self.fps = []\n\n    @property\n    def run(self):\n        if self._run is None:\n            self._run = self.swanlab.get_run() or self.swanlab.init(**self.swanlab_params)\n        return self._run\n\n    def on_step_end(self, status):\n        if dist.get_world_size() < 2 or dist.get_rank() == 0 and status['mode'] == 'train':\n            training_status = status['training_staus'].get()\n            batch_time = status['batch_time']\n            data_time = status['data_time']\n            batch_size = self.model.cfg['{}Reader'.format(status['mode'].capitalize())]['batch_size']\n\n            ips = float(batch_size) / float(batch_time.avg)\n            metrics = {\n                \"train/\" + k: float(v) for k, v in training_status.items()\n            }\n            metrics.update({\n                \"train/ips\": ips,\n                \"train/data_cost\": float(data_time.avg),\n                \"train/batch_cost\": float(batch_time.avg)\n            })\n\n            self.fps.append(ips)\n            self.run.log(metrics)\n\n    def on_epoch_end(self, status):\n        if dist.get_world_size() < 2 or dist.get_rank() == 0:\n            mode = status['mode']\n            epoch_id = status['epoch_id']\n            \n            if mode == 'train':\n                fps = sum(self.fps) / len(self.fps)\n                self.fps = []\n\n                end_epoch = self.model.cfg.epoch\n                if (epoch_id + 1) % self.model.cfg.snapshot_epoch == 0 or epoch_id == end_epoch - 1:\n                    save_name = str(epoch_id) if epoch_id != end_epoch - 1 else \"model_final\"\n                    tags = [\"latest\", f\"epoch_{epoch_id}\"]\n            \n            elif mode == 'eval':\n                fps = status['sample_num'] / status['cost_time']\n\n                merged_dict = {\n                    f\"eval/{key}-mAP\": map_value[0]\n                    for metric in self.model._metrics\n                    for key, map_value in metric.get_results().items()\n                }\n                merged_dict.update({\n                    \"epoch\": status[\"epoch_id\"],\n                    \"eval/fps\": fps\n                })\n\n                self.run.log(merged_dict)\n\n                if status.get('save_best_model'):\n                    for metric in self.model._metrics:\n                        map_res = metric.get_results()\n                        key = next((k for k in ['bbox', 'keypoint', 'mask'] if k in map_res), None)\n                        \n                        if not key:\n                            logger.warning(\"Evaluation results empty, this may be due to \"\n                                           \"training iterations being too few or not \"\n                                           \"loading the correct weights.\")\n                            return\n                        \n                        if map_res[key][0] >= self.best_ap:\n                            self.best_ap = map_res[key][0]\n                            save_name = 'best_model'\n                            tags = [\"best\", f\"epoch_{epoch_id}\"]\n\n    def on_train_end(self, status):\n        self.run.finish()\n```",
    "548": "一级标题：PaddleDetection\n二级标题：2. 修改trainer代码\n内容：\n在`ppdet/engine/trainer.py`文件中，在`from .callbacks import`那一行添加SwanLabCallback：\n\n```python\nfrom .callbacks import Callback, ComposeCallback, LogPrinter, Checkpointer, WiferFaceEval, VisualDLWriter, SniperProposalsGenerator, WandbCallback, SemiCheckpointer, SemiLogPrinter, SwanLabCallback\n```\n\n接着，我们找到`Trainer`类的`__init_callbacks`方法，在`if self.mode == 'train':`下添加如下代码：\n\n```python\nif self.cfg.get('use_swanlab', False) or 'swanlab' in self.cfg:\n    self._callbacks.append(SwanLabCallback(self))\n```\n\n至此，你已经完成了SwanLab与PaddleYolo的集成！接下来，只需要在训练的配置文件中添加`use_swanlab: True`，即可开始可视化跟踪训练。",
    "549": "一级标题：PaddleDetection\n二级标题：3. 修改配置文件\n内容：\n我们以`yolov3_mobilenet_v1_roadsign`为例。\n\n在`configs/yolov3/yolov3_mobilenet_v1_roadsign.yml`文件中，在下面添加如下代码：\n\n```yaml\nuse_swanlab: true\nswanlab_project: PaddleYOLO # 可选\nswanlab_experiment_name: yolov3_mobilenet_v1_roadsign # 可选\nswanlab_description: 对PaddleYOLO的一次训练测试 # 可选\n# swanlab_workspace: swanhub # 组织名，可选\n```",
    "550": "一级标题：PaddleDetection\n二级标题：4. 开始训练\n内容：\n```bash\npython -u tools/train.py -c configs/yolov3/yolov3_mobilenet_v1_roadsign.yml --eval\n```\n\n在训练过程中，即可看到整个训练过程的日志，以及训练结束后自动生成的可视化图表。",
    "551": "一级标题：PaddleNLP\n二级标题：无\n内容：\n[PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP) 是一款基于飞桨深度学习框架的大语言模型(LLM)开发套件，支持在多种硬件上进行高效的大模型训练、无损压缩以及高性能推理。PaddleNLP 具备简单易用和性能极致的特点，致力于助力开发者实现高效的大模型产业级应用。\n\n![paddlenlp-image](./paddlenlp/logo.png)\n\n你可以使用`PaddleNLP`快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。",
    "552": "一级标题：PaddleNLP\n二级标题：1. 引入SwanLabCallback\n内容：\n```python\nfrom swanlab.integration.paddlenlp import SwanLabCallback\n```\n\n**SwanLabCallback**是适配于PaddleNLP的日志记录类。\n\n**SwanLabCallback**可以定义的参数有：\n\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。\n- 你也可以在外部通过`swanlab.init`创建项目，集成会将实验记录到你在外部创建的项目中。",
    "553": "一级标题：PaddleNLP\n二级标题：2. 传入Trainer\n内容：\n```python (1,7,12)\nfrom swanlab.integration.paddlenlp import SwanLabCallback\nfrom paddlenlp.trainer import  TrainingArguments, Trainer\n\n...\n\n# 实例化SwanLabCallback\nswanlab_callback = SwanLabCallback(project=\"paddlenlp-demo\")\n\ntrainer = Trainer(\n    ...\n    # 传入callbacks参数\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n```",
    "554": "一级标题：PaddleNLP\n二级标题：3. 完整案例代码\n内容：\n> 需要能连接上HuggingFace服务器下载数据集。\n\n```python {8,19,28}\n\"\"\"\n测试于：\npip install paddlepaddle-gpu==3.0.0 -i https://www.paddlepaddle.org.cn/packages/stable/cu126/\npip install paddlenlp==3.0.0b4\n\"\"\"\nfrom paddlenlp.trl import SFTConfig, SFTTrainer\nfrom datasets import load_dataset\nfrom swanlab.integration.paddlenlp import SwanLabCallback\n\ndataset = load_dataset(\"ZHUI/alpaca_demo\", split=\"train\")\n\ntraining_args = SFTConfig(\n    output_dir=\"Qwen/Qwen2.5-0.5B-SFT\",\n    device=\"gpu\",\n    per_device_train_batch_size=1,\n    logging_steps=20\n    )\n\nswanlab_callback = SwanLabCallback(\n    project=\"Qwen2.5-0.5B-SFT-paddlenlp\",\n    experiment_name=\"Qwen2.5-0.5B\",\n)\n\ntrainer = SFTTrainer(\n    args=training_args,\n    model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n    train_dataset=dataset,\n    callbacks=[swanlab_callback],\n)\ntrainer.train()\n```",
    "555": "一级标题：PaddleNLP\n二级标题：4. GUI效果展示\n内容：\n超参数自动记录：\n\n![ig-paddlenlp-gui-1](./paddlenlp/config.png)\n\n指标记录：\n\n![ig-paddlenlp-gui-2](./paddlenlp/chart.png)",
    "556": "一级标题：PaddleNLP\n二级标题：5 拓展：增加更多回调\n内容：\n试想一个场景，你希望在每个epoch结束时，让模型推理测试样例，并用swanlab记录推理的结果，那么你可以创建一个继承自`SwanLabCallback`的新类，增加或重构生命周期函数。比如：\n\n```python\nclass NLPSwanLabCallback(SwanLabCallback):    \n    def on_epoch_end(self, args, state, control, **kwargs):\n        test_text_list = [\"example1\", \"example2\"]\n        log_text_list = []\n        for text in test_text_list:\n            result = model(text)\n            log_text_list.append(swanlab.Text(result))\n            \n        swanlab.log({\"Prediction\": test_text_list}, step=state.global_step)\n```\n\n上面是一个在NLP任务下的新回调类，增加了`on_epoch_end`函数，它会在`transformers`训练的每个epoch结束时执行。",
    "557": "一级标题：PaddleYolo\n二级标题：无\n内容：\n[PaddleYolo](https://github.com/PaddlePaddle/PaddleYOLO) 是飞桨（PaddlePaddle）框架下的一个目标检测库，主要用于图像和视频中的物体检测。PaddleYOLO包含YOLO系列模型的相关代码，支持YOLOv3、PP-YOLO、PP-YOLOv2、PP-YOLOE、PP-YOLOE+、RT-DETR、YOLOX、YOLOv5、YOLOv6、YOLOv7、YOLOv8、YOLOv5u、YOLOv7u、YOLOv6Lite、RTMDet等模型\n\n你可以使用PaddleYolo快速进行目标检测模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n[Demo](https://swanlab.cn/@ZeyiLin/PaddleYOLO/runs/10zy8zickn2062kubch34/chart)",
    "558": "一级标题：PaddleYolo\n二级标题：1. 引入SwanLabCallback\n内容：\n首先在你clone的PaddleYolo项目中，找到`ppdet/engine/callbacks.py`文件，在代码的底部添加如下代码：\n\n```python\nclass SwanLabCallback(Callback):\n    def __init__(self, model):\n        super(SwanLabCallback, self).__init__(model)\n\n        try:\n            import swanlab\n            self.swanlab = swanlab\n        except Exception as e:\n            logger.error('swanlab not found, please install swanlab. '\n                         'Use: `pip install swanlab`.')\n            raise e\n\n        self.swanlab_params = {k[8:]: v for k, v in model.cfg.items() if k.startswith(\"swanlab_\")}\n\n        self._run = None\n        if dist.get_world_size() < 2 or dist.get_rank() == 0:\n            _ = self.run\n            self.run.config.update(self.model.cfg)\n\n        self.best_ap = -1000.\n        self.fps = []\n\n    @property\n    def run(self):\n        if self._run is None:\n            self._run = self.swanlab.get_run() or self.swanlab.init(**self.swanlab_params)\n        return self._run\n\n    def on_step_end(self, status):\n        if dist.get_world_size() < 2 or dist.get_rank() == 0 and status['mode'] == 'train':\n            training_status = status['training_staus'].get()\n            batch_time = status['batch_time']\n            data_time = status['data_time']\n            batch_size = self.model.cfg['{}Reader'.format(status['mode'].capitalize())]['batch_size']\n\n            ips = float(batch_size) / float(batch_time.avg)\n            metrics = {\n                \"train/\" + k: float(v) for k, v in training_status.items()\n            }\n            metrics.update({\n                \"train/ips\": ips,\n                \"train/data_cost\": float(data_time.avg),\n                \"train/batch_cost\": float(batch_time.avg)\n            })\n\n            self.fps.append(ips)\n            self.run.log(metrics)\n\n    def on_epoch_end(self, status):\n        if dist.get_world_size() < 2 or dist.get_rank() == 0:\n            mode = status['mode']\n            epoch_id = status['epoch_id']\n            \n            if mode == 'train':\n                fps = sum(self.fps) / len(self.fps)\n                self.fps = []\n\n                end_epoch = self.model.cfg.epoch\n                if (epoch_id + 1) % self.model.cfg.snapshot_epoch == 0 or epoch_id == end_epoch - 1:\n                    save_name = str(epoch_id) if epoch_id != end_epoch - 1 else \"model_final\"\n                    tags = [\"latest\", f\"epoch_{epoch_id}\"]\n            \n            elif mode == 'eval':\n                fps = status['sample_num'] / status['cost_time']\n\n                merged_dict = {\n                    f\"eval/{key}-mAP\": map_value[0]\n                    for metric in self.model._metrics\n                    for key, map_value in metric.get_results().items()\n                }\n                merged_dict.update({\n                    \"epoch\": status[\"epoch_id\"],\n                    \"eval/fps\": fps\n                })\n\n                self.run.log(merged_dict)\n\n                if status.get('save_best_model'):\n                    for metric in self.model._metrics:\n                        map_res = metric.get_results()\n                        key = next((k for k in ['bbox', 'keypoint', 'mask'] if k in map_res), None)\n                        \n                        if not key:\n                            logger.warning(\"Evaluation results empty, this may be due to \"\n                                           \"training iterations being too few or not \"\n                                           \"loading the correct weights.\")\n                            return\n                        \n                        if map_res[key][0] >= self.best_ap:\n                            self.best_ap = map_res[key][0]\n                            save_name = 'best_model'\n                            tags = [\"best\", f\"epoch_{epoch_id}\"]\n\n    def on_train_end(self, status):\n        self.run.finish()\n```",
    "559": "一级标题：PaddleYolo\n二级标题：2. 修改trainer代码\n内容：\n在`ppdet/engine/trainer.py`文件中，在`from .callbacks import`那一行添加SwanLabCallback：\n\n```python\nfrom .callbacks import Callback, ComposeCallback, LogPrinter, Checkpointer, VisualDLWriter, WandbCallback, SwanLabCallback\n```\n\n接着，我们找到`Trainer`类的`__init_callbacks`方法，在`if self.mode == 'train':`下添加如下代码：\n\n```python\nif self.cfg.get('use_swanlab', False) or 'swanlab' in self.cfg:\n    self._callbacks.append(SwanLabCallback(self))\n```\n\n至此，你已经完成了SwanLab与PaddleYolo的集成！接下来，只需要在训练的配置文件中添加`use_swanlab: True`，即可开始可视化跟踪训练。",
    "560": "一级标题：PaddleYolo\n二级标题：3. 修改配置文件\n内容：\n我们以`yolov3_mobilenet_v1_roadsign`为例。\n\n在`configs/yolov3/yolov3_mobilenet_v1_roadsign.yml`文件中，在下面添加如下代码：\n\n```yaml\nuse_swanlab: true\nswanlab_project: PaddleYOLO # 可选\nswanlab_experiment_name: yolov3_mobilenet_v1_roadsign # 可选\nswanlab_description: 对PaddleYOLO的一次训练测试 # 可选\n# swanlab_workspace: swanhub # 组织名，可选\n```",
    "561": "一级标题：PaddleYolo\n二级标题：4. 开始训练\n内容：\n```bash\npython -u tools/train.py -c configs/yolov3/yolov3_mobilenet_v1_roadsign.yml --eval\n```\n\n在训练过程中，即可看到整个训练过程的日志，以及训练结束后自动生成的可视化图表。\n\n![paddleyolo-image](/assets/ig-paddleyolo.png)",
    "562": "一级标题：PyTorch Lightning\n二级标题：无\n内容：\n[![](/assets/colab.svg)](https://colab.research.google.com/drive/1g1s86qobSvIuaFVxzDgzyZ-B4VzdTCym?usp=sharing)\n\n[PyTorch Lightning](https://github.com/Lightning-AI/pytorch-lightning)是一个开源的机器学习库，它建立在 PyTorch 之上，旨在帮助研究人员和开发者更加方便地进行深度学习模型的研发。Lightning 的设计理念是将模型训练中的繁琐代码（如设备管理、分布式训练等）与研究代码（模型架构、数据处理等）分离，从而使研究人员可以专注于研究本身，而不是底层的工程细节。\n\n![pytorch-lightning-image](/assets/ig-pytorch-lightning.png)\n\n你可以使用PyTorch Lightning快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。",
    "563": "一级标题：PyTorch Lightning\n二级标题：1. 引入SwanLabLogger\n内容：\n```python\nfrom swanlab.integration.pytorch_lightning import SwanLabLogger\n```\n\n**SwanLabLogger**是适配于PyTorch Lightning的日志记录类。\n\n**SwanLabLogger**可以定义的参数有：\n\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。\n- 你也可以在外部通过`swanlab.init`创建项目，集成会将实验记录到你在外部创建的项目中。",
    "564": "一级标题：PyTorch Lightning\n二级标题：2. 传入Trainer\n内容：\n```python (6,11)\nimport pytorch_lightning as pl\n\n...\n\n# 实例化SwanLabLogger\nswanlab_logger = SwanLabLogger(project=\"lightning-visualization\")\n\ntrainer = pl.Trainer(\n    ...\n    # 传入callbacks参数\n    logger=swanlab_logger,\n)\n\ntrainer.fit(...)\n```",
    "565": "一级标题：PyTorch Lightning\n二级标题：3. 完整案例代码\n内容：\n```python (1,65,70)\nfrom swanlab.integration.pytorch_lightning import SwanLabLogger\n\nimport importlib.util\nimport os\n\nimport pytorch_lightning as pl\nfrom torch import nn, optim, utils\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\n\n# define any number of nn.Modules (or use your current ones)\nencoder = nn.Sequential(nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 3))\ndecoder = nn.Sequential(nn.Linear(3, 128), nn.ReLU(), nn.Linear(128, 28 * 28))\n\n\n# define the LightningModule\nclass LitAutoEncoder(pl.LightningModule):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def training_step(self, batch, batch_idx):\n        # training_step defines the train loop.\n        # it is independent of forward\n        x, y = batch\n        x = x.view(x.size(0), -1)\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        loss = nn.functional.mse_loss(x_hat, x)\n        # Logging to TensorBoard (if installed) by default\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        # test_step defines the test loop.\n        # it is independent of forward\n        x, y = batch\n        x = x.view(x.size(0), -1)\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        loss = nn.functional.mse_loss(x_hat, x)\n        # Logging to TensorBoard (if installed) by default\n        self.log(\"test_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n        return optimizer\n\n\n# init the autoencoder\nautoencoder = LitAutoEncoder(encoder, decoder)\n\n\n# setup data\ndataset = MNIST(os.getcwd(), train=True, download=True, transform=ToTensor())\ntrain_dataset, val_dataset = utils.data.random_split(dataset, [55000, 5000])\ntest_dataset = MNIST(os.getcwd(), train=False, download=True, transform=ToTensor())\n\ntrain_loader = utils.data.DataLoader(train_dataset)\nval_loader = utils.data.DataLoader(val_dataset)\ntest_loader = utils.data.DataLoader(test_dataset)\n\nswanlab_logger = SwanLabLogger(\n    project=\"swanlab_example\",\n    experiment_name=\"example_experiment\",\n)\n\ntrainer = pl.Trainer(limit_train_batches=100, max_epochs=5, logger=swanlab_logger)\n\n\ntrainer.fit(model=autoencoder, train_dataloaders=train_loader, val_dataloaders=val_loader)\ntrainer.test(dataloaders=test_loader)\n\n```",
    "566": "一级标题：PyTorch Lightning\n二级标题：4. 注意：如多次调用trainer.fit\n内容：\n如果你在一次进程中多次调用`trainer.fit`（如N折交叉验证），那么需要在`trainer.fit`之后添加一行：\n\n```python\nswanlab_logger.experiment.finish()\n# 或swanlab.finish()\n```\n\n示例程序：\n\n```python\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom sklearn.model_selection import KFold\nimport pytorch_lightning as pl\nfrom swanlab.integration.pytorch_lightning import SwanLabLogger\nimport datetime\nimport argparse\n\nclass RandomDataset(Dataset):\n    def __init__(self, size=100):\n        self.x = torch.randn(size, 10)\n        self.y = (self.x.sum(dim=1) > 0).long()  # 简单分类任务\n\n    def __len__(self):\n        return len(self.x)\n\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n\nclass SimpleClassifier(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = torch.nn.Linear(10, 2)\n        self.loss_fn = torch.nn.CrossEntropyLoss()\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss_fn(logits, y)\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss_fn(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log(\"val_loss\", loss)\n        self.log(\"val_acc\", acc)\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=1e-3)\n\ndef main(args):\n    dataset = RandomDataset(size=100)\n    kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n\n    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n        print(f\"\\nFold {fold + 1}/3\")\n\n        train_loader = DataLoader(Subset(dataset, train_idx), batch_size=16, shuffle=True)\n        val_loader = DataLoader(Subset(dataset, val_idx), batch_size=16)\n\n        # 日志名称\n        current_time = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n        run_name = f\"{args.save_name}_fold{fold + 1}_{current_time}\"\n\n        swanlab_logger = SwanLabLogger(\n            project=\"swanlab_example\",\n            experiment_name=run_name,\n        )\n\n        model = SimpleClassifier()\n\n        trainer = pl.Trainer(\n            max_epochs=5,\n            logger=swanlab_logger,\n            log_every_n_steps=1\n        )\n\n        trainer.fit(model, train_loader, val_loader)\n        swanlab_logger.experiment.finish()\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--save_name\", type=str, default=\"test_swan\")\n    args = parser.parse_args()\n    main(args)\n```",
    "567": "一级标题：Torchtune\n二级标题：无\n内容：\n[Torchtune](https://github.com/pytorch/torchtune)是一个 PyTorch 库，用于轻松编写、微调和试验LLMs。\n\n你可以使用`torchtune`快速进行LLM微调，同时使用SwanLab进行实验跟踪与可视化。",
    "568": "一级标题：Torchtune\n二级标题：1. 修改配置文件，引入SwanLabLogger\n内容：\n我们以使用`torchtune`微调Google的`gemma-2b`模型为例。\n\ntorchtune在微调一个模型时，需要训练者先准备一个配置文件，如用QLoRA微调Gemma-2b模型：[2B_qlora_single_device.yaml](https://github.com/pytorch/torchtune/blob/main/recipes/configs/gemma/2B_qlora_single_device.yaml)。\n\n下载后，编辑这个配置文件。我们在文件中找到下面的代码段：\n\n```yaml\n# Logging\nmetric_logger:\n  _component_: torchtune.utils.metric_logging.DiskLogger\n  log_dir: ${output_dir}\n```\n\n将该代码段替换为：\n\n```yaml\n# Logging\nmetric_logger:\n  _component_: swanlab.integration.torchtune.SwanLabLogger\n  project: \"gemma-fintune\"\n  experiment_name: \"gemma-2b\"\n  log_dir: ${output_dir}\n```\n\n其中，`_component_`对应的`swanlab.integration.torchtune.SwanLabLogger`是适配于PyTorch torchtune的日志记录类。而`project`、`experiment_name`等则是创建SwanLab项目传入的参数，支持传入的参数与[swanlab.init](/zh/api/py-init.html)规则一致。",
    "569": "一级标题：Torchtune\n二级标题：2. 开始训练\n内容：\n```bash\ntune run lora_finetune_single_device --config 2B_qlora_single_device.yaml\n```",
    "570": "一级标题：PyTorch\n二级标题：无\n内容：\n[![](/assets/colab.svg)](https://colab.research.google.com/drive/1RWsrY_1bS8ECzaHvYtLb_1eBkkdzekR3?usp=sharing)\n\n在学术研究者当中，[PyTorch](https://pytorch.org/) 是最流行的 Python 深度学习框架。  \n\n![PyTorch](/assets/ig-pytorch.png)\n\n你可以使用PyTorch进行深度学习模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n::: warning Pytorch生态的其他集成\n\n- [Lightning](/guide_cloud/integration/integration-pytorch-lightning.md)\n- [Torchtune](/guide_cloud/integration/integration-pytorch-torchtune.md)\n\n:::",
    "571": "一级标题：PyTorch\n二级标题：记录Tensor图像\n内容：\n你可以将带有图像数据的PyTorch `Tensors`传递给`swanlab.Image`，`swanlab.Image`将使用`torchvision`把它们转换成图像：\n\n```python\nimage_tensors = ...  # shape为[B, C, H, W]的Tensor图像\nswanlab.log({\"examples\": [swanlab.Image(im) for im in image_tensors]})\n```",
    "572": "一级标题：Ray\n二级标题：无\n内容：\n[Ray](https://github.com/ray-project/ray) 是一个分布式计算框架，专为大规模并行任务和强化学习应用设计。它由加州大学伯克利分校的研究团队开发，旨在简化构建高性能、可扩展的分布式应用程序的过程。Ray 支持 Python 和 Java，并且可以轻松集成到现有的机器学习、数据处理和强化学习工作流中。\n\n![ray](./ray/logo.png)\n\nSwanLab 支持 Ray 的实验记录，通过 `SwanLabLoggerCallback` 可以方便地记录实验指标和超参数。",
    "573": "一级标题：Ray\n二级标题：1. 引入SwanLabCallback\n内容：\n```python\nfrom swanlab.integration.ray import SwanLabLoggerCallback\n```\n\n`SwanLabLoggerCallback` 是适配于 `Ray` 的日志记录类。\n\n`SwanLabLoggerCallback`可以定义的参数有：\n- `project`: 项目名称\n- `workspace`: 工作空间名称\n- 其他和`swanlab.init`一致的参数",
    "574": "一级标题：Ray\n二级标题：2. 与`tune.Tuner`集成\n内容：\n```python\ntuner = tune.Tuner(\n    ...\n    run_config=tune.RunConfig(\n        callbacks=[SwanLabLoggerCallback(project=\"Ray_Project\")],\n    ),\n)\n```",
    "575": "一级标题：Ray\n二级标题：3. 完整案例\n内容：\n```python\nimport random\nfrom ray import tune\nfrom swanlab.integration.ray import SwanLabLoggerCallback\n\ndef train_func(config):\n    offset = random.random() / 5\n    for epoch in range(2, config[\"epochs\"]):\n        acc = 1 - (2 + config[\"lr\"]) ** -epoch - random.random() / epoch - offset\n        loss = (2 + config[\"lr\"]) ** -epoch + random.random() / epoch + offset\n        tune.report({\"acc\": acc, \"loss\": loss})\n\n\ntuner = tune.Tuner(\n    train_func,\n    param_space={\n        \"lr\": tune.grid_search([0.001, 0.01, 0.1, 1.0]),\n        \"epochs\": 10,\n    },\n    run_config=tune.RunConfig(\n        callbacks=[SwanLabLoggerCallback(project=\"Ray_Project\")],\n    ),\n)\nresults = tuner.fit()\n```\n\n![ray-tune](./ray/demo.png)",
    "576": "一级标题：ROLL\n二级标题：无\n内容：\n[ROLL](https://github.com/alibaba/ROLL) 是一个高效且用户友好的强化学习库，专为利用大规模 GPU 资源的大型语言模型 (LLM) 而设计。它显著提升了 LLM 在人类偏好对齐、复杂推理和多轮代理交互等关键领域的性能。\n\nROLL 利用 Ray 的多角色分布式架构实现灵活的资源分配和异构任务调度，并集成 Megatron-Core、SGLang 和 vLLM 等尖端技术来加速模型训练和推理。\n\n![ROLL](./roll/logo.png)\n\n在ROLL中使用SwanLab非常简单，只需要设置一些参数即可，详情参考 [agentic_pipeline_config.yaml](https://github.com/alibaba/ROLL/blob/main/tests/pipeline/agentic_pipeline_config.yaml) 中的`track_with: swanlab`部分。",
    "577": "一级标题：Stable-Baseline3\n二级标题：无\n内容：\n[![](/assets/colab.svg)](https://colab.research.google.com/drive/1JfU4oCKCS7FQE_AXqZ3k9Bt1vmK-6pMO?usp=sharing)\n\n\nStable Baselines3 (SB3) 是一个强化学习的开源库，基于 PyTorch 框架构建。它是 Stable Baselines 项目的继任者，旨在提供一组可靠且经过良好测试的RL算法实现，便于研究和应用。StableBaseline3主要被应用于机器人控制、游戏AI、自动驾驶、金融交易等领域。\n\n![sb3](/assets/ig-sb3.png)\n\n你可以使用sb3快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。",
    "578": "一级标题：Stable-Baseline3\n二级标题：1.引入SwanLabCallback\n内容：\n```python\nfrom swanlab.integration.sb3 import SwanLabCallback\n```\n\n**SwanLabCallback**是适配于 Stable Baselines3 的日志记录类。\n\n**SwanLabCallback**可以定义的参数有：\n\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。\n- 你也可以在外部通过`swanlab.init`创建项目，集成会将实验记录到你在外部创建的项目中。",
    "579": "一级标题：Stable-Baseline3\n二级标题：2.传入model.learn\n内容：\n```python (1,7)\nfrom swanlab.integration.sb3 import SwanLabCallback\n\n...\n\nmodel.learn(\n    ...\n    callback=SwanLabCallback(),\n)\n```\n在`model.learn`的`callback`参数传入`SwanLabCallback`实例，即可开始跟踪。",
    "580": "一级标题：Stable-Baseline3\n二级标题：3.完整案例代码\n内容：\n下面是一个PPO模型的简单训练案例，使用SwanLab做训练可视化和监控：\n\n```python (6,31)\nimport gymnasium as gym\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nimport swanlab\nfrom swanlab.integration.sb3 import SwanLabCallback\n\n\nconfig = {\n    \"policy_type\": \"MlpPolicy\",\n    \"total_timesteps\": 25000,\n    \"env_name\": \"CartPole-v1\",\n}\n\n\ndef make_env():\n    env = gym.make(config[\"env_name\"], render_mode=\"rgb_array\")\n    env = Monitor(env)\n    return env\n\n\nenv = DummyVecEnv([make_env])\nmodel = PPO(\n    config[\"policy_type\"],\n    env,\n    verbose=1,\n)\n\nmodel.learn(\n    total_timesteps=config[\"total_timesteps\"],\n    callback=SwanLabCallback(\n        project=\"PPO\",\n        experiment_name=\"MlpPolicy\",\n        verbose=2,\n    ),\n)\n\nswanlab.finish()\n\n```",
    "581": "一级标题：Sentence Transformers\n二级标题：无\n内容：\n[Sentence Transformers](https://github.com/UKPLab/sentence-transformers)(又名SBERT)是访问、使用和训练文本和图像嵌入（Embedding）模型的Python库。\n\n![](/assets/ig-sentence-transformers.png)\n\n你可以使用Sentence Transformers快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。",
    "582": "一级标题：Sentence Transformers\n二级标题：1. 引入SwanLabCallback\n内容：\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\n```\n\n**SwanLabCallback**是适配于HuggingFace系列工具（Transformers等）的日志记录类。\n\n**SwanLabCallback**可以定义的参数有：\n\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。\n- 你也可以在外部通过`swanlab.init`创建项目，集成会将实验记录到你在外部创建的项目中。",
    "583": "一级标题：Sentence Transformers\n二级标题：2. 传入Trainer\n内容：\n```python (1,7,12)\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom sentence_transformers import SentenceTransformer, SentenceTransformerTrainer\n\n...\n\n# 实例化SwanLabCallback\nswanlab_callback = SwanLabCallback(project=\"hf-visualization\")\n\ntrainer = SentenceTransformerTrainer(\n    ...\n    # 传入callbacks参数\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n```",
    "584": "一级标题：Sentence Transformers\n二级标题：3.完整案例代码\n内容：\n```python (4,12,19)\nfrom datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer, SentenceTransformerTrainer\nfrom sentence_transformers.losses import MultipleNegativesRankingLoss\nfrom swanlab.integration.transformers import SwanLabCallback\n\nmodel = SentenceTransformer(\"bert-base-uncased\")\n\ntrain_dataset = load_dataset(\"sentence-transformers/all-nli\", \"pair\", split=\"train[:10000]\")\neval_dataset = load_dataset(\"sentence-transformers/all-nli\", \"triplet\", split=\"dev\")\nmnrl_loss = MultipleNegativesRankingLoss(model)\n\nswanlab_callback = SwanLabCallback(project=\"sentence-transformers\", experiment_name=\"bert-all-nli\")\n\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    loss=mnrl_loss,\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n```",
    "585": "一级标题：Modelscope Swift\n二级标题：无\n内容：\n> SwanLab已经与Swift官方集成，见：[#3142](https://github.com/modelscope/ms-swift/pull/3142)  \n> 可视化在线Demo：[swift-robot](https://swanlab.cn/@ZeyiLin/swift-robot/runs/9lc9rmmwm4hh7ay1vkzd7/chart)\n\n[Modelscope魔搭社区](https://modelscope.cn/) 的 [Swift](https://github.com/modelscope/swift) 是一个集模型训练、微调、推理、部署于一体的框架。\n\n![logo](./swift/logo.png)\n\n🍲 **ms-swift** 是 ModelScope 社区提供的官方框架，用于微调和部署大型语言模型和多模态大型模型。它目前支持 **450+** 大型模型和 **150+** 多模态大型模型的训练（预训练、微调、人工对齐）、推理、评估、量化和部署。\n\n🍔 此外，ms-swift 还采用了**最新的训练技术**，包括 **LoRA、QLoRA、Llama-Pro、LongLoRA、GaLore、Q-GaLore、LoRA+、LISA、DoRA、FourierFt、ReFT、UnSloth 和 Liger 等轻量级技术**，以及 **DPO、GRPO、RM、PPO、KTO、CPO、SimPO 和 ORPO** 等人工对齐训练方法。\n\nms-swift 支持使用 vLLM 和 LMDeploy 加速推理、评估和部署模块，并支持使用 GPTQ、AWQ 和 BNB 等技术进行模型量化。此外，ms-swift 还提供了基于 Gradio 的 Web UI 和丰富的最佳实践。\n\n你可以使用Swift快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n[[toc]]",
    "586": "一级标题：Modelscope Swift\n二级标题：0. 安装ms-swift和swanlab\n内容：\n安装ms-swift（>=3.1.1）：\n\n```bash\npip install ms-swift\n```\n\n安装swanlab：\n\n```bash\npip install swanlab\n```",
    "587": "一级标题：Modelscope Swift\n二级标题：1. CLI微调\n内容：\n你只需要在ms-swift的CLI中添加`--report_to`和`--swanlab_project`两个参数，即可使用SwanLab进行实验跟踪与可视化：\n\n```bash\nswift sft \\\n    ...\n    --report_to swanlab \\  # [!code ++]\n    --swanlab_project swift-robot \\  # [!code ++]\n    ...\n```\n\n下面是在swift官方的CLI微调案例，中结合SwanLab的示例（见代码最后）：\n\n```bash {29-30}\n# 22GB\nCUDA_VISIBLE_DEVICES=0 \\\nswift sft \\\n    --model Qwen/Qwen2.5-7B-Instruct \\\n    --train_type lora \\\n    --dataset 'AI-ModelScope/alpaca-gpt4-data-zh#500' \\\n              'AI-ModelScope/alpaca-gpt4-data-en#500' \\\n              'swift/self-cognition#500' \\\n    --torch_dtype bfloat16 \\\n    --num_train_epochs 1 \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --learning_rate 1e-4 \\\n    --lora_rank 8 \\\n    --lora_alpha 32 \\\n    --target_modules all-linear \\\n    --gradient_accumulation_steps 16 \\\n    --eval_steps 50 \\\n    --save_steps 50 \\\n    --save_total_limit 5 \\\n    --logging_steps 5 \\\n    --max_length 2048 \\\n    --output_dir output \\\n    --system 'You are a helpful assistant.' \\\n    --warmup_ratio 0.05 \\\n    --dataloader_num_workers 4 \\\n    --model_author swift \\\n    --model_name swift-robot \\\n    --report_to swanlab \\\n    --swanlab_project swift-robot\n```\n\n运行指令后，就可以在SwanLab看到训练过程：\n\n![](./swift/dashboard-1.png)\n\n支持的完整参数：\n\n- `swanlab_token`: SwanLab的api-key\n- `swanlab_project`: swanlab的project\n- `swanlab_workspace`: 默认为None，会使用api-key对应的username\n- `swanlab_exp_name`: 实验名，可以为空，为空时默认传入--output_dir的值\n- `swanlab_mode`: 可选cloud和local，云模式或者本地模式",
    "588": "一级标题：Modelscope Swift\n二级标题：2. WebUI微调\n内容：\nSwift不仅支持CLI微调，还为开发者提供非常方便的**WebUI（网页端）**的微调界面。你同样可以在WebUI当中启动SwanLab跟踪实验。\n\n启动WebUI方式：\n\n```bash\nswift web-ui\n```\n\n启动后，会自动打开浏览器，显示微调界面（或者访问 `http://localhost:7860/` ）：\n\n![ig-swift-2](./swift/dashboard-2.png)\n\n在下方的「训练记录」模块中，在`训练记录方式`部分选择`swanlab`：\n\n![ig-swift-3](./swift/webui-1.png)\n\n你还可以在「训练记录」模块的其他填写更细致的swanlab参数，包括：\n\n- `swanlab_token`: SwanLab的api-key\n- `swanlab_project`: swanlab的project\n- `swanlab_workspace`: 默认为None，会使用api-key对应的username\n- `swanlab_exp_name`: 实验名，可以为空，为空时默认传入--output_dir的值\n- `swanlab_mode`: 可选cloud和local，云模式或者本地模式\n\n然后，点击「🚀开始训练」按钮，即可启动训练，并使用SwanLab跟踪实验：\n\n![ig-swift-4](./swift/webui-2.png)",
    "589": "一级标题：Modelscope Swift\n二级标题：3. Python代码微调\n内容：\n**3.1 引入SwanLabCallback**\n\n因为`Swift`的`trainer`集成自`transformers`，所以可以直接使用`swanlab`与`huggingface`集成的`SwanLabCallback`：\n\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\n```\n\nSwanLabCallback可以定义的参数有：\n\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。 你也可以在外部通过swanlab.init创建项目，集成会将实验记录到你在外部创建的项目中。\n\n**3.2 引入Trainer**\n\n```python {1,7,11}\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom swift import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\n···\n\n#实例化SwanLabCallback\nswanlab_callback = SwanLabCallback(project=\"swift-visualization\")\n\ntrainer = Seq2SeqTrainer(\n    ...\n    callbacks=[swanlab_callback],\n    )\n\ntrainer.train()\n```\n\n**3.3 使用SwanLabCallback**\n\n> Lora微调一个Qwen2-0.5B模型\n\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom swift import Seq2SeqTrainer, Seq2SeqTrainingArguments\nfrom swift.llm import get_model_tokenizer, load_dataset, get_template, EncodePreprocessor\nfrom swift.utils import get_logger, find_all_linears, get_model_parameter_info, plot_images, seed_everything\nfrom swift.tuners import Swift, LoraConfig\nfrom swift.trainers import Seq2SeqTrainer, Seq2SeqTrainingArguments\nfrom functools import partial\nimport os\n\nlogger = get_logger()\nseed_everything(42)\n\n# Hyperparameters for training\n# model\nmodel_id_or_path = 'Qwen/Qwen2.5-3B-Instruct'  # model_id or model_path\nsystem = 'You are a helpful assistant.'\noutput_dir = 'output'\n\n# dataset\ndataset = ['AI-ModelScope/alpaca-gpt4-data-zh#500', 'AI-ModelScope/alpaca-gpt4-data-en#500',\n           'swift/self-cognition#500']  # dataset_id or dataset_path\ndata_seed = 42\nmax_length = 2048\nsplit_dataset_ratio = 0.01  # Split validation set\nnum_proc = 4  # The number of processes for data loading.\n# The following two parameters are used to override the placeholders in the self-cognition dataset.\nmodel_name = ['小黄', 'Xiao Huang']  # The Chinese name and English name of the model\nmodel_author = ['魔搭', 'ModelScope']  # The Chinese name and English name of the model author\n\n# lora\nlora_rank = 8\nlora_alpha = 32\n\n# training_args\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=output_dir,\n    learning_rate=1e-4,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_checkpointing=True,\n    weight_decay=0.1,\n    lr_scheduler_type='cosine',\n    warmup_ratio=0.05,\n    logging_first_step=True,\n    save_strategy='steps',\n    save_steps=50,\n    eval_strategy='steps',\n    eval_steps=50,\n    gradient_accumulation_steps=16,\n    num_train_epochs=1,\n    metric_for_best_model='loss',\n    save_total_limit=5,\n    logging_steps=5,\n    dataloader_num_workers=1,\n    data_seed=data_seed,\n)\n\noutput_dir = os.path.abspath(os.path.expanduser(output_dir))\nlogger.info(f'output_dir: {output_dir}')\n\n# Obtain the model and template, and add a trainable Lora layer on the model.\nmodel, tokenizer = get_model_tokenizer(model_id_or_path)\nlogger.info(f'model_info: {model.model_info}')\ntemplate = get_template(model.model_meta.template, tokenizer, default_system=system, max_length=max_length)\ntemplate.set_mode('train')\n\ntarget_modules = find_all_linears(model)\nlora_config = LoraConfig(task_type='CAUSAL_LM', r=lora_rank, lora_alpha=lora_alpha,\n                         target_modules=target_modules)\nmodel = Swift.prepare_model(model, lora_config)\nlogger.info(f'lora_config: {lora_config}')\n\n# Print model structure and trainable parameters.\nlogger.info(f'model: {model}')\nmodel_parameter_info = get_model_parameter_info(model)\nlogger.info(f'model_parameter_info: {model_parameter_info}')\n\n# Download and load the dataset, split it into a training set and a validation set,\n# and encode the text data into tokens.\ntrain_dataset, val_dataset = load_dataset(dataset, split_dataset_ratio=split_dataset_ratio, num_proc=num_proc,\n        model_name=model_name, model_author=model_author, seed=data_seed)\n\nlogger.info(f'train_dataset: {train_dataset}')\nlogger.info(f'val_dataset: {val_dataset}')\nlogger.info(f'train_dataset[0]: {train_dataset[0]}')\n\ntrain_dataset = EncodePreprocessor(template=template)(train_dataset, num_proc=num_proc)\nval_dataset = EncodePreprocessor(template=template)(val_dataset, num_proc=num_proc)\nlogger.info(f'encoded_train_dataset[0]: {train_dataset[0]}')\n\n# Print a sample\ntemplate.print_inputs(train_dataset[0])\n\n# Get the trainer and start the training.\nmodel.enable_input_require_grads()  # Compatible with gradient checkpointing\n\nswanlab_callback = SwanLabCallback(\n    project=\"swift-visualization\",\n    experiment_name=\"lora-qwen2-0.5b\",\n    description=\"Lora微调一个Qwen2-0.5B模型\"\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    data_collator=template.data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    template=template,\n    callbacks=[swanlab_callback],\n)\ntrainer.train()\n\nlast_model_checkpoint = trainer.state.last_model_checkpoint\nlogger.info(f'last_model_checkpoint: {last_model_checkpoint}')\n```\n\n运行可视化结果：\n\n![ig-swift-5](./swift/dashboard-3.png)",
    "590": "一级标题：Tensorboard\n二级标题：无\n内容：\n[TensorBoard](https://github.com/tensorflow/tensorboard) 是 Google TensorFlow 提供的一个可视化工具，用于帮助理解、调试和优化机器学习模型。它通过图形界面展示训练过程中的各种指标和数据，让开发者更直观地了解模型的性能和行为。\n\n![TensorBoard](/assets/ig-tensorboard.png)\n\n:::warning 其他工具的同步教程\n\n- [Wandb](/guide_cloud/integration/integration-wandb.md)\n- [MLFlow](/guide_cloud/integration/integration-mlflow.md)\n:::\n\n**你可以用两种方式将使用Tensorboard跟踪的项目同步到SwanLab：**\n\n- **同步跟踪**：如果你现在的项目使用了Tensorboard进行实验跟踪，你可以使用`swanlab.sync_tensorboardX()`或`swanlab.sync_tensorboard_torch()`命令，在运行训练脚本时同步记录指标到SwanLab。\n- **转换已存在的项目**：如果你想要将Tensorboard上的项目复制到SwanLab，你可以使用`swanlab convert`，将存放TFevent文件的目录转换成SwanLab项目。\n\n::: info\n在当前版本暂仅支持转换标量和图像图表。\n:::\n\n[[toc]]",
    "591": "一级标题：Tensorboard\n二级标题：1. 同步跟踪\n内容：\n### 1.1 TensorboardX: 添加sync_tensorboardX命令\n\n如果你使用的是TensorboardX，可以在代码执行`tensorboardX.SummaryWriter()`之前的任何位置，添加一行`swanlab.sync_tensorboardX()`命令，即可在训练时同步记录指标到SwanLab。\n\n```python\nimport swanlab\nfrom tensorboardX import SummaryWriter\n\nswanlab.sync_tensorboardX()\n\nwriter = SummaryWriter(log_dir='./runs')\n```\n\n### 1.2 PyTorch: 添加sync_tensorboard_torch命令\n\n如果你使用的是PyTorch自带的tensorboard，那么可以在代码执行`torch.utils.tensorboard.SummaryWriter()`之前的任何位置，添加一行`swanlab.sync_tensorboard_torch()`命令，即可在训练时同步记录指标到SwanLab。\n\n```python\nimport swanlab\nimport torch\n\nswanlab.sync_tensorboard_torch()\n\nwriter = torch.utils.tensorboard.SummaryWriter(log_dir='./runs')\n```\n\n### 1.3 另一种写法\n\n你也可以先手动初始化swanlab，再运行tensorboard的代码。\n\n::: code-group\n\n```python [TensorboardX]\nimport swanlab\nfrom tensorboardX import SummaryWriter\n\nswanlab.init(...)\nswanlab.sync_tensorboardX()\n\n...\n\nwriter = SummaryWriter(log_dir='./runs')\n```\n\n```python [PyTorch]\nimport swanlab\nfrom torch.utils.tensorboard import SummaryWriter\n\nswanlab.init(...)\nswanlab.sync_tensorboard_torch()\n\n...\n\nwriter = SummaryWriter(log_dir='./runs')\n```\n:::\n\n### 1.4 测试代码\n\n::: code-group\n\n```python [TensorboardX]\nimport swanlab\nfrom tensorboardX import SummaryWriter\n\nswanlab.sync_tensorboardX()\n\nwriter = SummaryWriter(log_dir='./runs')\n\nepochs = 10\noffset = random.random() / 5\nfor epoch in range(2, epochs):\n  acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n  loss = 2 ** -epoch + random.random() / epoch + offset\n\n  writer.add_scalar(\"acc\", acc, epoch)\n  writer.add_scalar(\"loss\", loss, epoch)\n```\n\n```python [PyTorch]\nimport swanlab\nfrom torch.utils.tensorboard import SummaryWriter\n\nswanlab.sync_tensorboard_torch()\n\nwriter = SummaryWriter(log_dir='./runs')\n\nepochs = 10\noffset = random.random() / 5\nfor epoch in range(2, epochs):\n  acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n  loss = 2 ** -epoch + random.random() / epoch + offset\n\n  writer.add_scalar(\"acc\", acc, epoch)\n  writer.add_scalar(\"loss\", loss, epoch)\n```\n\n:::",
    "592": "一级标题：Tensorboard\n二级标题：2. 转换已存在的项目\n内容：\n### 2.1 方式一：命令行转换\n\n```bash\nswanlab convert -t tensorboard --tb_logdir [TFEVENT_LOGDIR]\n```\n\n支持的参数如下：\n\n- `-t`: 转换类型，可选wandb、tensorboard和mlflow。\n- `-p`: SwanLab项目名。\n- `-w`: SwanLab工作空间名。\n- `--cloud`: (bool) 是否上传模式为\"cloud\"，默认为True\n- `-l`: logdir路径。\n- `--tb_logdir`: Tensorboard日志文件路径。\n\n这里的`[TFEVENT_LOGDIR]`是指你先前用Tensorboard记录实验时，生成的日志文件路径。\n\nSwanLab Converter将会自动检测文件路径及其子目录下的`tfevent`文件（默认子目录深度为3），并为每个`tfevent`文件创建一个SwanLab实验。\n\n### 2.2 方式二：代码内转换\n\n```python\nfrom swanlab.converter import TFBConverter\n\ntfb_converter = TFBConverter(convert_dir=\"[TFEVENT_LOGDIR]\")\ntfb_converter.run()\n```\n\n效果与命令行转换一致。\n\n### 2.3 参数列表\n\n| 参数 | 对应CLI参数       | 描述                  | \n| ---- | ---------- | --------------------- | \n| convert_dir    | -      | Tfevent文件路径       | \n| project    | -p, --project      | SwanLab项目名       |\n| workspace  | -w, --workspace      | SwanLab工作空间名 |\n| cloud    | --cloud      | 是否使用云端版，默认为True       | \n| logdir    | -l, --logdir      | SwanLab日志文件保存路径       | \n\n例子：\n\n```python\nfrom swanlab.converter import TFBConverter\n\ntfb_converter = TFBConverter(\n    convert_dir=\"./runs\",\n    project=\"Tensorboard-Converter\",\n    workspace=\"SwanLab\",\n    logdir=\"./logs\",\n    )\ntfb_converter.run()\n```\n\n与之作用相同的CLI：\n```bash\nswanlab convert -t tensorboard --tb_logdir ./runs -p Tensorboard-Converter -w SwanLab -l ./logs\n```\n\n执行上面的脚本，将会在`SwanLab`空间下，创建一个名为`Tensorboard-Converter`的项目，将`./runs`目录下tfevent文件创建为一个个swanlab实验，并将swanlab运行时产生的日志保存在`./logs`目录下。",
    "593": "一级标题：Tensorboard\n二级标题：3. API映射表\n内容：\n| 功能 | Tensorboard | SwanLab | \n| ---- | ---------- | --------------------- | \n| 创建实验    |  writer = SummaryWriter(logdir=\"./runs\")   | swanlab.init(logdir=\"./runs\")    | \n| 记录标量指标 | writer.add_scalar(key, value, step) | swanlab.log({key, value}, step=step) |\n| 记录多个标量指标 | writer.add_scalar(key1, value1, step)<br> writer.add_scalar(key2, value2, step) | swanlab.log({key1: value1, key2: value2}, step=step) |\n| 记录图像指标 | writer.add_image(key, data, step) | swanlab.log({key: swanlab.Image(data), step=step}) |\n| 记录文本指标 | writer.add_text(key, data, step) | swanlab.log({key: swanlab.Text(data)}, step=step) |\n| 记录音频指标 | writer.add_audio(key, data, step) | swanlab.log({key: swanlab.Audio(data), step=step}) |\n| 记录视频指标 | writer.add_video(key, data, step) | swanlab.log({key: swanlab.Video(data), step=step}) |\n| 记录PR曲线 | writer.add_pr_curve(key, labels, predictions, step) | swanlab.log({key: swanlab.PRCurve(labels, predictions), step=step}) |\n| 关闭实验 | writer.close() | swanlab.finish() |",
    "594": "一级标题：Ultralytics\n二级标题：无\n内容：\n[![](/assets/colab.svg)](https://colab.research.google.com/drive/1RAT2vSrvET4wEDd9syeDrgz0KBUDQAR1?usp=sharing)\n\n[Ultralytics](https://github.com/ultralytics/ultralytics) YOLOv8 是一款尖端、最先进的 （SOTA） 模型，它建立在以前 YOLO 版本的成功基础上，并引入了新功能和改进，以进一步提高性能和灵活性。YOLOv8 设计为快速、准确且易于使用，使其成为各种对象检测和跟踪、实例分割、图像分类和姿态估计任务的绝佳选择。\n\n![ultralytics](./ultralytics/logo.png)\n\n你可以使用Ultralytics快速进行计算机视觉模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n下面介绍两种引入SwanLab的方式：  \n1. `add_swanlab_callback`：无需修改源码，适用于单卡训练场景\n2. `return_swanlab_callback`：需要修改源码，适用于单卡以及多卡DDP训练场景",
    "595": "一级标题：Ultralytics\n二级标题：1.1 引入add_swanlab_callback\n内容：\n```python\nfrom swanlab.integration.ultralytics import add_swanlab_callback\n```\n\n`add_swanlab_callback`的作用是为Ultralytics模型添加回调函数，以在模型训练的各个生命周期执行SwanLab记录。",
    "596": "一级标题：Ultralytics\n二级标题：1.2 代码案例\n内容：\n下面是使用yolov8n模型在coco数据集上的训练，只需将model传入`add_swanlab_callback`函数，即可完成与SwanLab的集成。\n\n```python {9}\nfrom ultralytics import YOLO\nfrom swanlab.integration.ultralytics import add_swanlab_callback\n\n\nif __name__ == \"__main__\":\n    model = YOLO(\"yolov8n.yaml\")\n    model.load()\n    # 添加swanlab回调\n    add_swanlab_callback(model)\n\n    model.train(\n        data=\"./coco128.yaml\",\n        epochs=3, \n        imgsz=320,\n    )\n```\n\n如果需要自定义SwanLab的项目、实验名等参数，则可以在`add_swanlab_callback`中添加：\n\n```python\nadd_swanlab_callback(\n    model,\n    project=\"ultralytics\",\n    experiment_name=\"yolov8n\",\n    description=\"yolov8n在coco128数据集上的训练。\",\n    mode=\"local\",\n    )\n```",
    "597": "一级标题：Ultralytics\n二级标题：2.1 多卡训练/DDP训练\n内容：\n> swanlab>=0.3.7\n\n在Ultralytics多卡训练的场景下，由于启动训练的方式与单卡完全不同，所以需要用一种不同的方式接入SwanLab回调。\n\n这是一个ultralytics开启DDP训练的样例代码：\n\n```python\nfrom ultralytics import YOLO\n\nif __name__ == \"__main__\":\n    model = YOLO(\"yolov8n.pt\")\n\n    model.train(\n        data=\"./coco128.yaml\",\n        epochs=3, \n        imgsz=320,\n        # 开启DDP\n        device=[0,1],\n    )\n```\n\n我们需要修改ultralytics的源码，去到`ultralytics/utils/callbacks/base.py`，找到`add_integration_callbacks`函数，添加下面的三行代码：\n\n```python (15,16,18)\ndef add_integration_callbacks(instance):\n    ...\n    \n    # Load training callbacks\n    if \"Trainer\" in instance.__class__.__name__:\n        from .clearml import callbacks as clear_cb\n        from .comet import callbacks as comet_cb\n        from .dvc import callbacks as dvc_cb\n        from .mlflow import callbacks as mlflow_cb\n        from .neptune import callbacks as neptune_cb\n        from .raytune import callbacks as tune_cb\n        from .tensorboard import callbacks as tb_cb\n        from .wb import callbacks as wb_cb\n\n        from swanlab.integration.ultralytics import return_swanlab_callback\n        sw_cb = return_swanlab_callback()\n\n        callbacks_list.extend([..., sw_cb])\n```\n\n然后运行，就可以在ddp下正常跟踪实验了。\n\n如果需要自定义SwanLab的项目、实验名等参数，则可以在`return_swanlab_callback`中添加：\n\n```python\nreturn_swanlab_callback(\n    model,\n    project=\"ultralytics\",\n    experiment_name=\"yolov8n\",\n    description=\"yolov8n在coco128数据集上的训练。\",\n    mode=\"local\",\n    )\n```\n\n:::warning ps\n1. 写入源码之后，之后运行就不需要在训练脚本中增加`add_swanlab_callback`了。\n2. 项目名由model.train()的project参数定义，实验名由name参数定义。\n:::",
    "598": "一级标题：Ultralytics\n二级标题：2.2 代码案例\n内容：\n```python\nfrom ultralytics import YOLO\n\nif __name__ == \"__main__\":\n    model = YOLO(\"yolov8n.pt\")\n\n    model.train(\n        data=\"./coco128.yaml\",\n        epochs=3, \n        imgsz=320,\n        # 开启DDP\n        device=[0,1,2,3],\n        # 可以通过project参数设置SwanLab的project，name参数设置SwanLab的experiment_name\n        project=\"Ultralytics\",\n        name=\"yolov8n\"\n    )\n```",
    "599": "一级标题：Unsloth\n二级标题：无\n内容：\n[微信公众号文章](https://mp.weixin.qq.com/s/re7R7WhTYNuiDj0fSwAnWQ)\n\n[Unsloth](https://github.com/unslothai/unsloth) 是一个用于加速 LLM（大型语言模型）微调的轻量级库 。它与 Hugging Face 生态系统完全兼容，包括 Hub、transformers 和 PEFT 。\n\n![logo](./unsloth/logo.png)\n\n你可以使用Unsloth与Tranformers或TRL结合加速LLM模型训练，同时使用SwanLab进行实验跟踪与可视化。",
    "600": "一级标题：Unsloth\n二级标题：1. 引入SwanLabCallback\n内容：\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\n```\n\nSwanLabCallback是适配于Transformers的日志记录类。\n\nSwanLabCallback可以定义的参数有：\n\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。\n- 你也可以在外部通过swanlab.init创建项目，集成会将实验记录到你在外部创建的项目中。",
    "601": "一级标题：Unsloth\n二级标题：2. 传入Trainer\n内容：\n```python {1,7,12}\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom trl import GRPOTrainer\n\n...\n\n# 实例化SwanLabCallback\nswanlab_callback = SwanLabCallback(project=\"unsloth-example\")\n\ntrainer = GRPOTrainer(\n    ...\n    # 传入callbacks参数\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n```",
    "602": "一级标题：Unsloth\n二级标题：3. 与Unsloth结合的案例模板\n内容：\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom unsloth import FastLanguageModel, PatchFastRL\n\nPatchFastRL(\"GRPO\", FastLanguageModel)  # 对 TRL 进行补丁处理\nfrom trl import GRPOConfig, GRPOTrainer, ModelConfig, TrlParser\n\n...\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n...\n) \n\n# PEFT 模型\nmodel = FastLanguageModel.get_peft_model(\n...\n)\n\n# 实例化SwanLabCallback\nswanlab_callback = SwanLabCallback(\n  project=\"trl_integration\",\n  experiment_name=\"qwen2.5-sft\",\n  description=\"测试swanlab和trl的集成\",\n  config={\"framework\": \"🤗TRL\"},\n)\n\n# 定义GRPOTrainer\ntrainer = GRPOTrainer(\n    ...\n    # 传入callbacks参数\n    callbacks=[swanlab_callback],\n)\n\n#开启训练！\ntrainer.train()\n```",
    "603": "一级标题：verl\n二级标题：无\n内容：\n[verl](https://github.com/volcengine/verl) 是一个灵活、高效且可用于生产环境的强化学习（RL）训练框架，专为大型语言模型（LLMs）的后训练设计。它由字节跳动火山引擎团队开源，是 [HybridFlow](https://arxiv.org/abs/2409.19256) 论文的开源实现。\n\n<div style=\"text-align: center;\">\n    <img src=\"./verl/verl_logo.svg\" alt=\"verl_logo\" style=\"width: 70%;\">\n</div>\n\n**verl 具有以下特点，使其灵活且易于使用：**\n\n1. **易于扩展的多样化 RL 算法**：Hybrid 编程模型结合了单控制器和多控制器范式的优点，能够灵活表示并高效执行复杂的后训练数据流。用户只需几行代码即可构建 RL 数据流。\n\n2. **与现有 LLM 基础设施无缝集成的模块化 API**：通过解耦计算和数据依赖，verl 能够与现有的 LLM 框架（如 PyTorch FSDP、Megatron-LM 和 vLLM）无缝集成。此外，用户可以轻松扩展到其他 LLM 训练和推理框架。\n\n3. **灵活的设备映射和并行化**：支持将模型灵活地映射到不同的 GPU 组上，以实现高效的资源利用，并在不同规模的集群上具有良好的扩展性。\n\n4. **与流行的 HuggingFace 模型轻松集成**：verl 能够方便地与 HuggingFace 模型进行集成。\n\n**verl 也具有以下优势，使其运行速度快：**\n\n1. **最先进的吞吐量**：通过无缝集成现有的 SOTA LLM 训练和推理框架，verl 实现了高生成和训练吞吐量。\n\n2. **基于 3D-HybridEngine 的高效 Actor 模型重分片**：消除了内存冗余，并显著减少了在训练和生成阶段之间切换时的通信开销。\n\n更多信息可参考如下链接\n\n> * verl GitHub仓库链接: [https://github.com/volcengine/verl](https://github.com/volcengine/verl)\n> * 官方文档: [https://verl.readthedocs.io/en/latest/index.html](https://verl.readthedocs.io/en/latest/index.html)\n> * HybridFlow论文地址: [https://arxiv.org/pdf/2409.19256v2](https://arxiv.org/pdf/2409.19256v2)\n\n\n你可以使用verl快速进行大模型强化学习训练，同时使用SwanLab进行实验跟踪与可视化。",
    "604": "一级标题：verl\n二级标题：环境安装\n内容：\n需要环境：\n\n* Python: Version >= 3.9\n\n* CUDA: Version >= 12.1\n\n参考verl官方文档安装：[https://verl.readthedocs.io/en/latest/start/install.html](https://verl.readthedocs.io/en/latest/start/install.html)\n\n以及需要额外安装SwanLab\n\n```bash\npip install -U swanlab\n```",
    "605": "一级标题：verl\n二级标题：使用方法\n内容：\n以verl官方文档的[Post-train a LLM using PPO with GSM8K dataset](https://verl.readthedocs.io/en/latest/start/quickstart.html)为例。\n\n你仅需要通过在实验的启动命令中，增加`trainer.logger=['swanlab']`，即可选择swanlab进行实验跟踪。\n\n**完整的测试命令如下：**\n\n```bash {4}\nPYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo \\\n data.train_files=$HOME/data/gsm8k/train.parquet \\\n data.val_files=$HOME/data/gsm8k/test.parquet \\\n trainer.logger=['console','swanlab'] \\\n data.train_batch_size=256 \\\n data.val_batch_size=1312 \\\n data.max_prompt_length=512 \\\n data.max_response_length=256 \\\n actor_rollout_ref.model.path=Qwen/Qwen2.5-0.5B-Instruct \\\n actor_rollout_ref.actor.optim.lr=1e-6 \\\n actor_rollout_ref.actor.ppo_mini_batch_size=64 \\\n actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 \\\n actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8 \\\n actor_rollout_ref.rollout.tensor_model_parallel_size=1 \\\n actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \\\n actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 \\\n critic.optim.lr=1e-5 \\\n critic.model.path=Qwen/Qwen2.5-0.5B-Instruct \\\n critic.ppo_micro_batch_size_per_gpu=4 \\\n algorithm.kl_ctrl.kl_coef=0.001 \\\n trainer.val_before_train=False \\\n trainer.default_hdfs_dir=null \\\n trainer.n_gpus_per_node=1 \\\n trainer.nnodes=1 \\\n trainer.save_freq=10 \\\n trainer.test_freq=10 \\\n trainer.total_epochs=15 2>&1 | tee verl_demo.log\n```\n\n:::info\n如果你需要设置项目和实验名，可以设置`trainer.project_name`和`trainer.experiment_name`。  \n如：\n```bash\nPYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo \\\n ...\n trainer.project_name=\"verl_demo\" \\\n trainer.experiment_name=\"ppo\" \\\n ...\n```\n:::\n\n如果启动训练时你还未登陆SwanLab，会出现如下提示。\n\n![select](./verl/select.png)\n\n选择**1、2**则为使用云端跟踪模式，选择后根据引导输入官网的API即可实现在线跟踪。可以在线查看训练跟踪结果。选择**3**则不上传训练数据，采用离线跟踪。\n\n\n当然，你也可以通过[环境变量](/api/environment-variable)的方式登陆或者设置跟踪模式：\n\n```bash\nexport SWANLAB_API_KEY=<你的登陆API>           # 设置在线跟踪模式API\nexport SWANLAB_LOG_DIR=<设置本地日志存储路径>    # 设置本地日志存储路径\nexport SWANLAB_MODE=<设置SwanLab的运行模式>     # 包含四种模式：cloud云端跟踪模式（默认）、cloud-only仅云端跟踪本地不保存文件、local本地跟踪模式、disabled完全不记录用于debug\n```",
    "606": "一级标题：verl\n二级标题：查看训练日志\n内容：\n完成登陆后会显示如下登陆信息：\n\n![track](./verl/track.png)\n\n运行进程，即可在[SwanLab官网](https://swanlab.cn)上查看训练日志：\n\n![remote](./verl/remote.png)\n\n更多使用方法可以参考[SwanLab查看使用结果](https://docs.swanlab.cn/guide_cloud/experiment_track/view-result.html)\n\n---\n\n如果你使用本地看板模式，则可以通过如下命令打开本地看板\n\n```bash\nswanlab watch\n```\n\n更多详细可以参考[SwanLab离线看板模式](https://docs.swanlab.cn/guide_cloud/self_host/offline-board.html)\n\n服务器设置端口号可以查看[离线看板端口号](https://docs.swanlab.cn/api/cli-swanlab-watch.html#%E8%AE%BE%E7%BD%AEip%E5%92%8C%E7%AB%AF%E5%8F%A3%E5%8F%B7)",
    "607": "一级标题：verl\n二级标题：每轮评估时记录生成文本\n内容：\n如果你希望在每轮评估（val）时将生成的文本记录到SwanLab中，只需在命令行钟增加一行`trainer.log_val_generations=1`即可：\n\n```bash {5}\nPYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo \\\n data.train_files=$HOME/data/gsm8k/train.parquet \\\n data.val_files=$HOME/data/gsm8k/test.parquet \\\n trainer.logger=['console','swanlab'] \\\n trainer.log_val_generations=1 \\\n ...\n```\n\n> 如果你希望每轮评估时生成多条结果，如10条，那么修改`trainer.log_val_generations=10`即可",
    "608": "一级标题：verl\n二级标题：断点续训\n内容：\n如果你训练时崩溃或希望补充实验，可以使用[resume](/guide_cloud/experiment_track/resume-experiment.html)功能来恢复实验。\n\n在verl训练中，你可以通过设置环境变量来执行resume：\n\n```bash\nexport SWANLAB_RESUME=must\nexport SWANLAB_RUN_ID=<exp_id>\n```",
    "609": "一级标题：Weights & Biases\n二级标题：无\n内容：\nWeights & Biases (Wandb) 是一个用于机器学习和深度学习项目的实验跟踪、模型优化和协作平台。W&B 提供了强大的工具来记录和可视化实验结果，帮助数据科学家和研究人员更好地管理和分享他们的工作。\n\n![wandb](/assets/ig-wandb.png)\n\n:::warning 其他工具的同步教程\n\n- [TensorBoard](/guide_cloud/integration/integration-tensorboard.md)\n- [MLFlow](/guide_cloud/integration/integration-mlflow.md)\n:::\n\n**你可以用两种方式将Wandb上的项目同步到SwanLab：**\n\n1. **同步跟踪**：如果你现在的项目使用了wandb进行实验跟踪，你可以使用`swanlab.sync_wandb()`命令，在运行训练脚本时同步记录指标到SwanLab。\n2. **转换已存在的项目**：如果你想要将wandb上的项目复制到SwanLab，你可以使用`swanlab convert`，将Wandb上已存在的项目转换成SwanLab项目。\n\n::: info\n在当前版本暂仅支持转换标量图表。\n:::\n\n[[toc]]",
    "610": "一级标题：Weights & Biases\n二级标题：1. 同步跟踪\n内容：\n### 1.1 添加sync_wandb命令\n\n在你的代码执行`wandb.init()`之前的任何位置，添加一行`swanlab.sync()`命令，即可在训练时同步wandb的指标到SwanLab。\n\n```python\nimport swanlab\n\nswanlab.sync_wandb()\n\n...\n\nwandb.init()\n```\n\n在上述这种代码写法中，`wandb.init()`的同时会初始化swanlab，项目名、实验名和配置和`wandb.init()`中的`project`、`name`、`config`一致，因此你不需要再手动初始化swanlab。\n\n:::info\n\n**`sync_wandb`支持设置两个参数：**\n\n- `mode`: swanlab的记录模式，支持cloud、local和disabled三种模式。\n- `wandb_run`: 如果此参数设置为**False**，则不会将数据上传到wandb，等同于设置wandb.init(mode=\"offline\")\n\n:::\n\n### 1.2 另一种写法\n\n另一种用法是先手动初始化swanlab，再运行wandb的代码。\n\n```python\nimport swanlab\n\nswanlab.init(...)\nswanlab.sync_wandb()\n\n...\n\nwandb.init()\n```\n\n在这种写法中，项目名、实验名、配置和`swanlab.init()`中的`project`、`experiment_name`、`config`一致，而后续`wandb.init()`中的`project`、`name`会被忽略，`config`会更新进`swanlab.config`中。\n\n### 1.3 测试代码\n\n```python\nimport wandb\nimport random\nimport swanlab\n\nswanlab.sync_wandb()\n# swanlab.init(project=\"sync_wandb\")\n\nwandb.init(\n  project=\"test\",\n  config={\"a\": 1, \"b\": 2},\n  name=\"test\",\n  )\n\nepochs = 10\noffset = random.random() / 5\nfor epoch in range(2, epochs):\n  acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n  loss = 2 ** -epoch + random.random() / epoch + offset\n\n  wandb.log({\"acc\": acc, \"loss\": loss})\n```\n\n![alt text](/assets/ig-wandb-4.png)",
    "611": "一级标题：Weights & Biases\n二级标题：2. 转换已存在的项目\n内容：\n### 2.1 找到你在wandb.ai上的projecy、entity和runid\n\nprojecy、entity和runid是转换所需要的（runid是可选的）。  \nproject和entity的位置：\n![alt text](/assets/ig-wandb-2.png)\n\nrunid的位置：\n\n![alt text](/assets/ig-wandb-3.png)\n\n### 2.2 方式一：命令行转换\n\n首先，需要确保当前环境下，你已登录了wandb，并有权限访问目标项目。\n\n转换命令行：\n\n```bash\nswanlab convert -t wandb --wb-project [WANDB_PROJECT_NAME] --wb-entity [WANDB_ENTITY]\n```\n\n支持的参数如下：\n\n- `-t`: 转换类型，可选wandb与tensorboard。\n- `-p`: SwanLab项目名。\n- `-w`: SwanLab工作空间名。\n- `--mode`: (str) 选择模式，默认为\"cloud\"，可选 [\"cloud\", \"local\", \"offline\", \"disabled\"]\n- `-l`: logdir路径。\n- `--wb-project`：待转换的wandb项目名。\n- `--wb-entity`：wandb项目所在的空间名。\n- `--wb-runid`: wandb Run（项目下的某一个实验）的id。\n\n如果不填写`--wb-runid`，则会将指定项目下的全部Run进行转换；如果填写，则只转换指定的Run。\n\n---\n\n**异步转换方法（先将数据下载到本地，再上传到swanlab）**\n\n1. 数据下载到本地：\n\n```bash\nswanlab convert --mode 'offline' -t wandb --wb-project [WANDB_PROJECT_NAME] --wb-entity [WANDB_ENTITY]\n```\n\n2. 上传到swanlab：\n\n```bash\nswanlab sync [日志文件夹路径]\n```\n\n[swanlab sync文档](/zh/api/cli-swanlab-sync.md)\n\n\n### 2.3 方式二：代码内转换\n\n```python\nfrom swanlab.converter import WandbConverter\n\nwb_converter = WandbConverter()\n# wb_runid可选\nwb_converter.run(wb_project=\"WANDB_PROJECT_NAME\", wb_entity=\"WANDB_USERNAME\")\n```\n\n效果与命令行转换一致。\n\n`WandbConverter`支持的参数：\n\n- `project`: SwanLab项目名。\n- `workspace`: SwanLab工作空间名。\n- `mode`: (str) 选择模式，默认为\"cloud\"，可选 [\"cloud\", \"local\", \"offline\", \"disabled\"]\n- `logdir`: logdir路径。\n\n`WandbConverter.run`支持的参数：\n\n- `wb_project`: wandb项目名。\n- `wb_entity`: wandb项目所在的空间名。\n- `wb_runid`: wandb Run（项目下的某一个实验）的id。\n\n**异步转换方法（先将数据下载到本地，再上传到swanlab）**\n\n1. 数据下载到本地：\n\n```python\nfrom swanlab.converter import WandbConverter\n\nwb_converter = WandbConverter(mode=\"offline\")\n# wb_runid可选\nwb_converter.run(wb_project=\"WANDB_PROJECT_NAME\", wb_entity=\"WANDB_USERNAME\")\n```\n\n2. 上传到swanlab：\n\n```bash\nswanlab sync [日志文件夹路径]\n```\n\n[swanlab sync文档](/zh/api/cli-swanlab-sync.md)",
    "612": "一级标题：XGBoost\n二级标题：无\n内容：\nXGBoost（eXtreme Gradient Boosting）是一种高效、灵活且广泛使用的梯度提升框架，由陈天奇在2014年提出。它基于决策树算法，通过集成多个弱学习器（通常是决策树）来构建一个强大的预测模型。XGBoost在各种机器学习竞赛和实际应用中表现出色，尤其是在分类、回归和排序任务中。\n\n![xgboost](/zh/guide_cloud/integration/xgboost/logo.png)\n\n你可以使用XGBoost快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。",
    "613": "一级标题：XGBoost\n二级标题：1. 引入SwanLabCallback\n内容：\n```python\nfrom swanlab.integration.xgboost import SwanLabCallback\n```\n\nSwanLabCallback是适配于XGBoost的日志记录类。",
    "614": "一级标题：XGBoost\n二级标题：2. 初始化SwanLab\n内容：\n```python\nswanlab.init(\n    project=\"xgboost-example\", \n)\n```",
    "615": "一级标题：XGBoost\n二级标题：3. 传入`xgb.train`\n内容：\n```python\nimport xgboost as xgb\n\nbst = xgb.train(\n    ...\n    callbacks=[SwanLabCallback()]\n)\n```",
    "616": "一级标题：XGBoost\n二级标题：4. 完整测试代码\n内容：\n```python\nimport xgboost as xgb\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nimport swanlab\nfrom swanlab.integration.xgboost import SwanLabCallback\n\n# 初始化swanlab\nswanlab.init(\n    project=\"xgboost-breast-cancer\",\n    config={\n        \"learning_rate\": 0.1,\n        \"max_depth\": 3,\n        \"subsample\": 0.8,\n        \"colsample_bytree\": 0.8,\n        \"num_round\": 100\n    }\n)\n\n# 加载数据集\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# 将数据集分为训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 转换为DMatrix格式，这是XGBoost的内部数据格式\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n\n# 设置参数\nparams = {\n    'objective': 'binary:logistic',  # 二分类任务\n    'max_depth': 3,                  # 树的最大深度\n    'eta': 0.1,                      # 学习率\n    'subsample': 0.8,                # 样本采样比例\n    'colsample_bytree': 0.8,         # 特征采样比例\n    'eval_metric': 'logloss'         # 评估指标\n}\n\n# 训练模型\nnum_round = 100  # 迭代次数\nbst = xgb.train(\n    params, \n    dtrain, \n    num_round,\n    evals=[(dtrain, 'train'), (dtest, 'test')], \n    callbacks=[SwanLabCallback()]\n)\n\n# 进行预测\ny_pred = bst.predict(dtest)\ny_pred_binary = [round(value) for value in y_pred]  # 将概率转换为二分类结果\n\n# 评估模型\naccuracy = accuracy_score(y_test, y_pred_binary)\nprint(f\"Accuracy: {accuracy:.4f}\")\n\n# 打印分类报告\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred_binary, target_names=data.target_names))\n\n# 保存模型\nbst.save_model('xgboost_model.model')\n\n# 结束swanlab会话\nswanlab.finish()\n```",
    "617": "一级标题：Xtuner\n二级标题：无\n内容：\n[XTuner](https://github.com/InternLM/xtuner) 是一个高效、灵活、全能的轻量化大模型微调工具库。\n\n<div align=\"center\">\n<img src=\"/assets/integration-xtuner.png\" width=440>\n</div>\n\nXtuner支持与书生·浦语（InternLM）、Llama等多款开源大模型的适配，可执行增量预训练、指令微调、工具类指令微调等任务类型。硬件要求上，在Tesla T4、A100等传统数据中心之外，开发者最低使用消费级显卡便可进行训练，实现大模型特定需求能力。\n\n<div align=\"center\">\n<img src=\"/assets/integration-xtuner-intro.png\">\n</div>\n\nXtuner 支持通过 MMEngine 使用 SwanLab 进行在线跟踪，只需在配置文件中添加几行代码，就可以跟踪和可视化损失、显存占用等指标。",
    "618": "一级标题：Xtuner\n二级标题：使用SwanLab可视化跟踪Xtuner微调进展\n内容：\n打开要训练的配置文件（比如[qwen1_5_7b_chat_full_alpaca_e3.py](https://github.com/InternLM/xtuner/blob/main/xtuner/configs/qwen/qwen1_5/qwen1_5_7b_chat/qwen1_5_7b_chat_full_alpaca_e3.py)）），找到`visualizer`参数的位置，将它替换成：\n\n```python\n# set visualizer\nfrom mmengine.visualization import Visualizer\nfrom swanlab.integration.mmengine import SwanlabVisBackend\n\nvisualizer = dict(type=Visualizer, vis_backends=[dict(type=SwanlabVisBackend)])\n```\n\n然后照样运行微调命令，即可实现SwanLab实验跟踪：\n\n```bash\nxtuner train qwen1_5_7b_chat_full_alpaca_e3.py\n```\n\n---\n\n如果希望像平常使用SwanLab那样指定项目名、实验名等信息，可以在实例化`SwanlabVisBackend`时在`init_kwargs`参数中指定，可以参考 [swanlab init](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/sdk.py#L71) 查看可配置的参数。\n\n通过以字典的形式传入`init_kwargs`，该参数最终会传给 `swanlab.init` 方法，下面举了个指定项目名称的案例。\n\n```python (5)\nvisualizer = dict(\n  type=Visualizer,\n  vis_backends=[dict(\n        type=SwanlabVisBackend,\n        init_kwargs=dict(project='toy-example', experiment_name='Qwen'),\n    )])\n```\n\n有关MM系列的其他引入方法和更灵活的配置，可以参考[MMEngine接入SwanLab](https://docs.swanlab.cn/zh/guide_cloud/integration/integration-mmengine.html)。",
    "619": "一级标题：ZhipuAI\n二级标题：无\n内容：\n[zhipuai](https://github.com/MetaGLM/zhipuai-sdk-python-v4)是[智谱开放平台](https://open.bigmodel.cn/dev/api) 大模型接口的Python SDK，让开发者更便捷的调用智谱开放API。\n\n![](/assets/integration-zhipu.jpg)\n\n你可以使用zhipuai获得ChatGLM的回复，同时使用SwanLab自动进行过程记录。",
    "620": "一级标题：ZhipuAI\n二级标题：1. 引入autolog\n内容：\n```python\nfrom swanlab.integration.zhipuai import autolog\n```\n\nautolog是一个为zhipuai适配的过程记录类，能够自动记录你的zhipuai交互的过程。",
    "621": "一级标题：ZhipuAI\n二级标题：2. 传入参数\n内容：\n```python\nautolog(init=dict(project=\"zhipuai_logging\"))\n```\n\n这里给`init`传入的参数与`swanlab.init`的参数形式完全一致。",
    "622": "一级标题：ZhipuAI\n二级标题：3. 自动记录\n内容：\n```python\nfrom swanlab.integration.zhipuai import autolog\n\nautolog(init=dict(project=\"zhipuai_logging\"))\nclient = autolog.client\n\nresponse = client.chat.completions.create(\n    model=\"glm-4\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"作为一名营销专家，请为我的产品创作一个吸引人的slogan\"},\n        {\"role\": \"assistant\", \"content\": \"当然，为了创作一个吸引人的slogan，请告诉我一些关于您产品的信息\"},\n        {\"role\": \"user\", \"content\": \"智谱AI开放平台\"},\n        {\"role\": \"assistant\", \"content\": \"智启未来，谱绘无限一智谱AI，让创新触手可及!\"},\n        {\"role\": \"user\", \"content\": \"创造一个更精准、吸引人的slogan\"},\n    ],\n)\n\nresponse2 = client.chat.completions.create(\n    model=\"glm-4\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"谁获得了NBA2015年的总冠军\"},\n    ],\n)\n```",
    "623": "一级标题：阿里云计算巢应用部署\n二级标题：无\n内容：\n:::warning 关于第三方部署\n\n第三方部署是由社区贡献的部署方式，官方不保证能实时同步最新版本。\n\n:::\n\n目前 SwanLab 社区版本已上线阿里云计算巢服务市场，欢迎各位训练师通过阿里云一键部署使用~",
    "624": "一级标题：阿里云计算巢应用部署\n二级标题：⚠️ 前提条件\n内容：\n部署 SwanLab 社区版服务实例，需要对部分阿里云资源进行访问和创建操作。因此您的账号需要包含如下资源的权限。\n**说明**：当您的账号是RAM账号时，才需要添加此权限。\n\n| 权限策略名称                          | 备注                         |\n|---------------------------------|----------------------------|\n| AliyunECSFullAccess             | 管理云服务器服务（ECS）的权限           |\n| AliyunVPCFullAccess             | 管理专有网络（VPC）的权限             |\n| AliyunROSFullAccess             | 管理资源编排服务（ROS）的权限           |\n| AliyunComputeNestUserFullAccess | 管理计算巢服务（ComputeNest）的用户侧权限 |",
    "625": "一级标题：阿里云计算巢应用部署\n二级标题：💰 计费说明\n内容：\nSwanLab社区版在计算巢部署的费用主要涉及：\n\n- 所选vCPU与内存规格\n- 系统盘类型及容量\n- 公网带宽",
    "626": "一级标题：阿里云计算巢应用部署\n二级标题：🚀 部署流程\n内容：\n1. 访问计算巢SwanLab社区版[部署链接](https://computenest.console.aliyun.com/service/instance/create/cn-hangzhou?type=user&ServiceId=service-cb2da57160444c3ebdbf)\n，按提示填写部署参数：\n<img src=\"./alibabacloud-computenest/deploy_service_instance.jpg\" width=\"800\"/>\n\n2. 参数填写完成后可以看到对应询价明细，确认参数后点击**下一步：确认订单**。\n\n3. 确认订单完成后同意服务协议并点击**立即创建**进入部署阶段。\n\n4. 等待部署完成后就可以开始使用服务，进入服务实例详情点击服务地址。\n   <img src=\"./alibabacloud-computenest/get_service_instance.jpg\" width=\"800\"/>\n\n5. 访问服务地址注册账号并使用SwanLab服务。\n   <img src=\"./alibabacloud-computenest/swanlab_service.jpg\" width=\"800\"/>",
    "627": "一级标题：使用Docker进行部署\n二级标题：无\n内容：\n如果你想要使用SwanLab私有化部署（社区版），请按照下面的流程进行安装。\n\n![logo](./docker-deploy/swanlab-docker.jpg)",
    "628": "一级标题：使用Docker进行部署\n二级标题：先决条件\n内容：\n> 在安装 SwanLab 之前，请确保您的机器满足以下最低系统要求：\n>\n> - CPU >= 2核\n> - 内存 >= 4GB\n> - 存储空间 >= 20GB\n\nSwanLab 私有化部署版，需要使用 **Docker Compose** 进行安装与部署（暂不支持K8S部署），请根据你的操作系统，对表下面的表格选择正确的Docker及compose版本。\n\n**如果你已经安装了Docker，请跳过这一步。**\n\n| 操作系统               | 软件                                                   | 解释                                                                                                                                                                                                                                                                                                                                                 |\n| ---------------------- | ------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| macOS 10.14 或更高版本 | Docker Desktop                                         | 将 Docker 虚拟机 (VM) 设置为至少使用 2 个虚拟 CPU (vCPU) 和 8 GB 初始内存。否则，安装可能会失败。有关更多信息，请参阅[Mac 版 Docker Desktop 安装指南](https://docs.docker.com/desktop/install/mac-install/)。                                                                                                                                        |\n| Windows（启用了WSL 2） | Docker Desktop                                         | 我们建议将源代码和其他与 Linux 容器绑定的数据存储在 Linux 文件系统中，而不是 Windows 文件系统中。有关更多信息，请参阅 [Windows上使用WSL安装Linux](https://learn.microsoft.com/zh-cn/windows/wsl/install) 与 [在 Windows 上使用 WSL 2 后端的 Docker Desktop 安装指南](https://docs.docker.com/desktop/setup/install/windows-install/#wsl-2-backend)。 |\n| Linux                  | Docker 19.03 或更高版本 Docker Compose 1.28 或更高版本 | 有关如何安装Docker和Docker Compose 的更多信息，请参阅[Docker 安装指南](https://docs.docker.com/engine/install/)和[Docker Compose 安装指南](https://docs.docker.com/compose/install/)。                                                                                                                                                               |\n\n> 如果你还未安装Docker，可以运行我们提供的[安装脚本](https://docs.docker.com/desktop/install/mac-install/)。\n\n---\n\n**端口说明**\n\n如果你将SwanLab部署在服务器上，并希望能够远程访问与实验记录，那么请确保服务器开放以下两个端口：\n\n| 端口号 | 是否可配置 | 用途说明                                                      |\n| ------ | ---------- | ------------------------------------------------------------- |\n| 8000   | 是         | 网关服务端口，可用于接收外部请求，建议在公网环境中设置为 `80` |\n| 9000   | 否         | MinIO 签名端口，用于对象存储访问，端口固定不可修改            |\n\n> 由于网关服务端口（默认为`8000`）支持在部署前后修改，所以请确保你开放的是最终修改的端口。",
    "629": "一级标题：使用Docker进行部署\n二级标题：1. 克隆仓库\n内容：\n使用Git克隆`self-hosted`仓库：\n\n```bash\ngit clone https://github.com/SwanHubX/self-hosted.git\ncd self-hosted\n```",
    "630": "一级标题：使用Docker进行部署\n二级标题：2. 一键脚本安装\n内容：\n> 如果你使用的是Windows系统，请确保已安装并开启 WSL2 和 Docker Desktop\n> <img src=\"./docker-deploy/wsl-dockerinfo.png\" width=\"600\"/>\n\n> 在WSL2的文件系统中执行 `.sh` 安装脚本\n> <img src=\"./docker-deploy/wsl-bash.png\" width=\"600\"/>\n\n默认的安装脚本在`docker/install.sh`，直接执行即可一键安装所有需要的容器以及执行初始化配置。\n\n```bash\ncd ./docker\n./install.sh\n```\n\n默认脚本链接的镜像源在中国，所以中国地区的下载速度非常快！\n\n如果你需要使用 [DockerHub](https://hub.docker.com/) 作为镜像源，则可以使用下面的脚本进行安装：\n\n```bash\n./install-dockerhub.sh\n```",
    "631": "一级标题：使用Docker进行部署\n二级标题：3. 激活主账号\n内容：\nSwanLab社区版默认会使用`8000`端口，如果你使用的是默认配置，那么可以直接访问：`http://localhost:8000`，就可以访问到SwanLab社区版。\n\n> 也有可能社区版部署在了其他端口，请打开 Docker Desktop，找到`traefik`容器旁边的port映射，比如`64703:80`，那么你应该访问`http://localhost:64703`。\n\n![](./docker-deploy/create-account.png)\n\n现在，你需要激活你的主账号。激活需要1个License，个人使用可以免费在[SwanLab官网](https://swanlab.cn)申请一个，位置在 「设置」-「账户与许可证」。\n\n:::warning 离线验证\n\n在私有化部署 > `v1.1`的版本中，支持在离线环境下验证License。\n\n:::\n\n![](./docker-deploy/apply-license.png)\n\n拿到License后，回到激活页面，填写用户名、密码、确认密码和License，点击激活即可完成创建。\n\n![](./docker-deploy/quick-start.png)",
    "632": "一级标题：使用Docker进行部署\n二级标题：4. 开始你的第一个实验\n内容：\n在Python SDK完成登录：\n\n```bash\nswanlab login --host <IP地址>\n```\n\n> 如果你之前登录过swanlab，想要重新登录，请使用：\n> `swanlab login --host <IP地址> --relogin`。\n\n按回车，填写API Key，完成登录。之后你的SwanLab实验将会默认传到私有化部署的SwanLab上。\n\n---\n\n测试脚本：\n\n```bash\nimport swanlab\nimport random\n\n# 创建一个SwanLab项目\nswanlab.init(\n    # 设置项目名\n    project=\"my-awesome-project\",\n\n    # 设置超参数\n    config={\n        \"learning_rate\": 0.02,\n        \"architecture\": \"CNN\",\n        \"dataset\": \"CIFAR-100\",\n        \"epochs\": 10\n    }\n)\n\n# 模拟一次训练\nepochs = 10\noffset = random.random() / 5\nfor epoch in range(2, epochs):\n  acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n  loss = 2 ** -epoch + random.random() / epoch + offset\n\n  # 记录训练指标\n  swanlab.log({\"acc\": acc, \"loss\": loss})\n\n# [可选] 完成训练，这在notebook环境中是必要的\nswanlab.finish()\n```\n\n运行后在网页查看实验：\n\n![](./docker-deploy/test-experiment.png)",
    "633": "一级标题：使用Docker进行部署\n二级标题：升级版本\n内容：\n如果你想要将你本地的私有化部署版本升级到最新版，请使用下面的命令：\n\n```bash\n# 在你之前本地部署的 self-hosted 项目目录下\ncd ./docker\n./upgrade.sh\n```\n\n升级完成的命令行样式：\n\n![](./docker-deploy/upgrade.png)",
    "634": "一级标题：团队/企业版\n二级标题：无\n内容：\n私有化个人版（免费）目前支持现在公有云版的绝大部分功能，但不支持多人协作、创建组织、权限控制、统计看板等高级功能。\n\n对 商业版/团队版/企业版/多租户云版 有需求的伙伴，欢迎联系我们：[contact@swanlab.cn](mailto:contact@swanlab.cn)，并备注您的公司/机构与职位。",
    "635": "一级标题：常见问题\n二级标题：无\n内容：",
    "636": "一级标题：常见问题\n二级标题：如何修改端口？\n内容：\nSwanLab 自托管版本基于 [Docker](https://www.docker.com/) 部署，默认情况下使用 `8000` 端口，修改自托管服务默认访问端口实际上是修改 **swanlab-traefik** 容器的映射端口，分为以下两种情况：\n\n### 部署前修改\n\n安装脚本提供有一些配置可选项，包括数据存储位置和映射的端口，我们通过修改脚本启动参数来实现修改端口。\n\n- 执行 `install.sh` 安装脚本后，命令行会提示配置可选项，可以交互式输入对应的参数。在命令行输出 `2. Use the default port  (8000)? (y/n):` 后输入 `n`，然后会提示 `Enter a custom port:`，输入对应的端口号即可，例如 `80` 。\n\n```bash\n❯ bash install.sh\n🤩 Docker is installed, so let's get started.\n🧐 Checking if Docker is running...\n\n1. Use the default path  (./data)? (y/n):\n   The selected path is: ./data\n2. Use the default port  (8000)? (y/n):\n```\n\n- 启动脚本时添加参数，安装脚本提供有命令行参数 `-p` 可以用于修改端口，例如： `./install.sh -p 80`。\n\n> 更多命令行参数详见：[通过 Docker 部署](https://github.com/SwanHubX/self-hosted/tree/main/docker)\n\n### 部署后修改\n\n如果需要 SwanLab 服务部署完成后需要修改访问端口，则需要修改生成的 `docker-compose.yaml` 配置文件。\n\n在脚本执行的位置找到 `swanlab/` 目录，执行 `cd swanlab/` 后进入到 `swanlab` 目录下找到对应的 `docker-compose.yaml` 配置文件，然后修改 `traefik` 容器对应的端口 `ports`，如下所示：\n\n```yaml\n  traefik:\n    <<: *common\n    image: ccr.ccs.tencentyun.com/self-hosted/traefik:v3.0\n    container_name: swanlab-traefik\n    ports:\n      - \"8000:80\" # [!code --]\n      - \"80:80\" # [!code ++]\n```\n\n> 上面将访问端口修改为了 `80`\n\n修改完成后执行 `docker compose up -d` 重启容器，重启完成后即可通过 `http://{ip}:80` 访问",
    "637": "一级标题：常见问题\n二级标题：上传媒体文件报错怎么办\n内容：\n当你使用`swanlab.log`记录媒体文件，如图像、音频时，发现报错，如：\n\n```bash\nswanlab: Upload error: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The Access Key Id you provided does not exist in our records.\n```\n\n请检查你的服务器是否开放了`9000`端口，如果未开放，请在服务器防火墙/安全组中开放`9000`端口。",
    "638": "一级标题：离线看板接口文档\n二级标题：无\n内容：\n:::info 提示\n\n本 API 适用于 Swanlab **离线看板模式**。主要用于获取项目、实验、图表等数据，便于进行数据分析。\n\n:::",
    "639": "一级标题：离线看板接口文档\n二级标题：接口 1：获取项目详情\n内容：\n- **URL**：`/api/v1/project`\n- **方法**：`GET`\n- **接口说明**：获取当前 Swanlab 实例中加载的项目及其所有实验信息。\n\n### 响应示例\n\n```json\n{\n  \"code\": 0,\n  \"message\": \"success\",\n  \"data\":{\n    \"id\": 1,\n    \"name\": \"llamafactory\",\n    \"experiments\": [\n        {\n        \"id\": 1,\n        \"name\": \"Qwen2.5-7B/20250321-1130-16bed2e2\",\n        \"run_id\": \"run-20250321_125806-a3b1799d\",\n        \"status\": 0,\n        \"config\": { ... },\n        \"create_time\": \"2025-03-21T04:58:06.387383+00:00\"\n        },\n        ...\n    ]\n  }\n}\n```\n\n### 字段说明\n\n- `id` / `name`：项目唯一标识与名称。\n- `experiments`：该项目下的所有实验信息。\n- `logdir`：日志文件存储路径。\n- `charts`：图表数量。\n- `pinned_opened` / `hidden_opened`：控制面板的默认展开状态。\n\n---",
    "640": "一级标题：离线看板接口文档\n二级标题：接口 2：获取单个实验详情\n内容：\n- **URL**：`/api/v1/experiment/<experiment_id>`\n- **方法**：`GET`\n- **示例**：`/api/v1/experiment/1`\n- **接口说明**：获取指定实验的详细配置信息与系统环境。\n\n### 响应示例\n\n```json\n{\n  \"code\": 0,\n  \"message\": \"success\",\n  \"data\": {\n    \"id\": 1,\n    \"run_id\": \"run-20250321_125806-a3b1799d\",\n    \"name\": \"Qwen2.5-7B/20250321-1130-16bed2e2\",\n    \"config\": { ... },\n    \"system\": {\n      \"cpu\": { \"brand\": \"Intel...\", \"cores\": 104 },\n      \"gpu\": {\n        \"nvidia\": {\n          \"type\": [\"NVIDIA A100-PCIE-40GB\", ...],\n          \"memory\": [40, 40, 40, 40],\n          \"cuda\": \"11.6\"\n        }\n      },\n      \"os\": \"Linux...\",\n      \"python\": \"3.10.14\",\n      \"command\": \"/path/to/cli config.yaml\",\n      \"swanlab\": {\n        \"version\": \"0.5.2\",\n        \"logdir\": \"/path/to/logs\",\n        \"_monitor\": 3\n      }\n    }\n  }\n}\n```\n\n### 字段说明\n\n- `config`：实验的完整参数配置。\n- `system`：运行时主机的系统信息，包括 CPU、GPU、Python 版本、命令等。\n- `run_id`：实验的唯一标识符，通常与日志文件关联。\n\n---",
    "641": "一级标题：离线看板接口文档\n二级标题：接口 3：获取实验图表信息\n内容：\n- **URL**：`/api/v1/experiment/<experiment_id>/chart`\n- **方法**：`GET`\n- **示例**：`/api/v1/experiment/1/chart`\n- **接口说明**：获取指定实验的所有图表定义和元信息（如 loss 曲线、学习率曲线等）。\n\n### 响应示例\n\n```json\n{\n  \"code\": 0,\n  \"message\": \"success\",\n  \"data\": {\n    \"charts\": [\n      {\n        \"id\": 1,\n        \"name\": \"train/loss\",\n        \"type\": \"line\",\n        \"reference\": \"step\",\n        \"source\": [\"train/loss\"],\n        \"multi\": false\n      },\n      ...\n    ],\n    \"namespaces\": [\n      {\n        \"id\": 1,\n        \"name\": \"train\",\n        \"opened\": 1,\n        \"charts\": [1, 3, 5, 7]\n      }\n    ]\n  }\n}\n```\n\n### 字段说明\n\n- `charts`：图表定义列表，包括图表名称、类型、数据来源等。\n- `namespaces`：图表命名空间，用于分类展示。\n- `reference`：图表的 X 轴参考，5982 `step`（训练步数）。\n\n---",
    "642": "一级标题：离线看板接口文档\n二级标题：接口 4：获取指标数据\n内容：\n- **URL**：`/api/v1/experiment/<experiment_id>/tag/<namespace>/<metric_name>`\n- **方法**：`GET`\n- **示例**：`/api/v1/experiment/1/tag/train/loss`\n- **接口说明**：获取指定实验中某个具体指标的历史数据（如 loss、accuracy 等）。\n\n### 响应示例\n\n```json\n{\n  \"code\": 0,\n  \"message\": \"success\",\n  \"data\": {\n    \"sum\": 207,\n    \"max\": 1.7614,\n    \"min\": 0.8499,\n    \"experiment_id\": 1,\n    \"list\": [\n      {\n        \"index\": 1,\n        \"data\": 1.6858,\n        \"create_time\": \"2025-03-21T04:58:32.095272+00:00\"\n      },\n      ...,\n      {\n        \"index\": 207,\n        \"data\": 1.1845,\n        \"create_time\": \"2025-03-21T06:05:16.716693+00:00\",\n        \"_last\": true\n      }\n    ]\n  }\n}\n```\n\n### 字段说明\n\n- `sum`：数据总条数。\n- `max` / `min`：指标最大值与最小值。\n- `experiment_id`：所属实验 ID。\n- `list`：具体数据项，每项包括：\n  - `index`：数据点序号。\n  - `data`：具体数值。\n  - `create_time`：记录时间。\n  - `_last`：是否为最后一个数据点（仅最后一条为 true）。\n\n---",
    "643": "一级标题：离线看板接口文档\n二级标题：接口 5：获取实验最新日志\n内容：\n- **URL**：`/api/v1/experiment/<experiment_id>/recent_log`\n- **方法**：`GET`\n- **示例**：`/api/v1/experiment/1/recent_log`\n- **接口说明**：获取指定实验最新的日志输出。包括 Swanlab 自身日志信息和用户自定义的输出。\n\n### 响应示例\n\n```json\n{\n  \"code\": 0,\n  \"message\": \"success\",\n  \"data\": {\n    \"recent\": [\n      \"swanlab:\",\n      \"{'loss':\"\n    ],\n    \"logs\": [\n      \"swanlab: Tracking run with swanlab version 0.5.2\",\n      \"swanlab: Run data will be saved locally in /data/project/...\",\n      \"{'loss': 1.6858, 'grad_norm': ..., 'epoch': 0.02, ...}\",\n      \"...\"\n    ]\n  }\n}\n```\n\n### 字段说明\n\n- `recent`：最新日志段落，通常用于快速预览。\n- `logs`：日志输出列表，包含 swanlab 系统日志和运行中的配置、输出数据。\n\n---",
    "644": "一级标题：离线看板接口文档\n二级标题：接口 6：获取实验状态信息\n内容：\n- **URL**：`/api/v1/experiment/<experiment_id>/status`\n- **方法**：`GET`\n- **示例**：`/api/v1/experiment/1/status`\n- **接口说明**：获取指定实验的最新状态、更新时间、图表结构等信息。\n\n### 响应示例\n\n```json\n{\n  \"code\": 0,\n  \"message\": \"success\",\n  \"data\": {\n    \"status\": 0,\n    \"update_time\": \"2025-03-21T04:58:06.387487+00:00\",\n    \"finish_time\": null,\n    \"charts\": {\n      \"charts\": [\n        {\n          \"id\": 1,\n          \"name\": \"train/loss\",\n          \"type\": \"line\",\n          \"reference\": \"step\",\n          \"status\": 0,\n          \"source\": [\"train/loss\"],\n          \"multi\": false,\n          \"source_map\": {\"train/loss\": 1}\n        },\n        {\n          \"id\": 3,\n          \"name\": \"train/grad_norm\",\n          \"type\": \"line\",\n          \"reference\": \"step\",\n          \"status\": 0,\n          \"source\": [\"train/grad_norm\"],\n          \"multi\": false,\n          \"source_map\": {\"train/grad_norm\": 1}\n        },\n        {\n          \"id\": 5,\n          \"name\": \"train/learning_rate\",\n          \"type\": \"line\",\n          \"reference\": \"step\",\n          \"status\": 0,\n          \"source\": [\"train/learning_rate\"],\n          \"multi\": false,\n          \"source_map\": {\"train/learning_rate\": 1}\n        },\n        ...\n      ],\n      \"namespaces\": [\n        {\n          \"id\": 1,\n          \"name\": \"train\",\n          \"opened\": 1,\n          \"charts\": [1, 3, 5, 7, 9, 11]\n        }\n      ]\n    }\n  }\n}\n```\n\n### 字段说明\n\n- `status`：实验当前状态，整型（如 0 表示运行中）。\n- `update_time`：实验状态最近更新时间。\n- `finish_time`：实验完成时间，未完成为 `null`。\n- `charts`：实验中的图表结构信息。\n  - `charts`：图表定义数组，字段与 `/chart` 接口一致。\n  - `namespaces`：图表命名空间，标识图表分类与分组。\n\n---",
    "645": "一级标题：离线看板接口文档\n二级标题：接口 7：获取实验指标汇总\n内容：\n- **URL**：`/api/v1/experiment/<experiment_id>/summary`\n- **方法**：`GET`\n- **示例**：`/api/v1/experiment/1/summary`\n- **接口说明**：获取指定实验在当前状态下的各项关键指标的最新值汇总。\n\n### 响应示例\n\n```json\n{\n  \"code\": 0,\n  \"message\": \"success\",\n  \"data\": {\n    \"summaries\": [\n      { \"key\": \"train/loss\", \"value\": 1.1845 },\n      { \"key\": \"train/grad_norm\", \"value\": 1.0172306299209595 },\n      { \"key\": \"train/learning_rate\", \"value\": 0.000037463413651718303 },\n      { \"key\": \"train/epoch\", \"value\": 3.288 },\n      { \"key\": \"train/num_input_tokens_seen\", \"value\": 597776 },\n      { \"key\": \"train/global_step\", \"value\": 207 }\n    ]\n  }\n}\n```\n\n### 字段说明\n\n- `summaries`：包含多个指标的汇总值，每项包括：\n  - `key`：指标名称（如 `train/loss`）。\n  - `value`：该指标当前最新值。",
    "646": "一级标题：离线看板\n二级标题：无\n内容：\n:::warning 注意\n\n离线看板是SwanLab的历史功能，现阶段仅做简单维护，不再更新。\n\n如果您有私有化部署的需求，推荐使用[Docker版](/guide_cloud/self_host/docker-deploy)。\n\n:::\n\n离线看板是一种使用模式接近`tensorboard`的轻量级离线web看板。\n\nGithub：https://github.com/SwanHubX/SwanLab-Dashboard",
    "647": "一级标题：离线看板\n二级标题：安装\n内容：\n> 在swanlab>=0.5.0版本后，不再自带离线看板，需要使用dashboard扩展安装。\n\n使用离线看板，需要安装`swanlab`的`dashboard`扩展：\n\n```bash\npip install swanlab[dashboard]\n```",
    "648": "一级标题：离线看板\n二级标题：离线实验跟踪\n内容：\n在`swanlab.init`中设置`logdir`和`mode`这两个参数，即可离线跟踪实验：\n\n```python\n...\n\nswanlab.init(\n  logdir='./logs',\n  mode=\"local\",\n)\n\n...\n```\n\n- 参数`mode`设置为`local`，关闭将实验同步到云端\n- 参数`logdir`的设置是可选的，它的作用是指定了SwanLab日志文件的保存位置（默认保存在`swanlog`文件夹下）\n  - 日志文件会在跟踪实验的过程中被创建和更新，离线看板的启动也将基于这些日志文件\n\n其他部分和云端使用完全一致。",
    "649": "一级标题：离线看板\n二级标题：开启离线看板\n内容：\n打开终端，使用下面的指令，开启一个SwanLab仪表板:\n\n```bash\nswanlab watch ./logs\n```\n\n> 谐音：用swanlab看 ./logs 里的文件\n\n运行完成后，将启动一个后端服务，SwanLab会给你1个本地的URL链接（默认是http://127.0.0.1:5092）\n\n访问该链接，就可以在浏览器用离线看板查看实验了。\n\n[如何设置端口号和IP](/api/cli-swanlab-watch.md#设置ip和端口号)",
    "650": "一级标题：纯离线环境部署\n二级标题：无\n内容：\n> [!NOTE] \n>\n> 该教程适用于将 SwanLab 私有化部署在无法联网的服务器上。",
    "651": "一级标题：纯离线环境部署\n二级标题：部署流程\n内容：\n### 1. 下载镜像\n\n由于私有化版 [SwanLab](https://github.com/SwanHubX/self-hosted) 基于 Docker 部署，因此我们需要先在一台联网的机器上提前下载好所有镜像。\n\n> [!NOTE]\n>\n> 注意需要在相同 CPU 架构的服务器上下载镜像。比如你的服务器为 AMD64 架构，那么也需要在 AMD64 架构的服务器上拉取镜像，不能在 MacBook 这类采用 ARM64 架构的电脑上下载镜像。\n\n找到一台联网的电脑，确保其安装有 [Docker](https://docs.docker.com/engine/install/)，然后执行 [pull-images.sh](https://github.com/SwanHubX/self-hosted/blob/main/scripts/pull-images.sh) 脚本下载镜像包。执行完成后会得到一个 `swanlab_images.tar` 的压缩包。\n\n::: details pull-images.sh 脚本详情\n\n```shell\n#!/bin/bash\n\n# 定义要下载的镜像列表\nimages=(\n  \"ccr.ccs.tencentyun.com/self-hosted/traefik:v3.0\"\n  \"ccr.ccs.tencentyun.com/self-hosted/postgres:16.1\"\n  \"ccr.ccs.tencentyun.com/self-hosted/redis-stack-server:7.2.0-v15\"\n  \"ccr.ccs.tencentyun.com/self-hosted/clickhouse:24.3\"\n  \"ccr.ccs.tencentyun.com/self-hosted/logrotate:v1\"\n  \"ccr.ccs.tencentyun.com/self-hosted/fluent-bit:3.0\"\n  \"ccr.ccs.tencentyun.com/self-hosted/minio:RELEASE.2025-02-28T09-55-16Z\"\n  \"ccr.ccs.tencentyun.com/self-hosted/minio-mc:RELEASE.2025-04-08T15-39-49Z\"\n  \"ccr.ccs.tencentyun.com/self-hosted/swanlab-server:v1.1.1\"\n  \"ccr.ccs.tencentyun.com/self-hosted/swanlab-house:v1.1\"\n  \"ccr.ccs.tencentyun.com/self-hosted/swanlab-cloud:v1.1\"\n  \"ccr.ccs.tencentyun.com/self-hosted/swanlab-next:v1.1\"\n)\n\n# 下载镜像\nfor image in \"${images[@]}\"; do\n  docker pull \"$image\"\ndone\n\n# 保存镜像到文件\necho \"正在打包所有镜像到 swanlab_images.tar...\"\ndocker save -o ./swanlab_images.tar \"${images[@]}\"\n\necho \"所有镜像都打包至 swanlab_images.tar，可直接上传该文件到目标服务器!\"\n```\n\n:::\n\n###  2. 上传镜像到目标服务器\n\n可以使用 [sftp](https://www.ssh.com/academy/ssh/sftp-ssh-file-transfer-protocol) 等命令。例如：\n\n- 首先连接到服务器\n\n```bash\n$ sftp username@remote_host\n```\n\n- 上传文件\n\n```sftp\n> put swanlab_images.tar swanlab_images.tar\n```\n\n> [!TIP]\n>\n> 借助 [Termius](https://termius.com/) 这类 SSH 工具可以更方便地向服务器上传下载文件\n\n### 3. 加载镜像\n\n> [!NOTE]  \n>\n> 需求确保服务器上安装有 [Docker](https://docs.docker.com/engine/install/)\n\n将镜像上传到目标服务器之后，需要加载镜像，命令如下：\n\n```bash\n$ docker load -i swanlab_images.tar\n```\n\n等待加载成功后，可以通过命令 `docker images` 查看镜像列表。\n\n```bash\n(base) root@swanlab:~# docker images\nREPOSITORY                                              TAG                            IMAGE ID       CREATED         SIZE\nccr.ccs.tencentyun.com/self-hosted/swanlab-server       v1.1.1                         a2b992161a68   8 days ago      1.46GB\nccr.ccs.tencentyun.com/self-hosted/swanlab-next         v1.1                           7a33e5b1afc5   3 weeks ago     265MB\nccr.ccs.tencentyun.com/self-hosted/swanlab-cloud        v1.1                           0bc15f138d79   3 weeks ago     53.3MB\nccr.ccs.tencentyun.com/self-hosted/swanlab-house        v1.1                           007b252f5b6c   3 weeks ago     48.5MB\nccr.ccs.tencentyun.com/self-hosted/minio-mc             RELEASE.2025-04-08T15-39-49Z   f33e36a42eec   5 weeks ago     84.1MB\nccr.ccs.tencentyun.com/self-hosted/clickhouse           24.3                           6ffc1e932ef1   2 months ago    942MB\nccr.ccs.tencentyun.com/self-hosted/fluent-bit           3.0                            97e65b999a4d   2 months ago    84.9MB\nccr.ccs.tencentyun.com/self-hosted/traefik              v3.0                           0f62db80c71d   2 months ago    190MB\nccr.ccs.tencentyun.com/self-hosted/minio                RELEASE.2025-02-28T09-55-16Z   377fe6127f60   2 months ago    180MB\nccr.ccs.tencentyun.com/self-hosted/redis-stack-server   7.2.0-v15                      110cc99f3057   3 months ago    520MB\nccr.ccs.tencentyun.com/self-hosted/postgres             16.1                           86414087c100   16 months ago   425MB\nccr.ccs.tencentyun.com/self-hosted/logrotate            v1                             e07b32a4bfda   6 years ago     45.6MB\n```\n\n### 4. 安装 SwanLab 服务\n\n在完成镜像载入之后，需要使用安装脚本完成服务安装并启动。\n\n首先在一台有网络的计算机上，使用 Git 克隆仓库到本地目录：\n\n```bash\n$ git clone https://github.com/SwanHubX/self-hosted.git\n```\n\n然后，将 `self-hosted` 文件夹上传到目标服务器。\n\n---\n\n在目标服务器，进入 `self-hosted` 目录，执行脚本 `./docker/install.sh` 用于安装，安装成功会看到以下标志：\n\n```bash\n$ ./docker/install.sh\n\n...\n   _____                    _           _     \n  / ____|                  | |         | |    \n | (_____      ____ _ _ __ | |     __ _| |__  \n  \\___ \\ \\ /\\ / / _` | '_ \\| |    / _` | '_ \\ \n  ____) \\ V  V / (_| | | | | |___| (_| | |_) |\n |_____/ \\_/\\_/ \\__,_|_| |_|______\\__,_|_.__/ \n                                              \n Self-Hosted Docker v1.1 - @SwanLab\n\n🎉 Wow, the installation is complete. Everything is perfect.\n🥰 Congratulations, self-hosted SwanLab can be accessed using {IP}:8000\n```\n\n> [!TIP]\n>\n> 默认脚本使用的镜像源在中国，所以中国地区不需要担心网络问题\n>\n> 如果你需要使用 [DockerHub](https://hub.docker.com/) 作为镜像源，可以使用下面的脚本进行安装：\n>\n> ```bash\n> $ ./docker/install-dockerhub.sh\n> ```\n\n脚本执行成功后，将会在当前目录下创建一个 `swanlab/` 目录，并在目录下生成两个文件：\n\n- `docker-compose.yaml`：用于 Docker Compose 的配置文件\n- `.env`：对应的密钥文件，保存数据库对应的初始化密码\n\n在 `swanlab` 目录下执行 `docker compose ps -a` 可以查看所有容器的运行状态：\n\n```bash\n$ docker compose ps -a                                                                                                                                             \nNAME                 IMAGE                                                                   COMMAND                  SERVICE          CREATED          STATUS                    PORTS\nswanlab-clickhouse   ccr.ccs.tencentyun.com/self-hosted/clickhouse:24.3                      \"/entrypoint.sh\"         clickhouse       22 minutes ago   Up 22 minutes (healthy)   8123/tcp, 9000/tcp, 9009/tcp\nswanlab-cloud        ccr.ccs.tencentyun.com/self-hosted/swanlab-cloud:v1                     \"/docker-entrypoint.…\"   swanlab-cloud    22 minutes ago   Up 21 minutes             80/tcp\nswanlab-fluentbit    ccr.ccs.tencentyun.com/self-hosted/fluent-bit:3.0                       \"/fluent-bit/bin/flu…\"   fluent-bit       22 minutes ago   Up 22 minutes             2020/tcp\nswanlab-house        ccr.ccs.tencentyun.com/self-hosted/swanlab-house:v1                     \"./app\"                  swanlab-house    22 minutes ago   Up 21 minutes (healthy)   3000/tcp\nswanlab-logrotate    ccr.ccs.tencentyun.com/self-hosted/logrotate:v1                         \"/sbin/tini -- /usr/…\"   logrotate        22 minutes ago   Up 22 minutes             \nswanlab-minio        ccr.ccs.tencentyun.com/self-hosted/minio:RELEASE.2025-02-28T09-55-16Z   \"/usr/bin/docker-ent…\"   minio            22 minutes ago   Up 22 minutes (healthy)   9000/tcp\nswanlab-next         ccr.ccs.tencentyun.com/self-hosted/swanlab-next:v1                      \"docker-entrypoint.s…\"   swanlab-next     22 minutes ago   Up 21 minutes             3000/tcp\nswanlab-postgres     ccr.ccs.tencentyun.com/self-hosted/postgres:16.1                        \"docker-entrypoint.s…\"   postgres         22 minutes ago   Up 22 minutes (healthy)   5432/tcp\nswanlab-redis        ccr.ccs.tencentyun.com/self-hosted/redis-stack-server:7.2.0-v15         \"/entrypoint.sh\"         redis            22 minutes ago   Up 22 minutes (healthy)   6379/tcp\nswanlab-server       ccr.ccs.tencentyun.com/self-hosted/swanlab-server:v1                    \"docker-entrypoint.s…\"   swanlab-server   22 minutes ago   Up 21 minutes (healthy)   3000/tcp\nswanlab-traefik      ccr.ccs.tencentyun.com/self-hosted/traefik:v3.0                         \"/entrypoint.sh trae…\"   traefik          22 minutes ago   Up 22 minutes (healthy)   0.0.0.0:8000->80/tcp, [::]:8000->80/tcp\n```\n\n通过执行 `docker compose logs <container_name>` 可以查看每个容器的日志。\n\n### 5. 访问 SwanLab\n\n安装成功后，可以通过 `http://localhost:8000` （默认端口为8000）直接打开网站。第一次打开需要激活主账户，流程见[文档](https://docs.swanlab.cn/guide_cloud/self_host/docker-deploy.html#_3-%E6%BF%80%E6%B4%BB%E4%B8%BB%E8%B4%A6%E5%8F%B7)。\n\n### 6. 升级 SwanLab\n\n如果你希望升级私有化部署版，那么回到联网的机器上，同步github上最新的`self-hosted`仓库，然后执行升级脚本：\n\n```bash\n$ ./docker/upgrade.sh\n```\n\n将升级后的镜像导出到目标服务器，载入镜像以覆盖之前的镜像。\n\n同时，将新同步的`self-hosted` 文件夹也上传到目标服务器（⚠️注意：不要覆盖存储原先私有化部署数据的文件夹）。\n\n然后在离线机器上，进入`self-hosted`目录，执行`./docker/upgrade.sh`进行升级。\n\n```bash\ncd self-hosted\n./docker/upgrade.sh\n```\n\n脚本运行完成后即完成升级。",
    "652": "一级标题：远程访问离线看板\n二级标题：无\n内容：\n`swanlab watch`命令让离线访问实验变得非常简单，而在机器学习训练中，使用远程服务器的情况是十分常见的。\n\n本节将教您：\n\n- 如何设定实验看板的IP和端口。\n- 如何在本机访问实验看板",
    "653": "一级标题：远程访问离线看板\n二级标题：准备工作\n内容：\n- `记下远程端IP`：比如你使用的是云服务器，那么它自带的公网IP（形如8.141.192.68）就是你之后本机访问实验看板的IP；如果你使用的是局域网服务器，那么则记下它的局域网IP。\n- `开放端口`：首先需要检查一下远程端的安全组/防火墙，比如你希望实验看板所用的端口为`5092`，那么需要检查服务器是否开放了该端口。\n\n> 可使用telnet <服务器IP> <端口号>命令查看linux服务器端口是否开放",
    "654": "一级标题：远程访问离线看板\n二级标题：在远程端设定实验看板的IP与端口\n内容：\n我们需要在远程端（跑训练所在的机器）运行实验看板服务。\n\n在swanlab watch命令中，可设置的参数主要有`-p`和`-h`：\n\n| API         | 描述                                     | 例子                                             |\n|-------------|------------------------------------------|--------------------------------------------------|\n| `-p, --port`| 设置实验看板Web服务运行的端口，默认为5092。 | `swanlab watch -p 8080`：将实验看板Web服务设置为8080端口 |\n| `-h, --host`| 设置实验看板Web服务运行的IP地址，默认为127.0.0.1。 | `swanlab watch -h 0.0.0.0`：将实验看板Web服务的IP地址设置为0.0.0.0 |\n\n\n一般远程访问实验看板需要将`-h`设置为`0.0.0.0`，`-p`的设置则根据你的需求。这里我们将端口设置为`8080`：\n\n```shell\nswanlab watch -h 0.0.0.0 -p 8080\n```\n\n运行上面的命令，得到：\n\n![image](/assets/self-host_im.jpg)",
    "655": "一级标题：远程访问离线看板\n二级标题：本机访问实验看板\n内容：\n这时我们在本机端打开浏览器，访问`远程端IP地址:端口号`。\n\n比如我的远程服务器的公网IP是`8.146.xxx.71`，端口号设置为`8080`，那么在浏览器就访问`8.146.xxx.71:8080`。",
    "656": "一级标题：腾讯云应用部署\n二级标题：无\n内容：\n:::warning 关于第三方部署\n\n第三方部署是由社区贡献的部署方式，官方不保证能实时同步最新版本。\n\n:::\n\n目前 SwanLab 自托管版本已上线腾讯云应用市场，欢迎各位训练师通过腾讯云开箱使用~\n\n![](./tencentcloud-app/head.png)\n\n- [SwanLab 腾讯云应用](https://app.cloud.tencent.com/detail/SPU_BHEEJEJCDD1984)",
    "657": "一级标题：腾讯云应用部署\n二级标题：先决条件\n内容：\n1. 首先需要一个腾讯云账号，并确保账号拥有 **安装云应用的权限**，参考链接：[腾讯云应用购买安装指引](https://cloud.tencent.com/document/product/1689/113848)\n\n2. 在 [腾讯云控制台-私有网络](https://console.cloud.tencent.com/vpc/vpc) 中，创建一个默认的 `VPC`（Vitual Private Cloud， 虚拟私有云），为云应用提供目标网络，\n目前支持的地域如下：\n    - 境内：南京; 北京; 广州; 成都; 上海; 重庆; 成都\n    - 境外：中国香港; 新加坡; 硅谷; 圣保罗; 法兰克福\n\n<img src=\"./tencentcloud-app/setup-vpc.png\" width=\"600\"/>\n\n以`南京`区域为例，CIDR与子网可以按需修改，必填项只有`名称`、`子网名称`与`可用区`\n\n<img src=\"./tencentcloud-app/setup-vpc-option.png\" width=\"600\"/>",
    "658": "一级标题：腾讯云应用部署\n二级标题：安装教程\n内容：\n1. 进入 [SwanLab 腾讯云应用](https://app.cloud.tencent.com/detail/SPU_BHEEJEJCDD1984) 页面，\n勾选 `我已阅读并同意《腾讯云云应用通用商品用户协议》`，并点击 `安装应用`，跳转到控制台界面\n\n<img src=\"./tencentcloud-app/intro.png\" width=\"800\"/>\n\n2. 在控制台界面，只需要配置 `目标网络`、`云服务器类型` 以及 `数据盘大小` 三项云资源设置：\n<img src=\"./tencentcloud-app/resource-option.png\" width=\"800\"/>\n\n各云资源代表的含义如下：\n\n| 配置项 | 说明 | 配置要求 |\n| ---- | ---- | ---- |\n| 目标网络 | 云服务托管地域 | 可以根据之前创建 `VPC` 的地域进行选择 |\n| 云服务器类型 | 云服务器实例配置 | 最低配置：<br>- CPU: ≥ 4 核<br>- 内存：≥ 8GB<br>- 系统存储空间：默认 40GB |\n| 数据盘大小 | 记录实验数据的硬盘大小 | 默认为 `100GB`，最低 `40GB` |\n\n云资源配置完成之后，点击 `下一步：确定资源`\n\n<img src=\"./tencentcloud-app/resource-confirm.png\" width=\"800\"/>\n\n3. 接着进入`确认订单信息`信息界面，腾讯云会根据上一步选用的云资源整理账单费用，此时需要确保腾讯云账号中有一定的余额。确认订单无误后，点击`允许服务角色调用其他云服务接口`，并点击 `下一步：安装应用`\n\n\n<img src=\"./tencentcloud-app/resource-setupapp.png\" width=\"800\"/>\n\n4. 接下来进入应用安装界面，需要等待所有资源创建并启动，需要等待 5 分钟左右\n\n<img src=\"./tencentcloud-app/app-setup.png\" width=\"600\"/>\n\n5. 完成之后，即可在腾讯云控制台界面看到已创建完成的云应用，点击 `打开应用`，即可使用自托管版的SwanLab\n\n<img src=\"./tencentcloud-app/open-app.png\" width=\"800\"/>\n\n\n:::info 提示\n\n在应用创建完成后，如果立即打开应用，可能会看到 404 页面，这是因为云服务器实例创建后需要执行一些容器初始化操作，稍等 1~2 分钟再打开即可。\n\n:::",
    "659": "一级标题：腾讯云应用部署\n二级标题：激活主账号\n内容：\n现在，你可以在腾讯云上使用自托管版本的 SwanLab\n\n<img src=\"./tencentcloud-app/swanlab-hello.png\" width=\"600\"/>\n\n个人使用可以免费在 [SwanLab官网](https://swanlab.cn) 申请一个License，位置在 「设置」-「账户与许可证」。\n\n<img src=\"./tencentcloud-app/swanlab-license-1.png\" width=\"600\"/>\n\n<img src=\"./tencentcloud-app/swanlab-license-2.png\" width=\"600\"/>\n\n输入账号、密码、License 后即可激活自托管版的 SwanLab\n\n<img src=\"./tencentcloud-app/swanlab-main.png\" width=\"600\"/>",
    "660": "一级标题：腾讯云应用部署\n二级标题：启动实验\n内容：\n在Python SDK完成登录：\n\n```bash\nswanlab login --host <IP地址>\n```\n\n> 如果你之前登录过swanlab，想要重新登录，请使用：  \n> `swanlab login --host <IP地址> --relogin`。\n\n按回车，填写API Key，完成登录。之后你的SwanLab实验将会默认传到私有化部署的SwanLab上。\n\n---\n\n测试脚本：\n\n```bash\nimport swanlab\nimport random\n\n# 创建一个SwanLab项目\nswanlab.init(\n    # 设置项目名\n    project=\"my-awesome-project\",\n    \n    # 设置超参数\n    config={\n        \"learning_rate\": 0.02,\n        \"architecture\": \"CNN\",\n        \"dataset\": \"CIFAR-100\",\n        \"epochs\": 10\n    }\n)\n\n# 模拟一次训练\nepochs = 10\noffset = random.random() / 5\nfor epoch in range(2, epochs):\n  acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n  loss = 2 ** -epoch + random.random() / epoch + offset\n\n  # 记录训练指标\n  swanlab.log({\"acc\": acc, \"loss\": loss})\n\n# [可选] 完成训练，这在notebook环境中是必要的\nswanlab.finish()\n```\n\n运行后，可在网页查看实验\n\n<img src=\"./tencentcloud-app/swanlab-dashboard.png\" width=\"600\"/>\n\n\n:::info 提示\n\n如您不再需要使用，请及时在 [腾讯云应用控制台](https://console.cloud.tencent.com/app) 中销毁应用，避免继续计费。\n\n:::",
    "661": "一级标题：版本对照表\n二级标题：无\n内容：\n| 版本 | 发布时间 | 主要更新 | 兼容的Python包版本 |\n| --- | --- | --- | --- |\n| v1.3 | 2025-07-08 | 同步发布时间的云端版更新 |  All |\n| v1.2 | 25-05-30 | 同步发布时间的云端版更新 |  <=0.6.4 |\n| v1.1 | 25-04-27 | 新增License离线验证功能；<br> 同步发布时间的云端版更新 |  <=0.6.4 |\n| v1.0 | 25-03-12 | 初始版本 |  <=0.5.5 |\n\n[升级版本](/guide_cloud/self_host/docker-deploy.html#升级版本)",
    "662": "一级标题：制作你的自定义插件\n二级标题：无\n内容：\n很开心，在`swanlab>=0.5.0`之后，我们正式开启了插件时代！\n\n插件是SwanLab诞生之初我们便一直探讨的话题，这不仅是增强SwanLab的功能与开放性，更是一种全新的视角来看待SwanLab ——\n\nSwanLab不只是1个训练跟踪工具与实验管理平台，同时可以是一个训练过程中的**数据核心**（比如Chrome core），`swanlab.init`与`swanlab.log`被赋予不同的意义。\n\n---\n\n我们将SwanLab的插件模式定义为三种类型：\n\n- **`Python库插件`**：SwanLab Python库中的回调类（Callback）。通过往SwanLab的生命周期阶段（比如`on_init`、`on_run`、`on_stop`等）注入代码的方式，来实现插件功能。\n- **`开放API插件`**：基于SwanLab平台提供的开放API，通过调用API进行组合的方式，来实现插件功能。\n- **`GUI插件`**：基于SwanLab平台开放的前端API，实现对图表、表格等组件的定制化。\n\n::: warning 👋 支持情况\n目前我们支持的插件类型为`Python库插件`，下面我将重点介绍如何制作你的`Python库插件`。\n:::",
    "663": "一级标题：制作你的自定义插件\n二级标题：认识SwanKitCallback类\n内容：\n> 仓库：[swanlab-toolkit](https://github.com/swanhubx/swanlab-toolkit)\n\n`SwanKitCallback`类是SwanLab的回调类，所有插件都必须继承自该类。\n\n```python\nfrom swankit.callback import SwanKitCallback\n```\n\n`SwanKitCallback`类中定义了所有SwanLab的生命周期阶段，你只需要重写你感兴趣的生命周期阶段即可：\n\n常用的生命周期阶段有：\n\n- `on_init`：初始化阶段，执行`swanlab.init`时调用\n- `before_init_experiment`：在初始化`SwanLabRun`之前调用\n- `on_run`：当`SwanLabRun`初始化完毕时调用\n- `on_log`：每次执行`swanlab.log`时调用\n- `on_stop`：停止阶段，当SwanLab停止时调用\n\n更多的生命周期阶段，请参考：[SwanKitCallback](https://github.com/SwanHubX/SwanLab-Toolkit/blob/main/swankit/callback/__init__.py)",
    "664": "一级标题：制作你的自定义插件\n二级标题：实现一个简单的插件\n内容：\n下面以1个案例为例，介绍如何实现一个插件。\n\n```python\nclass MyPlugin(SwanKitCallback):\n    def on_init(self, proj_name: str, workspace: str, logdir: str = None, *args, **kwargs):\n        print(f\"插件初始化: {proj_name} {workspace} {logdir}\")\n\n    def on_stop(self, error: str = None, *args, **kwargs):\n        print(f\"插件停止: {error}\")\n\n    def __str__(self):\n        return \"MyPlugin\"\n```\n\n这个插件实现的功能非常简单，就是在`swanlab.init()`调用时打印1条消息，在进程停止或`swanlab.finish()`调用时打印1条消息。\n\n而在SwanLab中使用着这个插件非常简单，只需要在`swanlab.init()`的`callbacks`参数中传入插件实例即可。\n\n```python {14,16}\nfrom swankit.callback import SwanKitCallback\nimport swanlab\n\nclass MyPlugin(SwanKitCallback):\n    def on_init(self, proj_name: str, workspace: str, logdir: str = None, *args, **kwargs):\n        print(f\"插件初始化: {proj_name} {workspace} {logdir}\")\n\n    def on_stop(self, error: str = None, *args, **kwargs):\n        print(f\"插件停止: {error}\")\n\n    def __str__(self):\n        return \"MyPlugin\"\n\nmy_plugin = MyPlugin()\n\nswanlab.init(callbacks=[my_plugin])\n```\n\n执行上述代码，你会在控制台看到\n\n![image](./custom-plugin/print.png)",
    "665": "一级标题：制作你的自定义插件\n二级标题：案例：指标打印与告警\n内容：\n我们来实现一个插件，这个插件的功能是打印指标，并当指标`acc`大于0.9时，打印1条消息，并发送告警。\n\n### 1. 定义插件\n\n> 在`SwanKitCallback`类中，定义了`on_log`方法，每次执行`swanlab.log`时都会调用该方法。\n\n```python\nclass ThresholdPlugin(SwanKitCallback):\n    def __init__(self, key: str, threshold: float = 0.9):\n        self.key = key\n        self.threshold = threshold\n\n    def on_log(self, data: dict, step: Optional[int] = None, *args, **kwargs):\n        print(f\"data: {data} step: {step}\")\n        if data[self.key] > self.threshold:\n            print(f\"{self.key} > {self.threshold} !!\")\n```\n\n### 2. 使用插件\n\n```python\nfrom swankit.callback import SwanKitCallback\nfrom typing import Optional\nimport swanlab\nimport random\n\nclass ThresholdPlugin(SwanKitCallback):\n    def __init__(self, key: str, threshold: float = 0.9):\n        self.key = key\n        self.threshold = threshold\n\n    def on_log(self, data: dict, step: Optional[int] = None, *args, **kwargs):\n        print(f\"data: {data} step: {step}\")\n        if data[self.key] > self.threshold:\n            print(f\"{self.key} > {self.threshold} !!\")\n\n    def __str__(self):\n        return \"ThresholdPlugin\"\n\nthreshold_plugin = ThresholdPlugin(key=\"acc\", threshold=0.9)\nswanlab.init(callbacks=[threshold_plugin])\n\n# 模拟一次训练\nepochs = 10\noffset = random.random() / 5\nfor epoch in range(2, epochs):\n  acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n  loss = 2 ** -epoch + random.random() / epoch + offset\n\n  # 记录训练指标\n  swanlab.log({\"acc\": acc, \"loss\": loss}, step=epoch)\n```\n\n执行上述代码，你会在控制台看到\n\n![image](./custom-plugin/threshold.png)",
    "666": "一级标题：制作你的自定义插件\n二级标题：学习更多插件\n内容：\n- [EmailCallback](/zh/plugin/notification-email.md)：训练完成/发生错误时，发送消息到邮箱\n- [LarkCallback](/zh/plugin/notification-lark.md)：训练完成/发生错误时，发送消息到飞书",
    "667": "一级标题：钉钉\n二级标题：无\n内容：\n![image](./notification-dingtalk/logo.jpg)\n\n如果你希望在训练完成/发生错误时，第一时间发送[钉钉](https://www.dingtalk.com/)信息通知你，那么非常推荐你使用钉钉通知插件。\n\n:::warning 改进插件\nSwanLab插件均为开源代码，你可以在[Github源代码](https://github.com/swanhubx/swanlab/blob/main/swanlab/plugin/notification.py)中查看，欢迎提交你的建议和PR！\n:::\n\n[[toc]]",
    "668": "一级标题：钉钉\n二级标题：准备工作\n内容：\n1. 在1个钉钉群（企业群）中，点击右上角的 **「设置」按钮**\n\n<img src=\"./notification-dingtalk/setting.png\" width=\"400\"/>\n\n2. 向下滚动，找到 **「机器人」**\n\n<img src=\"./notification-dingtalk/group-robot.png\" width=\"400\"/>\n\n3. 点击 **「添加机器人」**\n\n<img src=\"./notification-dingtalk/add-robot.png\" width=\"400\"/>\n\n4. 添加 **「自定义机器人」**\n\n<img src=\"./notification-dingtalk/custom-robot.png\" width=\"600\"/>\n\n<img src=\"./notification-dingtalk/add-robot-2.png\" width=\"600\"/>\n\n勾选「加签」，复制token到外部。\n\n<img src=\"./notification-dingtalk/add-robot-3.png\" width=\"600\"/>\n\n复制webhook，完成机器人创建：\n\n<img src=\"./notification-dingtalk/add-robot-4.png\" width=\"600\"/>\n\n至此，你完成了准备工作。",
    "669": "一级标题：钉钉\n二级标题：基本用法\n内容：\n使用钉钉通知插件的方法非常简单，只需要初始化1个`DingTalkCallback`对象：\n\n```python\nfrom swanlab.plugin.notification import DingTalkCallback\n\ndingtalk_callback = DingTalkCallback(\n    webhook_url=\"https://oapi.dingtalk.com/robot/xxxx\", \n    secret=\"xxxx\",\n)\n```\n\n然后将`dingtalk_callback`对象传入`swanlab.init`的`callbacks`参数中：\n\n```python\nswanlab.init(callbacks=[dingtalk_callback])\n```\n\n这样，当训练完成/发生错误时（触发`swanlab.finish()`），你将会收到钉钉信息通知。\n\n<img src=\"./notification-dingtalk/show.png\" width=\"600\"/>",
    "670": "一级标题：钉钉\n二级标题：自由提醒\n内容：\n你还可以使用`DingTalkCallback`对象的`send_msg`方法，发送自定义的钉钉信息。\n\n这在提醒你某些指标达到某个阈值时非常有用！\n\n```python \nif accuracy > 0.95:\n    # 自定义场景发送消息\n    dingtalk_callback.send_msg(\n        content=f\"Current Accuracy: {accuracy}\",  # 通知内容\n    )\n```",
    "671": "一级标题：钉钉\n二级标题：外部注册插件\n内容：\n<!--@include: ./shared-snippet.md-->",
    "672": "一级标题：钉钉\n二级标题：限制\n内容：\n- 钉钉通知插件的训练完成/异常通知，使用的是`SwanKitCallback`的`on_stop`生命周期回调，所以如果你的进程被突然`kill`，或者训练机异常关机，那么会因为无法触发`on_stop`回调，从而导致未发送钉钉通知。\n\n- 完善方案请期待`SwanLab`的`平台开放API`上线。",
    "673": "一级标题：Discord\n二级标题：无\n内容：\n如果你希望在训练完成/发生错误时，第一时间发送[Discord](https://discord.com/)信息通知你，那么非常推荐你使用Discord通知插件。\n\n![](./notification-discord/logo.jpg)\n\n:::warning 改进插件\nSwanLab插件均为开源代码，你可以在[Github源代码](https://github.com/swanhubx/swanlab/blob/main/swanlab/plugin/notification.py)中查看，欢迎提交你的建议和PR！\n:::\n\n[[toc]]",
    "674": "一级标题：Discord\n二级标题：准备工作\n内容：\n::: info 参考文档\n- [Discord-Webhook群机器人配置说明](https://support.discord.com/hc/en-us/articles/228383668-Intro-to-Webhooks)\n:::\n\n\n1. 选择您想要接收SwanLab事件通知的 Discord 频道\n\n\n2. 点击对应频道右侧的 **「⚙️」** 对应的 **「编辑频道」** 按钮\n\n<img src=\"./notification-discord/edit-channel.png\" width=\"400\"/>\n\n3. 展开菜单后，选择 **「整合」 -> 「Webhhook」**\n\n<img src=\"./notification-discord/integration-webhook.png\" width=\"400\"/>\n\n\n4. 点击选项卡 **「新Webhook」** 自动创建新的 webhook 机器人\n\n<img src=\"./notification-discord/new-webhook.png\" width=\"400\"/>\n\n5. 点击 **「复制 Webhook URL」** 即可获取到对应的 webhook 地址",
    "675": "一级标题：Discord\n二级标题：基本用法\n内容：\n使用Discord通知插件的方法非常简单，只需要初始化1个`DiscordCallback`对象：\n\n```python\nfrom swanlab.plugin.notification import DiscordCallback\n\ndiscord_callback = DiscordCallback(\n    webhook_url='https://discord.com/api/webhooks/xxxxx/xxx', \n    language='zh'\n)\n```\n\n然后将`discord_callback`对象传入`swanlab.init`的`callbacks`参数中：\n\n```python\nswanlab.init(callbacks=[discord_callback])\n```\n\n这样，当训练完成/发生错误时（触发`swanlab.finish()`），你将会收到Discord消息通知。\n\n\n\n<img src=\"./notification-discord/discord-finish.png\" width=\"500\"/>",
    "676": "一级标题：Discord\n二级标题：自由提醒\n内容：\n你还可以使用`DiscordCallback`对象的`send_msg`方法，发送自定义的的Discord消息。\n\n这在提醒你某些指标达到某个阈值时非常有用！\n\n```python \nif accuracy > 0.95:\n    # 自定义场景发送消息\n    discord_callback.send_msg(\n        content=f\"Current Accuracy: {accuracy}\",  # 通知内容\n    )\n```",
    "677": "一级标题：Discord\n二级标题：限制\n内容：\n- Discord通知插件的训练完成/异常通知，使用的是`SwanKitCallback`的`on_stop`生命周期回调，所以如果你的进程被突然`kill`，或者训练机异常关机，那么会因为无法触发`on_stop`回调，从而导致未发送Discord通知。\n\n- 完善方案请期待`SwanLab`的`平台开放API`上线。",
    "678": "一级标题：邮件通知\n二级标题：无\n内容：\n![image](./notification-email/logo.jpg)\n\n如果你希望在训练完成/发生错误时，第一时间发送邮件通知你，那么非常推荐你使用`邮件通知`插件。\n\n:::warning 改进插件\nSwanLab插件均为开源代码，你可以在[Github源代码](https://github.com/swanhubx/swanlab/blob/main/swanlab/plugin/notification.py)中查看，欢迎提交你的建议和PR！\n:::\n\n[[toc]]",
    "679": "一级标题：邮件通知\n二级标题：准备工作\n内容：\n在使用插件前，首先你需要准备开通你的邮箱的**STMP服务**。以QQ邮箱为例：\n\n**步骤 1：进入邮箱设置**\n\n- 进入QQ邮箱网页，点击顶部的 ​​“设置”​ \n- 在设置菜单中，选择 ​​“账号”​ 选项。\n\n**​步骤 2：开启SMTP服务**\n\n- 找到 **“POP3/IMAP/SMTP/Exchange/CardDAV/CalDAV服务”**\n- 在“服务状态”旁边，点击 **“开启服务”**\n- 经过一些身份验证流程后，完成**STMP服务的开启**\n- （重要）保存给到你的**授权码**\n\n**​步骤 3：记录以下信息**\n- **SMTP服务器地址**: smtp.qq.com\n- **端口**: 465（SSL加密）或 587（TLS加密）\n- **发送邮箱**: 你的完整QQ邮箱地址（如 123456789@qq.com）\n- **密码**: 使用你刚刚获取的 ​授权码，而不是QQ邮箱的登录密码。\n\n其他的邮箱服务基本都支持STMP，可按照相似的流程开启服务。",
    "680": "一级标题：邮件通知\n二级标题：基本用法\n内容：\n使用邮件通知插件的方法非常简单，只需要初始化1个`EmailCallback`对象：\n\n```python\nfrom swanlab.plugin.notification import EmailCallback\n\n# 初始化邮件通知插件\nemail_callback = EmailCallback(\n    sender_email=\"<发送者邮箱，即开启SMTP服务的邮箱>\",\n    receiver_email=\"<接收者邮箱，即你想要收到邮件的邮件>\",\n    password=\"<你的授权码>\",\n    smtp_server=\"<你的邮箱服务器>\",\n    port=587,\n    language=\"zh\",\n)\n```\n\n然后将`email_callback`对象传入`swanlab.init`的`callbacks`参数中：\n\n```python\nswanlab.init(callbacks=[email_callback])\n```\n\n这样，当训练完成/发生错误时（触发`swanlab.finish()`），你将会收到邮件通知。\n\n\n![image](./notification-email/email.png)",
    "681": "一级标题：邮件通知\n二级标题：自由提醒\n内容：\n你还可以使用`EmailCallback`对象的`send_email`方法，发送自定义的邮件。\n\n这在提醒你某些指标达到某个阈值时非常有用！\n\n```python \nif accuracy > 0.95:\n    # 自定义场景发送邮件\n    email_callback.send_email(\n        subject=\"SwanLab | Accuracy > 0.95\",  # 邮件标题\n        content=f\"Current Accuracy: {accuracy}\",  # 邮件内容\n    )\n```",
    "682": "一级标题：邮件通知\n二级标题：外部注册插件\n内容：\n<!--@include: ./shared-snippet.md-->",
    "683": "一级标题：邮件通知\n二级标题：限制\n内容：\n- 邮件通知插件的训练完成/异常通知，使用的是`SwanKitCallback`的`on_stop`生命周期回调，所以如果你的进程被突然`kill`，或者训练机异常关机，那么会因为无法触发`on_stop`回调，从而导致未发送邮件通知。\n\n- 完善方案请期待`SwanLab`的`平台开放API`上线。",
    "684": "一级标题：飞书通知\n二级标题：无\n内容：\n![image](./notification-lark/logo.jpg)\n\n如果你希望在训练完成/发生错误时，第一时间发送飞书信息通知你，那么非常推荐你使用飞书通知插件。\n\n:::warning 改进插件\nSwanLab插件均为开源代码，你可以在[Github源代码](https://github.com/swanhubx/swanlab/blob/main/swanlab/plugin/notification.py)中查看，欢迎提交你的建议和PR！\n:::\n\n[[toc]]",
    "685": "一级标题：飞书通知\n二级标题：准备工作\n内容：\n::: info 参考文档\n- [自定义机器人API使用指南](https://open.feishu.cn/document/client-docs/bot-v3/add-custom-bot?lang=zh-CN#f62e72d5)·\n- [在飞书群组中使用机器人](https://www.feishu.cn/hc/zh-CN/articles/360024984973-%E5%9C%A8%E7%BE%A4%E7%BB%84%E4%B8%AD%E4%BD%BF%E7%94%A8%E6%9C%BA%E5%99%A8%E4%BA%BA)\n:::\n\n\n1. 在1个飞书群中，点击右上角的 **「···」-「设置」**\n\n<img src=\"./notification-lark/setting.png\" width=\"400\"/>\n\n2. 点击 **「群机器人」**\n\n<img src=\"./notification-lark/group-robot.png\" width=\"400\"/>\n\n3. 点击 **「添加机器人」**\n\n<img src=\"./notification-lark/add-robot.png\" width=\"400\"/>\n\n4. 添加 **「自定义机器人」**\n\n<img src=\"./notification-lark/custom-robot.png\" width=\"600\"/>\n\n<img src=\"./notification-lark/custom-robot-detail.png\" width=\"600\"/>\n\n5. 复制 **「Webhook 地址」和 「签名」**\n\n<img src=\"./notification-lark/webhookurl.png\" width=\"600\"/>\n\n至此，你完成了准备工作。",
    "686": "一级标题：飞书通知\n二级标题：基本用法\n内容：\n使用飞书通知插件的方法非常简单，只需要初始化1个`LarkCallback`对象：\n\n```python\nfrom swanlab.plugin.notification import LarkCallback\n\nlark_callback = LarkCallback(\n    webhook_url=\"https://open.larkoffice.com/open-apis/bot/v2/hook/xxxx\", \n    secret=\"xxxx\",\n)\n```\n\n然后将`lark_callback`对象传入`swanlab.init`的`callbacks`参数中：\n\n```python\nswanlab.init(callbacks=[lark_callback])\n```\n\n这样，当训练完成/发生错误时（触发`swanlab.finish()`），你将会收到飞书信息通知。\n\n<img src=\"./notification-lark/show.png\" width=\"600\"/>",
    "687": "一级标题：飞书通知\n二级标题：自由提醒\n内容：\n你还可以使用`LarkCallback`对象的`send_msg`方法，发送自定义的飞书信息。\n\n这在提醒你某些指标达到某个阈值时非常有用！\n\n```python \nif accuracy > 0.95:\n    # 自定义场景发送消息\n    lark_callback.send_msg(\n        content=f\"Current Accuracy: {accuracy}\",  # 通知内容\n    )\n```",
    "688": "一级标题：飞书通知\n二级标题：外部注册插件\n内容：\n<!--@include: ./shared-snippet.md-->",
    "689": "一级标题：飞书通知\n二级标题：限制\n内容：\n- 飞书通知插件的训练完成/异常通知，使用的是`SwanKitCallback`的`on_stop`生命周期回调，所以如果你的进程被突然`kill`，或者训练机异常关机，那么会因为无法触发`on_stop`回调，从而导致未发送飞书通知。\n\n- 完善方案请期待`SwanLab`的`平台开放API`上线。",
    "690": "一级标题：Slack\n二级标题：无\n内容：\n如果你希望在训练完成/发生错误时，第一时间发送[Slack](https://slack.com)信息通知你，那么非常推荐你使用Slack通知插件。\n\n![](./notification-slack/logo.jpg)\n\n:::warning 改进插件\nSwanLab插件均为开源代码，你可以在[Github源代码](https://github.com/swanhubx/swanlab/blob/main/swanlab/plugin/notification.py)中查看，欢迎提交你的建议和PR！\n:::\n\n[[toc]]",
    "691": "一级标题：Slack\n二级标题：准备工作\n内容：\n::: info 参考文档\n- [Slack-使用传入的webhooks发送消息](https://api.slack.com/messaging/webhooks)\n- [腾讯云-Slack群接收消息](https://cloud.tencent.com/document/product/1263/74219)\n:::\n\n\n1. 前往 [Slack-API](https://api.slack.com/apps) 页面，点击 **「Create an App」**\n\n<img src=\"./notification-slack/slack-create-app.png\" width=\"400\"/>\n\n\n2. 在弹窗中点击 **「From scratch」**\n\n<img src=\"./notification-slack/from-scratch.png\" width=\"400\"/>\n\n3. 填写 **「App Name」** ，并选择用于通知的 workspace，点击右下角的 **「Create App」**\n\n<img src=\"./notification-slack/name-app.png\" width=\"400\"/>\n\n4. 进入 App 配置菜单后，点击左侧的 **「Incoming Webhooks」**，并开启 **「Activate Incoming Webhooks」** 按钮；\n\n<img src=\"./notification-slack/slack-webhook-option.png\" width=\"400\"/>\n\n5. 在页面下方，点击 **「Add New Webhook to Workspace」**，将APP添加到工作区的频道中；\n\n\n<img src=\"./notification-slack/add-new-webhook-workspace.png\" width=\"400\"/>\n\n6. 在跳转的应用请求页面中，选择好APP要发送消息的频道，点击 **「允许」**\n\n<img src=\"./notification-slack/allow-channel.png\" width=\"400\"/>\n\n7.最后返回 APP 配置页面，复制APP的 Webhook URL\n\n<img src=\"./notification-slack/copy-url.png\" width=\"500\"/>",
    "692": "一级标题：Slack\n二级标题：基本用法\n内容：\n使用Slack通知插件的方法非常简单，只需要初始化1个`SlackCallback`对象：\n\n```python\nfrom swanlab.plugin.notification import SlackCallback\n\nslack_callback = SlackCallback(\n    webhook_url='https://hooks.slack.com/services/xxxx/xxxx/xxxx', \n    language='zh'\n)\n```\n\n然后将`slack_callback`对象传入`swanlab.init`的`callbacks`参数中：\n\n```python\nswanlab.init(callbacks=[slack_callback])\n```\n\n这样，当训练完成/发生错误时（触发`swanlab.finish()`），你将会收到Slack消息通知。\n\n\n<img src=\"./notification-slack/slack-finish.png\" width=\"500\"/>",
    "693": "一级标题：Slack\n二级标题：自由提醒\n内容：\n你还可以使用`SlackCallback`对象的`send_msg`方法，发送自定义的的Slack消息。\n\n这在提醒你某些指标达到某个阈值时非常有用！\n\n```python \nif accuracy > 0.95:\n    # 自定义场景发送消息\n    slack_callback.send_msg(\n        content=f\"Current Accuracy: {accuracy}\",  # 通知内容\n    )\n```",
    "694": "一级标题：Slack\n二级标题：限制\n内容：\n- Slack通知插件的训练完成/异常通知，使用的是`SwanKitCallback`的`on_stop`生命周期回调，所以如果你的进程被突然`kill`，或者训练机异常关机，那么会因为无法触发`on_stop`回调，从而导致未发送Slack通知。\n\n- 完善方案请期待`SwanLab`的`平台开放API`上线。",
    "695": "一级标题：企业微信\n二级标题：无\n内容：\n![](./notification-wxwork/logo.jpg)\n\n如果你希望在训练完成/发生错误时，第一时间发送[企业微信](https://work.weixin.qq.com/)信息通知你，那么非常推荐你使用企业微信通知插件。\n\n:::warning 改进插件\nSwanLab插件均为开源代码，你可以在[Github源代码](https://github.com/swanhubx/swanlab/blob/main/swanlab/plugin/notification.py)中查看，欢迎提交你的建议和PR！\n:::\n\n[[toc]]",
    "696": "一级标题：企业微信\n二级标题：准备工作\n内容：\n::: info 参考文档\n- [企业微信-群机器人配置说明](https://developer.work.weixin.qq.com/document/path/91770)\n:::\n1. 在企业微信群中，点击右上角的 **「···」-「添加群机器人」**\n\n<img src=\"./notification-wxwork/wxwork-setting.png\" width=\"400\"/>\n\n2. 在弹出的对话框中点击 **「添加机器人」**\n\n<img src=\"./notification-wxwork/wxwork-addrobot.png\" width=\"400\"/>\n\n3. 继续点击  **「新创建一个机器人」**\n\n<img src=\"./notification-wxwork/wxwork-createnewrobot.png\" width=\"400\"/>\n\n4. 为机器人添加名称，点击 **「添加机器人」**\n\n<img src=\"./notification-wxwork/wxwork-name.png\" width=\"400\"/>\n\n5. 企业微信的机器人只需要复制 **「Webhook地址」** 即可\n\n<img src=\"./notification-wxwork/wxwork-webhook.png\" width=\"400\"/>",
    "697": "一级标题：企业微信\n二级标题：基本用法\n内容：\n使用企业微信通知插件的方法非常简单，只需要初始化1个`WXWorkCallback`对象：\n\n```python\nfrom swanlab.plugin.notification import WXWorkCallback\n\nwxwork_callback = WXWorkCallback(\n    webhook_url=\"https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=xxxx\",\n)\n```\n\n然后将`wxwork_callback`对象传入`swanlab.init`的`callbacks`参数中：\n\n```python\nswanlab.init(callbacks=[wxwork_callback])\n```\n\n这样，当训练完成/发生错误时（触发`swanlab.finish()`），你将会收到企业微信消息通知。\n\n\n<img src=\"./notification-wxwork/wxwork-show.png\" width=\"500\"/>",
    "698": "一级标题：企业微信\n二级标题：自由提醒\n内容：\n你还可以使用`WXWorkCallback`对象的`send_msg`方法，发送自定义的的企业微信消息。\n\n这在提醒你某些指标达到某个阈值时非常有用！\n\n```python \nif accuracy > 0.95:\n    # 自定义场景发送消息\n    wxwork_callback.send_msg(\n        content=f\"Current Accuracy: {accuracy}\",  # 通知内容\n    )\n```",
    "699": "一级标题：企业微信\n二级标题：外部注册插件\n内容：\n<!--@include: ./shared-snippet.md-->",
    "700": "一级标题：企业微信\n二级标题：限制\n内容：\n- 企业微信通知插件的训练完成/异常通知，使用的是`SwanKitCallback`的`on_stop`生命周期回调，所以如果你的进程被突然`kill`，或者训练机异常关机，那么会因为无法触发`on_stop`回调，从而导致未发送企业微信通知。\n\n- 完善方案请期待`SwanLab`的`平台开放API`上线。",
    "701": "一级标题：插件一览\n二级标题：无\n内容：\n- [自定义插件](custom-plugin)",
    "702": "一级标题：插件一览\n二级标题：✈️ 通知类\n内容：\n- [邮件](notification-email)\n- [飞书](notification-lark)\n- [钉钉](notification-dingtalk)\n- [企业微信](notification-wxwork)\n- [Discord](notification-discord)\n- [Slack](notification-slack)",
    "703": "一级标题：插件一览\n二级标题：📝 记录类\n内容：\n- [文件记录器](writer-filelogdir)\n- [CSV表格](writer-csv)",
    "704": "一级标题：等价于 swanlab.init(callbaks=[...])\n二级标题：无\n内容：\nswanlab.register_callbacks([...])\n```",
    "705": "一级标题：CSV表格记录器\n二级标题：无\n内容：\n如果你希望在训练过程中，将一些配置信息、指标信息记录在本地的CSV文件中（格式和SwanLab网页中的“表格视图“一致），那么非常推荐你使用`CSV记录器`插件。\n\n:::warning 改进插件\nSwanLab插件均为开源代码，你可以在[Github源代码](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/plugin/writer.py)中查看，欢迎提交你的建议和PR！\n:::",
    "706": "一级标题：CSV表格记录器\n二级标题：插件用法\n内容：\n**1. 初始化CSV记录器：**\n\n```python\nfrom swanlab.plugin.writer import CSVWriter\n\ncsv_writer = CSVWriter(dir=\"logs\")\n```\n\n`dir`参数指定了CSV文件的保存路径，默认保存到当前工作目录。\n\n**2. 传入插件：**\n\n```python\nswanlab.init(\n    ...\n    callbacks=[csv_writer]\n)\n```\n\n执行代码后，就会在`logs`目录下生成一个`swanlab_run.csv`文件，并开始记录数据。后续的每一次训练，都会在该csv文件中添加新的行。\n\n如果想要指定其他文件名，可以传入`filename`参数：\n\n```python\ncsv_writer = CSVWriter(dir=\"logs\", filename=\"my_csv_file.csv\")\n```",
    "707": "一级标题：CSV表格记录器\n二级标题：示例代码\n内容：\n```python\nimport swanlab\nfrom swanlab.plugin.writer import CSVWriter\nimport random\n\ncsv_writer = CSVWriter(dir=\"logs\")\n\n# 创建一个SwanLab项目\nswanlab.init(\n    # 设置项目名\n    project=\"my-awesome-project\",\n    \n    # 设置超参数\n    config={\n        \"learning_rate\": 0.02,\n        \"architecture\": \"CNN\",\n        \"dataset\": \"CIFAR-100\",\n        \"epochs\": 10,\n        \"batch_size\": 128\n    },\n    callbacks=[csv_writer]\n)\n\n# 模拟一次训练\nepochs = 10\noffset = random.random() / 5\nfor epoch in range(2, epochs):\n  acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n  loss = 2 ** -epoch + random.random() / epoch + offset\n\n  # 记录训练指标\n  swanlab.log({\"acc\": acc, \"loss2\": loss})\n\n# [可选] 完成训练，这在notebook环境中是必要的\nswanlab.finish()\n```",
    "708": "一级标题：CSV表格记录器\n二级标题：外部注册插件\n内容：\n<!--@include: ./shared-snippet.md-->",
    "709": "一级标题：文件记录器\n二级标题：无\n内容：\n如果你希望在训练开始时，指定一些文件复制到日志目录（run开头的目录下），那么非常推荐你使用`LogdirFileWriter`插件。\n\n:::warning 改进插件\nSwanLab插件均为开源代码，你可以在[Github源代码](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/plugin/writer.py#L175)中查看，欢迎提交你的建议和PR！\n:::",
    "710": "一级标题：文件记录器\n二级标题：插件用法\n内容：\n**1. 初始化LogdirFileWriter：**\n\n```python\nfrom swanlab.plugin.writer import LogdirFileWriter\n\nlogdirfile_writer = LogdirFileWriter(\n    sub_dir=\"code\",\n    files=[\n        \"config.yaml\",\n        \"README.md\",\n    ]\n)\n```\n\n- `sub_dir`参数如果不为None，则在run目录下创建1个sub_dir文件夹来保存文件\n- `files`参数指定了需要复制的文件列表（也支持仅传入1个str）。\n\n**2. 传入插件：**\n\n```python\nswanlab.init(\n    ...\n    callbacks=[logdirfile_writer]\n)\n```\n\n执行代码后，就会在`logdir`下对应的run开头目录下将`files`参数中的文件复制到该目录中（如果设置了`sub_dir`参数，则会复制到该子目录下）。",
    "711": "一级标题：文件记录器\n二级标题：示例代码\n内容：\n```python\nfrom swanlab.plugin.writer import LogdirFileWriter\nimport swanlab\n\nlogdirfile_writer = LogdirFileWriter(\n    sub_dir=\"code\",\n    file_path=[\"package.json\", \"README.md\"],\n)\n\nswanlab.init(project=\"test-plugin\", callbacks=[logdirfile_writer])\n\nswanlab.log({\"loss\": 0.2, \"acc\": 0.9})\nswanlab.finish()\n```\n\n![](./writer-filelogdir/paste.png)",
    "712": "一级标题：文件记录器\n二级标题：外部注册插件\n内容：\n<!--@include: ./shared-snippet.md-->",
    "713": "一级标题：定义数据\n二级标题：无\n内容：\nhours = [\"12a\", \"1a\", \"2a\", \"3a\", \"4a\", \"5a\", \"6a\", \"7a\", \"8a\", \"9a\", \"10a\", \"11a\", \"12p\", \"1p\", \"2p\", \"3p\", \"4p\", \"5p\", \"6p\", \"7p\", \"8p\", \"9p\", \"10p\", \"11p\"]\n\ndays = [\"Saturday\", \"Friday\", \"Thursday\", \"Wednesday\", \"Tuesday\", \"Monday\", \"Sunday\"]\n\ndata = [\n    [0, 0, 5], [0, 1, 1], [0, 2, 0], [0, 3, 0], [0, 4, 0], [0, 5, 0], [0, 6, 0], [0, 7, 0], [0, 8, 0], [0, 9, 0], [0, 10, 0], [0, 11, 2], [0, 12, 4], [0, 13, 1], [0, 14, 1], [0, 15, 3], [0, 16, 4], [0, 17, 6], [0, 18, 4], [0, 19, 4], [0, 20, 3], [0, 21, 3], [0, 22, 2], [0, 23, 5],\n    [1, 0, 7],[1, 1, 0],[1, 2, 0],[1, 3, 0],[1, 4, 0],[1, 5, 0],[1, 6, 0],[1, 7, 0],[1, 8, 0],[1, 9, 0],[1, 10, 5],[1, 11, 2],[1, 12, 2],[1, 13, 6],[1, 14, 9],[1, 15, 11],[1, 16, 6],[1, 17, 7],[1, 18, 8],[1, 19, 12],[1, 20, 5],[1, 21, 5],[1, 22, 7],[1, 23, 2],\n    [2, 0, 1],[2, 1, 1],[2, 2, 0],[2, 3, 0],[2, 4, 0],[2, 5, 0],[2, 6, 0],[2, 7, 0],[2, 8, 0],[2, 9, 0],[2, 10, 3],[2, 11, 2],[2, 12, 1],[2, 13, 9],[2, 14, 8],[2, 15, 10],[2, 16, 6],[2, 17, 5],[2, 18, 5],[2, 19, 5],[2, 20, 7],[2, 21, 4],[2, 22, 2],[2, 23, 4],\n    [3, 0, 7],[3, 1, 3],[3, 2, 0],[3, 3, 0],[3, 4, 0],[3, 5, 0],[3, 6, 0],[3, 7, 0],[3, 8, 1],[3, 9, 0],[3, 10, 5],[3, 11, 4],[3, 12, 7],[3, 13, 14],[3, 14, 13],[3, 15, 12],[3, 16, 9],[3, 17, 5],[3, 18, 5],[3, 19, 10],[3, 20, 6],[3, 21, 4],[3, 22, 4],[3, 23, 1],\n    [4, 0, 1],[4, 1, 3],[4, 2, 0],[4, 3, 0],[4, 4, 0],[4, 5, 1],[4, 6, 0],[4, 7, 0],[4, 8, 0],[4, 9, 2],[4, 10, 4],[4, 11, 4],[4, 12, 2],[4, 13, 4],[4, 14, 4],[4, 15, 14],[4, 16, 12],[4, 17, 1],[4, 18, 8],[4, 19, 5],[4, 20, 3],[4, 21, 7],[4, 22, 3],[4, 23, 0],\n    [5, 0, 2],[5, 1, 1],[5, 2, 0],[5, 3, 3],[5, 4, 0],[5, 5, 0],[5, 6, 0],[5, 7, 0],[5, 8, 2],[5, 9, 0],[5, 10, 4],[5, 11, 1],[5, 12, 5],[5, 13, 10],[5, 14, 5],[5, 15, 7],[5, 16, 11],[5, 17, 6],[5, 18, 0],[5, 19, 5],[5, 20, 3],[5, 21, 4],[5, 22, 2],[5, 23, 0],\n    [6, 0, 1],[6, 1, 0],[6, 2, 0],[6, 3, 0],[6, 4, 0],[6, 5, 0],[6, 6, 0],[6, 7, 0],[6, 8, 0],[6, 9, 0],[6, 10, 1],[6, 11, 0],[6, 12, 2],[6, 13, 1],[6, 14, 3],[6, 15, 4],[6, 16, 0],[6, 17, 0],[6, 18, 0],[6, 19, 0],[6, 20, 1],[6, 21, 2],[6, 22, 2],[6, 23, 6],\n]\ndata = [[d[1], d[0], d[2]] for d in data]\n\n# 创建echarts bar3d对象\nbar3d = swanlab.echarts.Bar3D()\n\n# 设置bar3d数据\nbar3d.add(\n    \"bar3d\",\n    data,\n    xaxis3d_opts=opts.Axis3DOpts(data=hours, type_=\"category\"),\n    yaxis3d_opts=opts.Axis3DOpts(data=days, type_=\"category\"),\n    zaxis3d_opts=opts.Axis3DOpts(data=data, type_=\"value\"),\n)\n\nbar3d.set_global_opts(\n        visualmap_opts=opts.VisualMapOpts(\n            max_=20,\n            range_color=[\n                \"#313695\",\n                \"#4575b4\",\n                \"#74add1\",\n                \"#abd9e9\",\n                \"#e0f3f8\",\n                \"#ffffbf\",\n                \"#fee090\",\n                \"#fdae61\",\n                \"#f46d43\",\n                \"#d73027\",\n                \"#a50026\",\n            ],\n        )\n    )\n\n# 记录到swanlab\nswanlab.log({\"bar3d\": bar3d})\n```",
    "714": "一级标题：定义数据\n二级标题：3D散点图 scatter3d\n内容：\n![scatter3d](/assets/py-echarts/scatter3d-1.png)\n\n```python\nimport asyncio\nfrom aiohttp import TCPConnector, ClientSession\nimport swanlab\nimport pyecharts.options as opts\n\nswanlab.init(project=\"swanlab-echarts-3d-demo\")\n\n# 定义数据\nasync def get_json_data(url: str) -> dict:\n    async with ClientSession(connector=TCPConnector(ssl=False)) as session:\n        async with session.get(url=url) as response:\n            return await response.json()\n\n\n# 获取echarts官方示例数据\ndata = asyncio.run(\n    get_json_data(\n        url=\"https://echarts.apache.org/examples/data/asset/data/nutrients.json\"\n    )\n)\n\n# 列名映射\nfield_indices = {\n    \"calcium\": 3,\n    \"calories\": 12,\n    \"carbohydrate\": 8,\n    \"fat\": 10,\n    \"fiber\": 5,\n    \"group\": 1,\n    \"id\": 16,\n    \"monounsat\": 14,\n    \"name\": 0,\n    \"polyunsat\": 15,\n    \"potassium\": 7,\n    \"protein\": 2,\n    \"saturated\": 13,\n    \"sodium\": 4,\n    \"sugars\": 9,\n    \"vitaminc\": 6,\n    \"water\": 11,\n}\n\n# 配置 config\nconfig_xAxis3D = \"protein\"\nconfig_yAxis3D = \"fiber\"\nconfig_zAxis3D = \"sodium\"\nconfig_color = \"fiber\"\nconfig_symbolSize = \"vitaminc\"\n\n# 构造数据\n\"\"\"\n数据结构为[[x, y, z, color, size, index]]\n例子：\n[[19.9, 0.4, 0.385, 0.4, 0.0769, 0],\n[35.8, 2, 0.717, 2, 0.138, 1],\n[23.5, 1.6, 0.78, 1.6, 0.0012, 2], ...]\n\"\"\"\ndata = [\n    [\n        item[field_indices[config_xAxis3D]],\n        item[field_indices[config_yAxis3D]],\n        item[field_indices[config_zAxis3D]],\n        item[field_indices[config_color]],\n        item[field_indices[config_symbolSize]],\n        index,\n    ]\n    for index, item in enumerate(data)\n]\n\n# 创建echarts scatter3d对象\nscatter3d = swanlab.echarts.Scatter3D()\n\n# 设置scatter3d数据\nscatter3d.add(\n    \"scatter3d\",\n    data,\n    xaxis3d_opts=opts.Axis3DOpts(name=config_xAxis3D, type_=\"value\"),\n    yaxis3d_opts=opts.Axis3DOpts(name=config_yAxis3D, type_=\"value\"),\n    zaxis3d_opts=opts.Axis3DOpts(name=config_zAxis3D, type_=\"value\"),\n    grid3d_opts=opts.Grid3DOpts(width=100, height=100, depth=100),\n)\nscatter3d.set_global_opts(\n        visualmap_opts=[\n            opts.VisualMapOpts(\n                type_=\"color\",\n                is_calculable=True,\n                dimension=3,\n                pos_top=\"10\",\n                max_=79 / 2,\n                range_color=[\n                    \"#1710c0\",\n                    \"#0b9df0\",\n                    \"#00fea8\",\n                    \"#00ff0d\",\n                    \"#f5f811\",\n                    \"#f09a09\",\n                    \"#fe0300\",\n                ],\n            ),\n            opts.VisualMapOpts(\n                type_=\"size\",\n                is_calculable=True,\n                dimension=4,\n                pos_bottom=\"10\",\n                max_=2.4 / 2,\n                range_size=[10, 40],\n            ),\n        ]\n    )\n\n# 记录到swanlab\nswanlab.log({\"scatter3d\": scatter3d})\n```",
    "715": "一级标题：定义数据\n二级标题：3D折线图 line3d\n内容：\n![line3d](/assets/py-echarts/line3d-1.png)\n\n```python\nimport math\nimport swanlab\nimport pyecharts.options as opts\nfrom pyecharts.faker import Faker\n\n\nswanlab.init(project=\"swanlab-echarts-3d-demo\")\n\n# 构造数据\ndata = []\nfor t in range(0, 25000):\n    _t = t / 1000\n    x = (1 + 0.25 * math.cos(75 * _t)) * math.cos(_t)\n    y = (1 + 0.25 * math.cos(75 * _t)) * math.sin(_t)\n    z = _t + 2.0 * math.sin(75 * _t)\n    data.append([x, y, z])\n\n\n# 创建echarts line3d对象\nline3d = swanlab.echarts.Line3D()\n\n# 设置line3d数据\nline3d.add(\n    \"line3d\",\n    data,\n    xaxis3d_opts=opts.Axis3DOpts(Faker.clock, type_=\"value\"),\n    yaxis3d_opts=opts.Axis3DOpts(Faker.week_en, type_=\"value\"),\n    grid3d_opts=opts.Grid3DOpts(width=100, depth=100),\n)\n\nline3d.set_global_opts(\n        visualmap_opts=opts.VisualMapOpts(\n            max_=30, min_=0, range_color=Faker.visual_color\n        ),\n    )\n\n# 记录到swanlab\nswanlab.log({\"line3d\": line3d})\n```",
    "716": "一级标题：定义数据\n二级标题：3D曲面图 3d_surface\n内容：\n![3d_surface](/assets/py-echarts/surface3d-1.png)\n\n```python\nimport math\nimport swanlab\nimport pyecharts.options as opts\nfrom typing import Union\n\nswanlab.init(project=\"swanlab-echarts-3d-demo\")\n\n# 构造数据\ndef float_range(start: int, end: int, step: Union[int, float], round_number: int = 2):\n    \"\"\"\n    浮点数 range\n    :param start: 起始值\n    :param end: 结束值\n    :param step: 步长\n    :param round_number: 精度\n    :return: 返回一个 list\n    \"\"\"\n    temp = []\n    while True:\n        if start < end:\n            temp.append(round(start, round_number))\n            start += step\n        else:\n            break\n    return temp\n\n\ndef surface3d_data():\n    for t0 in float_range(-3, 3, 0.05):\n        y = t0\n        for t1 in float_range(-3, 3, 0.05):\n            x = t1\n            z = math.sin(x**2 + y**2) * x / 3.14\n            yield [x, y, z]\n\n\n# 创建echarts surface3d对象\nsurface3d = swanlab.echarts.Surface3D()\n\n# 设置surface3d数据\nsurface3d.add(\n    \"surface3d\",\n    data=list(surface3d_data()),\n    xaxis3d_opts=opts.Axis3DOpts(type_=\"value\"),\n    yaxis3d_opts=opts.Axis3DOpts(type_=\"value\"),\n    grid3d_opts=opts.Grid3DOpts(width=100, height=40, depth=100),\n)\n\nsurface3d.set_global_opts(\n        visualmap_opts=opts.VisualMapOpts(\n            dimension=2,\n            max_=1,\n            min_=-1,\n            range_color=[\n                \"#313695\",\n                \"#4575b4\",\n                \"#74add1\",\n                \"#abd9e9\",\n                \"#e0f3f8\",\n                \"#ffffbf\",\n                \"#fee090\",\n                \"#fdae61\",\n                \"#f46d43\",\n                \"#d73027\",\n                \"#a50026\",\n            ],\n        )\n    )\n\n# 记录到swanlab\nswanlab.log({\"surface3d\": surface3d})\n```",
    "717": "一级标题：定义数据\n二级标题：无\n内容：\nweek_name_list = [\"周一\", \"周二\", \"周三\", \"周四\", \"周五\", \"周六\", \"周日\"]\nhigh_temperature = [11, 11, 15, 13, 12, 13, 10]\nlow_temperature = [1, -2, 2, 5, 3, 2, 0]\n\n# 创建echarts line对象\nline = swanlab.echarts.Line()\n\n# 设置line的轴\nline.add_xaxis(week_name_list)\n# 设置line的数据\nline.add_yaxis(\"high_temperature\", high_temperature)\nline.add_yaxis(\"low_temperature\", low_temperature)\n\n# 记录到swanlab\nswanlab.log({\"line\": line})\n```\n\n```python [样式-设置不透明度]\n\"\"\"\ndemo: \nhttps://swanlab.cn/@ZeyiLin/swanlab-echarts-demo/runs/trptzejp9037cimxd786e/chart#eDczbzM0-blJ6R3dXSFU=\n\"\"\"\n\nimport swanlab\nimport pyecharts.options as opts\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nweek_name_list = [\"周一\", \"周二\", \"周三\", \"周四\", \"周五\", \"周六\", \"周日\"]\nhigh_temperature = [11, 11, 15, 13, 12, 13, 10]\nlow_temperature = [1, -2, 2, 5, 3, 2, 0]\n\n# 创建echarts line对象\nline = swanlab.echarts.Line()\n# 设置line的轴\nline.add_xaxis(week_name_list)\n# 设置line的数据\nline.add_yaxis(\"high_temperature\", high_temperature)\nline.add_yaxis(\"low_temperature\", low_temperature)\n\n# 设置不透明度为0.5\nline.set_series_opts(areastyle_opts=opts.AreaStyleOpts(opacity=0.5))\n\n# 记录到swanlab\nswanlab.log({\"line_opacity\": line})\n```\n:::",
    "718": "一级标题：定义数据\n二级标题：柱状图 bar\n内容：\n![bar](/assets/py-echarts/bar-1.png)\n\n::: code-group\n\n```python [基础]\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nx = [\"a\", \"b\", \"c\"]\ny = [1, 2, 3]\n\n# 创建echarts bar对象\nbar = swanlab.echarts.Bar()\n\n# 设置x轴数据\nbar.add_xaxis(x)\n# 设置y轴数据\nbar.add_yaxis(\"value\", y)\n\n# 记录到swanlab\nswanlab.log({\"bar\": bar})\n```\n\n```python [水平方向]\n\"\"\"\ndemo:\nhttps://swanlab.cn/@ZeyiLin/swanlab-echarts-demo/runs/rtqyhofvc5080tpmdfxkz/chart#bGw5M2My-ZnRhOGRnWVE=\n\"\"\"\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nx = [\"a\", \"b\", \"c\"]\ny = [1, 2, 3]\n\n# 创建echarts bar对象\nbar = swanlab.echarts.Bar()\n\n# 设置x轴数据\nbar.add_xaxis(x)\n# 设置y轴数据\nbar.add_yaxis(\"value\", y)\n# 翻转\nbar.reversal_axis()\n\n# 记录到swanlab\nswanlab.log({\"bar_horizontal\": bar})\n```\n\n:::",
    "719": "一级标题：定义数据\n二级标题：饼状图 pie\n内容：\n![pie](/assets/py-echarts/pie-1.png)\n\n```python\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nx_data = [\"直接访问\", \"邮件营销\", \"联盟广告\", \"视频广告\", \"搜索引擎\"]\ny_data = [335, 310, 274, 235, 400]\n\n# 组合数据\ndata_pair = [list(z) for z in zip(x_data, y_data)]\ndata_pair.sort(key=lambda x: x[1])\n\n# 创建echarts pie对象\npie = swanlab.echarts.Pie()\n\n# 设置x轴数据并配置标签显示\npie.add(\n    \"访问来源\", \n    data_pair,\n    # 配置标签显示\n    label_opts={\n        \"formatter\": \"{b}: {d}%\",  # 显示百分比\n        \"position\": \"outside\"  # 标签位置\n    }\n)\n\n# 记录到swanlab\nswanlab.log({\"pie\": pie})\n```",
    "720": "一级标题：定义数据\n二级标题：热力图 heatmap\n内容：\n![heatmap](/assets/py-echarts/heatmap-1.png)\n\n:::code-group\n\n```python [基础]\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nhours = [\"12a\", \"1a\", \"2a\", \"3a\", \"4a\", \"5a\", \"6a\", \"7a\", \"8a\", \"9a\", \"10a\", \"11a\", \"12p\", \"1p\", \"2p\", \"3p\", \"4p\", \"5p\", \"6p\", \"7p\", \"8p\", \"9p\", \"10p\", \"11p\"]\n\ndays = [\"Saturday\", \"Friday\", \"Thursday\", \"Wednesday\", \"Tuesday\", \"Monday\", \"Sunday\"]\n\ndata = [\n    [0, 0, 5], [0, 1, 1], [0, 2, 0], [0, 3, 0], [0, 4, 0], [0, 5, 0], [0, 6, 0], [0, 7, 0], [0, 8, 0], [0, 9, 0], [0, 10, 0], [0, 11, 2], [0, 12, 4], [0, 13, 1], [0, 14, 1], [0, 15, 3], [0, 16, 4], [0, 17, 6], [0, 18, 4], [0, 19, 4], [0, 20, 3], [0, 21, 3], [0, 22, 2], [0, 23, 5],\n    [1, 0, 7],[1, 1, 0],[1, 2, 0],[1, 3, 0],[1, 4, 0],[1, 5, 0],[1, 6, 0],[1, 7, 0],[1, 8, 0],[1, 9, 0],[1, 10, 5],[1, 11, 2],[1, 12, 2],[1, 13, 6],[1, 14, 9],[1, 15, 11],[1, 16, 6],[1, 17, 7],[1, 18, 8],[1, 19, 12],[1, 20, 5],[1, 21, 5],[1, 22, 7],[1, 23, 2],\n    [2, 0, 1],[2, 1, 1],[2, 2, 0],[2, 3, 0],[2, 4, 0],[2, 5, 0],[2, 6, 0],[2, 7, 0],[2, 8, 0],[2, 9, 0],[2, 10, 3],[2, 11, 2],[2, 12, 1],[2, 13, 9],[2, 14, 8],[2, 15, 10],[2, 16, 6],[2, 17, 5],[2, 18, 5],[2, 19, 5],[2, 20, 7],[2, 21, 4],[2, 22, 2],[2, 23, 4],\n    [3, 0, 7],[3, 1, 3],[3, 2, 0],[3, 3, 0],[3, 4, 0],[3, 5, 0],[3, 6, 0],[3, 7, 0],[3, 8, 1],[3, 9, 0],[3, 10, 5],[3, 11, 4],[3, 12, 7],[3, 13, 14],[3, 14, 13],[3, 15, 12],[3, 16, 9],[3, 17, 5],[3, 18, 5],[3, 19, 10],[3, 20, 6],[3, 21, 4],[3, 22, 4],[3, 23, 1],\n    [4, 0, 1],[4, 1, 3],[4, 2, 0],[4, 3, 0],[4, 4, 0],[4, 5, 1],[4, 6, 0],[4, 7, 0],[4, 8, 0],[4, 9, 2],[4, 10, 4],[4, 11, 4],[4, 12, 2],[4, 13, 4],[4, 14, 4],[4, 15, 14],[4, 16, 12],[4, 17, 1],[4, 18, 8],[4, 19, 5],[4, 20, 3],[4, 21, 7],[4, 22, 3],[4, 23, 0],\n    [5, 0, 2],[5, 1, 1],[5, 2, 0],[5, 3, 3],[5, 4, 0],[5, 5, 0],[5, 6, 0],[5, 7, 0],[5, 8, 2],[5, 9, 0],[5, 10, 4],[5, 11, 1],[5, 12, 5],[5, 13, 10],[5, 14, 5],[5, 15, 7],[5, 16, 11],[5, 17, 6],[5, 18, 0],[5, 19, 5],[5, 20, 3],[5, 21, 4],[5, 22, 2],[5, 23, 0],\n    [6, 0, 1],[6, 1, 0],[6, 2, 0],[6, 3, 0],[6, 4, 0],[6, 5, 0],[6, 6, 0],[6, 7, 0],[6, 8, 0],[6, 9, 0],[6, 10, 1],[6, 11, 0],[6, 12, 2],[6, 13, 1],[6, 14, 3],[6, 15, 4],[6, 16, 0],[6, 17, 0],[6, 18, 0],[6, 19, 0],[6, 20, 1],[6, 21, 2],[6, 22, 2],[6, 23, 6],\n]\ndata = [[d[1], d[0], d[2] or \"-\"] for d in data]\n\n# 创建echarts heatmap对象\nheatmap = swanlab.echarts.HeatMap()\n\n# 设置x轴数据并配置标签显示\nheatmap.add_xaxis(hours)\nheatmap.add_yaxis(\n  \"Punch Card\", \n  days,\n  data,\n)\n\n# 记录到swanlab\nswanlab.log({\"heatmap\": heatmap})\n```\n\n```python [设置颜色映射范围]\n\"\"\"\ndemo:\nhttps://swanlab.cn/@ZeyiLin/swanlab-echarts-demo/runs/c1wm57rkfnwkyz7kaat8a/chart#OWJ5bWJl-c2M5bDFFc2I=\n\"\"\"\"\n\nimport swanlab\nfrom pyecharts import options as opts\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nhours = [\"12a\", \"1a\", \"2a\", \"3a\", \"4a\", \"5a\", \"6a\", \"7a\", \"8a\", \"9a\", \"10a\", \"11a\", \"12p\", \"1p\", \"2p\", \"3p\", \"4p\", \"5p\", \"6p\", \"7p\", \"8p\", \"9p\", \"10p\", \"11p\"]\n\ndays = [\"Saturday\", \"Friday\", \"Thursday\", \"Wednesday\", \"Tuesday\", \"Monday\", \"Sunday\"]\n\ndata = [\n    [0, 0, 5], [0, 1, 1], [0, 2, 0], [0, 3, 0], [0, 4, 0], [0, 5, 0], [0, 6, 0], [0, 7, 0], [0, 8, 0], [0, 9, 0], [0, 10, 0], [0, 11, 2], [0, 12, 4], [0, 13, 1], [0, 14, 1], [0, 15, 3], [0, 16, 4], [0, 17, 6], [0, 18, 4], [0, 19, 4], [0, 20, 3], [0, 21, 3], [0, 22, 2], [0, 23, 5],\n    [1, 0, 7],[1, 1, 0],[1, 2, 0],[1, 3, 0],[1, 4, 0],[1, 5, 0],[1, 6, 0],[1, 7, 0],[1, 8, 0],[1, 9, 0],[1, 10, 5],[1, 11, 2],[1, 12, 2],[1, 13, 6],[1, 14, 9],[1, 15, 11],[1, 16, 6],[1, 17, 7],[1, 18, 8],[1, 19, 12],[1, 20, 5],[1, 21, 5],[1, 22, 7],[1, 23, 2],\n    [2, 0, 1],[2, 1, 1],[2, 2, 0],[2, 3, 0],[2, 4, 0],[2, 5, 0],[2, 6, 0],[2, 7, 0],[2, 8, 0],[2, 9, 0],[2, 10, 3],[2, 11, 2],[2, 12, 1],[2, 13, 9],[2, 14, 8],[2, 15, 10],[2, 16, 6],[2, 17, 5],[2, 18, 5],[2, 19, 5],[2, 20, 7],[2, 21, 4],[2, 22, 2],[2, 23, 4],\n    [3, 0, 7],[3, 1, 3],[3, 2, 0],[3, 3, 0],[3, 4, 0],[3, 5, 0],[3, 6, 0],[3, 7, 0],[3, 8, 1],[3, 9, 0],[3, 10, 5],[3, 11, 4],[3, 12, 7],[3, 13, 14],[3, 14, 13],[3, 15, 12],[3, 16, 9],[3, 17, 5],[3, 18, 5],[3, 19, 10],[3, 20, 6],[3, 21, 4],[3, 22, 4],[3, 23, 1],\n    [4, 0, 1],[4, 1, 3],[4, 2, 0],[4, 3, 0],[4, 4, 0],[4, 5, 1],[4, 6, 0],[4, 7, 0],[4, 8, 0],[4, 9, 2],[4, 10, 4],[4, 11, 4],[4, 12, 2],[4, 13, 4],[4, 14, 4],[4, 15, 14],[4, 16, 12],[4, 17, 1],[4, 18, 8],[4, 19, 5],[4, 20, 3],[4, 21, 7],[4, 22, 3],[4, 23, 0],\n    [5, 0, 2],[5, 1, 1],[5, 2, 0],[5, 3, 3],[5, 4, 0],[5, 5, 0],[5, 6, 0],[5, 7, 0],[5, 8, 2],[5, 9, 0],[5, 10, 4],[5, 11, 1],[5, 12, 5],[5, 13, 10],[5, 14, 5],[5, 15, 7],[5, 16, 11],[5, 17, 6],[5, 18, 0],[5, 19, 5],[5, 20, 3],[5, 21, 4],[5, 22, 2],[5, 23, 0],\n    [6, 0, 1],[6, 1, 0],[6, 2, 0],[6, 3, 0],[6, 4, 0],[6, 5, 0],[6, 6, 0],[6, 7, 0],[6, 8, 0],[6, 9, 0],[6, 10, 1],[6, 11, 0],[6, 12, 2],[6, 13, 1],[6, 14, 3],[6, 15, 4],[6, 16, 0],[6, 17, 0],[6, 18, 0],[6, 19, 0],[6, 20, 1],[6, 21, 2],[6, 22, 2],[6, 23, 6],\n]\ndata = [[d[1], d[0], d[2] or \"-\"] for d in data]\n\n# 创建echarts heatmap对象\nheatmap = swanlab.echarts.HeatMap()\nheatmap.set_global_opts(\n    visualmap_opts=opts.VisualMapOpts(min_=0, max_=10, orient=\"horizontal\"),\n)\n\n# 设置x轴数据并配置标签显示\nheatmap.add_xaxis(hours)\nheatmap.add_yaxis(\n  \"Punch Card\", \n  days,\n  data,\n)\n\n# 记录到swanlab\nswanlab.log({\"heatmap_visualmapopts\": heatmap})\n```\n\n\n\n:::",
    "721": "一级标题：定义数据\n二级标题：散点图 scatter\n内容：\n![](/assets/py-echarts/scatter-1.png)\n\n```python\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\ndata = [\n    [10.0, 8.04],\n    [8.0, 6.95],\n    [13.0, 7.58],\n    [9.0, 8.81],\n    [11.0, 8.33],\n    [14.0, 9.96],\n    [6.0, 7.24],\n    [4.0, 4.26],\n    [12.0, 10.84],\n    [7.0, 4.82],\n    [5.0, 5.68],\n]\ndata.sort(key=lambda x: x[0])\nx_data = [d[0] for d in data]\ny_data = [d[1] for d in data]\n\n# 创建echarts scatter对象\nscatter = swanlab.echarts.Scatter()\n\n# 设置x轴数据并配置标签显示\nscatter.add_xaxis(x_data)\nscatter.add_yaxis(\n  \"\", \n  y_data,\n  symbol_size=20,\n)\n\n# 记录到swanlab\nswanlab.log({\"scatter\": scatter})\n```",
    "722": "一级标题：定义数据\n二级标题：雷达图 radar\n内容：\n![radar](/assets/py-echarts/radar-1.png)\n\n```python\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nv1 = [[4300, 10000, 28000, 35000, 50000, 19000]]\nv2 = [[5000, 14000, 28000, 31000, 42000, 21000]]\n\n# 创建echarts scatter对象\nradar = swanlab.echarts.Radar()\n\n# 设置雷达图维度与数据范围\nradar.add_schema(\n    schema=[\n        {\"name\": \"销售\", \"max\": 6500},\n        {\"name\": \"管理\", \"max\": 16000},\n        {\"name\": \"信息技术\", \"max\": 30000},\n        {\"name\": \"客服\", \"max\": 38000},\n        {\"name\": \"研发\", \"max\": 52000},\n        {\"name\": \"市场\", \"max\": 25000},\n    ]\n)\n\n# 添加数据1\nradar.add(\n    \"预算分配\",\n    v1,\n    color=\"#1f77b4\",\n)\n\n# 添加数据2\nradar.add(\n    \"实际开销\",\n    v2,\n    color=\"#ff7f0e\",\n)\n\n\n# 记录到swanlab\nswanlab.log({\"radar\": radar})\n```",
    "723": "一级标题：定义数据\n二级标题：箱线图 boxplot\n内容：\n![boxplot](/assets/py-echarts/boxplot-1.png)\n\n```python\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\ny_data = [\n    [850, 740, 900, 1070, 930, 850, 950, 980, 980, 880, 1000, 980, 930, 650, 760, 810, 1000, 1000, 960, 960, ],\n    [960, 940, 960, 940, 880, 800, 850, 880, 900, 840, 830, 790, 810, 880, 880, 830, 800, 790, 760, 800, ],\n    [880, 880, 880, 860, 720, 720, 620, 860, 970, 950, 880, 910, 850, 870, 840, 840, 850, 840, 840, 840, ],\n    [890, 810, 810, 820, 800, 770, 760, 740, 750, 760, 910, 920, 890, 860, 880, 720, 840, 850, 850, 780, ],\n    [890, 840, 780, 810, 760, 810, 790, 810, 820, 850, 870, 870, 810, 740, 810, 940, 950, 800, 810, 870, ],\n]\n\nscatter_data = [650, 620, 720, 720, 950, 970]\n# 创建echarts table对象\nboxplot = swanlab.echarts.Boxplot()\n\n# 设置表头\nboxplot.add_xaxis([\"expr 0\", \"expr 1\", \"expr 2\", \"expr 3\", \"expr 4\"])\nboxplot.add_yaxis(\"\", boxplot.prepare_data(y_data))\n\n# 记录到swanlab\nswanlab.log({\"boxplot\": boxplot})\n```",
    "724": "一级标题：定义数据\n二级标题：平行坐标系图 parallel\n内容：\n![parallel](/assets/py-echarts/parallel-1.png)\n\n```python\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nparallel_axis = [\n    {\"dim\": 0, \"name\": \"Price\"},\n    {\"dim\": 1, \"name\": \"Net Weight\"},\n    {\"dim\": 2, \"name\": \"Amount\"},\n    {\n        \"dim\": 3,\n        \"name\": \"Score\",\n        \"type\": \"category\",\n        \"data\": [\"Excellent\", \"Good\", \"OK\", \"Bad\"],\n    },\n]\n\ndata = [[12.99, 100, 82, \"Good\"], [9.99, 80, 77, \"OK\"], [20, 120, 60, \"Excellent\"]]\n\n# 创建echarts parallel对象\nparallel = swanlab.echarts.Parallel()\n\n# 设置parallel的轴\nparallel.add_schema(parallel_axis)\n# 设置parallel的数据\nparallel.add(\"data\", data=data)\n\n# 记录到swanlab\nswanlab.log({\"parallel\": parallel})\n```",
    "725": "一级标题：定义数据\n二级标题：仪表盘图 gauge\n内容：\n![gauge](/assets/py-echarts/gauge-1.png)\n\n```python\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 创建echarts gauge对象\ngauge = swanlab.echarts.Gauge()\ngauge.add(\"\", [(\"完成率\", 66.6)])\n\n# 记录到swanlab\nswanlab.log({\"gauge\": gauge})\n```",
    "726": "一级标题：定义数据\n二级标题：表格 table\n内容：\n![table](/assets/py-echarts/table-1.png)\n\n![](/assets/text-chart.gif)\n\n```python\nimport swanlab\n\nswanlab.init(\n    project=\"echarts-test\",\n)\n\n# 定义表头\nheaders = [\"NO\", \"Product\", \"Count\"]\n# 定义数据\nrows = [\n    [2, \"A\", 259],\n    [3, \"B\", 123],\n    [4, \"C\", 300],\n    [5, \"D\", 290],\n    [6, \"E\", 1145],\n]\n\n# 创建echarts table对象\ntable = swanlab.echarts.Table()\n\n# 添加数据\ntable.add(headers, rows)\n\n# 记录到swanlab\nswanlab.log({\"table\": table})\n```",
    "727": "一级标题：定义数据\n二级标题：树状图 tree\n内容：\n![tree](/assets/py-echarts/tree-1.png)\n\n```python\nimport swanlab\n\nswanlab.init(project=\"swanlab-echarts-demo\")\n\n# 构造数据\ndata = [\n    {\n        \"children\": [\n            {\"name\": \"B\"},\n            {\n                \"children\": [{\"children\": [{\"name\": \"I\"}], \"name\": \"E\"}, {\"name\": \"F\"}],\n                \"name\": \"C\",\n            },\n            {\n                \"children\": [\n                    {\"children\": [{\"name\": \"J\"}, {\"name\": \"K\"}], \"name\": \"G\"},\n                    {\"name\": \"H\"},\n                ],\n                \"name\": \"D\",\n            },\n        ],\n        \"name\": \"A\",\n    }\n]\n\n# 创建echarts tree对象\ntree = swanlab.echarts.Tree()\n\n# 设置tree数据\ntree.add(\"tree\", data=data)\n\n# 记录到swanlab\nswanlab.log({\"tree\": tree})\n```",
    "728": "一级标题：定义数据\n二级标题：桑基图 sankey\n内容：\n![sankey](/assets/py-echarts/sankey-1.png)\n\n```python\nimport swanlab\nfrom pyecharts import options as opts\n\nswanlab.init(project=\"swanlab-echarts-demo\")\n\n# 构造数据\nnodes = [\n    {\"name\": \"category1\"},\n    {\"name\": \"category2\"},\n    {\"name\": \"category3\"},\n    {\"name\": \"category4\"},\n    {\"name\": \"category5\"},\n    {\"name\": \"category6\"},\n]\n\nlinks = [\n    {\"source\": \"category1\", \"target\": \"category2\", \"value\": 10},\n    {\"source\": \"category2\", \"target\": \"category3\", \"value\": 15},\n    {\"source\": \"category3\", \"target\": \"category4\", \"value\": 20},\n    {\"source\": \"category5\", \"target\": \"category6\", \"value\": 25},\n]\n\n# 创建echarts sankey对象\nsankey = swanlab.echarts.Sankey()\n\n# 设置sankey数据\nsankey.add(\n    \"sankey\",\n    nodes=nodes,\n    links=links,\n    linestyle_opt=opts.LineStyleOpts(opacity=0.2, curve=0.5, color=\"source\"),\n    label_opts=opts.LabelOpts(position=\"right\"),\n)\n\n# 记录到swanlab\nswanlab.log({\"sankey\": sankey})\n```"
}