import os
import json
import sqlite3
from datetime import datetime
import requests
import numpy as np
import faiss
from collections import Counter
import re
from feishu_sync import sync_data_to_feishu
import time
#RAGæŸ¥è¯¢æ—¶é—´ï¼Œå“åº”æ—¶é—´ï¼ˆåˆ†ï¼‹æ€»ï¼‰ï¼Œå›å¤æ—¶é—´


class Chatbot:
    def __init__(self,
                 index_path="faiss_index_scratch_1",
                 url_map_path="swanlab_docs_Internet8-2.json",
                 db_path="user_questions1.db"):
        print("æ­£åœ¨åˆå§‹åŒ– Chatbot...")
        self.FAISS_INDEX_PATH = index_path
        self.URL_MAP_PATH = url_map_path
        self.LLM_API_KEY = "bcbe32ab-3117-4225-a751-c6aa24309de0"
        self.LLM_BASE_URL = "https://ark.cn-beijing.volces.com/api/v3"
        self.LLM_MODEL = "doubao-1-5-pro-32k-250115"
        self.EMB_API_KEY = "sk-ebaafxbagclzofzsruircunkmyizeytezzkkgdilzydpdxae"
        self.EMB_BASE_URL = "http://api.siliconflow.cn/v1"
        self.EMBEDDING_MODEL = "Qwen/Qwen3-Embedding-0.6B"
        self.VECTOR_DIMENSION = 1024

        self.db = self._init_database(db_path)
        self.url_map = self._load_url_map(self.URL_MAP_PATH)
        self.index, self.index_to_chunk = self._load_vector_store(self.FAISS_INDEX_PATH)
        self.VERSION = "V1.0"
        print("Chatbot åˆå§‹åŒ–å®Œæˆï¼")

    def _init_database(self, db_path):
        print(f"Initializing database at: {db_path}")
        conn = sqlite3.connect(db_path, check_same_thread=False)
        cursor = conn.cursor()
        try:
            # åˆ›å»ºç”¨æˆ·è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS users (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id TEXT UNIQUE NOT NULL,
                    enter_time TEXT NOT NULL,
                    exit_time TEXT,
                    question_count INTEGER DEFAULT 0
                )
            """)

            # åˆ›å»ºé—®é¢˜è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS questions (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id TEXT NOT NULL,
                    question TEXT NOT NULL,
                    answer TEXT NOT NULL,
                    feedback TEXT,
                    FOREIGN KEY (user_id) REFERENCES users(user_id)
                )
            """)

            # æ£€æŸ¥è¡¨æ˜¯å¦åˆ›å»ºæˆåŠŸ
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables = cursor.fetchall()
            print(f"Tables in database: {[table[0] for table in tables]}")

            conn.commit()
            print("Database initialized successfully")
            return conn
        except sqlite3.Error as e:
            print(f"Database initialization error: {e}")
            return None

    def _load_url_map(self, json_path):
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return {item['title']: item['html_url'] for item in data if 'title' in item and 'html_url' in item}
        except Exception as e:
            print(f"Error loading URL map: {e}")
            return {}

    def _load_vector_store(self, index_path):
        try:
            index_file = os.path.join(index_path, "index.faiss")
            map_file = os.path.join(index_path, "index_to_chunk.json")
            index = faiss.read_index(index_file)
            with open(map_file, 'r', encoding='utf-8') as f:
                index_to_chunk = json.load(f)
            print(f"å‘é‡æ•°æ®åº“åŠ è½½æˆåŠŸï¼ŒåŒ…å« {index.ntotal} ä¸ªå‘é‡ã€‚")
            return index, index_to_chunk
        except Exception as e:
            print(f"é”™è¯¯ï¼šæ— æ³•åŠ è½½å‘é‡æ•°æ®åº“ã€‚é”™è¯¯: {e}")
            return None, None

    def _get_query_embedding(self, text):
        headers = {"Authorization": f"Bearer {self.EMB_API_KEY}", "Content-Type": "application/json"}
        url = f"{self.EMB_BASE_URL.rstrip('/')}/embeddings"
        payload = {"model": self.EMBEDDING_MODEL, "input": [text]}
        response = requests.post(url, headers=headers, json=payload, timeout=10)
        response.raise_for_status()
        return response.json()['data'][0]['embedding']



    def get_user_enter_time(self, user_id):
        """æ ¹æ® user_id ä»æ•°æ®åº“è·å–ç”¨æˆ·çš„è¿›å…¥æ—¶é—´"""
        if not self.db:
            return None
        try:
            cursor = self.db.cursor()
            cursor.execute("SELECT enter_time FROM users WHERE user_id = ?", (user_id,))
            result = cursor.fetchone()
            return result[0] if result else None
        except sqlite3.Error as e:
            print(f"æŸ¥è¯¢ç”¨æˆ·è¿›å…¥æ—¶é—´å¤±è´¥: {e}")
            return None

    def create_user(self, user_id):
        """åˆ›å»ºæ–°ç”¨æˆ·è®°å½•"""
        print(f"Attempting to create user with ID: {user_id}")
        if not self.db:
            print("Database connection is None")
            return None
        try:
            cursor = self.db.cursor()
            enter_time = datetime.now().isoformat()
            print(f"Inserting user: {user_id}, enter_time: {enter_time}")

            # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å·²å­˜åœ¨
            cursor.execute("SELECT user_id FROM users WHERE user_id = ?", (user_id,))
            existing_user = cursor.fetchone()
            if existing_user:
                print(f"User {user_id} already exists")
                return user_id

            # æ’å…¥æ–°ç”¨æˆ·
            cursor.execute(
                "INSERT INTO users (user_id, enter_time) VALUES (?, ?)",
                (user_id, enter_time)
            )
            self.db.commit()

            # éªŒè¯ç”¨æˆ·æ˜¯å¦æˆåŠŸåˆ›å»º
            cursor.execute("SELECT user_id, enter_time FROM users WHERE user_id = ?", (user_id,))
            new_user = cursor.fetchone()
            if new_user:
                print(f"User {user_id} created successfully with enter_time: {new_user[1]}")
                return user_id
            else:
                print(f"Failed to create user {user_id}")
                return None
        except sqlite3.Error as e:
            print(f"Database error creating user: {e}")
            return None
        except Exception as e:
            print(f"Unexpected error creating user: {e}")
            return None

    def update_user_exit(self, user_id):
        """æ›´æ–°ç”¨æˆ·é€€å‡ºæ—¶é—´"""
        print(f"Attempting to update exit time for user: {user_id}")
        if not self.db:
            print("Database connection is None")
            return
        try:
            cursor = self.db.cursor()
            exit_time = datetime.now().isoformat()
            print(f"Updating user exit time: {user_id}, exit_time: {exit_time}")
            cursor.execute(
                "UPDATE users SET exit_time = ? WHERE user_id = ?",
                (exit_time, user_id)
            )
            self.db.commit()
            print(f"User {user_id} exit time updated successfully")
        except sqlite3.Error as e:
            print(f"Database error updating user exit: {e}")
        except Exception as e:
            print(f"Unexpected error updating user exit: {e}")

    def increment_question_count(self, user_id):
        """å¢åŠ ç”¨æˆ·é—®é¢˜è®¡æ•°"""
        if not self.db:
            return
        try:
            cursor = self.db.cursor()
            cursor.execute(
                "UPDATE users SET question_count = question_count + 1 WHERE user_id = ?",
                (user_id,)
            )
            self.db.commit()
        except sqlite3.Error as e:
            print(f"Database error incrementing question count: {e}")

    def save_question(self, user_id, question, answer):
        """ä¿å­˜é—®é¢˜è®°å½•"""
        if not self.db:
            return None
        try:
            cursor = self.db.cursor()
            cursor.execute(
                "INSERT INTO questions (user_id, question, answer) VALUES (?, ?, ?)",
                (user_id, question, answer)
            )
            self.db.commit()
            question_id = cursor.lastrowid
            self.increment_question_count(user_id)
            return question_id
        except sqlite3.Error as e:
            print(f"Database error saving question: {e}")
            return None

    def add_feedback(self, question_id, feedback):
        """ä¿å­˜ç”¨æˆ·åé¦ˆï¼Œå¹¶åªè§¦å‘é£ä¹¦åŒæ­¥"""
        if not self.db or not question_id:
            return "æ•°æ®åº“æœªè¿æ¥æˆ–é—®é¢˜IDä¸ºç©ºã€‚"
        if feedback not in ["correct", "incorrect"]:
            return "æ— æ•ˆçš„åé¦ˆã€‚"
        try:
            # æ›´æ–°æœ¬åœ°æ•°æ®åº“
            cursor = self.db.cursor()
            cursor.execute(
                "UPDATE questions SET feedback = ? WHERE id = ?",
                (feedback, question_id)
            )
            self.db.commit()
            print(f"æœ¬åœ°æ•°æ®åº“åé¦ˆå·²ä¿å­˜: question_id={question_id}, feedback={feedback}")

            # --- 3. ä¿®æ”¹ç‚¹ï¼šåªè°ƒç”¨åŒæ­¥å‡½æ•°æ¥æ›´æ–°åé¦ˆ ---
            feishu_data = {
                "question_id": question_id,
                "feedback": feedback
            }
            sync_data_to_feishu(feishu_data)
            # ------------------------------------------

            return f"æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼"
        except sqlite3.Error as e:
            print(f"Database error saving feedback: {e}")
            return "æ›´æ–°åé¦ˆå¤±è´¥ã€‚"

    @staticmethod
    def _extract_h1_title(text: str) -> str:
        prefix = "ä¸€çº§æ ‡é¢˜ï¼š"
        for line in text.split('\n'):
            if line.startswith(prefix):
                return line[len(prefix):].strip()
        return ""

    def _keyword_search(self, query: str, k: int = 10):
        keywords = [word for word in re.split(r'\W+', query.lower()) if word]
        if not keywords:
            return []

        all_chunks = list(self.index_to_chunk.values())
        chunk_scores = {}

        for chunk in all_chunks:
            score = 0
            chunk_lower = chunk.lower()
            for keyword in keywords:
                score += chunk_lower.count(keyword)

            if score > 0:
                chunk_scores[chunk] = score

        sorted_chunks = sorted(chunk_scores.items(), key=lambda item: item[1], reverse=True)
        return [chunk for chunk, score in sorted_chunks[:k]]

    def stream_chat(self, question, history, user_id):
        # --- 1. è®°å½•æµç¨‹å¼€å§‹æ—¶é—´ ---
        start_time = time.time()
        # ------------------------
        if not self.index or not self.index_to_chunk:
            yield "é”™è¯¯ï¼šå‘é‡æ•°æ®åº“æœªåŠ è½½ã€‚", None
            return

        # æ£€ç´¢é˜¶æ®µ
        query_embedding = self._get_query_embedding(question)
        query_vector = np.array([query_embedding]).astype('float32')
        distances, indices = self.index.search(query_vector, k=10)
        vector_retrieved_chunks = [self.index_to_chunk[str(i)] for i in indices[0]]

        keyword_retrieved_chunks = self._keyword_search(question, k=10)

        # åˆå¹¶ä¸å»é‡
        combined_chunks = {}
        all_retrieved = vector_retrieved_chunks + keyword_retrieved_chunks

        for chunk in all_retrieved:
            processed_chunk = chunk
            if isinstance(chunk, list):
                processed_chunk = "\n".join(chunk)

            if isinstance(processed_chunk, str) and processed_chunk:
                combined_chunks[processed_chunk] = None

        retrieved_chunks = list(combined_chunks.keys())

        # æ ‡é¢˜åˆ†æ
        all_h1_titles = [self._extract_h1_title(chunk) for chunk in retrieved_chunks if self._extract_h1_title(chunk)]
        title_counts = Counter(all_h1_titles)
        print(f"title_counts: {title_counts}")
        top_titles = [title for title, count in title_counts.items() if count >= 2]
        print(f"top_titles: {top_titles}")

        # --- 2. è®°å½•æ£€ç´¢å®Œæˆæ—¶é—´ ---
        rag_end_time = time.time()
        rag_duration = rag_end_time - start_time
        print(f"æ£€ç´¢é˜¶æ®µè€—æ—¶: {rag_duration:.2f} ç§’")
        # ------------------------

        # æ„å»ºPrompt
        context = "\n\n---\n\n".join(retrieved_chunks)
        history_prompt = "ğŸ‘‹ ä½ å¥½ï¼æˆ‘æ˜¯SwanLabæ–‡æ¡£çš„æ™ºèƒ½é—®ç­”åŠ©æ‰‹ã€‚ğŸ¤–è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼ŸğŸ˜Š".join([f"å†å²æé—®: {u}\nå†å²å›ç­”: {a}\n\n" for u, a in history])
        print(f"history_prompt:{history_prompt}")

        prompt = f"""
                # è§’è‰²
                ä½ æ˜¯ä¸€ä¸ª SwanLab å¼€æºé¡¹ç›®çš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ï¼Œä½ æ˜¯ä¸€ä¸ª SwanLab å¼€æºé¡¹ç›®çš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ï¼ŒSwanlabæ˜¯ç”±æƒ…æ„Ÿæœºå™¨ï¼ˆåŒ—äº¬ï¼‰ç§‘æŠ€æœ‰é™å…¬å¸æ¨å‡ºçš„äº§å“ã€‚
                #æŒ‡ä»¤
                1ï¼Œæ ¹æ®æä¾›çš„ [èƒŒæ™¯çŸ¥è¯†] å›ç­” [é—®é¢˜]ã€‚å¦‚æœæœªæ‰¾åˆ°ä»»ä½•ç›¸å…³çš„ï¼Œåˆ™æé†’ç”¨æˆ·æœªæ‰¾åˆ°å‚è€ƒèµ„æ–™ï¼Œæ ¹æ®ä½ ç°æœ‰çš„çŸ¥è¯†ç»™äºˆæ„è§ã€‚
                2.å›ç­”æ—¶ï¼Œå™è¿°é£æ ¼å¯ä»¥ä½¿ç”¨ä¸€äº›å›¾æ ‡ï¼Œä½¿å…¶æ›´äººæ€§åŒ–ã€‚
                4.å¦‚æœé‡åˆ°å›¾ç‰‡çš„ç›¸å¯¹åœ°å€ï¼Œå›ç­”çš„æ—¶å€™ä¸å‚è€ƒå›¾ç‰‡ã€‚
                5.å¦‚æœé‡åˆ°ä¸€äº›ç½‘é¡µåœ°å€å­˜åœ¨ä¸å¤Ÿå®Œæ•´ï¼Œè¯·åœ¨åœ°å€å‰é¢æ·»åŠ "https://docs.swanlab.cn/",å¹¶ä¸”æŠŠ".md"æ›¿æ¢ä¸º".html"ã€‚
                6.åŒæ—¶ä¹Ÿç»“åˆç”¨æˆ·å†å²æåˆ°çš„é—®é¢˜æ˜¯å¦ä¸å½“å‰é—®é¢˜æ˜¯å¦å­˜åœ¨ç›¸å…³æ€§ï¼Œç»“åˆèµ·æ¥å›ç­”ã€‚
                ---
                [å†å²é—®é¢˜]
                {history_prompt}
                ---
                [èƒŒæ™¯çŸ¥è¯†]
                {context}
                ---
                # æœ¬æ¬¡æé—®
                [é—®é¢˜]
                {question}
                ---
                ä½ çš„å›ç­”:
                """

        current_model_prompt = """
                # è§’è‰²
                ä½ æ˜¯ä¸€ä¸ª SwanLab å¼€æºé¡¹ç›®çš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ï¼Œä½ æ˜¯ä¸€ä¸ª SwanLab å¼€æºé¡¹ç›®çš„æ–‡æ¡£é—®ç­”åŠ©æ‰‹ï¼ŒSwanlabæ˜¯ç”±æƒ…æ„Ÿæœºå™¨ï¼ˆåŒ—äº¬ï¼‰ç§‘æŠ€æœ‰é™å…¬å¸æ¨å‡ºçš„äº§å“ã€‚
                #æŒ‡ä»¤
                1ï¼Œæ ¹æ®æä¾›çš„ [èƒŒæ™¯çŸ¥è¯†] å›ç­” [é—®é¢˜]ã€‚å¦‚æœæœªæ‰¾åˆ°ä»»ä½•ç›¸å…³çš„ï¼Œåˆ™æé†’ç”¨æˆ·æœªæ‰¾åˆ°å‚è€ƒèµ„æ–™ï¼Œæ ¹æ®ä½ ç°æœ‰çš„çŸ¥è¯†ç»™äºˆæ„è§ã€‚
                2.å›ç­”æ—¶ï¼Œå™è¿°é£æ ¼å¯ä»¥ä½¿ç”¨ä¸€äº›å›¾æ ‡ï¼Œä½¿å…¶æ›´äººæ€§åŒ–ã€‚
                3.å›ç­”å®Œæˆåï¼Œå¯ä»¥åé—®ç”¨æˆ·ä¸€äº›ç›¸å…³çš„é—®é¢˜ã€‚
                4.å¦‚æœé‡åˆ°å›¾ç‰‡çš„ç›¸å¯¹åœ°å€ï¼Œå›ç­”çš„æ—¶å€™ä¸å‚è€ƒå›¾ç‰‡ã€‚
                5.å¦‚æœé‡åˆ°ä¸€äº›ç½‘é¡µåœ°å€å­˜åœ¨ä¸å¤Ÿå®Œæ•´ï¼Œè¯·åœ¨åœ°å€å‰é¢æ·»åŠ "https://docs.swanlab.cn/",å¹¶ä¸”æŠŠ".md"æ›¿æ¢ä¸º".html"ã€‚
                6.åŒæ—¶ä¹Ÿç»“åˆç”¨æˆ·å†å²æåˆ°çš„é—®é¢˜æ˜¯å¦ä¸å½“å‰é—®é¢˜æ˜¯å¦å­˜åœ¨ç›¸å…³æ€§ï¼Œç»“åˆèµ·æ¥å›ç­”ã€‚
                ---
                [å†å²é—®é¢˜]
                {}
                ---
                [èƒŒæ™¯çŸ¥è¯†]
                {}
                ---
                # æœ¬æ¬¡æé—®
                [é—®é¢˜]
                {}
                ---
                ä½ çš„å›ç­”:
                """

        print(f'current_model_prompt:{current_model_prompt}')
        # å‘é€è¯·æ±‚
        headers = {"Authorization": f"Bearer {self.LLM_API_KEY}", "Content-Type": "application/json"}
        url = f"{self.LLM_BASE_URL.rstrip('/')}/chat/completions"
        payload = {"model": self.LLM_MODEL, "messages": [{"role": "user", "content": prompt}], "stream": True}

        # æµå¼å¤„ç†å“åº”
        full_answer = ""
        first_token_time = None  # åˆå§‹åŒ–é¦–ä¸ª token æ—¶é—´
        first_token_recorded = False  # æ·»åŠ ä¸€ä¸ªæ ‡å¿—ä½
        try:
            with requests.post(url, headers=headers, json=payload, stream=True, timeout=100) as response:
                response.raise_for_status()
                for line in response.iter_lines():
                    if line:
                        decoded_line = line.decode('utf-8')
                        if decoded_line.startswith("data: "):
                            json_str = decoded_line[len("data: "):]
                            if json_str.strip() == "[DONE]":
                                break
                            try:
                                data = json.loads(json_str)
                                if 'choices' in data and data['choices'][0].get('delta', {}).get('content'):

                                    # --- 3. è®°å½•é¦–ä¸ª Token æ—¶é—´ ---
                                    if not first_token_recorded:
                                        first_token_time = time.time()
                                        first_token_duration = first_token_time - rag_end_time
                                        print(f"LLM é¦–ä¸ª Token å“åº”è€—æ—¶: {first_token_duration:.2f} ç§’")
                                        first_token_recorded = True  # ç¡®ä¿åªè®°å½•ä¸€æ¬¡
                                    # ----------------------------

                                    token = data['choices'][0]['delta']['content']
                                    full_answer += token
                                    yield full_answer, None
                            except json.JSONDecodeError:
                                continue

            # --- 4. è®°å½•å›ç­”å®Œæˆæ—¶é—´ ---
            llm_end_time = time.time()
            # è®¡ç®—ä»æ”¶åˆ°é¦–ä¸ª token åˆ°å›ç­”ç»“æŸçš„è€—æ—¶
            llm_response_duration = llm_end_time - (first_token_time if first_token_time else rag_end_time)
            # è®¡ç®—æ•´ä¸ªæµç¨‹çš„æ€»è€—æ—¶
            total_duration = llm_end_time - start_time
            print(f"LLM è¾“å‡ºå®Œæˆè€—æ—¶: {llm_response_duration:.2f} ç§’")
            print(f"æ€»æµç¨‹è€—æ—¶: {total_duration:.2f} ç§’")
            # --------------------------

            # --- è¿™æ˜¯æ•°æ®è®°å½•åˆ°é£ä¹¦ä¿®æ”¹ç‚¹ ---
            # 1. ä¿å­˜åˆ°æœ¬åœ°æ•°æ®åº“å¹¶è·å– question_id
            question_id = self.save_question(user_id, question, full_answer)

            # 2. å‡†å¤‡åŒæ­¥åˆ°é£ä¹¦çš„å®Œæ•´æ•°æ®
            enter_time_str = self.get_user_enter_time(user_id)
            enter_time_timestamp = int(
                datetime.fromisoformat(enter_time_str).timestamp() * 1000) if enter_time_str else None

            feishu_data = {
                "question_id": question_id,
                "user_id": user_id,
                "enter_time": enter_time_timestamp,  # é£ä¹¦æ—¥æœŸéœ€è¦æ¯«ç§’çº§æ—¶é—´æˆ³
                "question": question,
                "retrieved_chunks_content": context,
                "retrieved_title_statistics": json.dumps(title_counts, ensure_ascii=False, indent=2),
                "answer": full_answer,

                ##æ–°å¢åˆ—è¡¨
                "history_prompt": history_prompt,
                "model_prompt": current_model_prompt,
                "embedding_model": self.EMBEDDING_MODEL,  #å¢åŠ 
                "llm_model": self.LLM_MODEL,
                "version": self.VERSION,
                "rag_duration": f"{rag_duration:.2f}",
                "first_token_duration": f"{first_token_duration:.2f}" if first_token_recorded else "N/A",
                "llm_response_duration": f"{llm_response_duration:.2f}",
                "total_duration": f"{total_duration:.2f}"
            }

            # 3. è°ƒç”¨åŒæ­¥å‡½æ•°
            sync_data_to_feishu(feishu_data)
            # --- ä¿®æ”¹ç»“æŸ ---

            # 4. æ ¼å¼åŒ–å¹¶æ·»åŠ å‚è€ƒèµ„æ–™
            source_documents = []
            for title in top_titles:
                html_url = self.url_map.get(title)
                print(f"title:{title}")
                print(f"url:{html_url}")
                source_documents.append({"title": title, "html_url": html_url})

            if source_documents:
                sources_md = "\n\n---\n\nğŸ“š **å‚è€ƒèµ„æ–™ï¼š**\n"
                for doc in source_documents:
                    if doc.get('html_url'):
                        sources_md += f"- [{doc['title']}]({doc['html_url']})\n"
                full_answer += sources_md
                yield full_answer, question_id

        except requests.exceptions.RequestException as e:
            yield f"API è¯·æ±‚å¤±è´¥: {e}", None