{
    "0": "一级标题：API文档\n二级标题：CLI\n内容：\n- [swanlab watch](/api/cli-swanlab-watch.md): 启动离线实验看板\n- [swanlab login](/api/cli-swanlab-login.md): 登录SwanLab\n- [swanlab logout](/api/cli-swanlab-logout.md): 登出SwanLab\n- [swanlab convert](/api/cli-swanlab-convert.md): 将外部日志转换为SwanLab项目",
    "1": "一级标题：API文档\n二级标题：Python SDK\n内容：\n- [init](/api/py-init.md)\n- [login](/api/py-login.md)\n- [Image](/api/py-Image.md)\n- [Audio](/api/py-Audio.md)\n- [Text](/api/py-Text.md)\n- [run](/api/py-run.md)\n- [convert](/api/py-converter.md)\n- [sync_wandb](/api/py-sync-wandb.md)\n- [sync_tensorboard](/api/py-sync-tensorboard.md)\n- [sync_mlflow](/api/py-sync-mlflow.md)\n- [register_callback](/api/py-register-callback.md)",
    "2": "一级标题：API文档\n二级标题：其他\n内容：\n- [环境变量](/api/environment-variable.md)",
    "3": "一级标题：swanlab convert\n二级标题：命令行选项\n内容：\n```bash\nswanlab convert [OPTIONS]\n```\n\n| 选项 | 描述 |\n| --- | --- |\n| `-t`, `--type` | 选择转换类型，可选`tensorboard`、`wandb`、`mlflow`，默认为`tensorboard`。 |\n| `-p`, `--project` | 设置转换创建的SwanLab项目名，默认为None。 |\n| `-w`, `--workspace` | 设置SwanLab项目所在空间，默认为None。 |\n| `-l`, `--logdir` | 设置SwanLab项目的日志文件保存路径，默认为None。 |\n| `--cloud` | 设置SwanLab项目是否将日志上传到云端，默认为True。 |\n| `--tb-logdir` | 需要转换的Tensorboard日志文件路径(tfevent) |\n| `--wb-project` | 需要转换的Wandb项目名 |\n| `--wb-entity` | 需要转换的Wandb项目所在实体 |\n| `--wb-runid` | 需要转换的Wandb Run的id |\n| `--mlflow-uri` | 需要转换的MLFlow项目URI |\n| `--mlflow-exp` | 需要转换的MLFlow实验ID |",
    "4": "一级标题：swanlab convert\n二级标题：介绍\n内容：\n将其他日志工具的内容转换为SwanLab项目。  \n支持转换的工具包括：`Tensorboard`、`Weights & Biases`、`MLFlow`。",
    "5": "一级标题：swanlab convert\n二级标题：使用案例\n内容：\n### Tensorboard\n\n[集成-Tensorboard](/guide_cloud/integration/integration-tensorboard.md)\n\n### Weights & Biases\n\n[集成-Weights & Biases](/guide_cloud/integration/integration-wandb.md)\n\n### MLFlow\n\n[集成-MLFlow](/guide_cloud/integration/integration-mlflow.md)",
    "6": "一级标题：swanlab login\n二级标题：命令格式\n内容：\n``` bash\nswanlab login [OPTIONS]\n```\n\n| 选项 | 描述 |\n| --- | --- |\n| `-r`, `--relogin` | 重新登录。|\n| `-h`, `--host` | 指定SwanLab服务所在的主机。比如`http://localhost:8000`。|\n| `-k`, `--api-key` | 指定API Key。如果您不喜欢使用命令行来输入 API 密钥，这将允许自动登录。|\n| `-w`, `--web-host` | 指定SwanLab前端所在的Web主机。|",
    "7": "一级标题：swanlab login\n二级标题：介绍\n内容：\n登录SwanLab账号，以同步实验到云端。\n\n执行下面的命令后，如果第一次登录，会让你填写[API_KEY](https://swanlab.cn/settings)：\n\n```bash\nswanlab login\n```\n\n登录过一次后，凭证会保存到本地，并覆盖之前登录过的凭证，无需再次通过`swanlab.login`或`swanlab login`登录。\n\n> 如果你不希望凭证保存在本地，请在python脚本中使用[swanlab.login()](./py-login.md)进行登录。\n\n如果你的电脑不太适合命令行粘贴API Key（比如一些Windows CMD）的方式登录，可以使用：\n\n```bash\nswanlab login -k <api-key>\n```",
    "8": "一级标题：swanlab login\n二级标题：重新登录\n内容：\n如果需要登录一个别的账号，则用下面的命令：\n\n```bash\nswanlab login --relogin\n```\n\n这会让你输入一个新的API Key以重新登录。",
    "9": "一级标题：swanlab login\n二级标题：退出登录\n内容：\n```bash\nswanlab logout\n```",
    "10": "一级标题：swanlab login\n二级标题：登录到私有化服务\n内容：\n```bash\nswanlab login --host <host>\n```",
    "11": "一级标题：swanlab logout\n二级标题：命令介绍\n内容：\n```bash\nswanlab logout\n```\n\n二级标题：功能描述\n内容：\n在编程环境上退出账号。",
    "12": "一级标题：其他CLI命令\n二级标题：查看SwanLab库版本\n内容：\n- `swanlab -v`：查看SwanLab库版本",
    "13": "一级标题：其他CLI命令\n二级标题：API帮助\n内容：\n- `swanlab --help`：API帮助",
    "14": "一级标题：swanlab sync\n二级标题：介绍\n内容：\n将本地日志，同步到SwanLab云端/私有化部署端。",
    "15": "一级标题：swanlab sync\n二级标题：命令行示例\n内容：\n找到你需要上传到云端的日志文件目录（默认是`swanlog`下的以`run-`开头的目录），然后执行命令：\n\n```bash\nswanlab sync ./swanlog/run-xxx\n```\n\n::: info\n默认同步到的项目的是日志文件中记录的`project`，即跑该实验时设置的`project`。  \n如果想要同步到其他项目，可以使用`-p`选项指定项目。\n:::\n\n看到下面的打印信息，则表示同步成功：\n\n![swanlab sync](./cli-swanlab-sync/console.png)",
    "16": "一级标题：swanlab sync\n二级标题：Python代码示例\n内容：\n```python\nimport swanlab\n\nswanlab.login(api_key=\"你的API Key\")\n\nswanlab.sync(\n    dir_path=\"./swanlog/run-xxx\",\n    workspace=\"swanlab\",\n    project_name=\"sync_test\",\n)\n```",
    "17": "一级标题：swanlab sync\n二级标题：批量上传\n内容：\n```bash\nswanlab sync ./swanlog/run-*\n```",
    "18": "一级标题：swanlab sync\n二级标题：选项\n内容：\n```bash\nswanlab sync [options] [logdir]\n```\n\n| 选项 | 描述 |\n| --- | --- |\n| `-k`, `--api-key` | 用于身份验证的API密钥。如果未指定，将使用环境中的默认API密钥。如果指定，将使用此API密钥登录但不会保存密钥。|\n| `-h`, `--host` | 同步日志的主机地址。如果未指定，将使用默认主机(`https://swanlab.cn`)。|\n| `-w`, `--workspace` | 同步日志的工作空间。如果未指定，将使用默认工作空间。|\n| `-p`, `--project` | 同步日志的项目。如果未指定，将使用默认项目。|",
    "19": "一级标题：swanlab watch\n二级标题：选项\n内容：\n``` bash\nswanlab watch [OPTIONS]\n```\n\n| 选项 | 描述 | 例子 |\n| --- | --- | --- |\n| `-p`, `--port` | 设置实验看板Web服务运行的端口，默认为**5092**。 | `swanlab watch -p 8080`：将实验看板Web服务设置为8080端口 |\n| `-h`, `--host` | 设置实验看板Web服务运行的IP地址，默认为**127.0.0.1**。 | `swanlab watch -h 0.0.0.0`：将实验看板Web服务的IP地址设置为0.0.0.0 |\n| `-l`, `--logdir` | 设置实验看板Web服务读取的日志文件路径，默认为`swanlog`。 | `swanlab watch --logdir ./logs`：将当前目录下的logs文件夹设置为日志文件读取路径 |\n| `--help` | 查看终端帮助信息。 | `swanlab watch --help` |",
    "20": "一级标题：swanlab watch\n二级标题：介绍\n内容：\n本地启动SwanLab[离线看板](/zh/guide_cloud/self_host/offline-board.md)。  \n在创建SwanLab实验时（并设置mode=\"local\"），会在本地目录下创建一个日志文件夹（默认名称为`swanlog`），使用`swanlab watch`可以本地离线打开实验看板，查看指标图表和配置。",
    "21": "一级标题：swanlab watch\n二级标题：使用案例\n内容：\n### 打开SwanLab离线看板\n\n首先，我们找到日志文件夹（默认名称为`swanlog`），然后在命令行执行下面的命令：\n\n```bash\nswanlab watch -l [logfile_path]\n```\n\n其中`logfile_path`是日志文件夹的路径，可以是绝对路径或相对路径。如果你的日志文件夹名称是默认的`swanlog`，那么也可以直接用`swanlab watch`启动而无需`-l`选项。\n\n执行命令后，会看到下面的输出：\n```bash{6}\nswanlab watch -l [logfile_path]\n\n*swanlab: Try to explore the swanlab experiment logs in: [logfile_path]\n*swanlab: SwanLab Experiment Dashboard ready in 465ms\n\n        ➜  Local:   http://127.0.0.1:5092\n```\n\n访问提供的URL，即可访问SwanLab离线看板。\n\n### 设置IP和端口号\n\n我们可以通过`-h`参数设置IP，`-p`参数设置端口号。  \n比如我们希望能够在本地访问云服务器上的离线看板，那么需要在云服务器上开启实验看板时，设置IP为0.0.0.0：\n\n```bash\nswanlab watch -h 0.0.0.0\n```\n\n如果需要设置端口的话：\n```bash\nswanlab watch -h 0.0.0.0 -p 8080\n```",
    "22": "一级标题：环境变量\n二级标题：全局配置\n内容：\n| 环境变量 | 描述 | 默认值 |\n| --- | --- | --- |\n| `SWANLAB_SAVE_DIR` | SwanLab 全局文件夹保存的路径 | 用户主目录下的 `.swanlab` 文件夹 |\n| `SWANLAB_LOG_DIR` | SwanLab 解析日志文件保存的路径 | 当前运行目录的 `swanlog` 文件夹 |\n| `SWANLAB_MODE` | SwanLab 的解析模式，涉及操作员注册的回调。目前有三种模式：`local`、`cloud`、`disabled`。**注意：大小写敏感** | `cloud` |",
    "23": "一级标题：环境变量\n二级标题：服务配置\n内容：\n| 环境变量 | 描述 | \n| --- | --- |\n| `SWANLAB_BOARD_PORT` | CLI 离线看板 `swanboard` 服务的端口 |\n| `SWANLAB_BOARD_HOST` | CLI 离线看板 `swanboard` 服务的地址 |\n| `SWANLAB_WEB_HOST` | SwanLab 云端环境的 Web 地址 |\n| `SWANLAB_API_HOST` | SwanLab 云端环境的 API 地址 |",
    "24": "一级标题：环境变量\n二级标题：实验配置\n内容：\n| 环境变量 | 描述 |\n| --- | --- |\n| `SWANLAB_PROJ_NAME` | 项目名称，效果等价于 `swanlab.init(project_name=\"...\")` |\n| `SWANLAB_WORKSPACE` | 工作空间名称，效果等价于 `swanlab.init(workspace=\"...\")` |\n| `SWANLAB_EXP_NAME` | 实验名称，效果等价于 `swanlab.init(experiment_name=\"...\")` |\n| `SWANLAB_RUN_ID` | 实验运行ID，效果等价于 `swanlab.init(id=\"...\")` |\n| `SWANLAB_RESUME` | 是否断点续训，效果等价于 `swanlab.init(resume=...)`，可选值为 `True`、`False`、`must`、`allow`、`never` |",
    "25": "一级标题：环境变量\n二级标题：登录认证\n内容：\n| 环境变量 | 描述 |\n| --- | --- | \n| `SWANLAB_API_KEY` | 云端 API Key。登录时会首先查找此环境变量，如果不存在，判断用户是否已登录，未登录则进入登录流程。<br>- 如果 `login` 接口传入字符串，此环境变量无效<br>- 如果用户已登录，此环境变量的优先级高于本地存储的登录信息 |",
    "26": "一级标题：环境变量\n二级标题：其他\n内容：\n| 环境变量 | 描述 |\n| --- | --- |\n| `SWANLAB_WEBHOOK` | Webhook 地址。<br> SwanLab 初始化完毕时，如果此环境变量存在，会调用此地址发送消息 |",
    "27": "一级标题：swanlab.Audio\n二级标题：介绍\n内容：\n对各种类型的音频数据做转换，以被`swanlab.log()`记录。\n\n![](/assets/media-audio-1.jpg)",
    "28": "一级标题：swanlab.Audio\n二级标题：从numpy array创建\n内容：\n记录单个音频：\n\n```python\nimport numpy as np\nimport swanlab\n\nrun = swanlab.init()\n\n# 创建一个numpy array类型的音频\nwhite_noise = np.random.randn(2, 100000)\n# 传入swanlab.Audio，设置采样率\naudio = swanlab.Audio(white_noise, caption=\"white_noise\")\n\nrun.log({\"examples\": audio})\n```\n\n记录多个音频：\n\n```python\nimport numpy as np\nimport swanlab\n\nrun = swanlab.init()\n\n# 创建一个列表\nexamples = []\nfor i in range(3):\n    white_noise = np.random.randn(100000)\n    audio = swanlab.Audio(white_noise, caption=\"audio_{i}\")\n    # 列表中添加swanlab.Audio类型对象\n    examples.append(audio)\n\nrun.log({\"examples\": examples})\n```",
    "29": "一级标题：swanlab.Audio\n二级标题：从文件路径创建\n内容：\n```python\nimport swanlab\n\nrun = swanlab.init()\naudio = swanlab.Audio(\"path/to/file\")\n\nrun.log({\"examples\": audio})\n```",
    "30": "一级标题：swanlab.Audio\n二级标题：参数说明\n内容：\n```python\nAudio(\n    data_or_path: Union[str, np.ndarray],\n    sample_rate: int = 44100,\n    caption: str = None,\n) -> None\n```\n\n| 参数          | 描述                                                                                                     |\n|-------------|--------------------------------------------------------------------------------------------------------|\n| data_or_path | (Union[str, np.ndarray]) 接收音频文件路径、numpy数组。Audio类将判断接收的数据类型做相应的转换。 |\n| sample_rate | (int) 音频的采样率，默认为44100。                                             |\n| caption     | (str) 音频的标签。用于在实验看板中展示音频时进行标记。                                                      |",
    "31": "一级标题：swanlab.converter\n二级标题：将其他日志工具的内容转换为SwanLab项目的API\n内容：\n- [swanlab.converter.TFBConverter](/guide_cloud/integration/integration-tensorboard)\n- [swanlab.converter.WandbConverter](/guide_cloud/integration/integration-wandb)\n- [swanlab.converter.MLFlowConverter](/guide_cloud/integration/integration-mlflow)",
    "32": "一级标题：swanlab.Echarts\n二级标题：折线图 line\n内容：\n![line](./py-echarts/line-1.png)\n\n```python\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nweek_name_list = [\"周一\", \"周二\", \"周三\", \"周四\", \"周五\", \"周六\", \"周日\"]\nhigh_temperature = [11, 11, 15, 13, 12, 13, 10]\nlow_temperature = [1, -2, 2, 5, 3, 2, 0]\n\n# 创建echarts line对象\nline = swanlab.echarts.Line()\n\n# 设置line的轴\nline.add_xaxis(week_name_list)\n# 设置line的数据\nline.add_yaxis(\"high_temperature\", high_temperature)\nline.add_yaxis(\"low_temperature\", low_temperature)\n\n# 记录到swanlab\nswanlab.log({\"line\": line})\n```",
    "33": "一级标题：swanlab.Echarts\n二级标题：柱状图 bar\n内容：\n![bar](./py-echarts/bar-1.png)\n\n```python\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nx = [\"a\", \"b\", \"c\"]\ny = [1, 2, 3]\n\n# 创建echarts bar对象\nbar = swanlab.echarts.Bar()\n\n# 设置x轴数据\nbar.add_xaxis(x)\n# 设置y轴数据\nbar.add_yaxis(\"value\", y)\n\n# 记录到swanlab\nswanlab.log({\"bar\": bar})\n```",
    "34": "一级标题：swanlab.Echarts\n二级标题：饼状图 pie\n内容：\n![pie](./py-echarts/pie-1.png)\n\n```python\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nx_data = [\"直接访问\", \"邮件营销\", \"联盟广告\", \"视频广告\", \"搜索引擎\"]\ny_data = [335, 310, 274, 235, 400]\n\n# 组合数据\ndata_pair = [list(z) for z in zip(x_data, y_data)]\ndata_pair.sort(key=lambda x: x[1])\n\n# 创建echarts pie对象\npie = swanlab.echarts.Pie()\n\n# 设置x轴数据并配置标签显示\npie.add(\n    \"访问来源\", \n    data_pair,\n    # 配置标签显示\n    label_opts={\n        \"formatter\": \"{b}: {d}%\",  # 显示百分比\n        \"position\": \"outside\"  # 标签位置\n    }\n)\n\n# 记录到swanlab\nswanlab.log({\"pie\": pie})\n```",
    "35": "一级标题：swanlab.Echarts\n二级标题：热力图 heatmap\n内容：\n![heatmap](./py-echarts/heatmap-1.png)\n\n```python\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nhours = [\"12a\", \"1a\", \"2a\", \"3a\", \"4a\", \"5a\", \"6a\", \"7a\", \"8a\", \"9a\", \"10a\", \"11a\", \"12p\", \"1p\", \"2p\", \"3p\", \"4p\", \"5p\", \"6p\", \"7p\", \"8p\", \"9p\", \"10p\", \"11p\"]\n\ndays = [\"Saturday\", \"Friday\", \"Thursday\", \"Wednesday\", \"Tuesday\", \"Monday\", \"Sunday\"]\n\ndata = [\n    [0, 0, 5], [0, 1, 1], [0, 2, 0], [0, 3, 0], [0, 4, 0], [0, 5, 0], [0, 6, 0], [0, 7, 0], [0, 8, 0], [0, 9, 0], [0, 10, 0], [0, 11, 2], [0, 12, 4], [0, 13, 1], [0, 14, 1], [0, 15, 3], [0, 16, 4], [0, 17, 6], [0, 18, 4], [0, 19, 4], [0, 20, 3], [0, 21, 3], [0, 22, 2], [0, 23, 5],\n    [1, 0, 7],[1, 1, 0],[1, 2, 0],[1, 3, 0],[1, 4, 0],[1, 5, 0],[1, 6, 0],[1, 7, 0],[1, 8, 0],[1, 9, 0],[1, 10, 5],[1, 11, 2],[1, 12, 2],[1, 13, 6],[1, 14, 9],[1, 15, 11],[1, 16, 6],[1, 17, 7],[1, 18, 8],[1, 19, 12],[1, 20, 5],[1, 21, 5],[1, 22, 7],[1, 23, 2],\n    [2, 0, 1],[2, 1, 1],[2, 2, 0],[2, 3, 0],[2, 4, 0],[2, 5, 0],[2, 6, 0],[2, 7, 0],[2, 8, 0],[2, 9, 0],[2, 10, 3],[2, 11, 2],[2, 12, 1],[2, 13, 9],[2, 14, 8],[2, 15, 10],[2, 16, 6],[2, 17, 5],[2, 18, 5],[2, 19, 5],[2, 20, 7],[2, 21, 4],[2, 22, 2],[2, 23, 4],\n    [3, 0, 7],[3, 1, 3],[3, 2, 0],[3, 3, 0],[3, 4, 0],[3, 5, 0],[3, 6, 0],[3, 7, 0],[3, 8, 1],[3, 9, 0],[3, 10, 5],[3, 11, 4],[3, 12, 7],[3, 13, 14],[3, 14, 13],[3, 15, 12],[3, 16, 9],[3, 17, 5],[3, 18, 5],[3, 19, 10],[3, 20, 6],[3, 21, 4],[3, 22, 4],[3, 23, 1],\n    [4, 0, 1],[4, 1, 3],[4, 2, 0],[4, 3, 0],[4, 4, 0],[4, 5, 1],[4, 6, 0],[4, 7, 0],[4, 8, 0],[4, 9, 2],[4, 10, 4],[4, 11, 4],[4, 12, 2],[4, 13, 4],[4, 14, 4],[4, 15, 14],[4, 16, 12],[4, 17, 1],[4, 18, 8],[4, 19, 5],[4, 20, 3],[4, 21, 7],[4, 22, 3],[4, 23, 0],\n    [5, 0, 2],[5, 1, 1],[5, 2, 0],[5, 3, 3],[5, 4, 0],[5, 5, 0],[5, 6, 0],[5, 7, 0],[5, 8, 2],[5, 9, 0],[5, 10, 4],[5, 11, 1],[5, 12, 5],[5, 13, 10],[5, 14, 5],[5, 15, 7],[5, 16, 11],[5, 17, 6],[5, 18, 0],[5, 19, 5],[5, 20, 3],[5, 21, 4],[5, 22, 2],[5, 23, 0],\n    [6, 0, 1],[6, 1, 0],[6, 2, 0],[6, 3, 0],[6, 4, 0],[6, 5, 0],[6, 6, 0],[6, 7, 0],[6, 8, 0],[6, 9, 0],[6, 10, 1],[6, 11, 0],[6, 12, 2],[6, 13, 1],[6, 14, 3],[6, 15, 4],[6, 16, 0],[6, 17, 0],[6, 18, 0],[6, 19, 0],[6, 20, 1],[6, 21, 2],[6, 22, 2],[6, 23, 6],\n]\ndata = [[d[1], d[0], d[2] or \"-\"] for d in data]\n\n# 创建echarts heatmap对象\nheatmap = swanlab.echarts.HeatMap()\n\n# 设置x轴数据并配置标签显示\nheatmap.add_xaxis(hours)\nheatmap.add_yaxis(\n  \"Punch Card\", \n  days,\n  data,\n)\n\n# 记录到swanlab\nswanlab.log({\"heatmap\": heatmap})\n```",
    "36": "一级标题：swanlab.Echarts\n二级标题：散点图 scatter\n内容：\n![](./py-echarts/scatter-1.png)\n\n```python\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\ndata = [\n    [10.0, 8.04],\n    [8.0, 6.95],\n    [13.0, 7.58],\n    [9.0, 8.81],\n    [11.0, 8.33],\n    [14.0, 9.96],\n    [6.0, 7.24],\n    [4.0, 4.26],\n    [12.0, 10.84],\n    [7.0, 4.82],\n    [5.0, 5.68],\n]\ndata.sort(key=lambda x: x[0])\nx_data = [d[0] for d in data]\ny_data = [d[1] for d in data]\n\n# 创建echarts scatter对象\nscatter = swanlab.echarts.Scatter()\n\n# 设置x轴数据并配置标签显示\nscatter.add_xaxis(x_data)\nscatter.add_yaxis(\n  \"\", \n  y_data,\n  symbol_size=20,\n)\n\n# 记录到swanlab\nswanlab.log({\"scatter\": scatter})\n```",
    "37": "一级标题：swanlab.Echarts\n二级标题：雷达图 radar\n内容：\n![radar](./py-echarts/radar-1.png)\n\n```python\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nv1 = [[4300, 10000, 28000, 35000, 50000, 19000]]\nv2 = [[5000, 14000, 28000, 31000, 42000, 21000]]\n\n# 创建echarts scatter对象\nradar = swanlab.echarts.Radar()\n\n# 设置雷达图维度与数据范围\nradar.add_schema(\n    schema=[\n        {\"name\": \"销售\", \"max\": 6500},\n        {\"name\": \"管理\", \"max\": 16000},\n        {\"name\": \"信息技术\", \"max\": 30000},\n        {\"name\": \"客服\", \"max\": 38000},\n        {\"name\": \"研发\", \"max\": 52000},\n        {\"name\": \"市场\", \"max\": 25000},\n    ]\n)\n\n# 添加数据1\nradar.add(\n    \"预算分配\",\n    v1,\n    color=\"#1f77b4\",\n)\n\n# 添加数据2\nradar.add(\n    \"实际开销\",\n    v2,\n    color=\"#ff7f0e\",\n)\n\n\n# 记录到swanlab\nswanlab.log({\"radar\": radar})\n```",
    "38": "一级标题：swanlab.Echarts\n二级标题：箱线图 boxplot\n内容：\n![boxplot](./py-echarts/boxplot-1.png)\n\n```python\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\ny_data = [\n    [850, 740, 900, 1070, 930, 850, 950, 980, 980, 880, 1000, 980, 930, 650, 760, 810, 1000, 1000, 960, 960, ],\n    [960, 940, 960, 940, 880, 800, 850, 880, 900, 840, 830, 790, 810, 880, 880, 830, 800, 790, 760, 800, ],\n    [880, 880, 880, 860, 720, 720, 620, 860, 970, 950, 880, 910, 850, 870, 840, 840, 850, 840, 840, 840, ],\n    [890, 810, 810, 820, 800, 770, 760, 740, 750, 760, 910, 920, 890, 860, 880, 720, 840, 850, 850, 780, ],\n    [890, 840, 780, 810, 760, 810, 790, 810, 820, 850, 870, 870, 810, 740, 810, 940, 950, 800, 810, 870, ],\n]\n\nscatter_data = [650, 620, 720, 720, 950, 970]\n# 创建echarts table对象\nboxplot = swanlab.echarts.Boxplot()\n\n# 设置表头\nboxplot.add_xaxis([\"expr 0\", \"expr 1\", \"expr 2\", \"expr 3\", \"expr 4\"])\nboxplot.add_yaxis(\"\", boxplot.prepare_data(y_data))\n\n# 记录到swanlab\nswanlab.log({\"boxplot\": boxplot})\n```",
    "39": "一级标题：swanlab.Echarts\n二级标题：平行坐标系图 parallel\n内容：\n![parallel](./py-echarts/parallel-1.png)\n\n```python\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nparallel_axis = [\n    {\"dim\": 0, \"name\": \"Price\"},\n    {\"dim\": 1, \"name\": \"Net Weight\"},\n    {\"dim\": 2, \"name\": \"Amount\"},\n    {\n        \"dim\": 3,\n        \"name\": \"Score\",\n        \"type\": \"category\",\n        \"data\": [\"Excellent\", \"Good\", \"OK\", \"Bad\"],\n    },\n]\n\ndata = [[12.99, 100, 82, \"Good\"], [9.99, 80, 77, \"OK\"], [20, 120, 60, \"Excellent\"]]\n\n# 创建echarts parallel对象\nparallel = swanlab.echarts.Parallel()\n\n# 设置parallel的轴\nparallel.add_schema(parallel_axis)\n# 设置parallel的数据\nparallel.add(\"data\", data=data)\n\n# 记录到swanlab\nswanlab.log({\"parallel\": parallel})\n```",
    "40": "一级标题：swanlab.Echarts\n二级标题：仪表盘图 gauge\n内容：\n![gauge](./py-echarts/gauge-1.png)\n\n```python\nimport swanlab\n\nswanlab.init(project=\"swanlab-echarts-demo\")\n\n# 创建echarts gauge对象\ngauge =\n\n根据提供的Markdown内容，按主题分块整理如下：\n\n```markdown\n一级标题：swanlab.Image\n二级标题：介绍\n内容：\n对各种类型的图像数据做转换，以被`swanlab.log()`记录。\n\n![](/assets/media-image-1.jpg)\n\n二级标题：从numpy array创建\n内容：\n记录单张图像：\n\n```python\nimport numpy as np\nimport swanlab\n\nrun = swanlab.init()\n\n# 1. 创建一个numpy array\nrandom_image = np.random.randint(low=0, high=256, size=(100, 100, 3), dtype=np.uint8)\n# 2. 传入swanlab.Image\nimage = swanlab.Image(random_image, caption=\"random image\")\n\nrun.log({\"examples\": image})\n```\n\n记录多张图像：\n\n```python\nimport numpy as np\nimport swanlab\n\nrun = swanlab.init()\n\n# 创建一个列表\nexamples = []\nfor i in range(3):\n    random_image = np.random.randint(low=0, high=256, size=(100, 100, 3), dtype=np.uint8)\n    image = swanlab.Image(random_image, caption=\"random image\")\n    # 列表中添加swanlab.Image类型对象\n    examples.append(image)\n\n# 记录图列\nrun.log({\"examples\": examples})\n```\n\n二级标题：从PyTorch Tensor创建\n内容：\n`swanlab.Image`支持传入尺寸为[B, C, H, W]与[C, H, W]的Tensor。\n\n```python\nimport torch\nimport swanlab\n\nrun = swanlab.init()\n···\nfor batch, ground_truth in train_dataloader():\n    # 假设batch是尺寸为[16, 3, 256, 256]的tensor\n    tensors = swanlab.Image(batch)\n    run.log({\"examples\": tensors})\n```\n\n二级标题：从PIL Image创建\n内容：\n```python\nimport numpy as np\nfrom PIL import Image\nimport swanlab\n\nrun = swanlab.init()\n\n# 创建一个列表\nexamples = []\nfor i in range(3):\n    random_image = np.random.randint(low=0, high=256, size=(100, 100, 3), dtype=np.uint8)\n    pil_image = Image.fromarray(random_image)\n    image = swanlab.Image(pil_image, caption=\"random image\")\n    examples.append(image)\n\nrun.log({\"examples\": examples})\n```\n\n二级标题：从文件路径创建\n内容：\n```python\nimport swanlab\n\nrun = swanlab.init()\nimage = swanlab.Image(\"path/to/file\", caption=\"random image\")\n\nrun.log({\"examples\": image})\n```\n\n`swanlab.Image`在默认情况下，是以`png`的格式做图像转换与存储。\n\n如果想要用`jpg`格式：\n\n```python{3}\nimage = swanlab.Image(\"path/to/file\",\n                      caption=\"random image\",\n                      file_type=\"jpg\")\n```\n\n二级标题：对传入图像做Resize\n内容：\n在默认情况，`swanlab.Image`不对图像做任何尺寸缩放。  \n\n如果需要放缩图像，我们可以通过设置`size`参数，来调节图像尺寸。\n\n放缩规则为：  \n\n1. 默认: 不对图像做任何缩放\n\n2. `size`为int类型: 如果最长边超过`size`, 则将最长边设为`size`, 另一边等比例缩放; 否则不缩放\n\n3. `size`为list/tuple类型: \n\n    - (int, int): 将图像缩放到宽为size[0], 高为size[1]\n    - (int, None): 将图像缩放到宽为size[0], 高等比例缩放\n    - (None, int): 将缩放缩放到高为size[1], 宽等比例缩放\n\n```python\nprint(im_array.shape)\n# [1024, 512, 3]\n\nim1 = swanlab.Image(im_array, size=512)\n# [512, 256, 3]\n\nim2 = swanlab.Image(im_array, size=(512, 512))\n# [512, 512, 3]\n\nim3 = swanlab.Image(im_array, size=(None, 1024))\n# [2048, 1024, 3]\n\nim4 = swanlab.Image(im_array, size=(256, None))\n# [256, 128, 3]\n```\n\n二级标题：记录Matplotlib图表\n内容：\n```python\nimport swanlab\nimport matplotlib.pyplot as plt\n\n# 定义横纵坐标的数据\nx = [1, 2, 3, 4, 5]\ny = [2, 3, 5, 7, 11]\n\n# plt创建折线图\nplt.plot(x, y)\n\n# 添加标题和标签\nplt.title(\"Examples\")\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")\n\nswanlab.init()\n\n# 记录plt\nswanlab.log({\"example\": swanlab.Image(plt)})\n```\n```",
    "41": "一级标题：swanlab.init\n二级标题：参数介绍\n内容：\n```python\ninit(\n    project: str = None,\n    workspace: str = None,\n    experiment_name: str = None,\n    description: str = None,\n    tags: List[str] = None,\n    config: Union[dict, str] = None,\n    logdir: str = None,\n    mode: str = \"cloud\",\n    load: str = None,\n    public: bool = None,\n    callbacks: list = None,\n    settings: Settings = None,\n    id: str = None,\n    resume: Union[Literal['must', 'allow', 'never'], bool] = None,\n    reinit: bool = None,\n    **kwargs,\n)\n```\n\n| 参数         | 描述 |\n|-------------|------|\n| project |(str)项目名，如果不指定则取运行目录的名称。|\n| workspace |(str)工作空间，默认将实验同步到你的个人空间下，如果要上传到组织，则填写组织的username。|\n| experiment_name | (str) 实验名称, 如果不指定则取\"swan-1\"这样的`动物名+序号`作为实验名。 |\n| tags       | (list) 实验标签。可以传入多个字符串组成的列表，标签会显示在实验顶部的标签栏。|\n| description   | (str) 实验描述, 如果不指定默认为None。                                   |\n| config       | (dict, str) 实验配置，在此处可以记录一些实验的超参数等信息。支持传入配置文件路径，支持yaml和json文件。                   |\n| logdir       | (str) 离线看板日志文件存储路径，默认为`swanlog `。                                 |\n| mode       | (str) 设置swanlab实验创建的模式，可选\"cloud\"、\"local\"、\"offline\"、\"disabled\"，默认设置为\"cloud\"。<br>`cloud`：将实验上传到云端。（公有云和私有化部署）<br>`offline`：仅将实验数据保存到本地。<br>`local`：不上传到云端，但会记录实验数据和一些可被`swanlab watch`打开的数据到本地。<br>`disabled`：不上传也不记录。|\n| load       | (str) 加载的配置文件路径，支持yaml和json文件。|\n| public       | (bool) 设置使用代码直接创建SwanLab项目的可见性，默认为False即私有。|\n| callbacks       | (list) 设置实验回调函数，支持`swankit.callback.SwanKitCallback`的子类。|\n| name       | (str) 与experiment_name效果一致，优先级低于experiment_name。|\n| notes       | (str) 与description效果一致，优先级低于description。|\n| settings       | (dict) 实验配置。支持传入1个`swanlab.Settings`对象。|\n| id       | (str) 上次实验的运行ID，用于恢复上次实验。ID必须为21位字符串。|\n| resume       | (str) 端点续训模式，可选True、False、\"must\"、\"allow\"、\"never\"，默认取None。<br>`True`： 效果同`resume=\"allow\"`。<br>`False`：效果同`resume=\"never\"`。<br>`must`：你必须传递 `id` 参数，并且实验必须存在。<br>`allow`：如果存在实验，则会resume该实验，否则将创建新的实验。<br>`never`：你不能传递 `id` 参数，将会创建一个新的实验。(即不开启resume的效果)|\n| reinit       | (bool) 是否重新创建实验，如果为True，则每次调用`swanlab.init()`时，会把上一次实验`finish`掉；默认取None。|",
    "42": "一级标题：swanlab.init\n二级标题：介绍\n内容：\n- 在机器学习训练流程中，我们可以将`swandb.init()`添加到训练脚本和测试脚本的开头，SwanLab将跟踪机器学习流程的每个环节。\n\n- `swanlab.init()`会生成一个新的后台进程来将数据记录到实验中，默认情况下，它还会将数据同步到swanlab.cn，以便你可以在线实时看到可视化结果。\n\n- 在使用`swanlab.log()`记录数据之前，需要先调用`swanlab.init()`：\n\n```python\nimport swanlab\n\nswanlab.init()\nswanlab.log({\"loss\": 0.1846})\n```\n\n- 调用`swanlab.init()`会返回一个`SwanLabRun`类型的对象，同样可以执行`log`操作：\n\n```python\nimport swanlab\n\nrun = swanlab.init()\nrun.log({\"loss\": 0.1846})\n```\n\n- 在脚本运行结束时，我们将自动调用`swanlab.finish`来结束SwanLab实验。但是，如果从子进程调用`swanlab.init()`，如在jupyter notebook中，则必须在子进程结束时显式调用`swanlab.finish`。\n\n```python\nimport swanlab\n\nswanlab.init()\nswanlab.finish()\n```",
    "43": "一级标题：swanlab.init\n二级标题：更多用法\n内容：\n### 设置项目、实验名、描述\n\n```python\nswanlab.init(\n    project=\"cats-detection\",\n    experiment_name=\"YoloX-baseline\",\n    description=\"YoloX检测模型的基线实验，主要用于后续对比。\",\n)\n```\n\n### 设置标签\n\n```python\nswanlab.init(\n    tags=[\"yolo\", \"detection\", \"baseline\"]\n)\n```\n\n### 设置日志文件保存位置\n\n> 仅在mode=\"local\"时有效\n\n下面的代码展示了如何将日志文件保存到自定义的目录下：\n\n```python\nswanlab.init(\n    logdir=\"path/to/my_custom_dir\",\n    mode=\"local\",\n)\n```\n\n### 将实验相关的元数据添加到实验配置中\n\n```python\nswanlab.init(\n    config={\n        \"learning-rate\": 1e-4,\n        \"model\": \"CNN\",\n    }\n)\n\n```\n\n### 上传到组织\n\n```python\nswanlab.init(\n    workspace=\"[组织的username]\"\n)\n```\n\n### 插件\n\n关于插件的更多信息，请参考[插件](/zh/plugin/plugin-index.md)。\n\n```python\nfrom swanlab.plugin.notification import EmailCallback\n\nemail_callback = EmailCallback(...)\n\nswanlab.init(\n    callbacks=[email_callback]\n)\n```\n\n### 断点续训\n\n断点续训的意思是，如果你之前有一个状态为`完成`或`中断`的实验，需要补一些实验数据，那么你可以通过`resume`和`id`参数来恢复这个实验。\n\n```python\nswanlab.init(\n    resume=True,\n    id=\"14pk4qbyav4toobziszli\",  # id必须为21位字符串\n)\n```\n\n实验id可以在实验的「环境」选项卡或URL中找到，必须为1个21位字符串。\n\n:::tip resume使用场景\n\n1. 之前的训练进程断了，基于checkpoint继续训练时，希望实验图表能和之前的swanlab实验续上，而非创建1个新swanlab实验\n2. 训练和评估分为了两个进程，但希望评估和训练记录在同一个swanlab实验中\n3. config中有一些参数填写有误，希望更新config参数\n\n:::\n\n:::warning ⚠️注意\n\n1. 由项目克隆产生的实验，不能被resume\n\n:::\n\n断点续训可以选择三种模式：\n\n1. `allow`：如果项目下存在`id`对应的实验，则会resume该实验，否则将创建新的实验。\n2. `must`：如果项目下存在`id`对应的实验，则会resume该实验，否则将报错\n3. `never`：不能传递 `id` 参数，将会创建一个新的实验。(即不开启resume的效果)\n\n::: info\n`resume=True` 效果同 `resume=\"allow\"`。<br>\n`resume=False` 效果同 `resume=\"never\"`。\n:::\n\n测试代码：\n\n```python\nimport swanlab\n\nrun = swanlab.init()\nswanlab.log({\"loss\": 2, \"acc\":0.4})\nrun.finish()\n\nrun = swanlab.init(resume=True, id=run.id)\nswanlab.log({\"loss\": 0.2, \"acc\": 0.9})\n```",
    "44": "一级标题：swanlab.init\n二级标题：过期参数\n内容：\n- `cloud`：在v0.3.4被`mode`参数取代。参数仍然可用，且会覆盖掉`mode`的设置。",
    "45": "一级标题：swanlab.integration\n二级标题：SwanLab与外部项目的集成API\n内容：\nSwanLab与外部项目的集成API。\n\n- [swanlab.integration.accelerate](/guide_cloud/integration/integration-huggingface-accelerate.md)\n- [swanlab.integration.fastai](/guide_cloud/integration/integration-fastai.md)\n- [swanlab.integration.keras](/guide_cloud/integration/integration-keras.md)\n- [swanlab.integration.lightgbm](/guide_cloud/integration/integration-lightgbm.md)\n- [swanlab.integration.mmengine](/guide_cloud/integration/integration-mmengine.md)\n- [swanlab.integration.pytorch_lightning](/guide_cloud/integration/integration-pytorch-lightning.md)\n- [swanlab.integration.sb3](/guide_cloud/integration/integration-sb3.md)\n- [swanlab.integration.torchtune](/guide_cloud/integration/integration-pytorch-torchtune.md)\n- [swanlab.integration.transformers](/guide_cloud/integration/integration-huggingface-transformers.md)\n- [swanlab.integration.ultralytics](/guide_cloud/integration/integration-ultralytics.md)\n- [swanlab.integration.xgboost](/guide_cloud/integration/integration-xgboost.md)",
    "46": "一级标题：log\n二级标题：参数说明\n内容：\n```python\nlog(\n    data: Dict[str, DataType],\n    step: int = None,\n    print_to_console: bool = False,\n)\n```\n\n| 参数   | 描述                                       |\n|--------|------------------------------------------|\n| data   | (Dict[str, DataType]) 必须。传入一个键值对字典，key为指标名，value为指标值。value支持int、float、可被float()转换的类型、或任何`BaseType`类型。 |\n| step   | (int) 可选，该参数设置了data的步数。如不设置step，则将以0开始，后续每1次step累加1。 |\n| print_to_console | (bool) 可选，默认值为False。当设置为True时，会将data的key和value以字典的形式打印到终端。 |",
    "47": "一级标题：log\n二级标题：介绍\n内容：\n`swanlab.log`是指标记录的核心API，使用它记录实验中的数据，例如标量、图像、音频和文本。  \n\n最基本的用法是如下面代码所示，这将会将准确率与损失值记录到实验中，生成可视化图表并更新这些指标的汇总值（summary）。：\n\n```python\nswanlab.log({\"acc\": 0.9, \"loss\":0.1462})\n```\n\n除了标量以外，`swanlab.log`支持记录多媒体数据，包括图像、音频、文本等，并在UI上有很好的显示效果。",
    "48": "一级标题：log\n二级标题：打印传入的字典\n内容：\n`swanlab.log`支持打印传入的`data`的`key`和`value`到终端，默认情况下不打印。要开启打印的话，需要设置`print_to_console=True`。\n\n```python\nswanlab.log({\"acc\": 0.9, \"loss\":0.1462}, print_to_console=True)\n```\n\n当然，你也可以用这种方式打印：\n\n```python\nprint(swanlab.log({\"acc\": 0.9, \"loss\":0.1462}))\n```",
    "49": "一级标题：log\n二级标题：更多用法\n内容：\n- 记录[图像](/api/py-Image.md)\n- 记录[音频](/api/py-Audio.md)\n- 记录[文本](/api/py-Text.md)",
    "50": "一级标题：swanlab.login\n二级标题：函数定义\n内容：\n``` bash\nlogin(\n    api_key: str = None,\n    host: str = None,\n    web_host: str = None,\n    save: bool = False\n):\n```\n\n| 参数 | 描述 |\n| --- | --- |\n| `api_key` | (str) 身份验证密钥，如果未提供，密钥将从密钥文件中读取。|\n| `host` | (str) SwanLab服务所在的API主机，如果未提供，将使用默认主机（即云端版）|\n| `web_host` | (str) SwanLab服务所在的Web主机，如果未提供，将使用默认主机（即云端版）|\n| `save` | (bool) 是否将API密钥保存到密钥文件中，默认值为False。|",
    "51": "一级标题：swanlab.login\n二级标题：介绍\n内容：\n在Python代码中登录SwanLab账号，以将实验上传到指定的云端服务器。API Key从你的SwanLab「设置」-「常规」页面中获取。",
    "52": "一级标题：swanlab.login\n二级标题：登录到公有云\n内容：\n```python\nimport swanlab\n\nswanlab.login(api_key='your-api-key', save=True)\n```\n\n默认将登录到`swanlab.cn`，即SwanLab公有云服务。\n\n如果需要登录到其他主机，可以指定`host`参数，如`http://localhost:8000`。\n\n将`save`参数设置为`True`，会将登录凭证保存到本地（会覆盖之前保存的凭证），无需再次通过`swanlab.login`或`swanlab login`登录。\n\n**如果你在公共机器上使用，请将`save`参数设置为`False`**，这样不会泄露你的API Key，也避免其他人不小心上传数据到你的空间。",
    "53": "一级标题：swanlab.login\n二级标题：登录到私有化服务\n内容：\n```python\nswanlab.login(api_key='your-api-key', host='your-private-host')\n```",
    "54": "一级标题：swanlab.Molecule\n二级标题：参数\n内容：\n| 参数        | 描述       |\n|-----------|------------------------------------------------------------------------------------------------|\n| pdb_data | (str) 接收的PDB数据（字符串形式）                                 |      \n| caption   | (str) 分子对象的标签。用于在实验看板中展示分子对象时进行标记。                |",
    "55": "一级标题：swanlab.Molecule\n二级标题：简介\n内容：\n对各种类型的生物化学分子做转换，以被`swanlab.log()`记录。\n\n![molecule gif](/assets/molecule.gif)",
    "56": "一级标题：swanlab.Molecule\n二级标题：从RDKit Mol对象创建\n内容：\n```python\nfrom rdkit import Chem\nimport swanlab\n\nmol = Chem.MolFromSmiles(\"CCO\")\nmolecule = swanlab.Molecule.from_mol(mol, caption=\"Ethanol\")\n\nswanlab.init(project=\"molecule_demo\")\nswanlab.log({\"molecule\": molecule})\n```",
    "57": "一级标题：swanlab.Molecule\n二级标题：从PDB文件创建\n内容：\n```python\nimport swanlab\n\nmolecule = swanlab.Molecule.from_pdb(\"path/to/your/pdb/file.pdb\")\n\nswanlab.init(project=\"molecule_demo\")\nswanlab.log({\"molecule\": molecule})\n```",
    "58": "一级标题：swanlab.Molecule\n二级标题：从SDF文件创建\n内容：\n```python\nimport swanlab\n\nmolecule = swanlab.Molecule.from_sdf(\"path/to/your/sdf/file.sdf\")\n\nswanlab.init(project=\"molecule_demo\")\nswanlab.log({\"molecule\": molecule})\n```",
    "59": "一级标题：swanlab.Molecule\n二级标题：从SMILES字符串创建\n内容：\n```python\nimport swanlab\n\nmolecule = swanlab.Molecule.from_smiles(\"CCO\")\n\nswanlab.init(project=\"molecule_demo\")\nswanlab.log({\"molecule\": molecule})\n```",
    "60": "一级标题：swanlab.Molecule\n二级标题：从MOL文件创建\n内容：\n```python\nimport swanlab\n\nmolecule = swanlab.Molecule.from_mol(\"path/to/your/mol/file.mol\")\n\nswanlab.init(project=\"molecule_demo\")\nswanlab.log({\"molecule\": molecule})\n```\n\n### 一级标题：swanlab.Object3D\n\n#### 二级标题：参数\n\n内容：\n| 参数        | 描述   |\n|-----------|---------------|\n| data | (Union[np.ndarray, str, Path]) 接收点云文件路径、numpy数组。Object3D类将判断接收的数据类型做相应的转换。                                      |              |\n| caption   | (str) 3D对象的标签。用于在实验看板中展示3D对象时进行标记。                                                                                                                 |\n\n#### 二级标题：介绍\n\n内容：\n对各种类型的点云数据做转换，以被`swanlab.log()`记录。\n\n![](./py-object3d/demo.png)\n\n#### 二级标题：从文件/字典创建\n\n内容：\n::: warning 示例文件\ndata.swanlab.pts.json：[Google Drive下载](https://drive.google.com/file/d/1mFill-BXw3cirPHwIHndb1wNX4pWvSXb/view)\n:::\n\n文件的格式为`json`，内容格式如下：\n\n```json\n{\n    \"points\": [\n        [x1, y1, z1, r1, g1, b1],\n        [x2, y2, z2, r2, g2, b2],\n        ...\n    ],\n    // （可选）检测框，用于点云检测等任务，会框住对应位置\n    \"boxes\": [\n        {\n            \"color\": [r, g, b],\n            \"corners\": [[x1,y1,z1], ..., [x8,y8,z8]],\n            // （可选）检测框的标签文本，会在视图中显示\n            \"label\": \"class_name\",\n            // （可选）置信度，会在视图中显示\n            \"score\": 0.95,\n        },\n        ...\n    ]\n}\n```\n\n**json文件参数详细解释：**\n\n* **`points`**：\n    * 这是一个数组，用于存储3D点云数据。\n    * 每个元素都是一个包含6个数值的数组 `[x, y, z, r, g, b]`，分别代表：\n        * `x`, `y`, `z`：点的三维坐标。\n        * `r`, `g`, `b`：点的颜色，分别代表红、绿、蓝三个通道的数值，通常取值范围为0-255。\n\n* **`boxes`**（可选）：\n    * 这是一个数组，用于存储3D检测框数据。\n    * 每个元素都是一个对象，代表一个检测框，包含以下字段：\n        * **`color`**：检测框的颜色，`[r, g, b]` 数组，代表红、绿、蓝三个通道的数值。\n        * **`corners`**：检测框的八个顶点坐标，`[[x1, y1, z1], ..., [x8, y8, z8]]` 数组，每个元素是一个三维坐标 `[x, y, z]`。\n        * **`label`**（可选）：检测框的标签文本，字符串类型，用于在视图中显示检测框的类别。\n        * **`score`**（可选）：检测框的置信度，数值类型，通常取值范围为0-1，用于表示检测框的可靠程度。\n\n---\n\n使用SwanLab从`json`文件中记录3D点云数据：\n\n::: code-group\n\n```python [Object3D]\nimport swanlab\n\nswanlab.init()\n\nobj = swanlab.Object3D(\"data.swanlab.pts.json\", caption=\"3d_point_cloud\")\nswanlab.log({\"examples\": obj})\n```\n\n```python [Object3D.from_point_data]\nimport swanlab\n\nswanlab.init()\n\nwith open(\"data.swanlab.pts.json\", \"r\") as f:\n    cloud_point = json.load(f)\n\nobj = swanlab.Object3D.from_point_data(\n    points=cloud_point[\"points\"],\n    boxes=cloud_point[\"boxes\"],\n    caption=\"3d_point_cloud\"\n)\n\nswanlab.log({\"examples\": obj})\n```\n:::\n\n\n<video controls src=\"./py-object3d/video.mp4\"></video>\n\n<br>\n\n#### 二级标题：从numpy数组创建\n\n内容：\n::: code-group\n\n```python [从坐标创建]\nimport numpy as np\n\n# Example 1: Create point cloud from coordinates\npoints_xyz = np.array([\n    [0, 0, 0],  # Point1: x=0, y=0, z=0\n    [1, 1, 1],  # Point2: x=1, y=1, z=1\n    [2, 0, 1]   # Point3: x=2, y=0, z=1\n])\n\ncloud_xyz = swanlab.Object3D(points_xyz, caption=\"Basic XYZ Points\")\nswanlab.log({\"examples\": cloud_xyz})\n```\n\n```python [从坐标和类别创建]\nimport numpy as np\n\n# Example 2: Create point cloud with categories\npoints_xyzc = np.array([\n    [0, 0, 0, 0],  # Point1: xyz + category 0\n    [1, 1, 1, 1],  # Point2: xyz + category 1\n    [2, 0, 1, 2]   # Point3: xyz + category 2\n])\n\ncloud_xyzc = swanlab.Object3D(points_xyzc, caption=\"Points with Categories\")\nswanlab.log({\"examples\": cloud_xyzc})\n```\n\n```python [从坐标和RGB创建]\nimport numpy as np\n\n# Example 3: Create point cloud with RGB colors\npoints_xyzrgb = np.array([\n    [0, 0, 0, 255, 0, 0],    # Point1: xyz + red\n    [1, 1, 1, 0, 255, 0],    # Point2: xyz + green\n    [2, 0, 1, 0, 0, 255]     # Point3: xyz + blue\n])\n\ncloud_xyzrgb = swanlab.Object3D(points_xyzrgb, caption=\"Colored Points\")\nswanlab.log({\"examples\": cloud_xyzrgb})\n```\n:::\n\n#### 二级标题：单步记录多个点云\n\n内容：\n```python\nimport swanlab\n\n...\n\ncloud1 = swanlab.Object3D(points1, caption=\"cloud1\")\ncloud2 = swanlab.Object3D(points2, caption=\"cloud2\")\ncloud3 = swanlab.Object3D(points3, caption=\"cloud3\")\n\n...\n\nswanlab.log({\"examples\": [cloud1, cloud2, cloud3, ...]})\n```",
    "61": "一级标题：swanlab.OpenApi\n二级标题：介绍\n内容：\n> 前置条件：需要在编程环境下登录过SwanLab账号。\n\n要使用 SwanLab 的开放 API, 只需实例化一个 `OpenApi` 对象。\n\n```python\nfrom swanlab import OpenApi\n\nmy_api = OpenApi() # 使用本地登录信息\nprint(my_api.list_workspaces().data) # 获取当前用户的工作空间列表\n```\n\n如果你需要获取其他用户的数据：\n```python\nfrom swanlab import OpenApi\n\nother_api = OpenApi(api_key='other_api_key') # 使用另一个账户的api_key\nprint(other_api.list_workspaces().data)\n```\n\n\n具体来说, **OpenApi**的认证逻辑如下：\n\n1. 如果显式提供了`api_key`参数, 则优先使用该`api_key`进行身份认证, 可以在[这里](https://swanlab.cn/space/~/settings)查看自己的 API 密钥；\n2. 否则,使用本地的认证信息。",
    "62": "一级标题：swanlab.OpenApi\n二级标题：常用参数\n内容：\n### 实验ID `exp_id`\n\n实验的唯一标识符**CUID**, 即`exp_id`, 可通过`list_experiments`方法获取对应的`cuid`字段\n\n要查看某一个实验的CUID, 可在云端版网页的\"环境\"标签页查看\"实验ID\"一行, 点击即可复制此实验的CUID\n\n![](./py-openapi/exp_id.png)\n\n### 工作空间名 `username`\n\n工作空间名即`username`, 用于标识用户所在的工作空间:\n\n- 若为个人空间, `username`即为用户的用户名\n- 若为组织空间, `username`为该组织的组织ID\n\n`username`可以通过`list_workspaces`方法获取, 返回的工作空间列表中每个元素的`username`字段即为工作空间名\n\n一般的, 若在开放API调用中不指定`username`, 则**默认**为当前用户的个人空间",
    "63": "一级标题：swanlab.OpenApi\n二级标题：模型定义\n内容：\n在使用开放 API 时, 获取到的部分云端资源组成较为复杂, 如实验、项目等, 难以用简单的Python数据类型表示\n\n因此, 这些资源在开放API的返回值中被定义为了对象, 支持 IDE 的自动补全与类型检查, 从而方便用户进行操作\n\n例如, 要获取一个实验对象的开始时间, 可以用:\n\n```python\napi_response: ApiResponse = my_api.get_experiment(project=\"project1\", exp_cuid=\"cuid1\")\nmy_exp: Experiment = api_response.data\ncreated_time: str = my_exp.createdAt\n```\n\n或者, 要获取一个项目对象所属工作空间的名字, 可以用:\n\n```python\napi_response: ApiResponse = my_api.list_projects()\nmy_project: Project = api_response.data[0]\nworkspace_name: str = my_project.group[\"name\"]\n```\n\n对于一个模型, 其属性可通过以下三种方式访问:\n\n- `my_exp.createdAt`\n- `my_exp[\"createdAt\"]`\n- `my_exp.get(\"createdAt\")`\n\n> Note: 模型可以通过字典风格访问, 但不是真正的字典, 可以通过`my_exp_dict: Dict = my_exp.model_dump()`获取此时模型对应的字典\n\n### API 响应 `ApiResponse`\n\n开放 API 方法返回`swanlab.api.openapi.types.ApiResponse`对象, 包含以下字段:\n\n| 字段 | 类型 |描述 |\n| --- | --- | --- |\n| `code` | `int` | HTTP 状态码 |\n| `errmsg` | `str` | 错误信息, 如果状态码不为`2XX`则非空 |\n| `data` | `Any` | 返回的具体数据, 下面API文档中提到的返回值即为该字段 |\n\n### 实验模型 `Experiment`\n\n实验对象的类型为`swanlab.api.openapi.types.Experiment`, 包含以下字段:\n\n| 字段 | 类型 | 描述 |\n| --- | --- | --- |\n| `cuid` | `str` | 实验CUID, 唯一标识符 |\n| `name` | `str` | 实验名 |\n| `description` | `str` | 实验描述 |\n| `state` | `str` | 实验状态, `FINISHED` 或 `RUNNING` |\n| `show` | `bool` | 显示状态 |\n| `createdAt` | `str` | 创建时间, 格式如 `2024-11-23T12:28:04.286Z` |\n| `finishedAt` | `str` | 完成时间, 格式如 `2024-11-23T12:28:04.286Z`, 若不存在则为 None |\n| `user` | `Dict[str, str]` | 实验创建者, 包含 `username` 与 `name` |\n| `profile` | `dict` | 详细包含了实验的所有配置信息, 如用户自定义配置与Python运行环境等 |\n\n### 项目模型 `Project`\n\n项目对象的类型为`swanlab.api.openapi.types.Project`, 包含以下字段:\n\n| 字段 | 类型 | 描述 |\n| --- | --- | --- |\n| `cuid` | `str` | 项目CUID, 唯一标识符 |\n| `name` | `str` | 项目名 |\n| `description` | `str` | 项目描述 |\n| `visibility` | `str` | 可见性, `PUBLIC` 或 `PRIVATE` |\n| `createdAt` | `str` | 创建时间, 格式如 `2024-11-23T12:28:04.286Z` |\n| `updatedAt` | `str` | 更新时间, 格式如 `2024-11-23T12:28:04.286Z` |\n| `group` | `Dict[str, str]` | 工作空间信息, 包含 `type`, `username`, `name` |\n| `count` | `Dict[str, int]` | 项目的统计信息, 如实验个数, 协作者数量等 |",
    "64": "一级标题：swanlab.OpenApi\n二级标题：OpenAPIs\n内容：\n每个开放 API 都是`OpenApi`对象的一个方法\n\n下面是所有可用的SwanLab 开放 API\n\n### WorkSpace\n\n#### `list_workspaces`\n\n获取当前用户的所有工作空间(组织)列表。\n\n**返回值**\n\n`data` `(List[Dict])`: 用户加入的工作空间列表, 每个元素是一个字典, 包含工作空间的基础信息:\n\n| 字段 | 类型 | 描述 |\n| --- | --- | --- |\n| `name` | `str` | 工作空间名称 |\n| `username` | `str` | 工作空间唯一标识(用于组织相关的 URL) |\n| `role` | `str` | 用户在该工作空间中的角色, 为 `OWNER` 或 `MEMBER` |\n\n**示例**\n\n::: code-group\n\n```python [获取工作区列表]\nfrom swanlab import OpenApi\nmy_api = OpenApi()\n\nmy_api.list_workspaces().data\n\"\"\"\n[\n    {\n        \"name\": \"workspace1\",\n        \"username\": \"kites-test3\",\n        \"role\": \"OWNER\"\n    },\n    {\n        \"name\": \"hello-openapi\",\n        \"username\": \"kites-test2\",\n        \"role\": \"MEMBER\"\n    }\n]\n\"\"\"\n```\n\n```python [获取第一个工作区名称]\nfrom swanlab import OpenApi\nmy_api = OpenApi()\n\nmy_api.list_workspaces().data[0][\"name\"]\n\"\"\"\n\"workspace1\"\n\"\"\"\n```\n\n```python [获取响应状态码]\nfrom swanlab import OpenApi\nmy_api = OpenApi()\n\nmy_api.list_workspaces().code\n\"\"\"\n200\n\"\"\"\n```\n\n:::\n\n<br>\n\n### Experiment\n\n#### `list_experiments`\n\n获取指定项目下的所有实验列表\n\n**方法参数**\n\n| 参数  | 类型 | 描述 |\n| --- | --- | --- |\n| `project` | `str` | 项目名 |\n| `username` | `str` | 工作空间名, 默认为用户个人空间 |\n\n**返回值**\n\n`data` `(List[Experiment])`: 包含实验[(Experiment)](#实验模型-experiment)对象的列表\n\n**示例**\n\n::: code-group\n\n```python [获取实验列表]\nmy_api.list_experiments(project=\"project1\").data\n\"\"\"\n[\n    {\n        \"cuid\": \"cuid1\",\n        \"name\": \"experiment1\",\n        \"description\": \"Description 1\",\n        \"state\": \"RUNNING\",\n        \"show\": true,\n        \"createdAt\": \"2024-11-23T12:28:04.286Z\",\n        \"finishedAt\": null,\n        \"user\": {\n            \"username\": \"kites-test3\",\n            \"name\": \"Kites Test\"\n        },\n        \"profile\": {\n            \"config\": {\n                \"lr\": 0.001,\n                \"epochs\": 10\n            }\n        }\n    },\n    ...\n]\n\"\"\"\n```\n\n```python [获取第一个实验的CUID]\nmy_api.list_experiments(project=\"project1\").data[0].cuid\n\"\"\"\n\"cuid1\"\n\"\"\"\n```\n\n```python [获取第一个实验的名称]\nmy_api.list_experiments(project=\"project1\").data[0].name\n\"\"\"\n\"experiment1\"\n\"\"\"\n```\n\n:::\n\n<br>\n\n#### `get_experiment`\n\n获取一个实验的详细信息\n\n**方法参数**\n\n| 参数 | 类型 | 描述 |\n| --- | --- | --- |\n| `project` | `str` | 项目名 |\n| `exp_id` | `str` | 实验CUID, 唯一标识符, 可通过`list_experiments`获取, 也可在云端版实验\"环境\"标签页查看 |\n| `username` | `str` | 工作空间名, 默认为用户个人空间 |\n\n**返回值**\n\n`data` `(Experiment)`: 返回一个实验[(Experiment)](#实验模型-experiment)类型的对象, 包含实验的详细信息\n\n**示例**\n\n::: code-group\n\n```python [获取实验信息]\nmy_api.get_experiment(project=\"project1\", exp_id=\"cuid1\").data\n\"\"\"\n{\n    \"cuid\": \"cuid1\",\n    \"name\": \"experiment1\",\n    \"description\": \"This is a test experiment\",\n    \"state\": \"FINISHED\",\n    \"show\": true,\n    \"createdAt\": \"2024-11-23T12:28:04.286Z\",\n    \"finishedAt\": \"2024-11-25T15:56:48.123Z\",\n    \"user\": {\n        \"username\": \"kites-test3\",\n        \"name\": \"Kites Test\"\n    },\n    \"profile\": {\n        \"conda\": \"...\",\n        \"requirements\": \"...\",\n        ...\n    }\n}\n\"\"\"\n```\n\n```python [获取实验的状态]\nmy_api.get_experiment(project=\"project1\", exp_id=\"cuid1\").data.state\n\"\"\"\n\"FINISHED\"\n\"\"\"\n```\n\n```python [获取实验的创建者用户名]\nmy_api.get_experiment(project=\"project1\", exp_id=\"cuid1\").data.user[\"username\"]\n\"\"\"\n\"kites-test3\"\n\"\"\"\n```\n\n:::\n\n<br>\n\n#### `delete_experiment`\n\n删除一个实验\n\n**方法参数**\n| 参数 | 类型 | 描述 |\n| --- | --- | --- |\n| `project` | `str` | 项目名 |\n| `exp_id` | `str` | 实验CUID, 唯一标识符, 可通过`list_experiments`获取, 也可在云端版实验\"环境\"标签页查看 |\n| `username` | `str` | 工作空间名, 默认为用户个人空间 |\n\n**返回值**\n`data` `(dict)`: 空字典, 仅表示删除操作成功\n\n**示例**\n\n::: code-group\n\n```python [删除实验]\nmy_api.delete_experiment(project=\"project1\", exp_id=\"cuid1\")\n```\n\n:::\n\n<br>\n\n#### `get_summary`\n\n获取一个实验的概要信息, 包含实验跟踪指标的最终值和最大最小值, 以及其对应的步数\n\n**方法参数**\n\n| 参数 | 类型 | 描述 |\n| --- | --- | --- |\n| `project` | `str` | 项目名 |\n| `exp_id` | `str` | 实验CUID, 唯一标识符, 可通过`list_experiments`获取, 也可在云端版实验\"环境\"标签页查看 |\n| `username` | `str` | 工作空间名, 默认为用户个人空间 |\n\n**返回值**\n\n`data` `(Dict[str, Dict])`: 返回一个字典, 包含实验的概要信息\n\n字典中的每个键是一个指标名称, 值是一个结构如下的字典:\n\n| 字段 | 类型 | 描述 |\n| --- | --- | --- |\n| `step` | `int` | 最后一个步数 |\n| `value` | `float` | 最后一个步数的指标值 |\n| `min` | `Dict[str, float]` | 最小值对应的步数和指标值 |\n| `max` | `Dict[str, float]` | 最大值对应的步数和指标值 |\n\n\n**示例**\n\n::: code-group\n\n```python [获取实验概要信息]\nmy_api.get_summary(project=\"project1\", exp_id=\"cuid1\").data\n\"\"\"\n{\n    \"loss\": {\n        \"step\": 47,\n        \"value\": 0.1907215012216071,\n        \"min\": {\n            \"step\": 33,\n            \"value\": 0.1745886406861026\n        },\n        \"max\": {\n            \"step\": 0,\n            \"value\": 0.7108771095136294\n        }\n    },\n    ...\n}\n\"\"\"\n```\n\n\n```python [获取指标的最大值]\nmy_api.get_summary(project=\"project1\", exp_id=\"cuid1\").data[\"loss\"][\"max\"][\"value\"]\n\"\"\"\n0.7108771095136294\n\"\"\"\n```\n\n```python [获取指标最小值所在步]\nmy_api.get_summary(project=\"project1\", exp_id=\"cuid1\").data[\"loss\"][\"min\"][\"step\"]\n\"\"\"\n33\n\"\"\"\n```\n:::\n\n<br>\n\n### Project\n\n#### `list_projects`\n\n获取指定工作空间下的所有项目列表\n\n**方法参数**\n\n| 参数  | 类型 | 描述 |\n| --- | --- | --- |\n| `username` | `str` | 工作空间名, 默认为用户个人空间 |\n| `detail` | `bool` | 是否项目统计信息, 默认为 True |\n\n**返回值**\n\n`data` `(List[Project])`: 包含项目[(Project)](#项目模型-project)对象的列表\n\n**示例**\n\n::: code-group\n\n```python [获取项目列表]\nmy_api.list_projects().data\n\"\"\"\n[\n    {\n        \"cuid\": \"project1\",\n        \"name\": \"Project 1\",\n        \"description\": \"Description 1\",\n        \"visibility\": \"PUBLIC\",\n        \"createdAt\": \"2024-11-23T12:28:04.286Z\",\n        \"updatedAt\": null,\n        \"group\": {\n            \"type\": \"PERSON\",\n            \"username\": \"kites-test3\",\n            \"name\": \"Kites Test\"\n        },\n        \"count\": {\n            \"experiments\": 4,\n            \"contributors\": 1,\n            \"children\": 0,\n            \"runningExps\": 0\n        }\n    },\n    ...\n]\n\"\"\"\n```\n\n:::\n\n#### `delete_project`\n\n删除一个项目\n\n**方法参数**\n\n| 参数 | 类型 | 描述 |\n| --- | --- | --- |\n| `project` | `str` | 项目名 |\n| `username` | `str` | 工作空间名, 默认为用户个人空间 |\n\n**返回值**\n\n`data` `(dict)`: 空字典, 仅表示删除操作成功\n\n**示例**\n\n::: code-group\n\n```python [删除项目]\nmy_api.delete_project(project=\"project1\")\n```\n\n:::\n\n<br>\n\n根据提供的Markdown内容，按照主题分块整理如下：",
    "65": "一级标题：其他Python API\n二级标题：get_run\n内容：\n获取当前运行的实验对象（`SwanLabRun`）。\n\n```python\nrun = swanlab.init(...)\n\n...\n\nrun = swanlab.get_run()\n```",
    "66": "一级标题：其他Python API\n二级标题：get_url\n内容：\n获取实验的URL（cloud模式，否则为None）。\n\n```python\nprint(swanlab.get_url())\n```",
    "67": "一级标题：其他Python API\n二级标题：get_project_url\n内容：\n获取项目的URL（cloud模式，否则为None）。\n\n```python\nprint(swanlab.get_project_url())\n```",
    "68": "一级标题：swanlab.register_callback\n二级标题：函数定义\n内容：\n```python\n@should_call_before_init(\"After calling swanlab.init(), you can't call it again.\")\ndef register_callbacks(\n    self,\n    callbacks: List[SwanKitCallback]\n) -> None:\n```\n\n| 参数 | 类型 | 描述 |\n| --- | --- | --- |\n| `callbacks` | `List[SwanKitCallback]` | 回调函数列表 |",
    "69": "一级标题：swanlab.register_callback\n二级标题：介绍\n内容：\n使用`swanlab.register_callbacks()`注册回调函数，以在SwanLab的执行生命周期中调用。\n\n```python {3}\nfrom swanlab.plugin.writer import EmailCallback\nemail_callback = EmailCallback(...)\nswanlab.register_callbacks([email_callback])\n\nswanlab.init(...)\n```\n\n效果等价于：\n\n```python\nfrom swanlab.plugin.writer import EmailCallback\nemail_callback = EmailCallback(...)\n\nswanlab.init(\n    ...\n    callbacks=[email_callback]\n)\n```\n\n**场景**：比如你使用时的是SwanLab与Transformers的集成，那么你要找到`swanlab.init()`是不容易的。那么，你可以在`trainer.train()`调用前，用`swanlab.register_callbacks()`注册回调函数，实现插件的注入。",
    "70": "一级标题：run\n二级标题：public\n内容：\npublic存储了SwanLabRun的一些公共信息，包括：\n- `project_name`: 项目名称\n- `version`: 版本\n- `run_id`: 实验ID\n- `swanlog_dir`: swanlog日志目录的路径\n- `run_dir`: 运行目录的路径\n- `cloud`: 云端信息\n    - `project_name`: 项目名称（仅在cloud模式时有效）\n    - `project_url`: 项目在云端的URL（仅在cloud模式时有效）\n    - `experiment_name`: 实验名称（仅在cloud模式时有效）\n    - `experiment_url`: 实验在云端的URL（仅在cloud模式时有效）\n\n以字典形式获取public信息：\n\n```python\nimport swanlab\nrun = swanlab.init()\nprint(run.public.json())\n```\n\n比如，你想要获取实验的URL，可以这样：\n\n```python\nprint(run.public.cloud.experiment_url)\n```",
    "71": "一级标题：swanlab.Settings\n二级标题：Settings 类定义\n内容：\n```python\nSettings(\n    model_config = ConfigDict(frozen=True),\n    metadata_collect: StrictBool = True,\n    collect_hardware: StrictBool = True,\n    collect_runtime: StrictBool = True,\n    security_mask: StrictBool = True,\n    requirements_collect: StrictBool = True,\n    conda_collect: StrictBool = False,\n    hardware_monitor: StrictBool = True,\n    disk_io_dir: DirectoryPath = Field(...),\n    upload_interval: PositiveInt = 1,\n    max_log_length: int = Field(ge=500, le=4096, default=1024),\n    log_proxy_type: Literal[\"all\", \"stdout\", \"stderr\", \"none\"] = \"all\",\n)\n```",
    "72": "一级标题：swanlab.Settings\n二级标题：参数描述\n内容：\n| 参数                     | 类型            | 描述                                                                              |\n|:-----------------------|:--------------|:--------------------------------------------------------------------------------|\n| `metadata_collect`     | StrictBool    | 是否开启元数据采集。默认值为 `True`。                                                          |\n| `collect_hardware`     | StrictBool    | 是否采集当前系统环境的硬件信息。默认值为 `True`。                                                    |\n| `collect_runtime`      | StrictBool    | 是否采集运行时信息。默认值为 `True`。                                                          |\n| `security_mask`        | StrictBool    | 是否自动隐藏隐私信息，如 api_key 等。开启后将在检测到隐私信息时，自动将其替换为加密字符（****）。默认值为 `True`。             |\n| `requirements_collect` | StrictBool    | 是否采集 Python 环境信息 (`pip list`)。默认值为 `True`。                                      |\n| `conda_collect`        | StrictBool    | 是否采集 Conda 环境信息。默认值为 `False`。                                                   |\n| `hardware_monitor`     | StrictBool    | 是否开启硬件监控。如果 `metadata_collect` 关闭，则此项无效。默认值为 `True`。                            |\n| `disk_io_dir`          | DirectoryPath | 磁盘 IO 监控的路径。默认值为系统根目录 (`/` 或 `C:\\`)。                                            |\n| `hardware_interval`    | PositiveInt   | 硬件监控采集间隔，以秒为单位，最小值为5秒。                                                          |\n| `backup`               | PositiveInt   | 日志备份开启功能，默认值为 `True`。开启后，日志将被备份到本地（默认为`swanlog`目录）。      |\n| `upload_interval`      | PositiveInt   | 日志上传间隔（单位：秒）。默认值为 `1`。                                                          |\n| `max_log_length`       | int           | 终端日志上传单行最大字符数（范围：500-4096）。默认值为 `1024`。                                         |\n| `log_proxy_type`       | Literal       | 日志代理类型，会影响实验的日志选项卡记录的内容。默认值为 `\"all\"`。\"stdout\" 表示只代理标准输出流，\"stderr\" 表示只代理标准错误流，\"all\" 表示代理标准输出流和标准错误流，\"none\" 表示不代理日志。|",
    "73": "一级标题：swanlab.Settings\n二级标题：介绍\n内容：\n- `swanlab.Settings`类用于和管理 SwanLab 的全局功能开关和设置。\n- 在`import swanlab`时，会创建一个默认的全局设置，各个设置及其默认值详见上表。\n- 如果我们要对某些设置进行调整，需要通过新建一个`Settings`实例如`new_settings`，在实例化时传入想要修改的配置参数，然后要通过运行`swanlab.merge_settings(new_settings)`来对全局设置进行更新。\n- 值得注意的是，`merge_settings()`方法只在`swanlab.init()`被调用之前可用，这意味着，在使用`swanlab`的过程中，一旦`swanlab.init()`被调用，全局设置将不再能被更改。",
    "74": "一级标题：swanlab.Settings\n二级标题：更多用法\n内容：\n### 更新全局设置\n::: code-group\n\n```python [方式一]\nimport swanlab\n\n# 创建新的设置对象\nnew_settings = swanlab.Settings(\n    metadata_collect=False,\n    hardware_monitor=False,\n    upload_interval=5\n)\n\nswanlab.init(settings=new_settings)\n...\n```\n\n```python [方式二]\nimport swanlab\n\n# 创建新的设置对象\nnew_settings = swanlab.Settings(\n    metadata_collect=False,\n    hardware_monitor=False,\n    upload_interval=5\n)\n\n# 更新全局设置\nswanlab.merge_settings(new_settings)\n\nswanlab.init()\n...\n```\n\n:::\n### 记录 conda 环境信息\n\n```python\nimport swanlab\nfrom swanlab import Settings\n\n# 创建新的设置对象\nnew_settings = Settings(\n    conda_collect=True  # 默认不开启\n)\n\n# 更新全局设置\nswanlab.merge_settings(new_settings)\n\nswanlab.init()\n...\n```",
    "75": "一级标题：swanlab.sync_mlflow\n二级标题：将MLFlow项目同步到SwanLab\n内容：\n将MLFlow项目同步到SwanLab，[文档](/guide_cloud/integration/integration-mlflow.md)\n```\n\n这样就按照要求将内容进行了分块整理。\n\n根据您提供的Markdown内容和要求，以下是按主题分块整理后的格式：\n\n```",
    "76": "一级标题：swanlab.sync_tensorboard\n二级标题：将tensorboard/tensorboardX的指标同步到SwanLab\n内容：\n[文档](/guide_cloud/integration/integration-tensorboard.md)",
    "77": "一级标题：swanlab.sync_wandb\n二级标题：将wandb的指标同步到SwanLab\n内容：\n将wandb的指标同步到SwanLab, [文档](/guide_cloud/integration/integration-wandb.md)",
    "78": "一级标题：swanlab.Text\n二级标题：介绍\n内容：\n对文本数据做转换，以被`swanlab.log()`记录。",
    "79": "一级标题：swanlab.Text\n二级标题：记录字符串文本\n内容：\n记录单个字符串文本：\n\n```python{4}\nimport swanlab\n\nswanlab.init()\ntext = swanlab.Text(\"an awesome text.\")\nswanlab.log({\"examples\": text})\n```\n\n记录多个字符串文本：\n\n```python\nimport swanlab\n\nswanlab.init()\n\nexamples = []\nfor i in range(3):\n    text = swanlab.Text(\"an awesome text.\")\n    examples.append(text)\n\nswanlab.log({\"examples\": examples})\n```",
    "80": "一级标题：swanlab.Text\n二级标题：参数描述\n内容：\n```python\nText(\n    data: Union[str],\n    caption: str = None,\n) -> None\n```\n\n| 参数    | 描述                                                              |\n|-------|-----------------------------------------------------------------|\n| data  | (Union[str]) 接收字符串。                                      |\n| caption | (str) 文本的标签。用于在实验看板中对data进行标记。                     |",
    "81": "一级标题：音频分类\n二级标题：音频分类逻辑\n内容：\n本教程对音频分类任务的逻辑如下：\n\n1. 载入音频数据集，数据集为音频WAV文件与对应的标签\n2. 以8:2的比例划分训练集和测试集\n3. 使用`torchaudio`库，将音频文件转换为梅尔频谱图，本质将其转换为图像分类任务\n4. 使用ResNet模型对梅尔频谱图进行训练迭代\n5. 使用SwanLab记录训练和测试阶段的loss、acc变化，并对比不同实验之间的效果差异",
    "82": "一级标题：音频分类\n二级标题：环境安装\n内容：\n本案例基于**Python>=3.8**，请在您的计算机上安装好Python。\n\n我们需要安装以下这几个Python库：\n\n```python\ntorch\ntorchvision\ntorchaudio\nswanlab\npandas\nscikit-learn\n```\n\n一键安装命令：\n\n```shellscript\npip install torch torchvision torchaudio swanlab pandas scikit-learn\n```",
    "83": "一级标题：音频分类\n二级标题：GTZAN数据集准备\n内容：\n本任务使用的数据集为GTZAN，这是一个在音乐流派识别研究中常用的公开数据集。GTZAN数据集包含 1000 个音频片段，每个音频片段的时长为 30 秒，共分为 10 种音乐流派：包括布鲁斯（Blues）、古典（Classical）、乡村（Country）、迪斯科（Disco）、嘻哈（Hip Hop）、爵士（Jazz）、金属（Metal）、流行（Pop）、雷鬼（Reggae）、摇滚（Rock），且每种流派都有 100 个音频片段。\n\n![alt text](/assets/examples/audio_classification/example-audio-classification-2.png)\n\nGTZAN数据集是在 2000-2001 年从各种来源收集的，包括个人 CD、收音机、麦克风录音等，代表了各种录音条件下的声音。\n\n**数据下载方式（大小1.4GB）：**\n\n1. 百度网盘下载：链接: [https://pan.baidu.com/s/14CTI\\_9MD1vXCqyVxmAbeMw?pwd=1a9e](https://pan.baidu.com/s/14CTI\\_9MD1vXCqyVxmAbeMw?pwd=1a9e) 提取码: 1a9e\n2. 通过Kaggle下载：[https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification](https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification)\n3. 在Hyper超神经网站下载BT种子进行下载：[https://hyper.ai/cn/datasets/32001](https://hyper.ai/cn/datasets/32001)\n\n> 注意，数据集中有一个音频是损坏的，在百度网盘版本里已经将其剔除。\n\n下载完成后，解压到项目根目录下即可。",
    "84": "一级标题：音频分类\n二级标题：生成数据集CSV文件\n内容：\n我们将数据集中的音频文件路径和对应的标签，处理成一个`audio_dataset.csv`文件，其中第一列为文件路径，第二列为标签：\n\n（这一部分先不执行，在完整代码里会带上）\n\n```python\nimport os\nimport pandas as pd\n\ndef create_dataset_csv():\n    # 数据集根目录\n    data_dir = './GTZAN/genres_original'\n    data = []\n    \n    # 遍历所有子目录\n    for label in os.listdir(data_dir):\n        label_dir = os.path.join(data_dir, label)\n        if os.path.isdir(label_dir):\n            # 遍历子目录中的所有wav文件\n            for audio_file in os.listdir(label_dir):\n                if audio_file.endswith('.wav'):\n                    audio_path = os.path.join(label_dir, audio_file)\n                    data.append([audio_path, label])\n    \n    # 创建DataFrame并保存为CSV\n    df = pd.DataFrame(data, columns=['path', 'label'])\n    df.to_csv('audio_dataset.csv', index=False)\n    return df\n\n\n# 生成或加载数据集CSV文件\nif not os.path.exists('audio_dataset.csv'):\n    df = create_dataset_csv()\nelse:\n    df = pd.read_csv('audio_dataset.csv')\n```\n\n处理后，你会在根目录下看到一个`audio_dataset.csv`文件：\n\n![alt text](/assets/examples/audio_classification/example-audio-classification-3.png)",
    "85": "一级标题：音频分类\n二级标题：配置训练跟踪工具SwanLab\n内容：\nSwanLab 是一款开源、轻量的 AI 实验跟踪工具，提供了一个跟踪、比较、和协作实验的平台。SwanLab 提供了友好的 API 和漂亮的界面，结合了超参数跟踪、指标记录、在线协作、实验链接分享等功能，让您可以快速跟踪 AI 实验、可视化过程、记录超参数，并分享给伙伴。\n\n![alt text](/assets/examples/audio_classification/example-audio-classification-4.png)\n\n配置SwanLab的方式很简单：\n\n1. 注册一个账号：[https://swanlab.cn](https://swanlab.cn)\n2. 在安装好swanlab后（pip install swanlab），登录：\n\n```bash\nswanlab login\n```\n\n在提示输入API Key时，去[设置页面](https://swanlab.cn/settings/overview)复制API Key，粘贴后按回车即可。\n\n![alt text](/assets/examples/audio_classification/example-audio-classification-5.png)",
    "86": "一级标题：音频分类\n二级标题：完整代码\n内容：\n开始训练时的目录结构：\n\n```\n|--- train.py\n|--- GTZAN\n```\n\ntrain.py做的事情包括：\n\n1. 生成数据集csv文件\n2. 加载数据集和resnet18模型（ImageNet预训练）\n3. 训练20个epoch，每个epoch进行训练和评估\n4. 记录loss和acc，以及学习率的变化情况，在swanlab中可视化\n\ntrain.py：\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchaudio\nimport torchvision.models as models\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport swanlab\n\n\ndef create_dataset_csv():\n    # 数据集根目录\n    data_dir = './GTZAN/genres_original'\n    data = []\n    \n    # 遍历所有子目录\n    for label in os.listdir(data_dir):\n        label_dir = os.path.join(data_dir, label)\n        if os.path.isdir(label_dir):\n            # 遍历子目录中的所有wav文件\n            for audio_file in os.listdir(label_dir):\n                if audio_file.endswith('.wav'):\n                    audio_path = os.path.join(label_dir, audio_file)\n                    data.append([audio_path, label])\n    \n    # 创建DataFrame并保存为CSV\n    df = pd.DataFrame(data, columns=['path', 'label'])\n    df.to_csv('audio_dataset.csv', index=False)\n    return df\n\n# 自定义数据集类\nclass AudioDataset(Dataset):\n    def __init__(self, df, resize, train_mode=True):\n        self.audio_paths = df['path'].values\n        # 将标签转换为数值\n        self.label_to_idx = {label: idx for idx, label in enumerate(df['label'].unique())}\n        self.labels = [self.label_to_idx[label] for label in df['label'].values]\n        self.resize = resize\n        self.train_mode = train_mode  # 添加训练模式标志\n    def __len__(self):\n        return len(self.audio_paths)\n    \n    def __getitem__(self, idx):\n        # 加载音频文件\n        waveform, sample_rate = torchaudio.load(self.audio_paths[idx])\n        \n        # 将音频转换为梅尔频谱图\n        transform = torchaudio.transforms.MelSpectrogram(\n            sample_rate=sample_rate,\n            n_fft=2048,\n            hop_length=640,\n            n_mels=128\n        )\n        mel_spectrogram = transform(waveform)\n\n        # 确保数值在合理范围内\n        mel_spectrogram = torch.clamp(mel_spectrogram, min=0)\n        \n        # 转换为3通道图像格式 (为了适配ResNet)\n        mel_spectrogram = mel_spectrogram.repeat(3, 1, 1)\n        \n        # 确保尺寸一致\n        resize = torch.nn.AdaptiveAvgPool2d((self.resize, self.resize))\n        mel_spectrogram = resize(mel_spectrogram)\n        \n        return mel_spectrogram, self.labels[idx]\n\n# 修改ResNet模型\nclass AudioClassifier(nn.Module):\n    def __init__(self, num_classes):\n        super(AudioClassifier, self).__init__()\n        # 加载预训练的ResNet\n        self.resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n        # 修改最后的全连接层\n        self.resnet.fc = nn.Linear(512, num_classes)\n        \n    def forward(self, x):\n        return self.resnet(x)\n\n# 训练函数\ndef train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n    \n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for i, (inputs, labels) in enumerate(train_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n            \n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            \n            loss.backward()\n\n            optimizer.step()\n            optimizer.zero_grad()\n            \n            running_loss += loss.item()\n            \n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n        \n        train_loss = running_loss/len(train_loader)\n        train_acc = 100.*correct/total\n        \n        # 验证阶段\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                \n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n        \n        val_loss = val_loss/len(val_loader)\n        val_acc = 100.*correct/total\n        \n        current_lr = optimizer.param_groups[0]['lr']\n        \n        # 记录训练和验证指标\n        swanlab.log({\n            \"train/loss\": train_loss,\n            \"train/acc\": train_acc,\n            \"val/loss\": val_loss,\n            \"val/acc\": val_acc,\n            \"train/epoch\": epoch,\n            \"train/lr\": current_lr\n        })\n            \n        print(f'Epoch {epoch+1}:')\n        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%')\n        print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%')\n        print(f'Learning Rate: {current_lr:.6f}')\n\n# 主函数\ndef main():\n    # 设置设备\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    run = swanlab.init(\n        project=\"PyTorch_Audio_Classification-simple\",\n        experiment_name=\"resnet18\",\n        config={\n            \"batch_size\": 16,\n            \"learning_rate\": 1e-4,\n            \"num_epochs\": 20,\n            \"resize\": 224,\n        },\n    )\n    \n    # 生成或加载数据集CSV文件\n    if not os.path.exists('audio_dataset.csv'):\n        df = create_dataset_csv()\n    else:\n        df = pd.read_csv('audio_dataset.csv')\n    \n    # 划分训练集和验证集\n    train_df = pd.DataFrame()\n    val_df = pd.DataFrame()\n    \n    for label in df['label'].unique():\n        label_df = df[df['label'] == label]\n        label_train, label_val = train_test_split(label_df, test_size=0.2, random_state=42)\n        train_df = pd.concat([train_df, label_train])\n        val_df = pd.concat([val_df, label_val])\n    \n    # 创建数据集和数据加载器 \n    train_dataset = AudioDataset(train_df, resize=run.config.resize, train_mode=True)\n    val_dataset = AudioDataset(val_df, resize=run.config.resize, train_mode=False)\n    \n    train_loader = DataLoader(train_dataset, batch_size=run.config.batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n    \n    # 创建模型\n    num_classes = len(df['label'].unique())  # 根据实际分类数量设置\n    print(\"num_classes\", num_classes)\n    model = AudioClassifier(num_classes).to(device)\n    \n    # 定义损失函数和优化器\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=run.config.learning_rate)  \n    \n    # 训练模型\n    train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=run.config.num_epochs, device=device)\n\nif __name__ == \"__main__\":\n    main()\n```\n\n看到下面的输出，则代表训练开始：\n\n![alt text](/assets/examples/audio_classification/example-audio-classification-6.png)\n\n访问打印的swanlab链接，可以看到训练的全过程：\n\n![alt text](/assets/examples/audio_classification/example-audio-classification-7.png)\n\n可以看到Reset18模型，且不加任何策略的条件下，在训练集的准确率为99.5%，验证集的准确率最高为71.5%，val loss在第3个epoch开始反而在上升，呈现「过拟合」的趋势。",
    "87": "一级标题：音频分类\n二级标题：进阶代码\n内容：\n下面是我训出验证集准确率87.5%的实验，具体策略包括：\n\n1. 将模型换成resnext101\\_32x8d\n2. 将梅尔顿图的resize提高到512\n3. 增加warmup策略\n4. 增加时间遮蔽、频率屏蔽、高斯噪声、随机响度这四种数据增强策略\n5. 增加学习率梯度衰减策略\n\n![alt text](/assets/examples/audio_classification/example-audio-classification-8.png)\n\n进阶代码（需要24GB显存，如果要降低显存消耗的话，可以调低batch\\_size）：\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchaudio\nimport torchvision.models as models\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport swanlab\nimport random\nimport numpy as np\n\n# 设置随机种子\ndef set_seed(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndef create_dataset_csv():\n    # 数据集根目录\n    data_dir = './GTZAN/genres_original'\n    data = []\n    \n    # 遍历所有子目录\n    for label in os.listdir(data_dir):\n        label_dir = os.path.join(data_dir, label)\n        if os.path.isdir(label_dir):\n            # 遍历子目录中的所有wav文件\n            for audio_file in os.listdir(label_dir):\n                if audio_file.endswith('.wav'):\n                    audio_path = os.path.join(label_dir, audio_file)\n                    data.append([audio_path, label])\n    \n    # 创建DataFrame并保存为CSV\n    df = pd.DataFrame(data, columns=['path', 'label'])\n    df.to_csv('audio_dataset.csv', index=False)\n    return df\n\n# 自定义数据集类\nclass AudioDataset(Dataset):\n    def __init__(self, df, resize, train_mode=True):\n        self.audio_paths = df['path'].values\n        # 将标签转换为数值\n        self.label_to_idx = {label: idx for idx, label in enumerate(df['label'].unique())}\n        self.labels = [self.label_to_idx[label] for label in df['label'].values]\n        self.resize = resize\n        self.train_mode = train_mode  # 添加训练模式标志\n    def __len__(self):\n        return len(self.audio_paths)\n    \n    def __getitem__(self, idx):\n        # 加载音频文件\n        waveform, sample_rate = torchaudio.load(self.audio_paths[idx])\n        \n        # 将音频转换为梅尔频谱图\n        transform = torchaudio.transforms.MelSpectrogram(\n            sample_rate=sample_rate,\n            n_fft=2048,\n            hop_length=640,\n            n_mels=128\n        )\n        mel_spectrogram = transform(waveform)\n        \n        # 仅在训练模式下进行数据增强\n        if self.train_mode:\n            # 1. 时间遮蔽 (Time Masking)：通过随机选择一个时间步，然后遮蔽掉20个时间步\n            time_mask = torchaudio.transforms.TimeMasking(time_mask_param=20)\n            mel_spectrogram = time_mask(mel_spectrogram)\n            \n            # 2. 频率遮蔽 (Frequency Masking)：通过随机选择一个频率步，然后遮蔽掉20个频率步\n            freq_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param=20)\n            mel_spectrogram = freq_mask(mel_spectrogram)\n            \n            # 3. 随机增加高斯噪声\n            if random.random() < 0.5:\n                noise = torch.randn_like(mel_spectrogram) * 0.01\n                mel_spectrogram = mel_spectrogram + noise\n            \n            # 4. 随机调整响度\n            if random.random() < 0.5:\n                gain = random.uniform(0.8, 1.2)\n                mel_spectrogram = mel_spectrogram * gain\n\n        # 确保数值在合理范围内\n        mel_spectrogram = torch.clamp(mel_spectrogram, min=0)\n        \n        # 转换为3通道图像格式 (为了适配ResNet)\n        mel_spectrogram = mel_spectrogram.repeat(3,",
    "88": "一级标题：BERT文本分类\n二级标题：概述\n内容：\n**BERT**（Bidirectional Encoder Representations from Transformers）是由Google提出的一种自然语言处理预训练模型，广泛应用于各种自然语言处理任务。BERT 通过在大规模语料库上进行预训练，能够捕捉词汇之间的上下文关系，从而在很多任务上取得了优秀的效果。\n\n在这个任务中，我们将使用 BERT 模型对 IMDB 电影评论进行情感分类，具体来说是将电影评论分类为“正面”或“负面”。\n\n![IMDB](/assets/example-bert-1.png)\n\n**IMDB 电影评论数据集**包含50,000条电影评论，分为25,000条训练数据和25,000条测试数据，每部分又包含50%正面评论和50%负面评论。我们将使用预训练的 BERT 模型，通过微调(finetuning)的方式，来对这些评论进行情感分类。",
    "89": "一级标题：BERT文本分类\n二级标题：环境安装\n内容：\n本案例基于`Python>=3.8`，请在您的计算机上安装好Python。 环境依赖：\n\n```txt\ntransformers\ndatasets\nswanlab\n```\n\n快速安装命令：\n\n```bash\npip install transformers datasets swanlab\n```\n\n> 本文的代码测试于transformers==4.41.0、datasets==2.19.1、swanlab==0.3.3",
    "90": "一级标题：BERT文本分类\n二级标题：完整代码\n内容：\n```python\n\"\"\"\n用预训练的Bert模型微调IMDB数据集，并使用SwanLabCallback回调函数将结果上传到SwanLab。\nIMDB数据集的1是positive，0是negative。\n\"\"\"\n\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom swanlab.integration.transformers import SwanLabCallback\nimport swanlab\n\ndef predict(text, model, tokenizer, CLASS_NAME):\n    inputs = tokenizer(text, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        predicted_class = torch.argmax(logits).item()\n\n    print(f\"Input Text: {text}\")\n    print(f\"Predicted class: {int(predicted_class)} {CLASS_NAME[int(predicted_class)]}\")\n    return int(predicted_class)\n\n# 加载IMDB数据集\ndataset = load_dataset('imdb')\n\n# 加载预训练的BERT tokenizer\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n\n# 定义tokenize函数\ndef tokenize(batch):\n    return tokenizer(batch['text'], padding=True, truncation=True)\n\n# 对数据集进行tokenization\ntokenized_datasets = dataset.map(tokenize, batched=True)\n\n# 设置模型输入格式\ntokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\ntokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n\n# 加载预训练的BERT模型\nmodel = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n\n# 设置训练参数\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    eval_strategy='epoch',\n    save_strategy='epoch',\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    logging_first_step=100,\n    # 总的训练轮数\n    num_train_epochs=3,\n    weight_decay=0.01,\n    report_to=\"none\",\n    # 单卡训练\n)\n\nCLASS_NAME = {0: \"negative\", 1: \"positive\"}\n\n# 设置swanlab回调函数\nswanlab_callback = SwanLabCallback(project='BERT',\n                                   experiment_name='BERT-IMDB',\n                                   config={'dataset': 'IMDB', \"CLASS_NAME\": CLASS_NAME})\n\n# 定义Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['test'],\n    callbacks=[swanlab_callback],\n)\n\n# 训练模型\ntrainer.train()\n\n# 保存模型\nmodel.save_pretrained('./sentiment_model')\ntokenizer.save_pretrained('./sentiment_model')\n\n# 测试模型\ntest_reviews = [\n    \"I absolutely loved this movie! The storyline was captivating and the acting was top-notch. A must-watch for everyone.\",\n    \"This movie was a complete waste of time. The plot was predictable and the characters were poorly developed.\",\n    \"An excellent film with a heartwarming story. The performances were outstanding, especially the lead actor.\",\n    \"I found the movie to be quite boring. It dragged on and didn't really go anywhere. Not recommended.\",\n    \"A masterpiece! The director did an amazing job bringing this story to life. The visuals were stunning.\",\n    \"Terrible movie. The script was awful and the acting was even worse. I can't believe I sat through the whole thing.\",\n    \"A delightful film with a perfect mix of humor and drama. The cast was great and the dialogue was witty.\",\n    \"I was very disappointed with this movie. It had so much potential, but it just fell flat. The ending was particularly bad.\",\n    \"One of the best movies I've seen this year. The story was original and the performances were incredibly moving.\",\n    \"I didn't enjoy this movie at all. It was confusing and the pacing was off. Definitely not worth watching.\"\n]\n\nmodel.to('cpu')\ntext_list = []\nfor review in test_reviews:\n    label = predict(review, model, tokenizer, CLASS_NAME)\n    text_list.append(swanlab.Text(review, caption=f\"{label}-{CLASS_NAME[label]}\"))\n\nif text_list:\n    swanlab.log({\"predict\": text_list})\n\nswanlab.finish()\n```",
    "91": "一级标题：BERT文本分类\n二级标题：演示效果\n内容：\n![](/assets/example-bert-2.png)",
    "92": "一级标题：猫狗分类\n二级标题：简介\n内容：\n:::info\n图像分类、机器学习入门、RGB图像、自定义数据集\n:::\n\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/Cats_Dogs_Classification/runs/jzo93k112f15pmx14vtxf/chart)\n\n[知乎](https://zhuanlan.zhihu.com/p/676430630)\n\n猫狗分类是计算机视觉最基础的任务之一——如果说完成MNIST手写体识别是实现CV的“Hello World”，那猫狗分类就是旅程的下一站～。这篇文章我将带大家使用SwanLab、PyTorch、Gradio三个开源工具，完成从数据集准备、代码编写、可视化训练到构建Demo网页的全过程。\n\n![](./cats_dogs/01.png)\n\n- 实验过程可看这个网页：[猫狗分类｜SwanLab](https://swanlab.cn/@ZeyiLin/Cats_Dogs_Classification/runs/jzo93k112f15pmx14vtxf/chart)\n- 代码：[Github](https://github.com/Zeyi-Lin/Resnet50-cats_vs_dogs) \n- 在线Demo：[HuggingFace](https://huggingface.co/spaces/TheEeeeLin/Resnet50-cats_vs_dogs)\n- 数据集：[百度云](https://pan.baidu.com/s/1qYa13SxFM0AirzDyFMy0mQ) 提取码: 1ybm\n- 三个开源库：[SwanLab](https://github.com/swanhubx/swanlab)、[Gradio](https://github.com/gradio-app/gradio)、[PyTorch](https://github.com/pytorch/pytorch)",
    "93": "一级标题：猫狗分类\n二级标题：1. 准备部分\n内容：\n### 1.1 安装Python库\n需要安装下面这4个库：\n```bash\ntorch>=1.12.0\ntorchvision>=0.13.0\nswanlab\ngradio\n```\n安装命令：\n```bash\npip install torch>=1.12.0 torchvision>=0.13.0 swanlab gradio\n```\n\n### 1.2 创建文件目录\n现在打开1个文件夹，新建下面这5个文件：\n\n![在这里插入图片描述](./cats_dogs/02.png)\n\n它们各自的作用分别是： \n| 文件 | 用途 |\n| --- | --- |\n| `checkpoint` | 这个文件夹用于存储训练过程中生成的模型权重。 |\n| `datasets` | 这个文件夹用于放置数据集。 |\n| `app.py` | 运行Gradio Demo的Python脚本。 |\n| `load_datasets.py` | 负责载入数据集，包含了数据的预处理、加载等步骤，确保数据以适当的格式提供给模型使用。 |\n| `train.py` | 模型训练的核心脚本。它包含了模型的载入、训练循环、损失函数的选择、优化器的配置等关键组成部分，用于指导如何使用数据来训练模型。 |\n\n### 1.3 下载猫狗分类数据集\n\n数据集来源是Modelscope上的[猫狗分类数据集](https://modelscope.cn/datasets/tany0699/cats_and_dogs/summary)，包含275张图像的数据集和70张图像的测试集，一共不到10MB。\n我对数据做了一些整理，所以更推荐使用下面的百度网盘链接下载：\n> 百度网盘：链接: https://pan.baidu.com/s/1qYa13SxFM0AirzDyFMy0mQ 提取码: 1ybm\n\n![在这里插入图片描述](./cats_dogs/03.png)\n\n将数据集放入`datasets`文件夹：\n\n![在这里插入图片描述](./cats_dogs/04.png)\n\nok，现在我们开始训练部分！\n> ps：如果你想要用更大规模的数据来训练猫狗分类模型，请前往文末的相关链接。",
    "94": "一级标题：猫狗分类\n二级标题：2. 训练部分\n内容：\nps：如果想直接看完整代码和效果，可直接跳转到第2.9。\n\n### 2.1 load_datasets.py\n我们首先需要创建1个类`DatasetLoader`，它的作用是完成数据集的读取和预处理，我们将它写在`load_datasets.py`中。 \n在写这个类之前，先分析一下数据集。\n在datasets目录下，`train.csv`和`val.csv`分别记录了训练集和测试集的图像相对路径（第一列是图像的相对路径，第二列是标签，0代表猫，1代表狗）： \n![在这里插入图片描述](./cats_dogs/05.png)\n![左图作为train.csv，右图为train文件夹中的cat文件夹中的图像](./cats_dogs/06.png)\n左图作为train.csv，右图为train文件夹中的cat文件夹中的图像。\n\n那么我们的目标就很明确： \n1. 解析这两个csv文件，获取图像相对路径和标签 \n2. 根据相对路径读取图像\n3. 对图像做预处理\n4. 返回预处理后的图像和对应标签\n\n明确了目标后，现在我们开始写`DatasetLoader`类：\n\n```python\nimport csv\nimport os\nfrom torchvision import transforms\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n\nclass DatasetLoader(Dataset):\n    def __init__(self, csv_path):\n        self.csv_file = csv_path\n        with open(self.csv_file, 'r') as file:\n            self.data = list(csv.reader(file))\n\n        self.current_dir = os.path.dirname(os.path.abspath(__file__))\n\n    def preprocess_image(self, image_path):\n        full_path = os.path.join(self.current_dir, 'datasets', image_path)\n        image = Image.open(full_path)\n        image_transform = transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n        return image_transform(image)\n\n    def __getitem__(self, index):\n        image_path, label = self.data[index]\n        image = self.preprocess_image(image_path)\n        return image, int(label)\n\n    def __len__(self):\n        return len(self.data)\n   ```\n   \n`DatasetLoader`类由四个部分组成：\n1. `__init__`：包含1个输入参数csv_path，在外部传入`csv_path`后，将读取后的数据存入`self.data`中。`self.current_dir`则是获取了当前代码所在目录的绝对路径，为后续读取图像做准备。\n\n2. `preprocess_image`：此函数用于图像预处理。首先，它构造图像文件的绝对路径，然后使用PIL库打开图像。接着，定义了一系列图像变换：调整图像大小至256x256、转换图像为张量、对图像进行标准化处理，最终，返回预处理后的图像。\n\n3. `__getitem__`：当数据集类被循环调用时，`__getitem__`方法会返回指定索引index的数据，即图像和标签。首先，它根据索引从`self.data`中取出图像路径和标签。然后，调用`preprocess_image`方法来处理图像数据。最后，将处理后的图像数据和标签转换为整型后返回。\n\n4. `__len__`：用于返回数据集的总图像数量。\n\n### 2.2 载入数据集\n> 从本节开始，代码将写在train.py中。\n```python\nfrom torch.utils.data import DataLoader\nfrom load_datasets import DatasetLoader\n\nbatch_size = 8\n\nTrainDataset = DatasetLoader(\"datasets/train.csv\")\nValDataset = DatasetLoader(\"datasets/val.csv\")\nTrainDataLoader = DataLoader(TrainDataset, batch_size=batch_size, shuffle=True)\nValDataLoader = DataLoader(ValDataset, batch_size=batch_size, shuffle=False)\n```\n\n我们传入那两个csv文件的路径实例化`DatasetLoader`类，然后用PyTorch的`DataLoader`做一层封装。`DataLoader`可以再传入两个参数：\n- `batch_size`：定义了每个数据批次包含多少张图像。在深度学习中，我们通常不会一次性地处理所有数据，而是将数据划分为小批次。这有助于模型更快地学习，并且还可以节省内存。在这里我们定义batch_size = 8，即每个批次将包含8个图像。\n- `shuffle`：定义了是否在每个循环轮次（epoch）开始时随机打乱数据。这通常用于训练数据集以保证每个epoch的数据顺序不同，从而帮助模型更好地泛化。如果设置为True，那么在每个epoch开始时，数据将被打乱。在这里我们让训练时打乱，测试时不打乱。\n\n### 2.3 载入ResNet50模型\n\n模型我们选用经典的**ResNet50**，模型的具体原理本文就不细说了，重点放在工程实现上。\n我们使用**torchvision**来创建1个resnet50模型，并载入在Imagenet1k数据集上预训练好的权重：\n\n```python\nfrom torchvision.models import ResNet50_Weights\n\n# 加载预训练的ResNet50模型\nmodel = torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n```\n\n因为猫狗分类是个2分类任务，而torchvision提供的resnet50默认是1000分类，所以我们需要把模型最后的全连接层的输出维度替换为2：\n\n```python\nfrom torchvision.models import ResNet50_Weights\n\nnum_classes=2\n\n# 加载预训练的ResNet50模型\nmodel = torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n\n# 将全连接层的输出维度替换为num_classes\nin_features = model.fc.in_features\nmodel.fc = torch.nn.Linear(in_features, num_classes)\n```\n\n### 2.4 设置cuda/mps/cpu\n如果你的电脑是**英伟达显卡**，那么cuda可以极大加速你的训练；\n如果你的电脑是**Macbook Apple Sillicon（M系列芯片）**，那么mps同样可以极大加速你的训练；\n如果都不是，那就选用cpu：\n```python\n#检测是否支持mps\ntry:\n    use_mps = torch.backends.mps.is_available()\nexcept AttributeError:\n    use_mps = False\n\n#检测是否支持cuda\nif torch.cuda.is_available():\n    device = \"cuda\"\nelif use_mps:\n    device = \"mps\"\nelse:\n    device = \"cpu\"\n```\n\n将模型加载到对应的device中：\n\n```python\nmodel.to(torch.device(device))\n```\n\n### 2.5 设置超参数、优化器、损失函数\n\n**超参数**\n设置训练轮次为20轮，学习率为1e-4，训练批次为8，分类数为2分类。\n\n```python\nnum_epochs = 20\nlr = 1e-4\nbatch_size = 8\nnum_classes = 2\n```\n### 损失函数与优化器\n设置损失函数为交叉熵损失，优化器为Adam。\n\n```python\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\n```\n\n### 2.6 初始化SwanLab\n\n在训练中我们使用`swanlab`库作为实验管理与指标可视化工具。\n[swanlab](https://github.com/SwanHubX/SwanLab)是一个类似Tensorboard的开源训练图表可视化库，有着更轻量的体积与更友好的API，除了能记录指标，还能自动记录训练的logging、硬件环境、Python环境、训练时间等信息。\n\n![在这里插入图片描述](./cats_dogs/07.png)\n\n#### 2.6.1 设置初始化配置参数\nswanlab库使用`swanlab.init`设置实验名、实验介绍、记录超参数以及日志文件的保存位置。\n后续打开可视化看板需要根据日志文件完成。\n\n```python\nimport swanlab\n\nswanlab.init(\n    # 设置实验名\n    experiment_name=\"ResNet50\",\n    # 设置实验介绍\n    description=\"Train ResNet50 for cat and dog classification.\",\n    # 记录超参数\n    config={\n        \"model\": \"resnet50\",\n        \"optim\": \"Adam\",\n        \"lr\": lr,\n        \"batch_size\": batch_size,\n        \"num_epochs\": num_epochs,\n        \"num_class\": num_classes,\n        \"device\": device,\n    }\n)\n```\n\n#### 2.6.2 跟踪关键指标\nswanlab库使用`swanlab.log`来记录关键指标，具体使用案例见2.7和2.8。\n\n### 2.7 训练函数\n\n我们定义1个训练函数`train`：\n```python\ndef train(model, device, train_dataloader, optimizer, criterion, epoch):\n    model.train()\n    for iter, (inputs, labels) in enumerate(train_loader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        print('Epoch [{}/{}], Iteration [{}/{}], Loss: {:.4f}'.format(epoch, num_epochs, iter + 1, len(TrainDataLoader),\n                                                                      loss.item()))\n        swanlab.log({\"train_loss\": loss.item()})\n```\n\n训练的逻辑很简单：我们循环调用`train_dataloader`，每次取出1个batch_size的图像和标签，传入到resnet50模型中得到预测结果，将结果和标签传入损失函数中计算交叉熵损失，最后根据损失计算反向传播，Adam优化器执行模型参数更新，循环往复。\n在训练中我们最关心的指标是损失值`loss`，所以我们用`swanlab.log`跟踪它的变化。\n  \n### 2.8 测试函数\n我们定义1个测试函数`test`：\n```python\ndef test(model, device, test_dataloader, epoch):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in test_dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    accuracy = correct / total * 100\n    print('Accuracy: {:.2f}%'.format(accuracy))\n    swanlab.log({\"test_acc\": accuracy})\n```\n\n测试的逻辑同样很简单：我们循环调用`test_dataloader`，将测试集的图像传入到resnet50模型中得到预测结果，与标签进行对比，计算整体的准确率。\n在测试中我们最关心的指标是准确率`accuracy`，所以我们用`swanlab.log`跟踪它的变化。\n\n### 2.9 完整训练代码\n\n我们一共训练`num_epochs`轮，每4轮进行测试，并在最后保存权重文件：\n\n```python\nfor epoch in range(1, num_epochs + 1):\n    train(model, device, TrainDataLoader, optimizer, criterion, epoch)\n    if epoch % 4 == 0: \n        accuracy = test(model, device, ValDataLoader, epoch)\n\nif not os.path.exists(\"checkpoint\"):\n    os.makedirs(\"checkpoint\")\ntorch.save(model.state_dict(), 'checkpoint/latest_checkpoint.pth')\nprint(\"Training complete\")\n```\n\n组合后的完整`train.py`代码：\n\n```python\nimport torch\nimport torchvision\nfrom torchvision.models import ResNet50_Weights\nimport swanlab\nfrom torch.utils.data import DataLoader\nfrom load_datasets import DatasetLoader\nimport os\n\n# 定义训练函数\ndef train(model, device, train_dataloader, optimizer, criterion, epoch):\n    model.train()\n    for iter, (inputs, labels) in enumerate(train_dataloader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        print('Epoch [{}/{}], Iteration [{}/{}], Loss: {:.4f}'.format(epoch, num_epochs, iter + 1, len(TrainDataLoader),\n                                                                      loss.item()))\n        swanlab.log({\"train_loss\": loss.item()})\n\n\n# 定义测试函数\ndef test(model, device, test_dataloader, epoch):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in test_dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    accuracy = correct / total * 100\n    print('Accuracy: {:.2f}%'.format(accuracy))\n    swanlab.log({\"test_acc\": accuracy})\n\n\nif __name__ == \"__main__\":\n    num_epochs = 20\n    lr = 1e-4\n    batch_size = 8\n    num_classes = 2\n    \n    # 设置device\n    try:\n        use_mps = torch.backends.mps.is_available()\n    except AttributeError:\n        use_mps = False\n\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    elif use_mps:\n        device = \"mps\"\n    else:\n        device = \"cpu\"\n\n    # 初始化swanlab\n    swanlab.init(\n        experiment_name=\"ResNet50\",\n        description=\"Train ResNet50 for cat and dog classification.\",\n        config={\n            \"model\": \"resnet50\",\n            \"optim\": \"Adam\",\n            \"lr\": lr,\n            \"batch_size\": batch_size,\n            \"num_epochs\": num_epochs,\n            \"num_class\": num_classes,\n            \"device\": device,\n        }\n    )\n\n    TrainDataset = DatasetLoader(\"datasets/train.csv\")\n    ValDataset = DatasetLoader(\"datasets/val.csv\")\n    TrainDataLoader = DataLoader(TrainDataset, batch_size=batch_size, shuffle=True)\n    ValDataLoader = DataLoader(ValDataset, batch_size=batch_size, shuffle=False)\n\n    # 载入ResNet50模型\n    model = torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n\n    # 将全连接层替换为2分类\n    in_features = model.fc.in_features\n    model.fc = torch.nn.Linear(in_features, num_classes",
    "95": "一级标题：CIFAR10 图像分类\n二级标题：概述\n内容：\nCIFAR-10是一个经典的图像分类数据集，包含60,000张32×32像素的彩色图像，分为10个类别（如飞机、汽车、鸟类等），其中50,000张用于训练，10,000张用于测试。\n\n![](./cifar10/dataset.png)\n\nCIFAR-10常被用于图像分类训练任务。该任务是构建模型对输入图像进行10分类，输出每个类别的概率。由于图像分辨率低、背景复杂且数据量有限，该数据集常被用于测试模型的泛化能力和特征提取效果，成为深度学习入门基准。典型方法包括CNN（如ResNet、AlexNet），配合数据增强和交叉熵损失优化，最高准确率可达95%以上。CIFAR-10的轻量级特性使其广泛用于教学和研究，并衍生出更复杂的变体（如CIFAR-100）。\n\nCIFAR-10 包含来自 10 个类别的图像。这些类别包括：\n\n- 飞机 (airplane)\n- 汽车 (automobile)\n- 鸟类 (bird)\n- 猫 (cat)\n- 鹿 (deer)\n- 狗 (dog)\n- 青蛙 (frog)\n- 马 (horse)\n- 船 (ship)\n- 卡车 (truck)\n\n本案例主要：\n\n- 使用`pytorch`进行[ResNet50](https://arxiv.org/abs/1512.03385)(残差神经网络)网络的构建、模型训练与评估\n- 使用`swanlab` 跟踪超参数、记录指标和可视化监控整个训练周期",
    "96": "一级标题：CIFAR10 图像分类\n二级标题：环境安装\n内容：\n本案例基于`Python>=3.8`，请在您的计算机上安装好Python。\n环境依赖：\n```\ntorch\ntorchvision\nswanlab\n```\n快速安装命令：\n```bash\npip install torch torchvision swanlab\n```",
    "97": "一级标题：CIFAR10 图像分类\n二级标题：完整代码\n内容：\n```python\nimport os\nimport random\nimport numpy as np\nimport torch\nfrom torch import nn, optim, utils\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.transforms import ToTensor, Compose, Resize, Lambda\nimport swanlab\n\ndef set_seed(seed=42):\n    \"\"\"设置所有随机种子以确保可重复性\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    # 设置CUDA的随机种子\n    if torch.cuda.is_available():\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n# 捕获并可视化前20张图像\ndef log_images(loader, num_images=16):\n    images_logged = 0\n    logged_images = []\n    for images, labels in loader:\n        # images: batch of images, labels: batch of labels\n        for i in range(images.shape[0]):\n            if images_logged < num_images:\n                # 使用swanlab.Image将图像转换为wandb可视化格式\n                logged_images.append(swanlab.Image(images[i], caption=f\"Label: {labels[i]}\", size=(128, 128)))\n                images_logged += 1\n            else:\n                break\n        if images_logged >= num_images:\n            break\n    swanlab.log({\"Preview/CIFAR10\": logged_images}) \n\n\nif __name__ == \"__main__\":\n    # 设置随机种子\n    set_seed(42)\n\n    # 设置device\n    try:\n        use_mps = torch.backends.mps.is_available()\n    except AttributeError:\n        use_mps = False\n\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    elif use_mps:\n        device = \"mps\"\n    else:\n        device = \"cpu\"\n\n    # 初始化swanlab\n    run = swanlab.init(\n        project=\"CIFAR10\",\n        experiment_name=\"resnet50-pretrained\",\n        config={\n            \"model\": \"Resnet50\",\n            \"optim\": \"Adam\",\n            \"lr\": 1e-4,\n            \"batch_size\": 32,\n            \"num_epochs\": 5,\n            \"train_dataset_num\": 45000,\n            \"val_dataset_num\": 5000,\n            \"device\": device,\n            \"num_classes\": 10,\n        },\n    )\n\n    # 定义转换：调整大小并转换为3通道\n    transform = Compose([\n        ToTensor(),\n        Resize((224, 224), antialias=True),  # ResNet期望224x224的输入\n        # Lambda(lambda x: x.repeat(3, 1, 1))  # 将单通道转换为3通道\n    ])\n\n    # 设置训练集、验证集和测试集\n    dataset = CIFAR10(os.getcwd(), train=True, download=True, transform=transform)\n    \n    # 确保划分数量正确\n    total_size = len(dataset)  # 应该是50000\n    train_dataset, val_dataset = utils.data.random_split(\n        dataset, \n        [run.config.train_dataset_num, run.config.val_dataset_num],\n        generator=torch.Generator().manual_seed(42)  # 保持划分的随机性一致\n    )\n\n    train_loader = utils.data.DataLoader(train_dataset, batch_size=run.config.batch_size, shuffle=True)\n    val_loader = utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False)\n\n    # 初始化模型、损失函数和优化器\n    if run.config.model == \"Resnet18\":\n        from torchvision.models import resnet18\n        model = resnet18(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n    elif run.config.model == \"Resnet34\":\n        from torchvision.models import resnet34\n        model = resnet34(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n    elif run.config.model == \"Resnet50\":\n        from torchvision.models import resnet50\n        model = resnet50(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n    elif run.config.model == \"Resnet101\":\n        from torchvision.models import resnet101\n        model = resnet101(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n    elif run.config.model == \"Resnet152\":\n        from torchvision.models import resnet152\n        model = resnet152(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n\n    model.to(torch.device(device))\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=run.config.lr)\n\n    # （可选）看一下数据集的前16张图像\n    log_images(train_loader, 8)\n\n    # 开始训练\n    for epoch in range(1, run.config.num_epochs+1):\n        swanlab.log({\"train/epoch\": epoch}, step=epoch)\n        model.train()  # 确保模型处于训练模式\n        train_correct = 0\n        train_total = 0\n        \n        # 训练循环\n        for iter, batch in enumerate(train_loader):\n            x, y = batch\n            x, y = x.to(device), y.to(device)\n            optimizer.zero_grad()\n            output = model(x)\n            loss = criterion(output, y)\n            loss.backward()\n            optimizer.step()\n\n            # 计算训练准确率\n            _, predicted = torch.max(output, 1)\n            train_total += y.size(0)\n            train_correct += (predicted == y).sum().item()\n\n            if iter % 40 == 0:\n                print(\n                    f\"Epoch [{epoch}/{run.config.num_epochs}], Iteration [{iter + 1}/{len(train_loader)}], Loss: {loss.item()}\"\n                )\n                swanlab.log({\"train/loss\": loss.item()}, step=(epoch - 1) * len(train_loader) + iter)\n\n        # 记录每个epoch的训练准确率\n        train_accuracy = train_correct / train_total\n        swanlab.log({\"train/acc\": train_accuracy}, step=(epoch - 1) * len(train_loader) + iter)\n\n        # 评估\n        model.eval()\n        correct = 0\n        total = 0\n        val_loss = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                x, y = batch\n                x, y = x.to(device), y.to(device)\n                output = model(x)\n                # 计算验证损失\n                loss = criterion(output, y)\n                val_loss += loss.item()\n                # 计算验证准确率\n                _, predicted = torch.max(output, 1)\n                total += y.size(0)\n                correct += (predicted == y).sum().item()\n\n        accuracy = correct / total\n        avg_val_loss = val_loss / len(val_loader)\n        swanlab.log({\n            \"val/acc\": accuracy,\n            \"val/loss\": avg_val_loss,\n            }, step=(epoch - 1) * len(train_loader) + iter)\n```",
    "98": "一级标题：CIFAR10 图像分类\n二级标题：切换其他ResNet模型\n内容：\n上面的代码支持切换以下ResNet模型：\n- ResNet18\n- ResNet34\n- ResNet50\n- ResNet101\n- ResNet152\n\n切换方式非常简单，只需要将`config`的`model`参数修改为对应的模型名称即可，如切换为ResNet50：\n\n```python (5)\n    # 初始化swanlab\n    run = swanlab.init(\n        ...\n        config={\n            \"model\": \"Resnet50\",\n        ...\n        },\n    )\n```\n\n- `config`是如何发挥作用的？ 👉 [设置实验配置](/guide_cloud/experiment_track/set-experiment-config)",
    "99": "一级标题：CIFAR10 图像分类\n二级标题：效果演示\n内容：\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/CIFAR10/runs/5q3sh20ni2zs6p28ja8qm/chart)\n\n![](./cifar10/show.png)",
    "100": "一级标题：DQN-CartPole\n二级标题：什么是DQN？\n内容：\n> 训练过程：[RL-All-In-One](https://swanlab.cn/@ZeyiLin/RL-All-In-One/runs/vjbnl6y3l99k0sqrd0f2s/chart)\n>\n> 代码：[Zeyi-Lin/SwanBook-RL](https://github.com/Zeyi-Lin/SwanBook-RL/blob/main/dqn-cartpole.py)\n>\n> 硬件环境：纯CPU可训，实测M1 Max训练3分30秒\n\nDQN（Deep Q-Network，深度Q网络）是Q-Learning的**深度学习扩展**，通过神经网络替代Q表的方式来解决高维状态空间问题（例如图像输入），开启了**深度强化学习时代**。它在2013年由DeepMind提出，并在**Atari**游戏上取得了突破性表现。\n\n![image](https://imagebed-1301372061.cos.ap-beijing.myqcloud.com/blogs/image-20250207132403-hte9grx.png)\n\n传统的Q-Learning方法很好，但是Q表是个离散的结构，无法处理状态是连续的任务；以及一些状态空间巨大的任务（比如视频游戏），Q表的开销也是无法接受的，所以DQN应运而生。DQN用神经网络（称为QNet）**近似Q函数**，输入状态S，输出所有动作的Q值。\n\n**DQN还做了以下改进：**\n\n1. **经验回放（Experience Replay）** ：存储历史经验(st,at,rt+1,st+1)(st,at,rt+1,st+1)到缓冲区，训练时随机采样，打破数据相关性。\n2. **目标网络（Target Network）** ：使用独立的网络计算目标Q值，减少训练波动。\n3. **端到端训练**：直接从原始输入（如像素）学习，无需人工设计状态特征。\n\n具体DQN原理本文不做过多赘述，结合本文提供的代码和网上其他教程/DeepSeek R1学习，会有更好效果。\n\n#### 二级标题：什么是CartPole推车倒立摆任务？\n内容：\n**CartPole（推车倒立摆）**  是强化学习中经典的基准测试任务，因为其直观可视、方便调试、状态和动作空间小等特性，常用于入门教学和算法验证。它的目标是训练一个智能体（agent）通过左右移动小车，使车顶的杆子尽可能长时间保持竖直不倒。\n\n![image](https://imagebed-1301372061.cos.ap-beijing.myqcloud.com/blogs/20250207134541.png)\n\n* **环境**：小车（cart）可以在水平轨道上左右移动，顶部通过关节连接一根自由摆动的杆子（pole）。\n* **目标**：通过左右移动小车，使杆子的倾斜角度不超出阈值（±12°或±15°），同时小车不超出轨道范围（如轨道长度的±2.4单位）。简单理解为，就是杆子不会倒下里，小车不会飞出屏幕。\n* **状态**：状态空间包含4个连续变量，分别是小车位置（x），小车速度（v），杆子角度（θ），杆子角速度（ω）\n* **动作**：动作空间只有2个离线动作，分别是0（向左移动）或1（向右移动）\n* **奖励机制**：每成功保持杆子不倒+1分，目前是让奖励最大化，即杆子永远不倒\n\n使用`gymnasium`库，启动cartpole环境非常容易，下面是一个简单的示例代码：\n\n```python\nimport gymnasium as gym\n\nenv = gym.make(\"CartPole-v1\")\nstate = env.reset()\ndone = False\n\nwhile not done:\n    action = 0 if state[2] < 0 else 1  # 根据杆子角度简单决策\n    next_state, reward, done, _ = env.step(action)\n    state = next_state\n    env.render()\n```\n\n#### 二级标题：安装环境\n内容：\n首先你需要1个Python>=3.8的环境，然后安装下面的库：\n\n```txt\nswanlab\ngymnasium\nnumpy\ntorch\npygame\nmoviepy\n```\n\n一键安装命令：\n\n```bash\npip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\npip install swanlab gymnasium numpy torch pygame moviepy\n```\n\n#### 二级标题：定义QNet\n内容：\nDQN使用神经网络来近似QLearning中的Q表，这个神经网络被称为QNetwork。\n\nQNetwork的输入是状态向量，输出是动作向量，这里用一个非常简单的神经网络：\n\n```python\nclass QNetwork(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(state_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, action_dim)\n        )\n        self.to(device)  # 将网络移到指定设备\n  \n    def forward(self, x):\n        return self.fc(x)\n```\n\n#### 二级标题：定义DQNAgent\n内容：\nDQNAgent定义了一系列强化学习训练的行为，代码略长，我拿部分内容进行解读：\n\n##### 初始配置\n内容：\n```python\nclass DQNAgent:\n    def __init__(self, state_dim, action_dim):\n        self.q_net = QNetwork(state_dim, action_dim)       # 当前网络\n        self.target_net = QNetwork(state_dim, action_dim)  # 目标网络\n        self.target_net.load_state_dict(self.q_net.state_dict())  # 将目标网络和当前网络初始化一致，避免网络不一致导致的训练波动\n        self.optimizer = optim.Adam(self.q_net.parameters(), lr=1e-3)\n        self.replay_buffer = deque(maxlen=10000)           # 经验回放缓冲区\n\t\tself.update_target_freq = 100  \n```\n\nDQN会定义2个神经网络，分别是q_net和target_net，结构是完全相同的。训练过程中，target_net负责计算预期值，即 **reward + target_net(next_state).max(1)[0]** ，q_net负责计算当前值，训练时将两个值送到MSELoss里计算差值，反向传播后更新q_net的参数；每过update_target_freq步，将q_net的参数赋给target_net。\n\n优化器使用Adam；经验回访缓冲区是最大长度为10000的队列，用于存储历史经验用于训练。\n\n##### 动作选择（ε-贪婪策略）\n内容：\n动作选择的ε-贪婪策略，指的是在当前状态下，选择下一个动作时的两种方式：\n\nA. 随机选择1个动作，这种被称为探索\n\nB. 按照先前训练得到的知识选择动作。\n\n在强化学习训练中，每一步会以epsilon（即ε）的概率选择A，否则选择B：\n\n```python\n    def choose_action(self, state):\n        if np.random.rand() < self.epsilon:\n            return np.random.randint(0, 2)  # CartPole有2个动作（左/右）\n        else:\n            state_tensor = torch.FloatTensor(state).to(device)\n            q_values = self.q_net(state_tensor)\n            return q_values.cpu().detach().numpy().argmax()\n```\n\n在训练中，开始时以高概率随机探索环境，逐渐转向利用学到的知识。\n\n#### 二级标题：完整代码\n内容：\n**下面是DQN训练的完整代码，做了这些事：**\n\n1. 开启gymnasium中的CartPole环境\n2. QAgent按照ε-贪婪策略选择动作，更新状态，训练模型更新q_net参数\n3. 每隔固定的步数，同步target_net的参数\n4. 一共训练600轮，每10轮会进行一次评估，并使用swanlab记录参数\n5. 保存评估时最高reward的模型权重\n6. 使用了经验回放与ε衰减策略\n7. 训练完成后，进行测试，并保存测试视频到本地目录下\n\n**完整代码如下：**\n\n```python\nimport gymnasium as gym\nfrom gymnasium.wrappers import RecordVideo\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom collections import deque\nimport random\nimport swanlab\nimport os\n\n# 设置随机数种子\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# 定义Q网络\nclass QNetwork(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(state_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 64),\n            nn.ReLU(),\n            nn.Linear(64, action_dim)\n        )\n  \n    def forward(self, x):\n        return self.fc(x)\n\n# DQN Agent\nclass DQNAgent:\n    def __init__(self, state_dim, action_dim):\n        self.q_net = QNetwork(state_dim, action_dim)       # 当前网络\n        self.target_net = QNetwork(state_dim, action_dim)  # 目标网络\n        self.target_net.load_state_dict(self.q_net.state_dict())  # 将目标网络和当前网络初始化一致，避免网络不一致导致的训练波动\n        self.best_net = QNetwork(state_dim, action_dim)\n        self.optimizer = optim.Adam(self.q_net.parameters(), lr=1e-3)\n        self.replay_buffer = deque(maxlen=10000)           # 经验回放缓冲区\n        self.batch_size = 64\n        self.gamma = 0.99\n        self.epsilon = 0.1\n        self.update_target_freq = 100  # 目标网络更新频率\n        self.step_count = 0\n        self.best_reward = 0\n        self.best_avg_reward = 0\n        self.eval_episodes = 5  # 评估时的episode数量\n\n    def choose_action(self, state):\n        if np.random.rand() < self.epsilon:\n            return np.random.randint(0, 2)  # CartPole有2个动作（左/右）\n        else:\n            state_tensor = torch.FloatTensor(state)\n            q_values = self.q_net(state_tensor)\n            return q_values.cpu().detach().numpy().argmax()\n\n    def store_experience(self, state, action, reward, next_state, done):\n        self.replay_buffer.append((state, action, reward, next_state, done))\n\n    def train(self):\n        if len(self.replay_buffer) < self.batch_size:\n            return\n      \n        # 从缓冲区随机采样\n        batch = random.sample(self.replay_buffer, self.batch_size)\n        states, actions, rewards, next_states, dones = zip(*batch)\n\n        states = torch.FloatTensor(np.array(states))\n        actions = torch.LongTensor(actions)\n        rewards = torch.FloatTensor(rewards)\n        next_states = torch.FloatTensor(np.array(next_states))\n        dones = torch.FloatTensor(dones)\n\n        # 计算当前Q值\n        current_q = self.q_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n\n        # 计算目标Q值（使用目标网络）\n        with torch.no_grad():\n            next_q = self.target_net(next_states).max(1)[0]\n            target_q = rewards + self.gamma * next_q * (1 - dones)\n\n        # 计算损失并更新网络\n        loss = nn.MSELoss()(current_q, target_q)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        # 定期更新目标网络\n        self.step_count += 1\n        if self.step_count % self.update_target_freq == 0:\n            # 使用深拷贝更新目标网络参数\n            self.target_net.load_state_dict({\n                k: v.clone() for k, v in self.q_net.state_dict().items()\n            })\n\n    def save_model(self, path=\"./output/best_model.pth\"):\n        if not os.path.exists(\"./output\"):\n            os.makedirs(\"./output\")\n        torch.save(self.q_net.state_dict(), path)\n        print(f\"Model saved to {path}\")\n      \n    def evaluate(self, env):\n        \"\"\"评估当前模型的性能\"\"\"\n        original_epsilon = self.epsilon\n        self.epsilon = 0  # 关闭探索\n        total_rewards = []\n\n        for _ in range(self.eval_episodes):\n            state = env.reset()[0]\n            episode_reward = 0\n            while True:\n                action = self.choose_action(state)\n                next_state, reward, done, _, _ = env.step(action)\n                episode_reward += reward\n                state = next_state\n                if done or episode_reward > 2e4:\n                    break\n            total_rewards.append(episode_reward)\n\n        self.epsilon = original_epsilon  # 恢复探索\n        return np.mean(total_rewards)\n\n# 训练过程\nenv = gym.make('CartPole-v1')\nstate_dim = env.observation_space.shape[0]\naction_dim = env.action_space.n\nagent = DQNAgent(state_dim, action_dim)\n\n\n# 初始化SwanLab日志记录器\nswanlab.init(\n    project=\"RL-All-In-One\",\n    experiment_name=\"DQN-CartPole-v1\",\n    config={\n        \"state_dim\": state_dim,\n        \"action_dim\": action_dim,\n        \"batch_size\": agent.batch_size,\n        \"gamma\": agent.gamma,\n        \"epsilon\": agent.epsilon,\n        \"update_target_freq\": agent.update_target_freq,\n        \"replay_buffer_size\": agent.replay_buffer.maxlen,\n        \"learning_rate\": agent.optimizer.param_groups[0]['lr'],\n        \"episode\": 600,\n        \"epsilon_start\": 1.0,\n        \"epsilon_end\": 0.01,\n        \"epsilon_decay\": 0.995,\n    },\n    description=\"增加了初始化目标网络和当前网络一致，避免网络不一致导致的训练波动\"\n)\n\n# ========== 训练阶段 ==========\n\nagent.epsilon = swanlab.config[\"epsilon_start\"]\n\nfor episode in range(swanlab.config[\"episode\"]):\n    state = env.reset()[0]\n    total_reward = 0\n  \n    while True:\n        action = agent.choose_action(state)\n        next_state, reward, done, _, _ = env.step(action)\n        agent.store_experience(state, action, reward, next_state, done)\n        agent.train()\n\n        total_reward += reward\n        state = next_state\n        if done or total_reward > 2e4:\n            break\n  \n    # epsilon是探索系数，随着每一轮训练，epsilon 逐渐减小\n    agent.epsilon = max(swanlab.config[\"epsilon_end\"], agent.epsilon * swanlab.config[\"epsilon_decay\"])  \n  \n    # 每10个episode评估一次模型\n    if episode % 10 == 0:\n        eval_env = gym.make('CartPole-v1')\n        avg_reward = agent.evaluate(eval_env)\n        eval_env.close()\n      \n        if avg_reward > agent.best_avg_reward:\n            agent.best_avg_reward = avg_reward\n            # 深拷贝当前最优模型的参数\n            agent.best_net.load_state_dict({k: v.clone() for k, v in agent.q_net.state_dict().items()})\n            agent.save_model(path=f\"./output/best_model.pth\")\n            print(f\"New best model saved with average reward: {avg_reward}\")\n\n    print(f\"Episode: {episode}, Train Reward: {total_reward}, Best Eval Avg Reward: {agent.best_avg_reward}\")\n  \n    swanlab.log(\n        {\n            \"train/reward\": total_reward,\n            \"eval/best_avg_reward\": agent.best_avg_reward,\n            \"train/epsilon\": agent.epsilon\n        },\n        step=episode,\n    )\n\n# 测试并录制视频\nagent.epsilon = 0  # 关闭探索策略\ntest_env = gym.make('CartPole-v1', render_mode='rgb_array')\ntest_env = RecordVideo(test_env, \"./dqn_videos\", episode_trigger=lambda x: True)  # 保存所有测试回合\nagent.q_net.load_state_dict(agent.best_net.state_dict())  # 使用最佳模型\n\nfor episode in range(3):  # 录制3个测试回合\n    state = test_env.reset()[0]\n    total_reward = 0\n    steps = 0\n  \n    while True:\n        action = agent.choose_action(state)\n        next_state, reward, done, _, _ = test_env.step(action)\n        total_reward += reward\n        state = next_state\n        steps += 1\n      \n        # 限制每个episode最多1500步,约30秒,防止录制时间过长\n        if done or steps >= 1500:\n            break\n  \n    print(f\"Test Episode: {episode}, Reward: {total_reward}\")\n\ntest_env.close()\n```\n\n---\n\n训练用的是SwanLab的记录过程，能更好地分析和总结知识。\n\n在开始训练之前，如果你没有使用过[SwanLab](https://swanlab.cn)，需要去它的官网（[https://swanlab.cn](https://swanlab.cn)）注册一下，然后按下面的步骤复制API Key：\n\n![image](https://imagebed-1301372061.cos.ap-beijing.myqcloud.com/blogs/20250207150845.png)\n\n接下来打开命令行，敲下面的命令：\n\n```python\nswanlab login\n```\n\n在弹出的提示中，把API Key粘贴进去（粘贴进去不会显示任何东西，放心这是正常的），然后按回车，登录完毕！\n\n然后，就可以运行训练代码了。\n\n#### 二级标题：训练结果\n内容：\n训练过程可以看：[RL-All-In-One - SwanLab](https://swanlab.cn/@ZeyiLin/RL-All-In-One/runs/vjbnl6y3l99k0sqrd0f2s/chart)\n\n我的机器是Macbook M1 Max，大概训练了3分30秒。\n\n![image](https://imagebed-1301372061.cos.ap-beijing.myqcloud.com/blogs/20250207151927.png)\n\n可以看到train的reward波动的很厉害，因为随机探索的缘故，但eval（关闭随机探索）可以看到是很快达到了20000分的上限。\n\n下面是训练好的Agent控制倒立摆的30s视频：\n\n<video controls src=\"/assets/rl-video-episode-0.mp4\"></video>",
    "101": "一级标题：FashionMNIST\n二级标题：概述\n内容：\nFashionMNIST 是一个广泛用于测试机器学习算法的图像数据集，特别是在图像识别领域。它由 Zalando 发布，旨在替代传统的 MNIST 数据集，后者主要包含手写数字的图片。FashionMNIST 的设计初衷是提供一个稍微更具挑战性的问题，同时保持与原始 MNIST 数据集相同的图像大小（28x28 像素）和结构（训练集60,000张图片，测试集10,000张图片）。\n\n![fashion-mnist](/assets/example-fashionmnist.png)\n\nFashionMNIST 包含来自 10 个类别的服装和鞋类商品的灰度图像。这些类别包括：\n\n1. T恤/上衣（T-shirt/top）\n2. 裤子（Trouser）\n3. 套头衫（Pullover）\n4. 裙子（Dress）\n5. 外套（Coat）\n6. 凉鞋（Sandal）\n7. 衬衫（Shirt）\n8. 运动鞋（Sneaker）\n9. 包（Bag）\n10. 短靴（Ankle boot）\n\n每个类别都有相同数量的图像，使得这个数据集成为一个平衡的数据集。这些图像的简单性和标准化尺寸使得 FashionMNIST 成为计算机视觉和机器学习领域入门级的理想选择。数据集被广泛用于教育和研究，用于测试各种图像识别方法的效果。\n\n本案例主要：\n\n- 使用`pytorch`进行[ResNet34](https://arxiv.org/abs/1512.03385)(残差神经网络)网络的构建、模型训练与评估\n- 使用`swanlab` 跟踪超参数、记录指标和可视化监控整个训练周期",
    "102": "一级标题：FashionMNIST\n二级标题：环境安装\n内容：\n本案例基于`Python>=3.8`，请在您的计算机上安装好Python。\n环境依赖：\n```\ntorch\ntorchvision\nswanlab\n```\n快速安装命令：\n```bash\npip install torch torchvision swanlab\n```",
    "103": "一级标题：FashionMNIST\n二级标题：完整代码\n内容：\n```python\nimport os\nimport random\nimport numpy as np\nimport torch\nfrom torch import nn, optim, utils\nfrom torchvision.datasets import FashionMNIST\nfrom torchvision.transforms import ToTensor, Compose, Resize, Lambda\nimport swanlab\n\n\ndef set_seed(seed=42):\n    \"\"\"设置所有随机种子以确保可重复性\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    # 设置CUDA的随机种子\n    if torch.cuda.is_available():\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n# 捕获并可视化前20张图像\ndef log_images(loader, num_images=16):\n    images_logged = 0\n    logged_images = []\n    for images, labels in loader:\n        # images: batch of images, labels: batch of labels\n        for i in range(images.shape[0]):\n            if images_logged < num_images:\n                # 使用swanlab.Image将图像转换为wandb可视化格式\n                logged_images.append(swanlab.Image(images[i], caption=f\"Label: {labels[i]}\", size=(128, 128)))\n                images_logged += 1\n            else:\n                break\n        if images_logged >= num_images:\n            break\n    swanlab.log({\"Preview/FashionMNIST\": logged_images})\n\n\nif __name__ == \"__main__\":\n    # 设置随机种子\n    set_seed(42)\n\n    # 设置device\n    try:\n        use_mps = torch.backends.mps.is_available()\n    except AttributeError:\n        use_mps = False\n\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    elif use_mps:\n        device = \"mps\"\n    else:\n        device = \"cpu\"\n\n    # 初始化swanlab\n    run = swanlab.init(\n        project=\"FashionMNIST\",\n        experiment_name=\"resnet50\",\n        config={\n            \"model\": \"Resnet50\",\n            \"optim\": \"Adam\",\n            \"lr\": 1e-4,\n            \"batch_size\": 32,\n            \"num_epochs\": 10,\n            \"train_dataset_num\": 55000,\n            \"val_dataset_num\": 5000,\n            \"device\": device,\n            \"num_classes\": 10,\n        },\n    )\n\n    # 定义转换：调整大小并转换为3通道\n    transform = Compose([\n        ToTensor(),\n        Resize((224, 224), antialias=True),  # ResNet期望224x224的输入\n        Lambda(lambda x: x.repeat(3, 1, 1))  # 将单通道转换为3通道\n    ])\n\n    # 设置训练集、验证集和测试集\n    dataset = FashionMNIST(os.getcwd(), train=True, download=True, transform=transform)\n    train_dataset, val_dataset = utils.data.random_split(\n        dataset, [run.config.train_dataset_num, run.config.val_dataset_num]\n    )\n\n    train_loader = utils.data.DataLoader(train_dataset, batch_size=run.config.batch_size, shuffle=True)\n    val_loader = utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False)\n\n    # 初始化模型、损失函数和优化器\n    if run.config.model == \"Resnet18\":\n        from torchvision.models import resnet18\n        model = resnet18(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n    elif run.config.model == \"Resnet34\":\n        from torchvision.models import resnet34\n        model = resnet34(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n    elif run.config.model == \"Resnet50\":\n        from torchvision.models import resnet50\n        model = resnet50(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n    elif run.config.model == \"Resnet101\":\n        from torchvision.models import resnet101\n        model = resnet101(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n    elif run.config.model == \"Resnet152\":\n        from torchvision.models import resnet152\n        model = resnet152(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, run.config.num_classes)\n\n    model.to(torch.device(device))\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=run.config.lr)\n\n    # （可选）看一下数据集的前8张图像\n    log_images(train_loader, 8)\n\n    # 开始训练\n    for epoch in range(1, run.config.num_epochs+1):\n        swanlab.log({\"train/epoch\": epoch}, step=epoch)\n        model.train()  # 确保模型处于训练模式\n        train_correct = 0\n        train_total = 0\n        \n        # 训练循环\n        for iter, batch in enumerate(train_loader):\n            x, y = batch\n            x, y = x.to(device), y.to(device)\n            optimizer.zero_grad()\n            output = model(x)\n            loss = criterion(output, y)\n            loss.backward()\n            optimizer.step()\n\n            # 计算训练准确率\n            _, predicted = torch.max(output, 1)\n            train_total += y.size(0)\n            train_correct += (predicted == y).sum().item()\n\n            if iter % 40 == 0:\n                print(\n                    f\"Epoch [{epoch}/{run.config.num_epochs}], Iteration [{iter + 1}/{len(train_loader)}], Loss: {loss.item()}\"\n                )\n                swanlab.log({\"train/loss\": loss.item()}, step=(epoch - 1) * len(train_loader) + iter)\n\n        # 记录每个epoch的训练准确率\n        train_accuracy = train_correct / train_total\n        swanlab.log({\"train/acc\": train_accuracy}, step=(epoch - 1) * len(train_loader) + iter)\n\n        #\n        model.eval()\n        correct = 0\n        total = 0\n        val_loss = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                x, y = batch\n                x, y = x.to(device), y.to(device)\n                output = model(x)\n                # 计算验证损失\n                loss = criterion(output, y)\n                val_loss += loss.item()\n                # 计算验证准确率\n                _, predicted = torch.max(output, 1)\n                total += y.size(0)\n                correct += (predicted == y).sum().item()\n\n        accuracy = correct / total\n        avg_val_loss = val_loss / len(val_loader)\n        swanlab.log({\n            \"val/acc\": accuracy,\n            \"val/loss\": avg_val_loss,\n            }, step=(epoch - 1) * len(train_loader) + iter)\n```",
    "104": "一级标题：FashionMNIST\n二级标题：切换其他ResNet模型\n内容：\n上面的代码支持切换以下ResNet模型：\n- ResNet18\n- ResNet34\n- ResNet50\n- ResNet101\n- ResNet152\n\n切换方式非常简单，只需要将`config`的`model`参数修改为对应的模型名称即可，如切换为ResNet50：\n\n```python (5)\n    # 初始化swanlab\n    run = swanlab.init(\n        ...\n        config={\n            \"model\": \"Resnet50\",\n        ...\n        },\n    )\n```\n\n- `config`是如何发挥作用的？ 👉 [设置实验配置](/guide_cloud/experiment_track/set-experiment-config)",
    "105": "一级标题：FashionMNIST\n二级标题：效果演示\n内容：\n![](/assets/example-fashionmnist-show.jpg)",
    "106": "一级标题：使用ChatGLM4进行大模型指令遵从微调（附代码和测试脚本）\n二级标题：摘要\n内容：\n![instruct](./images/glm4-instruct/instruct.png)\n\n本教程主要实现了一个大模型的指令遵从微调方法。为了便于实现，减少代码量，本文使用了🤗HuggingFace的TRL框架实现。该框架除了支持SFT外，对DPO、PPO、GRPO等流行的强化微调算法都有很好的支持。\n\n虽然使用框架能够极大的减少工作量，但是不可避免的为新手学习带来了困扰。因此本教程会尽量附上完整的文档引用来帮助读者进一步学习框架。诚然从使用pytorch实现微调过程能够极大的提升对过程的理解，社区也有相当多优秀的项目。但是笔者仍推荐大家多使用框架来完成训练，这样可以减少大量的时间来让大家更专注于创新。\n\n因此本教程建议对🤗HuggingFace Transformers框架有一定基础的读者阅读～。\n\n注意：由于ChatGLM的模型相对较大，实际运行大概需要显存>=16G\n\n🎉 **SwanLab被官方集成进入了🤗HuggingFace Transformers：** 如果本地环境安装了SwanLab会默认开启！也可以通过`report_to=\"swanlab\"`开启训练跟踪。\n\n![swanlabxhuggingface](./images/glm4-instruct/swanlabxhuggingface.png)\n\n**参考资料：**\n\n* 智谱AI官网：[https://www.zhipuai.cn/](https://www.zhipuai.cn/)\n\n* ChatGLM-9B基座模型：[https://huggingface.co/THUDM/glm-4-9b-hf](https://huggingface.co/THUDM/glm-4-9b-hf/tree/main)\n\n* ChatGLM-9B-Chat模型：[https://huggingface.co/THUDM/glm-4-9b-chat-hf](https://huggingface.co/THUDM/glm-4-9b-chat-hf/tree/main)\n\n* Alpaca数据集中文版：[https://huggingface.co/datasets/llamafactory/alpaca_gpt4_zh](https://huggingface.co/datasets/llamafactory/alpaca_gpt4_zh)\n\n* 本博客开源项目链接：[https://github.com/SwanHubX/glm4-finetune](https://github.com/SwanHubX/glm4-finetune)\n\n* SwanLab训练日志查看：[https://swanlab.cn/@ShaohonChen/chatglm-finetune/](https://swanlab.cn/@ShaohonChen/chatglm-finetune/)",
    "107": "一级标题：使用ChatGLM4进行大模型指令遵从微调\n二级标题：TRL包介绍+环境准备\n内容：\n![trl](./images/glm4-instruct/trl.png)\n\n本教程使用[🤗HuggingFace TRL](https://huggingface.co/docs/trl/index)框架来完成微调代码的实现。TRL是一个强大且便于使用的微调框架，除了支持SFT外，也能轻松的通过接口调用DPO、PPO、GRPO等流行的强化微调算法。此外也完美兼容Transformers架构。\n\n首先是安装本教程的环境，安装命令如下：\n\n```bash\npip install transformers trl datasets peft swanlab\n```\n\n其中`transformers trl peft`用于模型的加载和训练，`datasets`用于导入数据集，`swanlab`用于对训练过程可视化跟踪。\n\n下面列举一个简单的微调案例来介绍HF TRL框架的使用方法：\n\n```python\nfrom datasets import load_dataset\nfrom trl import SFTConfig, SFTTrainer\n\ndataset = load_dataset(\"stanfordnlp/imdb\", split=\"train\")   # 设置微调数据集，此处使用IMDB电影评论分类数据\n\ntraining_args = SFTConfig(  # 设置微调参数\n    max_length=512,\n    output_dir=\"/tmp\",\n)\ntrainer = SFTTrainer(   # 设置模型，此处使用facebook的opt-350M，参数量比较小便于下载\n    \"facebook/opt-350m\",\n    train_dataset=dataset,\n    args=training_args,\n)\ntrainer.train() # 开始训练，流程和TRL一样\n```\n\n上面的代码来自HF官方文档[https://huggingface.co/docs/trl/sft_trainer](https://huggingface.co/docs/trl/sft_trainer)，增加了注释便于读者理解。\n\n简单来说TRL包的使用方法和Transformers类似，不过多了两步：\n\n* 导入`SFTConfig`模块，这个模块基于`transformers`的`TrainingArguments`，不过针对SFT引入了一点额外的参数，以及lora的支持参数\n\n* 导入`SFTTrainer`模块，这个模块包含了SFT的代码实现，还有一些对`peft`的lora支持和数据集格式转换代码。\n\n后文将完整的介绍如何使用TRL包完成大模型的指令遵从功能。",
    "108": "一级标题：使用ChatGLM4进行大模型指令遵从微调\n二级标题：GLM4介绍+模型准备\n内容：\n![chatglm_history](images/glm4-instruct/chatglm_history.png)\n\nGLM-4-9B是[智谱AI](https://www.zhipuai.cn/)推出的最新一代预训练模型GLM-4系列中的开源版本。ChatGLM发布了多个版本，其中GLM-4-9B是第四代基座模型，其微调版本GLM-4-9B-Chat具备网页浏览、代码执行、自定义工具调用（Function Call）和长文本推理（支持最大 128K 上下文）等高级功能。\n\n本教程使用GLM-4-9B模型进行指令遵从功能微调，并使用SwanLab进行模型的结果跟踪。\n\n⚠️注意：ChatGLM为了配合Huggingface Transformers更新，发布了两个版本权重`THUDM/glm-4-9b`和`THUDM/glm-4-9b-hf`，后者对应更为新版本的transformers，因此本教程使用后者的权重。\n\n本教程以经提供好了下载模型的脚本，下载模型的方法如下：\n\n```bash\nhuggingface-cli download --local-dir ./weights/glm-4-9b-hf THUDM/glm-4-9b-hf\n```\n\n模型将会下载在项目目录下的`./weights/glm-4-9b-hf`中\n\n下面列举一个使用`transformers`加载ChatGLM模型并进行推理的代码：\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ndevice = \"cuda\"\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/glm-4-9b-chat-hf\")\nmodel = AutoModelForCausalLM.from_pretrained(\"THUDM/glm-4-9b-chat-hf\").eval().to(device)\ninputs = tokenizer.encode(\"我是ChatGLM，是\", return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n```\n\n由于是基座模型，没经过微调，因此模型只会完成`\"我是ChatGLM，是\"`这段文本的后续补全，运行后会生成如下代码：\n\n```bash\nLoading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.35it/s]\n[gMASK]<sop>我是ChatGLM，是人工智能助手。我是ChatGLM，是人工智能助手。我是ChatGLM，是人工智能助手\n```\n\n当然上面的例子是一个基座模型推理的例子，该模型只能进行文本生成，如果希望使用对话能力，还是需要加载已经微调好的对话模型，代码如下：\n\n```python\nfrom transformers import pipeline\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"你是谁\"},\n]\npipe = pipeline(\"text-generation\", model=\"THUDM/glm-4-9b-chat-hf\")\nprint(pipe(messages))\n```\n\n此处我们换了种推理接口，直接使用pipeline完成推理，运行后将会生成如下信息\n\n```bash\nLoading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.24it/s]\nDevice set to use cuda:0\n[{'generated_text': [{'role': 'user', 'content': '你是谁'}, {'role': 'assistant', 'content': '\\n我是一个人工智能助手，名为 ChatGLM。我是基于清华大学 KEG 实验室和'}]}]\n```\n\n使用`print(model)`将模型的结构打印出来，展示如下：\n\n```text\nGlmForCausalLM(\n  (model): GlmModel(\n    (embed_tokens): Embedding(151552, 4096, padding_idx=151329)\n    (layers): ModuleList(\n      (0-39): 40 x GlmDecoderLayer(\n        (self_attn): GlmAttention(\n          (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n          (k_proj): Linear(in_features=4096, out_features=256, bias=True)\n          (v_proj): Linear(in_features=4096, out_features=256, bias=True)\n          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n        )\n        (mlp): GlmMLP(\n          (gate_up_proj): Linear(in_features=4096, out_features=27392, bias=False)\n          (down_proj): Linear(in_features=13696, out_features=4096, bias=False)\n          (activation_fn): SiLU()\n        )\n        (input_layernorm): GlmRMSNorm((4096,), eps=1.5625e-07)\n        (post_attention_layernorm): GlmRMSNorm((4096,), eps=1.5625e-07)\n      )\n    )\n    (norm): GlmRMSNorm((4096,), eps=1.5625e-07)\n    (rotary_emb): GlmRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=151552, bias=False)\n)\n```\n\n可以看到GLM模型的层数达到了惊人的40层😂，因此本身使用Lora进行微调时其可训练参数会比其他模型大一些。",
    "109": "一级标题：使用ChatGLM4进行大模型指令遵从微调\n二级标题：数据集准备\n内容：\n数据集我已经提前包括在了github项目当中，可以直接使用如下命令下载完整的实验代码\n\n```bash\ngit clone https://github.com/SwanHubX/glm4-finetune.git\n```\n\n如果只想下载数据集，可以直接下载如下文件：\n\n```bash\nwget https://github.com/SwanHubX/glm4-finetune/blob/main/data/alpaca_gpt4_data_zh.json\n```\n\n也可以通过🤗huggingface上下载：[https://huggingface.co/datasets/llamafactory/alpaca_gpt4_zh](https://huggingface.co/datasets/llamafactory/alpaca_gpt4_zh)",
    "110": "一级标题：使用ChatGLM4进行大模型指令遵从微调\n二级标题：代码说明+超参数调整\n内容：\n完整的微调代码公开在了GitHub上，使用如下命令即可下载\n\n```bash\ngit clone https://github.com/SwanHubX/glm4-finetune.git\n```\n\n文章的附件中也有完整的实现代码[#代码附件](#附件完整代码)\n\n本文接下来重点介绍各个代码的功能模块\n\n加载模型的超参数设置，这里可以重点关注lora参数的设置，本文lora参数参考了ChatGLM官方微调代码的lora参数设置\n\n这里要注意学习率为5e-4，如果是全量微调要小一个数量级。\n\n```python\n@dataclass\nclass ChatGLM4ModelConfig(ModelConfig):\n    model_name_or_path: Optional[str] = field(\n        default=\"./weights/glm-4-9b-hf\",\n        metadata={\n            \"help\": \"Model checkpoint for weights initialization. default used glm4\"\n        },\n    )\n    torch_dtype: Optional[str] = field(\n        default=\"bfloat16\",\n        metadata={\n            \"help\": \"Override the default `torch.dtype` and load the model under this dtype.\",\n            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n        },\n    )\n    use_peft: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to use PEFT for training. Default true\"},\n    )\n    lora_r: int = field(\n        default=8,\n        metadata={\"help\": \"LoRA R value.\"},\n    )\n    lora_alpha: int = field(\n        default=32,\n        metadata={\"help\": \"LoRA alpha.\"},\n    )\n    lora_dropout: float = field(\n        default=0.1,\n        metadata={\"help\": \"LoRA dropout.\"},\n    )\n    lora_target_modules: Optional[list[str]] = field(\n        default_factory=lambda: [\"q_proj\", \"k_proj\", \"v_proj\"],\n        metadata={\"help\": \"LoRA target modules.\"},\n    )\n```\n\n数据集超参数设置，这里比较简单，只是加载了本地的数据集\n\n```python\n@dataclass\nclass DataTrainingArguments:\n    data_files: Optional[str] = field(\n        default=\"./data/alpaca_gpt4_data_zh.json.json\",\n        metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"},\n    )\n```\n\n不过为了方便读者理解数据集长什么样，仍旧提供数据集展示脚本\n\n```python\nimport datasets\nraw_dataset=datasets.load_dataset(\"json\", data_files=\"data/glaive_toolcall_zh_1k.json\")\nprint(raw_dataset)\n\"\"\"打印内容\nDatasetDict({\n    train: Dataset({\n        features: ['instruction', 'input', 'output'],\n        num_rows: 42677\n    })\n})\n\"\"\"\n```\n\n可以看到数据一共有1000条，并且包括`'conversations', 'tools'`两个字段\n\n进一步选取其中一条打印：\n\n```python\nprint(raw_dataset[\"train\"][0])\n```\n\n输出如下：\n\n```json\n{\n    \"instruction\": \"保持健康的三个提示。\",\n    \"input\": \"\",\n    \"output\": \"以下是保持健康的三个提示：\\n\\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\\n\\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\\n\\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。\"\n}\n```\n\n这里大家会注意到为什么会有Instruct和input两部分。实际上早期针对指令遵从的研究是为了获得一个通用的任务处理模型（比如既能做翻译又能做计算这样），因此我们通常把对任务的描述放到instruct中，将实际的任务文本放在input中。\n但是随着ChatGPT这种通用的AI助理出现，大家已经逐渐习惯直接下指令让其执行了。因此instruct和prompt的这种分离就显得没那么有必要了。实际上无论分离和不分离模型的本质都是根据前文补后文。因此分离不分离对模型的最终结果不会有太大影响，无非就是格式的不同。\n现在的开源Chat大语言模型流行把“人设”放在“system prompt”中，把用户的指令放在input中，因此后文我们会将Alpaca数据集处理成更适应于主流Chat的格式。\n\nChatGLM提供的推荐输入微调数据结构如下：\n\n```json\n{\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"类型#裤*材质#牛仔布*风格#性感\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"3x1的这款牛仔裤采用浅白的牛仔面料为裤身材质，其柔然的手感和细腻的质地，在穿着舒适的同时，透露着清纯甜美的个性气质。除此之外，流畅的裤身剪裁将性感的腿部曲线彰显的淋漓尽致，不失为一款随性出街的必备单品。\"\n    }\n  ]\n}\n```\n\n这里可能有一定经验的读者会说，不对呀，我们从0训练我们当然可以定义自己的数据结构。这么想是对的，但是让我们能够直接使用ChatGLM原生的`chat_template`，我还是建议咱们遵守chatglm官方定义的数据格式，这么做的话既能兼容ChatGLM的很多工具，又能充分利用官方定义的special_token。\n\n我们可以通过HuggingFace上开源的`glm-4-9b-chat-hf`的`tokenizer_config.json`中可以找到他们的原生`chat_template`，下面的脚本提供一个打印`chat_template`的代码\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ndevice = \"cuda\"\ntokenizer = AutoTokenizer.from_pretrained(\"THUDM/glm-4-9b-chat-hf\")\nprint(tokenizer.chat_template)\n```\n\n获取tokenizer配置的链接[https://huggingface.co/THUDM/glm-4-9b-chat-hf/blob/main/tokenizer_config.json](https://huggingface.co/THUDM/glm-4-9b-chat-hf/blob/main/tokenizer_config.json)\n\n这里我们简单打印一下转换完成后数据集最终的一个效果，参考脚本如下：\n\n```python\ndef formatting_func(example):\n    \"\"\"\n    process data format\n    \"\"\"\n    prompt = example[\"instruction\"]\n    if len(example[\"input\"]) != 0:\n        prompt += \"\\n\\n\" + example[\"input\"]\n    conversations = [\n        {\"role\": \"user\", \"content\": prompt},\n        {\"role\": \"assistant\", \"content\": example[\"output\"]},\n    ]\n    output_text = tokenizer.apply_chat_template(\n        conversation=conversations, tokenize=False\n    )\n    return output_text\n```\n\n输出效果如下，以下字段便是实际运用于模型微调时，输入给模型的数据样式：\n\n```text\n[gMASK]<sop><|user|>\n保持健康的三个提示。<|assistant|>\n以下是保持健康的三个提示：\n\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\n\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\n\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。\n\n最后便是训练的超参数设置和训练过程的实现，这里由于数据规模比较小，我们训练600个steps，每个GPU实际batch大小为1*4：\n```python\n# Train kwargs\n@dataclass\nclass MySFTConfig(SFTConfig):\n    output_dir: Optional[str] = field(\n        default=\"./output/lora-glm4-9b-alpaca\",\n        metadata={\n            \"help\": \"The output directory where the model predictions and checkpoints will be written. Defaults to 'lora-glm4-9b-toolcall' if not provided.\"\n        },\n    )\n    num_train_epochs: float = field(\n        default=3.0, metadata={\"help\": \"Total number of training epochs to perform.\"}\n    )\n    per_device_train_batch_size: int = field(\n        default=2,\n        metadata={\"help\": \"Batch size per GPU/TPU/MPS/NPU core/CPU for training.\"},\n    )\n    per_device_eval_batch_size: int = field(\n        default=4,\n        metadata={\"help\": \"Batch size per GPU/TPU/MPS/NPU core/CPU for evaluation.\"},\n    )\n    gradient_accumulation_steps: int = field(\n        default=1,\n        metadata={\n            \"help\": \"Number of updates steps to accumulate before performing a backward/update pass.\"\n        },\n    )\n    learning_rate: float = field(\n        default=5e-4, metadata={\"help\": \"The initial learning rate for AdamW.\"}\n    )\n    bf16: bool = field(\n        default=True,\n        metadata={\n            \"help\": (\n                \"Whether to use bf16 (mixed) precision instead of 32-bit. Requires Ampere or higher NVIDIA\"\n                \" architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change.\"\n            )\n        },\n    )\n    bf16_full_eval: bool = field(\n        default=True,\n        metadata={\n            \"help\": (\n                \"Whether to use full bfloat16 evaluation instead of 32-bit. This is an experimental API and it may\"\n                \" change.\"\n            )\n        },\n    )\n    max_seq_length: Optional[int] = field(\n        default=512,\n        metadata={\n            \"help\": \"Maximum length of the tokenized sequence. Sequences longer than `max_seq_length` are truncated \"\n            \"from the right. If `None`, no truncation is applied. When packing is enabled, this value sets the \"\n            \"sequence length.\"\n        },\n    )\n    eval_strategy: Union[str] = field(\n        default=\"steps\",\n        metadata={\"help\": \"The evaluation strategy to use.\"},\n    )\n    eval_steps: Optional[float] = field(\n        default=0.1,\n        metadata={\n            \"help\": (\n                \"Run an evaluation every X steps. Should be an integer or a float in range `[0,1)`. \"\n                \"If smaller than 1, will be interpreted as ratio of total training steps.\"\n            )\n        },\n    )\n    logging_steps: float = field(\n        default=10,\n        metadata={\n            \"help\": (\n                \"Log every X updates steps. Should be an integer or a float in range `[0,1)`. \"\n                \"If smaller than 1, will be interpreted as ratio of total training steps.\"\n            )\n        },\n    )\n    save_steps: float = field(\n        default=0.1,\n        metadata={\n            \"help\": (\n                \"Save checkpoint every X updates steps. Should be an integer or a float in range `[0,1)`. \"\n                \"If smaller than 1, will be interpreted as ratio of total training steps.\"\n            )\n        },\n    )\n```\n训练的流程这块如下,使用HF TRL后流程变得非常简洁。\n```python\n# Training\ntrainer = SFTTrainer(\n    model=model_args.model_name_or_path,\n    args=training_args,\n    data_collator=None,\n    train_dataset=raw_datasets[\"train\"],\n    eval_dataset=(\n        raw_datasets[\"test\"] if training_args.eval_strategy != \"no\" else None\n    ),\n    processing_class=tokenizer,\n    peft_config=get_peft_config(model_args),\n    formatting_func=formatting_func,\n    callbacks=[SavePredictCallback()],\n)\ntrainer.train()\n```",
    "111": "一级标题：使用ChatGLM4进行大模型指令遵从微调\n二级标题：启动训练+效果评测\n本代码在实现训练时默认是开启SwanLab的。SwanLab被官方集成进入了🤗HuggingFace Transformers。可以通过report_to=\"swanlab\"开启训练跟踪。如果本地环境安装了SwanLab会默认开启！\n\n启动训练的命令如下：\n```python\npython instruct_train.py\n```\n可以看到如下启动信息，如果没登录SwanLab可能会弹出登录提示，这里推荐选择1并在https://swanlab.cn完成注册。即可在线查看到训练进展。\n登陆命令如下\n```bash\nswanlab login\n```\n点击打印出的链接即可通过看板查看训练日志。\n\n通过配置callback，SwanLab还能自动记录模型的预测输出，代码和效果如下：\n```python\n# Print prediction text callback\nclass SavePredictCallback(TrainerCallback):\n    def __init__(self, num_steps=10):\n        self.num_steps = num_steps\n\n    def on_save(self, args, state, control, model, processing_class, **kwargs):\n        if state.is_world_process_zero:\n            tokenizer = processing_class\n            batch_test_message = [\n                [{\"role\": \"user\", \"content\": \"你好，告诉我你的名字。\"}],\n                [{\"role\": \"user\", \"content\": \"告诉我1+2等于多少？\"}],\n            ]\n            batch_inputs_text = tokenizer.apply_chat_template(\n                batch_test_message,\n                return_tensors=\"pt\",\n                return_dict=True,\n                padding=True,\n                padding_side=\"left\",\n                add_generation_prompt=True,\n            ).to(model.device)\n\n            # print(batch_inputs_text)\n            outputs = model.generate(**batch_inputs_text, max_new_tokens=512)\n            batch_reponse = tokenizer.batch_decode(\n                outputs, skip_special_tokens=False\n            )\n            log_text_list = [swanlab.Text(response) for response in batch_reponse]\n            swanlab.log({\"Prediction\": log_text_list}, step=state.global_step)\n```\n多卡实验:\n如果你的卡数比较多，推荐使用多卡训练来极大提升训练速度！首先安装huggingface accelerate和deepspeed来方便的开启zero2多卡训练：\n```bash\npip install accelerate deepspeed\n```\n接下来使用如下命令来开启多卡训练（默认8GPU，可更改num_processes参数为实际卡数）：\n```bash\naccelerate launch --num_processes 8 --config_file configs/zero2.yaml instruct_train.py\n```\n关于zero2的详细设置在configs/zero2.yaml中。\n模型将会保存在output/lora-glm4-9b-alpaca，由于笔者的硬盘空间有限，因此仅仅保存Lora权重，推理加载时也要记得加载原始模型。\n推理+效果对比\n可以通过使用如下命令进行命令行聊天：\n```bash\nbash chat_cli.py\n```",
    "112": "一级标题：使用ChatGLM4进行大模型指令遵从微调\n二级标题：附件-完整代码\n完整代码如下，推荐还是通过使用github获得完整的代码\nhttps://github.com/SwanHubX/glm4-finetune\n记得帮忙点个star🌟",
    "113": "一级标题：Hello World\n二级标题：环境准备\n内容：\n```bash\npip install swanlab\n```",
    "114": "一级标题：Hello World\n二级标题：完整代码\n内容：\n```python\nimport swanlab\nimport random\n\noffset = random.random() / 5\n\n# 初始化SwanLab\nrun = swanlab.init(\n    project=\"my-project\",\n    config={\n        \"learning_rate\": 0.01,\n        \"epochs\": 10,\n    },\n)\n\n# 模拟训练过程\nfor epoch in range(2, run.config.epochs):\n    acc = 1 - 2**-epoch - random.random() / epoch - offset\n    loss = 2**-epoch + random.random() / epoch + offset\n    print(f\"epoch={epoch}, accuracy={acc}, loss={loss}\")\n\n    swanlab.log({\"accuracy\": acc, \"loss\": loss})  # 记录指标\n```",
    "115": "一级标题：LSTM股票预测\n二级标题：概述\n内容：\nLSTM（Long Short-Term Memory），即长短时记忆网络，是一种特殊的RNN（递归神经网络），它改进了传统RNN在处理长序列数据时存在的梯度消失和梯度爆炸问题。LSTM由Hochreiter和Schmidhuber于1997年提出，已成为处理**时间序列数据**的经典模型之一。\n\n![](/assets/example-lstm-1.png)\n\n股票预测任务指的是根据一支股票的过去一段时间的数据，通过AI模型预测现在以及未来的股价变化，也是一种实用的时间序列任务。这里我们使用2016～2021年的Google股价数据数据集来进行训练和推理。",
    "116": "一级标题：LSTM股票预测\n二级标题：环境安装\n内容：\n本案例基于`Python>=3.8`，请在您的计算机上安装好Python。 环境依赖：\n\n```txt\npandas\ntorch\nmatplotlib\nswanlab\nscikit-learn\n```\n\n快速安装命令：\n\n```bash\npip install pandas torch matplotlib swanlab scikit-learn\n```\n\n> 本代码测试于torch==2.3.0、pandas==2.0.3、matplotlib==3.8.2、swanlab==0.3.8、scikit-learn==1.3.2",
    "117": "一级标题：LSTM股票预测\n二级标题：完整代码\n内容：\n请先在[Kaggle](https://www.kaggle.com/datasets/shreenidhihipparagi/google-stock-prediction)下载Google Stock Prediction数据集到根目录下。\n\n```python\nimport os\nimport swanlab\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nfrom copy import deepcopy as dc\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nclass LSTMModel(nn.Module):\n    \"\"\"\n    定义模型类\n    \"\"\"\n    def __init__(self, input_size=1, hidden_size1=50, hidden_size2=64, fc1_size=32, fc2_size=16, output_size=1):\n        super(LSTMModel, self).__init__()\n        self.lstm1 = nn.LSTM(input_size, hidden_size1, batch_first=True)\n        self.lstm2 = nn.LSTM(hidden_size1, hidden_size2, batch_first=True)\n        self.fc1 = nn.Linear(hidden_size2, fc1_size)\n        self.fc2 = nn.Linear(fc1_size, fc2_size)\n        self.fc3 = nn.Linear(fc2_size, output_size)\n\n    def forward(self, x):\n        x, _ = self.lstm1(x)\n        x, _ = self.lstm2(x)\n        x = self.fc1(x[:, -1, :])\n        x = self.fc2(x)\n        x = self.fc3(x)\n        return x\n\n\nclass TimeSeriesDataset(Dataset):\n    \"\"\"\n    定义数据集类\n    \"\"\"\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, i):\n        return self.X[i], self.y[i]\n\n\ndef prepare_dataframe_for_lstm(df, n_steps):\n    \"\"\"\n    处理数据集，使其适用于LSTM模型\n    \"\"\"\n    df = dc(df)\n    df['date'] = pd.to_datetime(df['date'])\n    df.set_index('date', inplace=True)\n    \n    for i in range(1, n_steps+1):\n        df[f'close(t-{i})'] = df['close'].shift(i)\n        \n    df.dropna(inplace=True)\n    return df\n\n\ndef get_dataset(file_path, lookback, split_ratio=0.9):\n    \"\"\"\n    归一化数据、划分训练集和测试集\n    \"\"\"\n    data = pd.read_csv(file_path)\n    data = data[['date','close']]\n    \n    shifted_df_as_np = prepare_dataframe_for_lstm(data, lookback)\n\n    scaler = MinMaxScaler(feature_range=(-1,1))\n    shifted_df_as_np = scaler.fit_transform(shifted_df_as_np)\n\n    X = shifted_df_as_np[:, 1:]\n    y = shifted_df_as_np[:, 0]\n\n    X = dc(np.flip(X,axis=1))\n\n    # 划分训练集和测试集\n    split_index = int(len(X) * split_ratio)\n    \n    X_train = X[:split_index]\n    X_test = X[split_index:]\n\n    y_train = y[:split_index]\n    y_test = y[split_index:]\n\n    X_train = X_train.reshape((-1, lookback, 1))\n    X_test = X_test.reshape((-1, lookback, 1))\n\n    y_train = y_train.reshape((-1, 1))\n    y_test = y_test.reshape((-1, 1))\n\n    # 转换为Tensor\n    X_train = torch.tensor(X_train).float()\n    y_train = torch.tensor(y_train).float()\n    X_test = torch.tensor(X_test).float()\n    y_test = torch.tensor(y_test).float()\n    \n    return scaler, X_train, X_test, y_train, y_test\n\n\ndef train(model, train_loader, optimizer, criterion):\n        model.train()\n        running_loss = 0\n        # 训练\n        for i, batch in enumerate(train_loader):\n            x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n            y_pred = model(x_batch)\n            loss = criterion(y_pred, y_batch)\n            running_loss += loss.item()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        \n        avg_loss_epoch = running_loss / len(train_loader)\n        print(f'Epoch: {epoch}, Batch: {i}, Avg. Loss: {avg_loss_epoch}')\n        swanlab.log({\"train/loss\": running_loss}, step=epoch)\n        running_loss = 0\n\n\ndef validate(model, test_loader, criterion, epoch):\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for _, batch in enumerate(test_loader):\n            x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n            y_pred = model(x_batch)\n            loss = criterion(y_pred, y_batch)\n            val_loss += loss.item()\n        avg_val_loss = val_loss / len(test_loader)\n        print(f'Epoch: {epoch}, Validation Loss: {avg_val_loss}')\n        swanlab.log({\"val/loss\": avg_val_loss}, step=epoch)\n       \n       \ndef inverse_transform_and_extract(scaler, data, lookback):\n    dummies = np.zeros((data.shape[0], lookback + 1))\n    dummies[:, 0] = data.flatten()\n    return dc(scaler.inverse_transform(dummies)[:, 0])\n\n\ndef plot_predictions(actual, predicted, title, xlabel='Date', ylabel='Close Price'):\n    \"\"\"\n    绘制最后的股价预测与真实值的对比图\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.plot(actual, color='red', label='Actual Close Price')\n    plt.plot(predicted, color='blue', label='Predicted Close Price', alpha=0.5)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(title)\n    plt.legend()\n    return swanlab.Image(plt, caption=title)\n\n\ndef visualize_predictions(train_predictions, val_predictions, scaler, y_train, y_test, lookback):    \n    train_predictions = inverse_transform_and_extract(scaler, train_predictions, lookback)\n    val_predictions = inverse_transform_and_extract(scaler, val_predictions, lookback)\n    new_y_train = inverse_transform_and_extract(scaler, y_train, lookback)\n    new_y_test = inverse_transform_and_extract(scaler, y_test, lookback)\n\n    plt_image = []\n    plt_image.append(plot_predictions(new_y_train, train_predictions, '(TrainSet) Google Stock Price Prediction with LSTM'))\n    plt_image.append(plot_predictions(new_y_test, val_predictions, '(TestSet) Google Stock Price Prediction with LSTM'))\n\n    swanlab.log({\"Prediction\": plt_image})\n\n\nif __name__ == '__main__':\n    # ------------------- 初始化一个SwanLab实验 -------------------\n    swanlab.init(\n        project='Google-Stock-Prediction',\n        experiment_name=\"LSTM\",\n        description=\"根据前7天的数据预测下一日股价\",\n        config={ \n            \"learning_rate\": 1e-3,\n            \"epochs\": 100,\n            \"batch_size\": 32,\n            \"lookback\": 7,\n            \"spilt_ratio\": 0.9, \n            \"save_path\": \"./checkpoint\",\n            \"optimizer\": \"Adam\",\n            \"device\": 'cuda' if torch.cuda.is_available() else 'cpu',\n        },\n    )\n    \n    config = swanlab.config\n    device = torch.device(config.device)\n    \n    # ------------------- 定义数据集 -------------------\n    scaler, X_train, X_test, y_train, y_test = get_dataset(file_path='./GOOG.csv',\n                                                           lookback=config.lookback,\n                                                           split_ratio=config.spilt_ratio,)\n    \n    train_dataset = TimeSeriesDataset(X_train, y_train)\n    test_dataset = TimeSeriesDataset(X_test, y_test)\n\n    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n\n    # ------------------- 定义模型、超参数 -------------------\n    model = LSTMModel(input_size=1, output_size=1)\n\n    model = model.to(device)\n    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n    criterion = nn.MSELoss()\n\n    # ------------------- 训练与验证 -------------------\n    for epoch in range(1, config.epochs+1):\n        train(model, train_loader, optimizer, criterion)\n        validate(model, test_loader, criterion, epoch)\n        \n    # ------------------- 使用最佳模型推理，与生成可视化结果 -------------------\n    with torch.no_grad():\n        model.eval()\n        train_predictions = model(X_train.to(device)).to('cpu').numpy()\n        val_predictions = model(X_test.to(device)).to('cpu').numpy()\n        visualize_predictions(train_predictions, val_predictions, scaler, y_train, y_test, config.lookback)\n    \n    # ------------------- 保存模型 -------------------\n    model_save_path = os.path.join(config.save_path, 'lstm.pth')\n    if not os.path.exists(config.save_path):\n        os.makedirs(config.save_path)\n    torch.save(model.state_dict(), model_save_path)\n```",
    "118": "一级标题：LSTM股票预测\n二级标题：演示效果\n内容：\n![](/assets/example-lstm-2.png)",
    "119": "一级标题：MNIST手写体识别\n二级标题：概述\n内容：\nMNIST手写体识别是深度学习最经典的入门任务之一，由 LeCun 等人提出。  \n该任务基于[MNIST数据集](https://paperswithcode.com/dataset/mnist)，研究者通过构建机器学习模型，来识别10个手写数字（0～9）。\n\n![mnist](/assets/mnist.jpg)\n\n本案例主要：\n- 使用`pytorch`进行CNN（卷积神经网络）的构建、模型训练与评估\n- 使用`swanlab`跟踪超参数、记录指标和可视化监控整个训练周期",
    "120": "一级标题：MNIST手写体识别\n二级标题：环境安装\n内容：\n本案例基于`Python>=3.8`，请在您的计算机上安装好Python。  \n环境依赖：\n```\ntorch\ntorchvision\nswanlab\n```\n快速安装命令：\n```bash\npip install torch torchvision swanlab\n```",
    "121": "一级标题：MNIST手写体识别\n二级标题：完整代码\n内容：\n```python\nimport os\nimport torch\nfrom torch import nn, optim, utils\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nimport swanlab\n\n# CNN网络构建\nclass ConvNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # 1,28x28\n        self.conv1 = nn.Conv2d(1, 10, 5)  # 10, 24x24\n        self.conv2 = nn.Conv2d(10, 20, 3)  # 128, 10x10\n        self.fc1 = nn.Linear(20 * 10 * 10, 500)\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x):\n        in_size = x.size(0)\n        out = self.conv1(x)  # 24\n        out = F.relu(out)\n        out = F.max_pool2d(out, 2, 2)  # 12\n        out = self.conv2(out)  # 10\n        out = F.relu(out)\n        out = out.view(in_size, -1)\n        out = self.fc1(out)\n        out = F.relu(out)\n        out = self.fc2(out)\n        out = F.log_softmax(out, dim=1)\n        return out\n\n\n# 捕获并可视化前20张图像\ndef log_images(loader, num_images=16):\n    images_logged = 0\n    logged_images = []\n    for images, labels in loader:\n        # images: batch of images, labels: batch of labels\n        for i in range(images.shape[0]):\n            if images_logged < num_images:\n                # 使用swanlab.Image将图像转换为wandb可视化格式\n                logged_images.append(swanlab.Image(images[i], caption=f\"Label: {labels[i]}\"))\n                images_logged += 1\n            else:\n                break\n        if images_logged >= num_images:\n            break\n    swanlab.log({\"MNIST-Preview\": logged_images})\n    \n\ndef train(model, device, train_dataloader, optimizer, criterion, epoch, num_epochs):\n    model.train()\n    # 1. 循环调用train_dataloader，每次取出1个batch_size的图像和标签\n    for iter, (inputs, labels) in enumerate(train_dataloader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        # 2. 传入到resnet18模型中得到预测结果\n        outputs = model(inputs)\n        # 3. 将结果和标签传入损失函数中计算交叉熵损失\n        loss = criterion(outputs, labels)\n        # 4. 根据损失计算反向传播\n        loss.backward()\n        # 5. 优化器执行模型参数更新\n        optimizer.step()\n        print('Epoch [{}/{}], Iteration [{}/{}], Loss: {:.4f}'.format(epoch, num_epochs, iter + 1, len(train_dataloader),\n                                                                      loss.item()))\n        # 6. 每20次迭代，用SwanLab记录一下loss的变化\n        if iter % 20 == 0:\n            swanlab.log({\"train/loss\": loss.item()})\n\ndef test(model, device, val_dataloader, epoch):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        # 1. 循环调用val_dataloader，每次取出1个batch_size的图像和标签\n        for inputs, labels in val_dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            # 2. 传入到resnet18模型中得到预测结果\n            outputs = model(inputs)\n            # 3. 获得预测的数字\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            # 4. 计算与标签一致的预测结果的数量\n            correct += (predicted == labels).sum().item()\n    \n        # 5. 得到最终的测试准确率\n        accuracy = correct / total\n        # 6. 用SwanLab记录一下准确率的变化\n        swanlab.log({\"val/accuracy\": accuracy}, step=epoch)\n    \n\nif __name__ == \"__main__\":\n\n    #检测是否支持mps\n    try:\n        use_mps = torch.backends.mps.is_available()\n    except AttributeError:\n        use_mps = False\n\n    #检测是否支持cuda\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    elif use_mps:\n        device = \"mps\"\n    else:\n        device = \"cpu\"\n\n    # 初始化swanlab\n    run = swanlab.init(\n        project=\"MNIST-example\",\n        experiment_name=\"PlainCNN\",\n        config={\n            \"model\": \"ResNet18\",\n            \"optim\": \"Adam\",\n            \"lr\": 1e-4,\n            \"batch_size\": 256,\n            \"num_epochs\": 10,\n            \"device\": device,\n        },\n    )\n\n    # 设置MNIST训练集和验证集\n    dataset = MNIST(os.getcwd(), train=True, download=True, transform=ToTensor())\n    train_dataset, val_dataset = utils.data.random_split(dataset, [55000, 5000])\n\n    train_dataloader = utils.data.DataLoader(train_dataset, batch_size=run.config.batch_size, shuffle=True)\n    val_dataloader = utils.data.DataLoader(val_dataset, batch_size=8, shuffle=False)\n    \n    # （可选）看一下数据集的前16张图像\n    log_images(train_dataloader, 16)\n\n    # 初始化模型\n    model = ConvNet()\n    model.to(torch.device(device))\n\n    # 打印模型\n    print(model)\n\n    # 定义损失函数和优化器\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=run.config.lr)\n\n    # 开始训练和测试循环\n    for epoch in range(1, run.config.num_epochs+1):\n        swanlab.log({\"train/epoch\": epoch}, step=epoch)\n        train(model, device, train_dataloader, optimizer, criterion, epoch, run.config.num_epochs)\n        if epoch % 2 == 0: \n            test(model, device, val_dataloader, epoch)\n\n    # 保存模型\n    # 如果不存在checkpoint文件夹，则自动创建一个\n    if not os.path.exists(\"checkpoint\"):\n        os.makedirs(\"checkpoint\")\n    torch.save(model.state_dict(), 'checkpoint/latest_checkpoint.pth')\n```",
    "122": "一级标题：MNIST手写体识别\n二级标题：效果演示\n内容：\n![mnist](/assets/example-mnist.jpg)",
    "123": "一级标题：Qwen2命名实体识别\n二级标题：什么是指令微调？\n内容：\n大模型指令微调（Instruction Tuning）是一种针对大型预训练语言模型的微调技术，其核心目的是增强模型理解和执行特定指令的能力，使模型能够根据用户提供的自然语言指令准确、恰当地生成相应的输出或执行相关任务。指令微调特别关注于提升模型在遵循指令方面的一致性和准确性，从而拓宽模型在各种应用场景中的泛化能力和实用性。在实际应用中，我的理解是，指令微调更多把LLM看作一个更智能、更强大的传统NLP模型（比如Bert），来实现更高精度的NLP任务。所以这类任务的应用场景覆盖了以往NLP模型的场景，甚至很多团队拿它来标注互联网数据。",
    "124": "一级标题：Qwen2命名实体识别\n二级标题：什么是命名实体识别？\n内容：\n命名实体识别 (NER) 是一种NLP技术，主要用于识别和分类文本中提到的重要信息（关键词）。这些实体可以是人名、地名、机构名、日期、时间、货币值等等。 NER 的目标是将文本中的非结构化信息转换为结构化信息，以便计算机能够更容易地理解和处理。NER 也是一项非常实用的技术，包括在互联网数据标注、搜索引擎、推荐系统、知识图谱、医疗保健等诸多领域有广泛应用。",
    "125": "一级标题：Qwen2命名实体识别\n二级标题：1.环境安装\n内容：\n本案例基于`Python>=3.8`，请在您的计算机上安装好Python，并且有一张英伟达显卡（显存要求并不高，大概10GB左右就可以跑）。我们需要安装以下这几个Python库，在这之前，请确保你的环境内已安装好了**pytorch**以及**CUDA**：\n```txt\nswanlab\nmodelscope\ntransformers\ndatasets\npeft\naccelerate\npandas\n```\n一键安装命令：\n```bash \npip install swanlab modelscope transformers datasets peft pandas accelerate\n```\n> 本案例测试于modelscope==1.14.0、transformers==4.41.2、datasets==2.18.0、peft==0.11.1、accelerate==0.30.1、swanlab==0.3.11\n\n#### 二级标题：2.准备数据集\n内容：\n本案例使用的是HuggingFace上的[chinese_ner_sft](https://huggingface.co/datasets/qgyd2021/chinese_ner_sft)数据集，该数据集主要被用于训练命名实体识别模型。chinese_ner_sft由不同来源、不同类型的几十万条数据组成，应该是我见过收录最齐全的中文NER数据集。这次训练我们不需要用到它的全部数据，只取其中的CCFBDCI数据集（中文命名实体识别算法鲁棒性评测数据集）进行训练，该数据集包含LOC（地点）、GPE（地理）、ORG（组织）和PER（人名）四种实体类型标注，每条数据的例子如下：\n```json\n{\n  \"text\": \"今天亚太经合组织第十二届部长级会议在这里开幕，中国外交部部长唐家璇、外经贸部部长石广生出席了会议。\",\n  \"entities\": [\n    {\n        \"start_idx\": 23,\n        \"end_idx\": 25,\n        \"entity_text\": \"中国\",\n        \"entity_label\": \"GPE\",\n        \"entity_names\": [\"地缘政治实体\", \"政治实体\", \"地理实体\", \"社会实体\"]},\n        {\n            \"start_idx\": 25,\n            \"end_idx\": 28,\n            \"entity_text\": \"外交部\",\n            \"entity_label\": \"ORG\",\n            \"entity_names\": [\"组织\", \"团体\", \"机构\"]\n        },\n        {\n            \"start_idx\": 30,\n            \"end_idx\": 33,\n            \"entity_text\": \"唐家璇\",\n            \"entity_label\": \"PER\",\n            \"entity_names\": [\"人名\", \"姓名\"]\n        }, \n        ...\n    ],\n\"data_source\": \"CCFBDCI\"\n}\n```\n其中`text`是输入的文本，`entities`是文本抽取出的实体。我们的目标是希望微调后的大模型能够根据由`text`组成的提示词，预测出一个json格式的实体信息：\n```txt\n输入：今天亚太经合组织第十二届部长级会议在这里开幕，中国外交部部长唐家璇、外经贸部部长石广生出席了会议。\n\n大模型输出：{\"entity_text\":\"中国\", \"entity_label\":\"组织\"}{\"entity_text\":\"唐家璇\", \"entity_label\":\"人名\"}...\n```\n现在我们将数据集下载到本地目录。下载方式是前往[chinese_ner_sft - huggingface](https://huggingface.co/datasets/qgyd2021/chinese_ner_sft/tree/main/data)下载`ccfbdci.jsonl`到项目根目录下即可：\n\n#### 二级标题：3. 加载模型\n内容：\n这里我们使用modelscope下载Qwen2-1.5B-Instruct模型（modelscope在国内，所以直接用下面的代码自动下载即可，不用担心速度和稳定性问题），然后把它加载到Transformers中进行训练：\n```python\nfrom modelscope import snapshot_download, AutoTokenizer\nfrom transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n\nmodel_id = \"qwen/Qwen2-1.5B-Instruct\"    \nmodel_dir = \"./qwen/Qwen2-1___5B-Instruct\"\n\n# 在modelscope上下载Qwen模型到本地目录下\nmodel_dir = snapshot_download(model_id, cache_dir=\"./\", revision=\"master\")\n\n# Transformers加载模型权重\ntokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\nmodel.enable_input_require_grads()  # 开启梯度检查点时，要执行该方法\n```\n\n#### 二级标题：4. 配置训练可视化工具\n内容：\n我们使用SwanLab来监控整个训练过程，并评估最终的模型效果。这里直接使用SwanLab和Transformers的集成来实现：\n```python\nfrom swanlab.integration.huggingface import SwanLabCallback\n\nswanlab_callback = SwanLabCallback(...)\n\ntrainer = Trainer(\n    ...\n    callbacks=[swanlab_callback],\n)\n```\n如果你是第一次使用SwanLab，那么还需要去[https://swanlab.cn](https://swanlab.cn)上注册一个账号，在**用户设置**页面复制你的API Key，然后在训练开始时粘贴进去即可：\n\n#### 二级标题：5. 完整代码\n内容：\n开始训练时的目录结构：\n```txt\n|--- train.py\n|--- train.jsonl\n|--- test.jsonl\n```\ntrain.py:\n```python\nimport json\nimport pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom modelscope import snapshot_download, AutoTokenizer\nfrom swanlab.integration.huggingface import SwanLabCallback\nfrom peft import LoraConfig, TaskType, get_peft_model\nfrom transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\nimport os\nimport swanlab\n\n\ndef dataset_jsonl_transfer(origin_path, new_path):\n    \"\"\"\n    将原始数据集转换为大模型微调所需数据格式的新数据集\n    \"\"\"\n    messages = []\n\n    # 读取旧的JSONL文件\n    with open(origin_path, \"r\") as file:\n        for line in file:\n            # 解析每一行的json数据\n            data = json.loads(line)\n            input_text = data[\"text\"]\n            entities = data[\"entities\"]\n            match_names = [\"地点\", \"人名\", \"地理实体\", \"组织\"]\n            \n            entity_sentence = \"\"\n            for entity in entities:\n                entity_json = dict(entity)\n                entity_text = entity_json[\"entity_text\"]\n                entity_names = entity_json[\"entity_names\"]\n                for name in entity_names:\n                    if name in match_names:\n                        entity_label = name\n                        break\n                \n                entity_sentence += f\"\"\"{{\"entity_text\": \"{entity_text}\", \"entity_label\": \"{entity_label}\"}}\"\"\"\n            \n            if entity_sentence == \"\":\n                entity_sentence = \"没有找到任何实体\"\n            \n            message = {\n                \"instruction\": \"\"\"你是一个文本实体识别领域的专家，你需要从给定的句子中提取 地点; 人名; 地理实体; 组织 实体. 以 json 格式输出, 如 {\"entity_text\": \"南京\", \"entity_label\": \"地理实体\"} 注意: 1. 输出的每一行都必须是正确的 json 字符串. 2. 找不到任何实体时, 输出\"没有找到任何实体\". \"\"\",\n                \"input\": f\"文本:{input_text}\",\n                \"output\": entity_sentence,\n            }\n            \n            messages.append(message)\n\n    # 保存重构后的JSONL文件\n    with open(new_path, \"w\", encoding=\"utf-8\") as file:\n        for message in messages:\n            file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n            \n            \ndef process_func(example):\n    \"\"\"\n    将数据集进行预处理\n    \"\"\"\n\n    MAX_LENGTH = 384 \n    input_ids, attention_mask, labels = [], [], []\n    system_prompt = \"\"\"你是一个文本实体识别领域的专家，你需要从给定的句子中提取 地点; 人名; 地理实体; 组织 实体. 以 json 格式输出, 如 {\"entity_text\": \"南京\", \"entity_label\": \"地理实体\"} 注意: 1. 输出的每一行都必须是正确的 json 字符串. 2. 找不到任何实体时, 输出\"没有找到任何实体\".\"\"\"\n    \n    instruction = tokenizer(\n        f\"",
    "126": "一级标题：openMind大模型微调教程\n二级标题：简介\n内容：\n魔乐社区（[Modelers.cn](https://modelers.cn)）是一个为人工智能开发者及爱好者打造的社区，提供工具链、数据集、模型和应用等AI领域生产要素的托管及展示服务和支撑系统。目前，魔乐社区已支持openMind Library。该工具通过简单的API接口，帮助开发者完成模型预训练、微调、推理等流程。同时，openMind Library原生兼容PyTorch 和 MindSpore 等主流框架，原生支持昇腾NPU处理器。openMind Library可以和PEFT、DeepSpeed等三方库配合使用，来提升模型微调效率。\n\n友情链接：\n\n* [魔乐社区](https://modelers.cn/docs/zh/openmind-library/0.9.1/overview.html)\n* [Huggingface](https://huggingface.co)\n* [SwanLab](https://swanlab.cn)\n\n---",
    "127": "一级标题：openMind大模型微调教程\n二级标题：1、基本概念\n内容：\n1、[openMind Library](https://modelers.cn/docs/zh/openmind-library/0.9.1/overview.html)--->[Huggingface Transformers](https://huggingface.co/docs/transformers/index)\n\nopenMind Library类似于transformers的大模型封装工具，其中就有AutoModelForSequenceClassification、AutoModelForCausalLM等等模型加载工具以及像TrainingArguments参数配置工具等等，原理基本一样，不过对NPU适配更友好些。\n![openmind vs transformers](/zh/examples/openMind/openmind_transformers.png)\n\n2、[魔乐社区](https://modelers.cn/)--->[HuggingFace](https://huggingface.co/)\n\n魔乐社区类似于huggingface这种模型托管社区，里面除了torch的模型还有使用MindSpore实现的模型。transformers可以直接从huggingface获取模型或者数据集，openMind也是一样的，可以从魔乐社区获取模型和数据集。\n![魔乐社区 vs huggingface](/zh/examples/openMind/mole.png)\n\n---\n\n2、微调代码\n内容：\n如果了解了上述的对应机制，那么就可以跑一个简单的微调代码了，该代码参考了[魔乐社区的教程文档](https://modelers.cn/docs/zh/openmind-library/0.9.1/overview.html)，稍作调整，可以对比NVIDIA显卡的结果。\n\n##### 概述\nopenMind Library是一个深度学习开发套件，通过简单易用的API支持模型预训练、微调、推理等流程。openMind Library通过一套接口兼容PyTorch和MindSpore等主流框架，同时原生支持昇腾NPU处理器，同时openMind Library可以和PEFT、DeepSpeed等三方库配合使用，来加速模型微调效率。\n\n##### 环境配置\n###### 直接安装openMind环境\n如果是昇腾AI卡系列的话，配置环境前需要先安装驱动等设备，具体可以参考[软件安装-CANN商用版8.0.RC3开发文档-昇腾社区](https://www.hiascend.com/document/detail/zh/canncommercial/80RC3/softwareinst/instg/instg_0000.html?Mode=PmIns&OS=Ubuntu&Software=cannToolKit)。\n\n**驱动安装&验证**\n\n首先得确定有NPU卡和NPU相关驱动，驱动是8.0.RC3.beta1，如果没安装可以参考上面软件安装的链接查看。\n\n安装好后的验证方法是运行下面的命令，该命令作用与nvidia-smi类似，这里是查看NPU的状态和性能\n\n```bash\nnpu-smi info\n```\n\n可以看到如下信息的话就表示驱动已经安装完成了，左侧是安装成功后运行代码后的结果，右侧是每一部分的含义\n\n![npu-smi info](/zh/examples/openMind/npu-info.png)\n\n然后安装好驱动了之后就可以配置环境了，本次微调代码使用pytorch框架，openMind中自带了基于pytorch框架的各类函数，因此正常安装openMind就行。\n\n安装命令如下：\n\n```bash\n# 下载PyTorch安装包\nwget https://download.pytorch.org/whl/cpu/torch-2.4.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\n# 下载torch_npu插件包\nwget https://gitee.com/ascend/pytorch/releases/download/v6.0.rc3-pytorch2.4.0/torch_npu-2.4.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\n# 安装命令\npip3 install torch-2.4.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\npip3 install torch_npu-2.4.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl\n# 安装openMind Library\npip install openmind[pt]\n# 安装SwanLab\npip install swanlab\n```\n\n> 注意以下几点：\n>\n> 1、可以使用镜像源来安装环境，不然会很浪费时间，可以使用清华源：\n>\n> ```bash\n> pip install -i https://pypi.tuna.tsinghua.edu.cn/simple name\n> ```\n>\n> 2、魔乐社区中有两个框架的分类，如果是pytorch就只能选择pytorch框架，同理如果是mindspore就只能选择mindspore框架\n> ![魔乐社区模型](/zh/examples/openMind/models.png)\n> 3、配置环境的时候，按照openmind官方文档说可以同时存在两个框架，使用的时候分别设置就行，但是实际使用的时候只能存在一个框架，一旦设置了两个框架，使用的时候无论如何设置都会报错说openmind不知道使用哪个框架，所以最好在环境里只安装一个\n>\n> ```bash\n> >>>import openmind\n> Traceback (most recent call last):\n>   File \"<stdin>\", line 1, in <module>\n>   File \"/home/miniconda3/envs/openmind-pt-cp39/lib/python3.9/site-packages/openmind/__init__.py\", line 20, in <module>\n>     from .utils import is_ms_available, is_torch_available\n>   File \"/home/miniconda3/envs/openmind-pt-cp39/lib/python3.9/site-packages/openmind/utils/__init__.py\", line 14, in <module>\n>     from .import_utils import (\n>   File \"/home/miniconda3/envs/openmind-pt-cp39/lib/python3.9/site-packages/openmind/utils/import_utils.py\", line 69, in <module>\n>     CURRENT_FRAMEWORK = get_framework()\n>   File \"/home/miniconda3/envs/openmind-pt-cp39/lib/python3.9/site-packages/openmind/utils/import_utils.py\", line 66, in get_framework\n>     raise RuntimeError(replace_invalid_characters(error_msg))\n> RuntimeError: Multiple frameworks detected, including: pt, ms.\n> ```\n\n###### docker环境安装（推荐）\nopenMind官方库也提供了模型的docker环境。\n\n推荐通过点击模型测试部分（下图红框）找到docker的链接，通过docker来拉起拉起环境。下面介绍docker环境的搭建教程。\n\n![bert模型环境](/zh/examples/openMind/bert.png)\n\n首先得确定有NPU卡和NPU相关驱动，驱动是**8.0.RC3.beta1**，如果没安装可以参考[CANN官方安装教程](https://www.hiascend.com/document/detail/zh/canncommercial/80RC3/softwareinst/instg/instg_0000.html?Mode=PmIns&OS=Ubuntu&Software=cannToolKit)\n\n完成安装后检测方法是运行\n\n```bash\nnpu-smi info\n```\n\n可以看到如下信息的话就表示驱动已经安装完成了。\n\n![npu-smi](/zh/examples/openMind/a_mask.png)\n\n接下来使用如下命令创建一个装好openmind环境的容器，这样可以省去大量安装环境的时间：\n\n```bash\ndocker run \\\n    --name openmind \\\n    --device /dev/davinci0 \\    # 指定NPU 0号设备\n    --device /dev/davinci_manager \\\n    --device /dev/devmm_svm \\\n    --device /dev/hisi_hdc \\\n    -v /usr/local/dcmi:/usr/local/dcmi \\\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n    -v /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/ \\\n    -v /etc/ascend_install.info:/etc/ascend_install.info \\\n    -v /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info \\\n    -tid registry.modelers.cn/base_image/openmind:openeuler-python3.10-cann8.0.rc3.beta1-pytorch2.1.0-openmind0.9.1 bash\n```\n\n这将在后台开启一个名为openmind容器。使用如下命令可进入到容器当中\n\n```bash\n docker exec -it openmind bash\n```\n\n出现如下界面即表示进入到容器当中\n\n![indocker](/zh/examples/openMind/indocker.png)\n\n最后在docker中运行如下命令安装swanlab即可完成环境安装。\n\n```bash\n# 安装swanlab命令\npip install swanlab\n```\n\n##### 数据集处理\nOmDataset.load_dataset()方法目前支持下载的数据集格式如下：\n\n* parquet\n* json或者jsonl\n* tar.gz\n* csv\n* 下载python脚本加载魔乐社区数据集\n* 下载python脚本加载三方站点数据集\n\n```python\nfrom openmind import OmDataset\nfrom openmind import AutoTokenizer\n \n### 准备数据集\ndataset = OmDataset.load_dataset(\"AI_Connect/glue\", \"cola\")\n \n### 结果\n\"\"\"\nDatasetDict({\n    train: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 8551\n    })\n    validation: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 1043\n    })\n    test: Dataset({\n        features: ['sentence', 'label', 'idx'],\n        num_rows: 1063\n    })\n})\n\"\"\"\n \n### 加载分词器\ntokenizer = AutoTokenizer.from_pretrained(\"PyTorch-NPU/bert_base_cased\")\n \n### 处理数据集\ndef tokenize_function(examples):\n    return tokenizer(examples[\"sentence\"],truncation=True,padding=\"max_length\",max_length=512)\n \n### 训练数据封装\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n \n# 训练数据+验证数据，验证发生在每个epoch之后\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\nsmall_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42)\n```\n\n##### 加载模型\n和transformers使用差不多，分别加载模型和分词器\n\n```python\nfrom openmind import AutoTokenizer\nfrom openmind import AutoModelForSequenceClassification  ## 做分类任务\n \n \n### 加载分词器\ntokenizer = AutoTokenizer.from_pretrained(\"PyTorch-NPU/bert_base_cased\")\n \n### 加载模型\nmodel = AutoModelForSequenceClassification.from_pretrained(\"PyTorch-NPU/bert_base_cased\", num_labels=2)  # 二分类任务\n```\n\n##### 训练参数配置\n创建一个TrainingArguments类，其中包含可以调整的所有超参数以及不同的训练选项。\n\n```python\nfrom openmind import TrainingArguments\n \n### 参数初始化\n# 指定保存训练检查点的路径\ntraining_args = TrainingArguments(logging_steps=1,\n                                  output_dir=\"test_trainer\",\n                                  evaluation_strategy=\"epoch\",\n                                  half_precision_backend=\"auto\",  # auto:自动选择合适的混合精度训练后端；apex：英伟达的 ；cpu_amp：在CPU上运行\n                                  per_device_train_batch_size=4,\n                                  optim=\"adamw_torch\",\n                                  learning_rate=2e-5)\n```\n\n##### 评估参数设置\nTrainer在训练过程中不会自动评估模型性能，需要向Trainer传递一个函数来计算和展示指标。\n\n```python\nimport numpy as np\nfrom openmind import metrics\n \n### 配置评估参数\nmetric = metrics.Accuracy()\n \n \ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    return metric.compute(preds=preds, labels=labels)\n```\n\n##### 可视化工具配置\nswanlab支持记录openMind Library。能够在线/离线查看训练日志。SwanLab支持openMind Library通过callback调用，调用代码可参考后文。\n\n![SwanLab可视化工具](/zh/examples/openMind/modelers&swanlab%20V2.png)\n关于SwanLab的使用方法可以参考[SwanLab官方文档-快速开始](https://docs.swanlab.cn/guide_cloud/general/quick-start.html)\n\n> 如果提示登录swanlab，可以在[官网完成注册](https://swanlab.cn)后，使用[获取API KEY](https://swanlab.cn/settings)找到对应的登陆密钥并粘贴，这样将能够使用**云上看版**随时查看训练过程与结果。\n\n```python\nfrom openmind import Trainer\nfrom swanlab.integration.transformers import SwanLabCallback\n \n### 使用swanlab监测\nswanlab_config = {\n    \"dataset\": \"glue\",\n    \"fp16_backend\":\"auto\",\n    \"datacollator\":\"transformer\"\n}\nswanlab_callback = SwanLabCallback(\n    project=\"new_qwen2.5-7B-finetune\",\n    experiment_name=\"跑的官方例子的微调\",\n    description=\"这个是使用transformers的datacollator封装函数\",\n    workspace=None,\n    config=swanlab_config,\n)\n \n### 创建训练器并且启动训练\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[swanlab_callback],\n)\n \ntrainer.train()\n \n### 保存模型\noutput_dir=\"./output\"\nfinal_save_path = join(output_dir)\ntrainer.save_model(final_save_path)\n```\n\n##### 全过程代码\n```python\nfrom openmind import OmDataset\nfrom openmind import AutoTokenizer\nfrom openmind import AutoModelForSequenceClassification\nfrom openmind import TrainingArguments\nfrom openmind import metrics\nimport numpy as np\nfrom openmind import Trainer\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom os.path import join\n \n \n### 准备数据集\ndataset = OmDataset.load_dataset(\"AI_Connect/glue\", \"cola\")\n \n### 加载分词器\ntokenizer = AutoTokenizer.from_pretrained(\"PyTorch-NPU/bert_base_cased\")\n \n \n### 处理数据集\ndef tokenize_function(examples):\n    # 填充\n    return tokenizer(examples[\"sentence\"],truncation=True,padding=\"max_length\",max_length=512)\n \n \ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n \n# 减少数据量\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\nsmall_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42)\n \n \n### 加载模型\nmodel = AutoModelForSequenceClassification.from_pretrained(\"PyTorch-NPU/bert_base_cased\", num_labels=2)\n \n### 参数初始化\n# 指定保存训练检查点的路径\ntraining_args = TrainingArguments(logging_steps=1,\n                                  output_dir=\"test_trainer\",\n                                  evaluation_strategy=\"epoch\",\n                                  half_precision_backend=\"auto\",  # auto:自动选择合适的混合精度训练后端；apex：英伟达的 ；cpu_amp：在CPU上运行\n                                  per_device_train_batch_size=4,\n                                  optim=\"adamw_torch\",\n                                  learning_rate=2e-5)\n \n### 配置评估参数\nmetric = metrics.Accuracy()\n \n \ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    return metric.compute(preds=preds, labels=labels)\n \n \n### 使用swanlab监测\nswanlab_config = {\n    \"dataset\": \"glue\",\n    \"fp16_backend\":\"auto\",\n    \"datacollator\":\"transformer\"\n}\nswanlab_callback = SwanLabCallback(\n    project=\"new_qwen2.5-7B-finetune\",\n    experiment_name=\"跑的官方例子的微调\",\n    description=\"这个是使用transformers的datacollator封装函数\",\n    workspace=None,\n    config=swanlab_config,\n)\n### 创建训练器并且启动训练\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n    callbacks=[swanlab_callback],\n)\n \ntrainer.train()\n \n### 保存模型\noutput_dir=\"./output\"\nfinal_save_path = join(output_dir)\ntrainer.save_model(final_save_path)\n```\n\n---\n\n这里使用HF Transformers实现同样的训练过程，使用NVIDIA-A100卡来跑了一次做个对比，A100对应的代码如下：\n\n```python\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import TrainingArguments\nimport evaluate\nimport numpy as np\nfrom transformers import Trainer\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom os.path import join\nimport os\n \n# 设置只使用第一个GPU\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # 使用第一块 GPU\n \n### 加载数据集\ndataset = load_dataset(\"nyu-mll/glue\",\"cola\")\n \n### 加载分词器\ntokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n \n### 处理数据集\ndef tokenize_function(examples):\n    # 填充\n    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True)\n \ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n \n# 减少数据量\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\nsmall_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed=42)\n \n### 加载模型\nmodel = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=2)\n \n### 参数初始化\n\n以下是按主题分块整理的内容：",
    "128": "一级标题：从零预训练一个自己的大模型\n二级标题：安装环境\n内容：\n首先，项目推荐使用python3.10。需要安装的python包如下：\n\n```txt\nswanlab\ntransformers\ndatasets\naccelerate\n```\n\n使用如下命令一键安装：\n\n```bash\npip install swanlab transformers datasets accelerate modelscope\n```",
    "129": "一级标题：从零预训练一个自己的大模型\n二级标题：下载数据集\n内容：\n本教程使用的是中文wiki数据，理论上预训练数据集种类越丰富、数据量越大越好，后续会增加别的数据集。\n\n![dataset](/assets/examples/pretrain_llm/dataset.png)\n\nhuggingface链接：[wikipedia-zh-cn](https://huggingface.co/datasets/fjcanyue/wikipedia-zh-cn)\n\n百度网盘下载地址：[百度网盘（j8ee）](https://pan.baidu.com/s/1p5F52bRlnpSY7F78q0hz7A?pwd=j8ee)\n\n下载`wikipedia-zh-cn-20240820.json`文件后放到项目目录下`./WIKI_CN/`文件夹中\n\n该数据集文件约1.99G大，共有1.44M条数据。虽然数据集中包含文章标题，但是实际上在预训练阶段用不上。正文片段参考：\n\n```txt\n数学是研究数量、结构以及空间等概念及其变化的一门学科，属于形式科学的一种。数学利用抽象化和逻辑推理，从计数、计算、量度、对物体形状及运动的观察发展而成。数学家们拓展这些概念...\n```\n\n使用[🤗Huggingface Datasets](https://huggingface.co/docs/datasets/index)加载数据集的代码如下：\n\n```python\nfrom datasets import load_dataset\n\nds = load_dataset(\"fjcanyue/wikipedia-zh-cn\")\n```\n\n如果使用百度网盘下载的json文件，可以通过如下代码加载\n\n```python\nraw_datasets = datasets.load_dataset(\n    \"json\", data_files=\"data/wikipedia-zh-cn-20240820.json\"\n)\n\nraw_datasets = raw_datasets[\"train\"].train_test_split(test_size=0.1, seed=2333)\nprint(\"dataset info\")\nprint(raw_datasets)\n```",
    "130": "一级标题：从零预训练一个自己的大模型\n二级标题：构建自己的大语言模型\n内容：\n本教程使用[🤗huggingface transformers](https://huggingface.co/docs/transformers/index)构建自己的大模型。\n\n因为目标是训练一个中文大模型。因此我们参考[通义千问2](https://qwen.readthedocs.io/zh-cn/latest/run_locally/mlx-lm.html)的tokenize和模型架构，仅仅做一些简单的更改让模型更小更好训练。\n\n因为国内无法直接访问到huggingface，推荐使用modelscope先把模型配置文件和checkpoint下载到本地，运行如下代码\n\n```python\nimport modelscope\n\nmodelscope.AutoConfig.from_pretrained(\"Qwen/Qwen2-0.5B\").save_pretrained(\n    \"Qwen2-0.5B\"\n)\nmodelscope.AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\").save_pretrained(\n    \"Qwen2-0.5B\"\n)\n```\n\n配置参数，并修改模型注意力头数量、模型层数和中间层大小，把模型控制到大概120M参数左右（跟GPT2接近）。\n\n```python\nimport transformers\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"./Qwen2-0.5B\")   # 这里使用qwen2的tokenzier\nconfig = transformers.AutoConfig.from_pretrained(\n        \"./Qwen2-0.5B\",\n        vocab_size=len(tokenizer),\n        hidden_size=512,\n        intermediate_size=2048,\n        num_attention_heads=8,\n        num_hidden_layers=12,\n        n_ctx=context_length,\n        bos_token_id=tokenizer.bos_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n    )\nprint(\"Model Config:\")\nprint(config)\n```\n\n使用transformers库初始化模型\n\n```python\nmodel = transformers.Qwen2ForCausalLM(config)\nmodel_size = sum(t.numel() for t in model.parameters())\nprint(f\"Model Size: {model_size/1000**2:.1f}M parameters\")\n```",
    "131": "一级标题：从零预训练一个自己的大模型\n二级标题：设置训练参数\n内容：\n设置预训练超参数：\n\n```python\nargs = transformers.TrainingArguments(\n    output_dir=\"checkpoints\",\n    per_device_train_batch_size=24,  # 每个GPU的训练batch数\n    per_device_eval_batch_size=24,  # 每个GPU的测试batch数\n    eval_strategy=\"steps\",\n    eval_steps=5_000,\n    logging_steps=500,\n    gradient_accumulation_steps=12,  # 梯度累计总数\n    num_train_epochs=2, # 训练epoch数\n    weight_decay=0.1,\n    warmup_steps=1_000,\n    optim=\"adamw_torch\",  # 优化器使用adamw\n    lr_scheduler_type=\"cosine\",  # 学习率衰减策略\n    learning_rate=5e-4,  # 基础学习率，\n    save_steps=5_000,\n    save_total_limit=10,\n    bf16=True,  # 开启bf16训练, 对于Amper架构以下的显卡建议替换为fp16=True\n)\nprint(\"Train Args:\")\nprint(args)\n```",
    "132": "一级标题：从零预训练一个自己的大模型\n二级标题：初始化训练+使用swanlab进行记录\n内容：\n使用transformers自带的train开始训练，并且引入swanlab作为可视化日志记录\n\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\ntrainer = transformers.Trainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=args,\n    data_collator=data_collator,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n    callbacks=[SwanLabCallback()],\n)\ntrainer.train()\n```\n\n如果是第一次使用SwanLab，需要登陆SwanLab官网[https://swanlab.cn/](https://swanlab.cn/)，注册，并且在如下位置找到和复制自己的key。\n\n![findkey](/assets/examples/pretrain_llm/findkey.png)\n\n接下来在命令行中输入\n\n```sh\nswanlab login\n```\n\n会看到提示输入key\n\n![login](/assets/examples/pretrain_llm/login.png)\n\n按照提示将key粘贴进去（注意key是不会显示到终端当中的）就可以完成配置，完成效果如下：\n\n![login2](/assets/examples/pretrain_llm/login2.png)",
    "133": "一级标题：从零预训练一个自己的大模型\n二级标题：完整代码\n内容：\n项目目录结构：\n\n```txt\n|---data\\\n|------wikipedia-zh-cn-20240820.json    # 数据集放在data文件夹中\n|--- pretrain.py\n```\n\n`pretrain.py`代码如下：\n\n```python\nimport datasets\nimport transformers\nimport swanlab\nfrom swanlab.integration.transformers import SwanLabCallback\nimport modelscope\n\ndef main():\n    # using swanlab to save log\n    swanlab.init(\"WikiLLM\")\n\n    # load dataset\n    raw_datasets = datasets.load_dataset(\n        \"json\", data_files=\"/data/WIKI_CN/wikipedia-zh-cn-20240820.json\"\n    )\n\n    raw_datasets = raw_datasets[\"train\"].train_test_split(test_size=0.1, seed=2333)\n    print(\"dataset info\")\n    print(raw_datasets)\n\n    # load tokenizers\n    # 因为国内无法直接访问HuggingFace，因此使用魔搭将模型的配置文件和Tokenizer下载下来\n    modelscope.AutoConfig.from_pretrained(\"Qwen/Qwen2-0.5B\").save_pretrained(\n        \"Qwen2-0.5B\"\n    )\n    modelscope.AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\").save_pretrained(\n        \"Qwen2-0.5B\"\n    )\n    context_length = 512  # use a small context length\n    # tokenizer = transformers.AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\")\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\n        \"./Qwen2-0.5B\"\n    )  # download from local\n\n    # preprocess dataset\n    def tokenize(element):\n        outputs = tokenizer(\n            element[\"text\"],\n            truncation=True,\n            max_length=context_length,\n            return_overflowing_tokens=True,\n            return_length=True,\n        )\n        input_batch = []\n        for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n            if length == context_length:\n                input_batch.append(input_ids)\n        return {\"input_ids\": input_batch}\n\n    tokenized_datasets = raw_datasets.map(\n        tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n    )\n    print(\"tokenize dataset info\")\n    print(tokenized_datasets)\n    tokenizer.pad_token = tokenizer.eos_token\n    data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\n    # prepare a model from scratch\n    config = transformers.AutoConfig.from_pretrained(\n        \"./Qwen2-0.5B\",\n        vocab_size=len(tokenizer),\n        hidden_size=512,\n        intermediate_size=2048,\n        num_attention_heads=8,\n        num_hidden_layers=12,\n        n_ctx=context_length,\n        bos_token_id=tokenizer.bos_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n    )\n    model = transformers.Qwen2ForCausalLM(config)\n    model_size = sum(t.numel() for t in model.parameters())\n    print(\"Model Config:\")\n    print(config)\n    print(f\"Model Size: {model_size/1000**2:.1f}M parameters\")\n\n    # train\n    args = transformers.TrainingArguments(\n        output_dir=\"WikiLLM\",\n        per_device_train_batch_size=32,  # 每个GPU的训练batch数\n        per_device_eval_batch_size=32,  # 每个GPU的测试batch数\n        eval_strategy=\"steps\",\n        eval_steps=5_00,\n        logging_steps=50,\n        gradient_accumulation_steps=8,  # 梯度累计总数\n        num_train_epochs=2,  # 训练epoch数\n        weight_decay=0.1,\n        warmup_steps=2_00,\n        optim=\"adamw_torch\",  # 优化器使用adamw\n        lr_scheduler_type=\"cosine\",  # 学习率衰减策略\n        learning_rate=5e-4,  # 基础学习率，\n        save_steps=5_00,\n        save_total_limit=10,\n        bf16=True,  # 开启bf16训练, 对于Amper架构以下的显卡建议替换为fp16=True\n    )\n    print(\"Train Args:\")\n    print(args)\n    # enjoy training\n    trainer = transformers.Trainer(\n        model=model,\n        tokenizer=tokenizer,\n        args=args,\n        data_collator=data_collator,\n        train_dataset=tokenized_datasets[\"train\"],\n        eval_dataset=tokenized_datasets[\"test\"],\n        callbacks=[SwanLabCallback()],\n    )\n    trainer.train()\n\n    # save model\n    model.save_pretrained(\"./WikiLLM/Weight\")  # 保存模型的路径\n\n    # generate\n    pipe = transformers.pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n    print(\"GENERATE:\", pipe(\"人工智能\", num_return_sequences=1)[0][\"generated_text\"])\n    prompts = [\"牛顿\", \"北京市\", \"亚洲历史\"]\n    examples = []\n    for i in range(3):\n        # 根据提示词生成数据\n        text = pipe(prompts[i], num_return_sequences=1)[0][\"generated_text\"]\n        text = swanlab.Text(text)\n        examples.append(text)\n    swanlab.log({\"Generate\": examples})\n\n\nif __name__ == \"__main__\":\n    main()\n\n```",
    "134": "一级标题：从零预训练一个自己的大模型\n二级标题：训练结果演示\n内容：\n运行如下命令\n\n```\npython pretrain.py\n```\n\n可以看到如下训练日志。由于训练时间较长，推荐使用tmux将训练任务hold住\n\n![terminal](/assets/examples/pretrain_llm/terminal.png)\n\n可以在[SwanLab](https://swanlab.cn)中查看最终的训练结果：\n\n![log](/assets/examples/pretrain_llm/log.png)\n\n<!-- 并且能够看到一些最终生成的案例：\n\n![sample]() -->",
    "135": "一级标题：从零预训练一个自己的大模型\n二级标题：使用训练好的模型进行推理\n内容：\n以“人工智能”为开头生成内容的代码如下：\n\n```python\npipe = transformers.pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\nprint(\"GENERATE:\", pipe(\"人工智能\", num_return_sequences=1)[0][\"generated_text\"])\n```\n\n推理效果如下：\n\n（模型训练ing，可以在[https://swanlab.cn/@ShaohonChen/WikiLLM/overview](https://swanlab.cn/@ShaohonChen/WikiLLM/overview)实时查看训练进展和推理效果）\n<!-- ![result]() -->",
    "136": "一级标题：从零预训练一个自己的大模型\n二级标题：参考链接\n内容：\n* 本教程完整代码:[GitHub](https://github.com/ShaohonChen/transformers_from_scratch)\n\n* 实验记录：[SwanLab](https://swanlab.cn/@ShaohonChen/WikiLLM/overview)\n\n* 数据集下载：[百度网盘（j8ee）](https://pan.baidu.com/s/1p5F52bRlnpSY7F78q0hz7A?pwd=j8ee)，[huggingface](https://huggingface.co/datasets/fjcanyue/wikipedia-zh-cn)",
    "137": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：知识点：什么是全参数微调？\n内容：\n大模型全参数微调是指对预训练大模型的所有参数进行更新和优化，区别于部分参数微调和LoRA微调。这种方法通过将整个模型权重（包括底层词嵌入、中间特征提取层和顶层任务适配层）在下游任务数据上进行梯度反向传播，使模型整体适应新任务的需求。相比仅微调部分参数，全参数微调能更充分地利用预训练模型的泛化能力，并针对特定任务进行深度适配，通常在数据差异较大或任务复杂度较高的场景下表现更优。不过，全参数微调往往需要更高的计算资源和存储开销，且存在过拟合风险（尤其在小数据集上）。实际应用中常结合学习率调整、参数分组优化或正则化技术来缓解这些问题。全参数微调多用于对模型表现性能要求较高的场景，例如专业领域知识问答或高精度文本生成。\n![09-03](./qwen3/03.png)\n更多微调技术可参考：https://zhuanlan.zhihu.com/p/682082440",
    "138": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：1. 环境安装\n内容：\n本案例基于Python>=3.8，请在您的计算机上安装好Python；另外，您的计算机上至少要有一张英伟达/昇腾显卡（显存要求大概32GB左右可以跑）。我们需要安装以下这几个Python库，在这之前，请确保你的环境内已安装了pytorch以及CUDA：\n```\nswanlab\nmodelscope==1.22.0\ntransformers>=4.50.0\ndatasets==3.2.0\naccelerate\npandas\naddict\n```\n一键安装命令：\n```bash\npip install swanlab modelscope==1.22.0 \"transformers>=4.50.0\" datasets==3.2.0 accelerate pandas addict\n```\n> 本案例测试于modelscope==1.22.0、transformers==4.51.3、datasets==3.2.0、peft==0.11.1、accelerate==1.6.0、swanlab==0.5.7",
    "139": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：2. 准备数据集\n内容：\n本案例使用的是 [delicate_medical_r1_data](https://modelscope.cn/datasets/krisfu/delicate_medical_r1_data) 数据集，该数据集主要被用于医学对话模型。该数据集由2000多条数据组成，每条数据包含Instruction、question、think、answer、metrics六列：\n![09-04](./qwen3/04.png)\n这里我们只取`question`、`think`、`answer`这三列：\n- `question`：用户提出的问题，即模型的输入\n- `think`：模型的思考过程。大家如果用过DeepSeek R1的话，回复中最开始的思考过程就是这个。\n- `answer`：模型思考完成后，回复的内容。\n我们的训练任务，便是希望微调后的大模型，能够根据`question`，给用户一个`think`+`answer`的组合回复，并且think和answer直接在网页展示上是有区分的。\n理清需求后，我们设计这样一个数据集样例：\n```json\n{\n\"question\": \"我父亲刚刚被诊断为活动性出血，医生说需要立即处理，我们该怎么做？\", \n\"think\": \"嗯，用户的问题是关于病人出现活动性出血时应采取哪些一般处理措施，...\",\n\"answer\": \"首先，您父亲需要卧床休息，活动性出血期间暂时不要进食。为了...\",\n}\n```\n在训练代码执行时，会将`think`和`answer`按下面这样的格式组合成一条完整回复：\n```\n<think>\n嗯，用户的问题是关于病人出现活动性出血时应采取哪些一般处理措施，...\n</think>\n\n首先，您父亲需要卧床休息，活动性出血期间暂时不要进食。为了...\n```\n---\n接下来我们来下载数据集，并进行必要的格式转换。这个流程非常简单，执行下面的代码即可：\n```python\nfrom modelscope.msdatasets import MsDataset\nimport json\nimport random\n\nrandom.seed(42)\n\nds = MsDataset.load('krisfu/delicate_medical_r1_data', subset_name='default', split='train')\ndata_list = list(ds)\nrandom.shuffle(data_list)\n\nsplit_idx = int(len(data_list) * 0.9)\n\ntrain_data = data_list[:split_idx]\nval_data = data_list[split_idx:]\n\nwith open('train.jsonl', 'w', encoding='utf-8') as f:\n    for item in train_data:\n        json.dump(item, f, ensure_ascii=False)\n        f.write('\\n')\n\nwith open('val.jsonl', 'w', encoding='utf-8') as f:\n    for item in val_data:\n        json.dump(item, f, ensure_ascii=False)\n        f.write('\\n')\n\nprint(f\"The dataset has been split successfully.\")\nprint(f\"Train Set Size：{len(train_data)}\")\nprint(f\"Val Set Size：{len(val_data)}\")\n```\n完成后，你的代码目录下会出现训练集`train.jsonl`和验证集`val.jsonl`文件。\n至此，数据集部分完成。",
    "140": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：3. 加载模型\n内容：\n这里我们使用modelscope下载Qwen3-1.7B模型（modelscope在国内，所以下载不用担心速度和稳定性问题），然后把它加载到Transformers中进行训练：\n```python\nfrom modelscope import snapshot_download, AutoTokenizer\nfrom transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n\n# 在modelscope上下载Qwen模型到本地目录下\nmodel_dir = snapshot_download(\"Qwen/Qwen3-1.7B\", cache_dir=\"./\", revision=\"master\")\n\n# Transformers加载模型权重\ntokenizer = AutoTokenizer.from_pretrained(\"./Qwen/Qwen3-1.7B\", use_fast=False, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\"./Qwen/Qwen3-1.7B\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n```",
    "141": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：4. 配置训练可视化工具\n内容：\n我们使用SwanLab来监控整个训练过程，并评估最终的模型效果。SwanLab 是一款开源、轻量的 AI 模型训练跟踪与可视化工具，面向人工智能与深度学习开发者，提供了一个跟踪、记录、比较、和协作实验的平台，常被称为\"中国版 Weights & Biases + Tensorboard\"。SwanLab同时支持云端和离线使用，并适配了从PyTorch、Transformers、Lightning再到LLaMA Factory、veRL等40+ AI训练框架。\n![09-05](./qwen3/05.png)\n![09-06](./qwen3/06.png)\n这里直接使用SwanLab和Transformers的集成来实现，更多用法可以参考[官方文档](https://link.zhihu.com/?target=https%3A//docs.swanlab.cn/zh/guide_cloud/integration/integration-huggingface-transformers.html)：\n```python\nfrom transformers import TrainingArguments, Trainer\n\nargs = TrainingArguments(\n    ...,\n    report_to=\"swanlab\",\n    run_name=\"qwen3-1.7B\",\n)\n\ntrainer = Trainer(..., args=args)\n```\n如果你是第一次使用SwanLab，那么还需要去[https://swanlab.cn](https://link.zhihu.com/?target=https%3A//swanlab.cn/)上注册一个账号，在用户设置页面复制你的API Key，然后在训练开始时，选择【2】，然后粘贴进去即可：\n![09-07](./qwen3/07.png)",
    "142": "一级标题：Qwen3大模型微调实战：医学推理对话\n二级标题：5. 完整代码\n内容：\n开始训练时的目录结构：\n```\n|--- train.py\n|--- train.jsonl\n|--- val.jsonl\n```\ntrain.py：\n```python\nimport json\nimport pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom modelscope import snapshot_download, AutoTokenizer\nfrom transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\nimport os\nimport swanlab\n\nos.environ[\"SWANLAB_PROJECT\"]=\"qwen3-sft-medical\"\nPROMPT = \"你是一个医学专家，你需要根据用户的问题，给出带有思考的回答。\"\nMAX_LENGTH = 2048\n\nswanlab.config.update({\n    \"model\": \"Qwen/Qwen3-1.7B\",\n    \"prompt\": PROMPT,\n    \"data_max_length\": MAX_LENGTH,\n    })\n\ndef dataset_jsonl_transfer(origin_path, new_path):\n    \"\"\"\n    将原始数据集转换为大模型微调所需数据格式的新数据集\n    \"\"\"\n    messages = []\n\n    # 读取旧的JSONL文件\n    with open(origin_path, \"r\") as file:\n        for line in file:\n            # 解析每一行的json数据\n            data = json.loads(line)\n            input = data[\"question\"]\n            output = f\"<think>{data[\"think\"]}</think> \\n {data[\"answer\"]}\"\n            message = {\n                \"instruction\": PROMPT,\n                \"input\": f\"{input}\",\n                \"output\": output,\n            }\n            messages.append(message)\n\n    # 保存重构后的JSONL文件\n    with open(new_path, \"w\", encoding=\"utf-8\") as file:\n        for message in messages:\n            file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n\ndef process_func(example):\n    \"\"\"\n    将数据集进行预处理\n    \"\"\" \n    input_ids, attention_mask, labels = [], [], []\n    instruction = tokenizer(\n        f\"\n\n```markdown\n一级标题：Qwen1.5微调案例\n二级标题：概述\n内容：\n[Qwen1.5](https://modelscope.cn/models/qwen/Qwen1.5-7B-Chat/summary)是通义千问团队的开源大语言模型，由阿里云通义实验室研发。以Qwen-1.5作为基座大模型，通过任务微调的方式实现高准确率的文本分类，是学习**大语言模型微调**的入门任务。\n\n![](/assets/example-qwen-1.png)\n\n微调是一种通过在由（输入，输出）对组成的数据集上进一步训练LLMs的过程。这个过程有助于让LLM在特定的下游任务上表现的更为主出色。\n\n\n\n在这个任务中我们会使用[Qwen-1.5-7b](https://modelscope.cn/models/qwen/Qwen1.5-7B-Chat/summary)模型在[zh_cls_fudan_news](https://modelscope.cn/datasets/swift/zh_cls_fudan-news)数据集上进行指令微调任务，同时使用SwanLab进行监控和可视化。",
    "143": "一级标题：Qwen1.5微调案例\n二级标题：环境安装\n内容：\n本案例基于`Python>=3.10`，请在您的计算机上安装好Python。  \n\n环境依赖:  \n```txt\nswanlab\nmodelscope\ntransformers\ndatasets\npeft\naccelerat\npandas\n```\n\n\n一键安装命令：\n\n```bash \npip install swanlab modelscope transformers datasets peft pandas\n```\n\n> 本案例测试于modelscope==1.14.0、transformers==4.41.2、datasets==2.18.0、peft==0.11.1、accelerate==0.30.1、swanlab==0.3.8",
    "144": "一级标题：Qwen1.5微调案例\n二级标题：数据集介绍\n内容：\n本案例使用的是[zh_cls_fudan-news](https://modelscope.cn/datasets/swift/zh_cls_fudan-news)数据集，该数据集主要被用于训练文本分类模型。\n\nzh_cls_fudan-news由几千条数据，每条数据包含text、category、output三列：\n- text 是训练语料，内容是书籍或新闻的文本内容\n- category 是text的多个备选类型组成的列表\n- output 则是text唯一真实的类型\n\n![](/assets/example-qwen-2.png)\n\n数据集例子如下：\n```\n\"\"\"\n[PROMPT]Text: 第四届全国大企业足球赛复赛结束新华社郑州５月３日电（实习生田兆运）上海大隆机器厂队昨天在洛阳进行的第四届牡丹杯全国大企业足球赛复赛中，以５：４力克成都冶金实验厂队，进入前四名。沪蓉之战，双方势均力敌，９０分钟不分胜负。最后，双方互射点球，沪队才以一球优势取胜。复赛的其它３场比赛，青海山川机床铸造厂队３：０击败东道主洛阳矿山机器厂队，青岛铸造机械厂队３：１战胜石家庄第一印染厂队，武汉肉联厂队１：０险胜天津市第二冶金机械厂队。在今天进行的决定九至十二名的两场比赛中，包钢无缝钢管厂队和河南平顶山矿务局一矿队分别击败河南平顶山锦纶帘子布厂队和江苏盐城无线电总厂队。４日将进行两场半决赛，由青海山川机床铸造厂队和青岛铸造机械厂队分别与武汉肉联厂队和上海大隆机器厂队交锋。本届比赛将于６日结束。（完）\nCategory: Sports, Politics\nOutput:[OUTPUT]Sports\n\"\"\"\n\n```\n\n我们的训练任务，便是希望微调后的大模型能够根据Text和Category组成的提示词，预测出正确的Output。",
    "145": "一级标题：Qwen1.5微调案例\n二级标题：准备工作\n内容：\n在开始训练之前，请先确保环境已安装完成，并保证你有一张 **显存>=16GB** 的GPU。\n\n然后，将数据集下载到本地目录下。下载方式是前往[zh_cls_fudan-news - 魔搭社区](https://modelscope.cn/datasets/swift/zh_cls_fudan-news/files) ，将`train.jsonl`和`test.jsonl`下载到本地根目录下即可：\n\n![](/assets/example-qwen-3.png)",
    "146": "一级标题：Qwen1.5微调案例\n二级标题：完整代码\n内容：\n开始训练时的目录结构：\n\n```txt\n|--- train.py\n|--- train.jsonl\n|--- test.jsonl\n```\n\ntrain.py:\n\n```python\nimport json\nimport pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom modelscope import snapshot_download, AutoTokenizer\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom peft import LoraConfig, TaskType, get_peft_model\nfrom transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\nimport os\nimport swanlab\n\n\ndef dataset_jsonl_transfer(origin_path, new_path):\n    \"\"\"\n    将原始数据集转换为大模型微调所需数据格式的新数据集\n    \"\"\"\n    messages = []\n\n    # 读取旧的JSONL文件\n    with open(origin_path, \"r\") as file:\n        for line in file:\n            # 解析每一行的json数据\n            data = json.loads(line)\n            context = data[\"text\"]\n            catagory = data[\"category\"]\n            label = data[\"output\"]\n            message = {\n                \"instruction\": \"你是一个文本分类领域的专家，你会接收到一段文本和几个潜在的分类选项，请输出文本内容的正确类型\",\n                \"input\": f\"文本:{context},类型选型:{catagory}\",\n                \"output\": label,\n            }\n            messages.append(message)\n\n    # 保存重构后的JSONL文件\n    with open(new_path, \"w\", encoding=\"utf-8\") as file:\n        for message in messages:\n            file.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n            \n            \ndef process_func(example):\n    \"\"\"\n    将数据集进行预处理\n    \"\"\"\n    MAX_LENGTH = 384 \n    input_ids, attention_mask, labels = [], [], []\n    instruction = tokenizer(\n        f\"\n\n### 一级标题：Qwen2.5复现DeepSeek-R1-ZERO\n\n#### 二级标题：简介\n内容：\n本文旨在对deepseek-r1-zero进行复现实验，简单介绍了从r1原理到代码实现，再到结果观测的整个过程。通过SwanLab监控实验过程，确保实验的每个阶段都能精确跟踪与调试。通过这一系列的实验步骤，能够掌握GRPO的实现方法。\n\n![](./grpo/r1-zero-ds-qwen.jpg)\n\n---\n\n#### 二级标题：链接资料\n内容：\n本次实验参考了优秀开源项目[philschmid/deep-learning-pytorch-huggingface](https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/mini-deepseek-r1-aha-grpo.ipynb)，该项目作者是google-deepmind工程师Philipp Schmid，Countdown用于R1训练的idea就是这个项目发起的。\n\n> 模型地址：Qwen2.5-3B-Instruct:[huggingface社区](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct)|[魔搭社区](https://modelscope.cn/models/Qwen/Qwen2.5-3B-Instruct)\n>\n> 数据集地址：Countdown-Tasks-3to4:[huggingface地址](https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4)|[魔搭社区地址](https://modelscope.cn/datasets/zouxuhong/Countdown-Tasks-3to4)\n>\n> 可视化工具SwanLab项目地址：[SwanLab结果可视化](https://swanlab.cn/@LiXinYu/Try_r1/overview)\n\n---\n\n#### 二级标题：DeepSeek-R1原理\n内容：\n论文标题：DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\n\n论文链接：[论文链接](https://arxiv.org/pdf/2501.12948?)\n\n代码地址：[github链接](https://github.com/deepseek-ai/DeepSeek-R1)\n\n**下面是论文里从DeepSeek-V3到DeepSeek-R1的流程图表示**\n\n本次教程仅考虑从DeepSeek-V3--->DeepSeek-R1-Zero的复现过程，基于Qwen2.5-3B-Instruct模型实现。\n\n![](./grpo/deepseek-r1-process.png)\n\n**GRPO原理：**\n\n`群体相对策略优化 (GRPO，Group Relative Policy Optimization) `是一种强化学习 (RL) 算法，专门用于增强大型语言模型 (LLM) 中的推理能力。与严重依赖外部评估模型（价值函数）指导学习的传统 RL 方法不同，GRPO 通过评估彼此相关的响应组来优化模型。这种方法可以提高训练效率，使 GRPO 成为需要复杂问题解决和长链思维的推理任务的理想选择。\n\n> GRPO 的本质思路：通过在同一个问题上生成多条回答，把它们彼此之间做“相对比较”，来代替传统 PPO 中的“价值模型”\n\n`传统的强化学习算法（如Proximal Policy Optimization，PPO）`在应用于LLMs的推理任务时面临着重大挑战：\n\n1、依赖批评者模型：\nPPO需要一个独立的批评者模型来评估每个回答的价值，这使内存和计算需求增加了一倍。\n训练批评者模型非常复杂且容易出错，尤其是在需要对主观或细微差别进行评价的任务中。\n\n2、高昂的计算成本：\n强化学习流程通常需要大量计算资源来迭代评估和优化回答。\n将这些方法扩展到更大的LLMs会进一步加剧成本。\n\n3、可扩展性问题：\n绝对奖励评估难以应对多样化任务，使得跨推理领域的泛化变得困难。\n---\n`GRPO如何应对这些挑战：`\n\n1、无批评者优化： GRPO通过比较组内回答，消除了对批评者模型的需求，显著降低了计算开销。\n\n2、相对评估： GRPO不依赖外部评价者，而是利用组内动态来评估每个回答在同一批次中的相对表现。\n\n3、高效训练： 通过专注于组内优势，GRPO简化了奖励估计流程，使其对大型模型的训练更快且更具可扩展性。\n\n下图是PPO与GRPO的对比，GRPO放弃了价值模型，从分组得分中估计，显著减少了训练资源\n\n![grpo](./grpo/grpo.png)\n\n> 看到一位作者的看法，把GRPO比作老师给学生上课，老师让一组学生解决一个问题。\n> 老师没有单独为每个学生打分，而是让学生在组内比较彼此的答案。表现更好的学生会得到鼓励，而其他人则从错误中学习。随着时间的推移，整个组会逐渐提高，变得更准确和一致。GRPO 将这一原理应用于训练AI模型，使其能够高效地学习。\n\n---\n\n#### 二级标题：实验代码\n内容：\n### 1、环境搭建\n\n> 环境设置如下：\n> \n> pip install transformers==4.48.1 \n> \n> pip install peft==0.14.0\n> \n> conda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 -c pytorch\n> \n> pip install datasets\n> \n> pip install accelerate\n> \n> pip install trl\n> \n> pip install -U swanlab\n> \n> pip install deepspeed\n\n### 2、数据预处理\n\n本次实验使用一个490k条数据的[Countdown数据集](https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4)来进行实验，内容如下图所示：\n\n![数据集内容](./grpo/data-countdown.png)\n\n该数据集仅有两项，一个是target结果数据，一个是nums组合数据，我们的目的是为了让模型思考如何从nums经过+、-、*、/计算得到target，为了让模型更好的激活思考能力，我们需要对其设置提示词模板，最重要让模型回答成如下模样：\n\n```text\n<think>:\n让我们来思考下,……\n</think>\n\n<answer>\n……\n</answer>\n```\n同时，由于每个模型都有对应的训练格式模板，比如Qwen的模板在其权重文件中的tokenizer_config.json文件里，具体[例子](https://modelscope.cn/models/Qwen/Qwen2.5-3B-Instruct/file/view/master?fileName=tokenizer_config.json&status=1)如下：\n\n```json\n\"chat_template\": \"{%- if tools %}\\n    {{- 'system\\\\n' }}\\n    {%- if messages[0]['role'] == 'system' %}\\n        {{- messages[0]['content'] }}\\n    {%- else %}\\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\\n    {%- endif %}\\n    {{- \\\"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\\\" }}\\n    {%- for tool in tools %}\\n        {{- \\\"\\\\n\\\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \\\"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within",
    "147": "一级标题：Qwen2-VL多模态大模型微调实战\n二级标题：环境配置\n内容：\n环境配置分为三步：\n\n1. 确保你的电脑上至少有一张英伟达显卡，并已安装好了CUDA环境。\n2. 安装Python（版本>=3.8）以及能够调用CUDA加速的PyTorch。\n3. 安装Qwen2-VL微调相关的第三方库，可以使用以下命令：\n\n```bash\npython -m pip install --upgrade pip\n# 更换 pypi 源加速库的安装\npip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n\npip install modelscope==1.18.0\npip install transformers==4.46.2\npip install sentencepiece==0.2.0\npip install accelerate==1.1.1\npip install datasets==2.18.0\npip install peft==0.13.2\npip install swanlab==0.3.25\npip install qwen-vl-utils==0.0.8\n```",
    "148": "一级标题：Qwen2-VL多模态大模型微调实战\n二级标题：准备数据集\n内容：\n本节使用的是 [coco_2014_caption](https://modelscope.cn/datasets/modelscope/coco_2014_caption/summary) 数据集（中的500张图），该数据集主要用于多模态（Image-to-Text）任务。\n\n> 数据集介绍：COCO 2014 Caption数据集是Microsoft Common Objects in Context (COCO)数据集的一部分，主要用于图像描述任务。该数据集包含了大约40万张图像，每张图像都有至少1个人工生成的英文描述语句。这些描述语句旨在帮助计算机理解图像内容，并为图像自动生成描述提供训练数据。\n\n![](./qwen_vl_coco/02.png)\n\n在本节的任务中，我们主要使用其中的前500张图像，并对它进行处理和格式调整，目标是组合成如下格式的json文件：\n\n```json\n[\n{\n    \"id\": \"identity_1\",\n    \"conversations\": [\n      {\n        \"from\": \"user\",\n        \"value\": \"COCO Yes:",
    "149": "一级标题：Stable Diffusion文生图微调\n二级标题：环境安装\n内容：\n本案例基于**Python>=3.8**，请在您的计算机上安装好Python；\n\n另外，您的计算机上至少要有一张英伟达显卡（显存大约要求22GB左右）。\n\n我们需要安装以下这几个Python库，在这之前，请确保你的环境内已安装了pytorch以及CUDA：\n\n```txt\nswanlab\ndiffusers\ndatasets\naccelerate\ntorchvision\ntransformers\n```\n\n一键安装命令：\n\n```bash\npip install swanlab diffusers datasets accelerate torchvision transformers\n```\n\n> 本文的代码测试于diffusers==0.29.0、accelerate==0.30.1、datasets==2.18.0、transformers==4.41.2、swanlab==0.3.11，更多库版本可查看[SwanLab记录的Python环境](https://swanlab.cn/@ZeyiLin/SD-Naruto/runs/21flglg1lbnqo67a6f1kr/environment/requirements)。",
    "150": "一级标题：Stable Diffusion文生图微调\n二级标题：准备数据集\n内容：\n本案例是用的是[火影忍者](https://huggingface.co/datasets/lambdalabs/naruto-blip-captions)数据集，该数据集主要被用于训练文生图模型。\n\n该数据集由1200条（图像、描述）对组成，左边是火影人物的图像，右边是对它的描述：\n\n![alt text](./images/stable_diffusion/02.png)\n\n我们的训练任务，便是希望训练后的SD模型能够输入提示词，生成火影风格的图像：\n\n![alt text](./images/stable_diffusion/03.png)\n\n---\n\n数据集的大小大约700MB左右；数据集的下载方式有两种：\n\n1. 如果你的网络与HuggingFace连接是通畅的，那么直接运行我下面提供的代码即可，它会直接通过HF的`datasets`库进行下载。\n2. 如果网络存在问题，我也把它放到[百度网盘](https://pan.baidu.com/s/1Yu5HjXnHxK0Wgymc8G-g5g?pwd=gtk8)（提取码: gtk8），下载`naruto-blip-captions.zip`到本地解压后，运行到与训练脚本同一目录下。",
    "151": "一级标题：Stable Diffusion文生图微调\n二级标题：准备模型\n内容：\n这里我们使用HuggingFace上Runway发布的[stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)模型。\n\n![alt text](./images/stable_diffusion/04.png)\n\n模型的下载方式同样有两种：\n\n1. 如果你的网络与HuggingFace连接是通畅的，那么直接运行我下面提供的代码即可，它会直接通过HF的`transformers`库进行下载。\n2. 如果网络存在问题，我也把它放到[百度网盘](https://pan.baidu.com/s/1Yu5HjXnHxK0Wgymc8G-g5g?pwd=gtk8)（提取码: gtk8），下载`stable-diffusion-v1-5.zip`到本地解压后，运行到与训练脚本同一目录下。",
    "152": "一级标题：Stable Diffusion文生图微调\n二级标题：配置训练可视化工具\n内容：\n我们使用[SwanLab](https://swanlab.cn)来监控整个训练过程，并评估最终的模型效果。\n\n如果你是第一次使用SwanLab，那么还需要去https://swanlab.cn上注册一个账号，在**用户设置**页面复制你的API Key，然后在训练开始时粘贴进去即可：\n\n![alt text](./images/stable_diffusion/05.png)",
    "153": "一级标题：Stable Diffusion文生图微调\n二级标题：开始训练\n内容：\n由于训练的代码比较长，所以我把它放到了[Github](https://github.com/Zeyi-Lin/Stable-Diffusion-Example/tree/main)里，请Clone里面的代码：\n\n```bash\ngit clone https://github.com/Zeyi-Lin/Stable-Diffusion-Example.git\n```\n\n如果你与HuggingFace的网络连接通畅，那么直接运行训练：\n\n```bash\npython train_sd1-5_naruto.py \\\n  --use_ema \\\n  --resolution=512 --center_crop --random_flip \\\n  --train_batch_size=1 \\\n  --gradient_accumulation_steps=4 \\\n  --gradient_checkpointing \\\n  --max_train_steps=15000 \\\n  --learning_rate=1e-05 \\\n  --max_grad_norm=1 \\\n  --seed=42 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --output_dir=\"sd-naruto-model\"\n```\n\n上面这些参数的含义如下：\n\n- `--use_ema`: 使用指数移动平均 (EMA) 技术，该技术可以提高模型的泛化能力，在训练过程中使用模型参数的移动平均值进行预测，而不是直接使用当前模型参数。\n- `--resolution=512`: 设置训练图像的分辨率为 512 像素。\n- `--center_crop`: 对图像进行中心裁剪，将图像的中心部分作为训练样本，忽略图像边缘的部分。\n- `--random_flip`: 在训练过程中对图像进行随机翻转，增加训练数据的多样性。\n- `--train_batch_size=1`: 设置训练批次大小为 1，即每次训练只使用一张图像。\n- `--gradient_accumulation_steps=4`: 梯度累积步数为 4，即每进行 4 次训练才进行一次参数更新。\n- `--gradient_checkpointing`: 使用梯度检查点技术，可以减少内存使用量，加快训练速度。\n- `--max_train_steps=15000`: 设置最大训练步数为 15000 步。\n- `--learning_rate=1e-05`: 设置学习率为 1e-05。\n- `--max_grad_norm=1`: 设置梯度范数的最大值为 1，防止梯度爆炸。\n- `--seed=42`: 设置随机种子为 42，确保每次训练的随机性一致。\n- `--lr_scheduler=\"constant\"`: 使用常数学习率调度器，即在整个训练过程中保持学习率不变。\n- `--lr_warmup_steps=0`: 设置学习率预热步数为 0，即不进行预热。\n- `--output_dir=\"sd-naruto-model\"`: 设置模型输出目录为 \"sd-naruto-model\"。\n\n---\n\n如果你的模型或数据集用的是**上面的网盘下载的**，那么你需要做下面的两件事：\n\n**第一步**：将数据集和模型文件夹放到训练脚本同一目录下，文件结构如下：\n\n```txt\n|--- sd_config.py\n|--- train_sd1-5_naruto.py\n|--- stable-diffusion-v1-5\n|--- naruto-blip-captions\n```\n\n`stable-diffusion-v1-5`是下载好的模型文件夹，`naruto-blip-captions`是下载好的数据集文件夹。\n\n**第二步**：修改`sd_config.py`的代码，将`pretrained_model_name_or_path`和`dataset_name`的default值分别改为下面这样：\n\n```python\n    parser.add_argument(\n        \"--pretrained_model_name_or_path\",\n        type=str,\n        default=\"./stable-diffusion-v1-5\",\n    )\n    parser.add_argument(\n        \"--dataset_name\",\n        type=str,\n        default=\"./naruto-blip-captions\",\n    )\n```\n\n然后运行启动命令即可。\n\n---\n\n看到下面的进度条即代表训练开始：\n\n![alt text](./images/stable_diffusion/05.png)",
    "154": "一级标题：Stable Diffusion文生图微调\n二级标题：训练结果演示\n内容：\n我们在[SwanLab](https://swanlab.cn/@ZeyiLin/SD-Naruto/runs/21flglg1lbnqo67a6f1kr/chart)上查看最终的训练结果：\n\n![alt text](./images/stable_diffusion/06.png)\n\n可以看到SD训练的特点是loss一直在震荡，随着epoch的增加，loss在最初下降后，后续的变化其实并不大：\n\n![alt text](./images/stable_diffusion/07.png)\n\n我们来看看主观生成的图像，第一个epoch的图像长这样：\n\n![alt text](./images/stable_diffusion/08.png)\n\n可以看到詹姆斯还是非常的“原生态”，迈克尔杰克逊生成的也怪怪的。。。\n\n再看一下中间的状态：\n\n![alt text](./images/stable_diffusion/09.png)\n\n![alt text](./images/stable_diffusion/10.png)\n\n经过比较长时间的训练后，效果就好了不少。\n\n> 比较有意思的是，比尔盖茨生成出来的形象总是感觉非常邪恶。。。\n\n![alt text](./images/stable_diffusion/11.png)\n\n至此，你已经完成了SD模型在火影忍者数据集上的训练。",
    "155": "一级标题：Stable Diffusion文生图微调\n二级标题：模型推理\n内容：\n训练好的模型会放到`sd-naruto-model`文件夹下，推理代码如下：\n\n```python\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"./sd-naruto-model\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"Lebron James with a hat\"\nimage = pipe(prompt).images[0]  \n    \nimage.save(\"result.png\")\n```",
    "156": "一级标题：Stable Diffusion文生图微调\n二级标题：相关链接\n内容：\n- 代码：[Github](https://github.com/Zeyi-Lin/Stable-Diffusion-Example)\n- 实验日志过程：[SD-naruto - SwanLab](https://swanlab.cn/@ZeyiLin/SD-Naruto/runs/21flglg1lbnqo67a6f1kr/chart)\n- 模型：[runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)\n- 数据集：[lambdalabs/naruto-blip-captions](https://huggingface.co/datasets/lambdalabs/naruto-blip-captions)\n- SwanLab：[https://swanlab.cn](https://swanlab.cn)",
    "157": "一级标题：UNet 医学影像分割\n二级标题：简介\n内容：\n:::info\n计算机视觉，医学影像，图像分割\n:::\n\n[![](/assets/badge1.svg)](https://swanlab.cn/@ZeyiLin/Unet-Medical-Segmentation/runs/67konj7kdqhnfdmusy2u6/chart)\n\n[训练过程](https://swanlab.cn/@ZeyiLin/Unet-Medical-Segmentation/runs/67konj7kdqhnfdmusy2u6/chart)\n\nUNet是一种基于卷积神经网络（CNN）的医学影像分割模型，由Ronneberger等人于2015年提出。本文我们将简要介绍基于PyTorch框架，使用UNet模型在脑瘤医学影像分割数据集上进行训练，同时通过SwanLab监控训练过程，实现对病灶区域或器官结构的智能定位。\n\n![](./unet-medical-segmentation/train_image.png)\n\n- 代码：完整代码直接看本文第5节 或 [Github](https://github.com/Zeyi-Lin/UNet-Medical)\n- 实验日志过程：[Unet-Medical-Segmentation - SwanLab](https://swanlab.cn/@ZeyiLin/Unet-Medical-Segmentation/runs/67konj7kdqhnfdmusy2u6/chart)\n- 模型：UNet（Pytorch代码直接写）\n- 数据集：[brain-tumor-image-dataset-semantic-segmentation - Kagggle](https://www.kaggle.com/datasets/pkdarabi/brain-tumor-image-dataset-semantic-segmentation)\n- SwanLab：[https://swanlab.cn](https://swanlab.cn)\n- 论文：[《U-Net: Convolutional Networks for Biomedical Image Segmentation》](https://arxiv.org/abs/1505.04597)",
    "158": "一级标题：UNet 医学影像分割\n二级标题：环境配置\n内容：\n环境配置分为三步：\n\n1. 确保你的电脑上至少有一张英伟达显卡，并已安装好了CUDA环境。\n2. 安装Python（版本>=3.8）以及能够调用CUDA加速的PyTorch。\n3. 安装UNet微调相关的第三方库，可以使用以下命令：\n\n```bash\ngit clone https://github.com/Zeyi-Lin/UNet-Medical.git\ncd UNet-Medical\npip install -r requirements.txt\n```",
    "159": "一级标题：UNet 医学影像分割\n二级标题：准备数据集\n内容：\n本节使用的是 [脑瘤图像分割](https://www.kaggle.com/datasets/pkdarabi/brain-tumor-image-dataset-semantic-segmentation) 数据集，该数据集主要用于医学影像分割任务。\n\n> ​​数据集介绍​​：Brain Tumor Segmentation Dataset 是专用于医学图像语义分割的数据集，旨在精准识别脑肿瘤区域。该数据集包含两类标注（肿瘤/非肿瘤），通过像素级分类实现肿瘤区域的细粒度分割，适用于训练和评估医学影像分割模型，为脑肿瘤诊断提供自动化分析支持。\n\n![](./unet-medical-segmentation/kaggle.png)\n\n在本节的任务中，我们主要是将数据集下载下来并解压，以供后续的训练。\n\n**下载数据集并解压：**\n\n```bash\npython download.py\nunzip dataset/Brain_Tumor_Image_DataSet.zip -d dataset/\n```\n\n完成上述步骤后，你应该可以根目录下看到这样的文件夹：\n\n![](./unet-medical-segmentation/dir.png)\n\n文件夹中包含训练集、验证集和测试集，里面有图像文件（`jpg`格式）和标注文件（`json`格式）。至此，我们完成了数据集的准备。\n\n下面是一些细节的代码展示，然后你想马上训练起来，可以直接跳到第五节。",
    "160": "一级标题：UNet 医学影像分割\n二级标题：模型代码\n内容：\n这里我们使用PyTorch来写UNet模型（在`net.py`中）。代码展示如下：\n\n```python\nimport torch\nimport torch.nn as nn\n\n# 定义U-Net模型的下采样块\nclass DownBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, dropout_prob=0, max_pooling=True):\n        super(DownBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(2) if max_pooling else None\n        self.dropout = nn.Dropout(dropout_prob) if dropout_prob > 0 else None\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        if self.dropout:\n            x = self.dropout(x)\n        skip = x\n        if self.maxpool:\n            x = self.maxpool(x)\n        return x, skip\n\n# 定义U-Net模型的上采样块\nclass UpBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(UpBlock, self).__init__()\n        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n        self.conv1 = nn.Conv2d(out_channels * 2, out_channels, 3, padding=1)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x, skip):\n        x = self.up(x)\n        x = torch.cat([x, skip], dim=1)\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        return x\n\n# 定义完整的U-Net模型\nclass UNet(nn.Module):\n    def __init__(self, n_channels=3, n_classes=1, n_filters=32):\n        super(UNet, self).__init__()\n        \n        # 编码器路径\n        self.down1 = DownBlock(n_channels, n_filters)\n        self.down2 = DownBlock(n_filters, n_filters * 2)\n        self.down3 = DownBlock(n_filters * 2, n_filters * 4)\n        self.down4 = DownBlock(n_filters * 4, n_filters * 8)\n        self.down5 = DownBlock(n_filters * 8, n_filters * 16)\n        \n        # 瓶颈层 - 移除最后的maxpooling\n        self.bottleneck = DownBlock(n_filters * 16, n_filters * 32, dropout_prob=0.4, max_pooling=False)\n        \n        # 解码器路径\n        self.up1 = UpBlock(n_filters * 32, n_filters * 16)\n        self.up2 = UpBlock(n_filters * 16, n_filters * 8)\n        self.up3 = UpBlock(n_filters * 8, n_filters * 4)\n        self.up4 = UpBlock(n_filters * 4, n_filters * 2)\n        self.up5 = UpBlock(n_filters * 2, n_filters)\n        \n        # 输出层\n        self.outc = nn.Conv2d(n_filters, n_classes, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # 编码器路径\n        x1, skip1 = self.down1(x)      # 128\n        x2, skip2 = self.down2(x1)     # 64\n        x3, skip3 = self.down3(x2)     # 32\n        x4, skip4 = self.down4(x3)     # 16\n        x5, skip5 = self.down5(x4)     # 8\n        \n        # 瓶颈层\n        x6, skip6 = self.bottleneck(x5)  # 8 (无下采样)\n        \n        # 解码器路径\n        x = self.up1(x6, skip5)    # 16\n        x = self.up2(x, skip4)     # 32\n        x = self.up3(x, skip3)     # 64\n        x = self.up4(x, skip2)     # 128\n        x = self.up5(x, skip1)     # 256\n        \n        x = self.outc(x)\n        x = self.sigmoid(x)\n        return x\n```\n\n该模型保存为`pth`文件，大约需要124MB。",
    "161": "一级标题：UNet 医学影像分割\n二级标题：使用SwanLab跟踪实验\n内容：\n[SwanLab](https://github.com/swanhubx/swanlab) 是一个开源的模型训练记录工具。SwanLab面向AI研究者，提供了训练可视化、自动日志记录、超参数记录、实验对比、多人协同等功能。在SwanLab上，研究者能基于直观的可视化图表发现训练问题，对比多个实验找到研究灵感，并通过在线链接的分享与基于组织的多人协同训练，打破团队沟通的壁垒。\n\n<video controls src=\"../guide_cloud/general/what_is_swanlab/demo.mp4\"></video>\n\n在本次训练中，我们设置swanlab的项目为`Unet-Medical-Segmentation`，实验名称为`bs32-epoch40`，并设置超参数如下：\n\n```python\nswanlab.init(\n    project=\"Unet-Medical-Segmentation\",\n    experiment_name=\"bs32-epoch40\",\n    config={\n        \"batch_size\": 32,\n        \"learning_rate\": 1e-4,\n        \"num_epochs\": 40,\n        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    },\n)\n```\n\n可以看到，这次训练的batch_size为32，学习率为1e-4，训练40个epoch。\n\n首次使用SwanLab，需要先在[官网](https://swanlab.cn)注册一个账号，然后在用户设置页面复制你的API Key，然后在训练开始提示登录时粘贴即可，后续无需再次登录：\n\n![](./qwen_vl_coco/04.png)",
    "162": "一级标题：UNet 医学影像分割\n二级标题：开始训练\n内容：\n查看可视化训练过程：<a href=\"https://swanlab.cn/@ZeyiLin/Unet-Medical-Segmentation/runs/67konj7kdqhnfdmusy2u6/chart\" target=\"_blank\">Unet-Medical-Segmentation</a>\n\n**本节代码做了以下几件事：**\n1. 加载UNet模型\n2. 加载数据集，分为训练集、验证集和测试集，数据处理为Resize为 (256, 256)和 Normalization \n3. 使用SwanLab记录训练过程，包括超参数、指标和最终的模型输出结果\n4. 训练40个epoch\n5. 生成最后的预测图像\n\n开始执行代码时的目录结构应该是：\n\n```\n|———— dataset/\n|———————— train/\n|———————— val/\n|———————— test/\n|———— readme_files/\n|———— train.py\n|———— data.py\n|———— net.py\n|———— download.py\n|———— requirements.txt\n```\n\n**完整代码如下**\n\ntrain.py：\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom pycocotools.coco import COCO\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nimport random\nimport swanlab\nfrom net import UNet\nfrom data import COCOSegmentationDataset\n\n\n# 数据路径设置\ntrain_dir = './dataset/train'\nval_dir = './dataset/valid'\ntest_dir = './dataset/test'\n\ntrain_annotation_file = './dataset/train/_annotations.coco.json'\ntest_annotation_file = './dataset/test/_annotations.coco.json'\nval_annotation_file = './dataset/valid/_annotations.coco.json'\n\n# 加载COCO数据集\ntrain_coco = COCO(train_annotation_file)\nval_coco = COCO(val_annotation_file)\ntest_coco = COCO(test_annotation_file)\n\n# 定义损失函数\ndef dice_loss(pred, target, smooth=1e-6):\n    pred_flat = pred.view(-1)\n    target_flat = target.view(-1)\n    intersection = (pred_flat * target_flat).sum()\n    return 1 - ((2. * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth))\n\ndef combined_loss(pred, target):\n    dice = dice_loss(pred, target)\n    bce = nn.BCELoss()(pred, target)\n    return 0.6 * dice + 0.4 * bce\n\n# 训练函数\ndef train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n    best_val_loss = float('inf')\n    patience = 8\n    patience_counter = 0\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        train_acc = 0\n        \n        for images, masks in train_loader:\n            images, masks = images.to(device), masks.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, masks)\n            \n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            train_acc += (outputs.round() == masks).float().mean().item()\n\n        train_loss /= len(train_loader)\n        train_acc /= len(train_loader)\n        \n        # 验证\n        model.eval()\n        val_loss = 0\n        val_acc = 0\n        \n        with torch.no_grad():\n            for images, masks in val_loader:\n                images, masks = images.to(device), masks.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, masks)\n                \n                val_loss += loss.item()\n                val_acc += (outputs.round() == masks).float().mean().item()\n        \n        val_loss /= len(val_loader)\n        val_acc /= len(val_loader)\n        \n        swanlab.log(\n            {\n                \"train/loss\": train_loss,\n                \"train/acc\": train_acc,\n                \"train/epoch\": epoch+1,\n                \"val/loss\": val_loss,\n                \"val/acc\": val_acc,\n            },\n            step=epoch+1)\n        \n        print(f'Epoch {epoch+1}/{num_epochs}:')\n        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n        \n        # 早停\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), 'best_model.pth')\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\"Early stopping triggered\")\n                break\n\ndef main():\n    swanlab.init(\n        project=\"Unet-Medical-Segmentation\",\n        experiment_name=\"bs32-epoch40\",\n        config={\n            \"batch_size\": 32,\n            \"learning_rate\": 1e-4,\n            \"num_epochs\": 40,\n            \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n        },\n    )\n    \n    # 设置设备\n    device = torch.device(swanlab.config[\"device\"])\n    \n    # 数据预处理\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Resize((256, 256)),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    # 创建数据集\n    train_dataset = COCOSegmentationDataset(train_coco, train_dir, transform=transform)\n    val_dataset = COCOSegmentationDataset(val_coco, val_dir, transform=transform)\n    test_dataset = COCOSegmentationDataset(test_coco, test_dir, transform=transform)\n    \n    # 创建数据加载器\n    BATCH_SIZE = swanlab.config[\"batch_size\"]\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n    \n    # 初始化模型\n    model = UNet(n_filters=32).to(device)\n    \n    # 设置优化器和学习率\n    optimizer = optim.Adam(model.parameters(), lr=swanlab.config[\"learning_rate\"])\n    \n    # 训练模型\n    train_model(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        criterion=combined_loss,\n        optimizer=optimizer,\n        num_epochs=swanlab.config[\"num_epochs\"],\n        device=device,\n    )\n    \n    # 在测试集上评估\n    model.eval()\n    test_loss = 0\n    test_acc = 0\n    \n    with torch.no_grad():\n        for images, masks in test_loader:\n            images, masks = images.to(device), masks.to(device)\n            outputs = model(images)\n            loss = combined_loss(outputs, masks)\n            test_loss += loss.item()\n            test_acc += (outputs.round() == masks).float().mean().item()\n    \n    test_loss /= len(test_loader)\n    test_acc /= len(test_loader)\n    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n    swanlab.log({\"test/loss\": test_loss, \"test/acc\": test_acc})\n    \n    # 可视化预测结果\n    visualize_predictions(model, test_loader, device, num_samples=10)\n    \n\ndef visualize_predictions(model, test_loader, device, num_samples=5, threshold=0.5):\n    model.eval()\n    with torch.no_grad():\n        # 获取一个批次的数据\n        images, masks = next(iter(test_loader))\n        images, masks = images.to(device), masks.to(device)\n        predictions = model(images)\n        \n        # 将预测结果转换为二值掩码\n        binary_predictions = (predictions > threshold).float()\n        \n        # 选择前3个样本\n        indices = random.sample(range(len(images)), min(num_samples, len(images)))\n        indices = indices[:8]\n        \n        # 创建一个大图\n        plt.figure(figsize=(15, 8))  # 调整图像大小以适应新增的行\n        plt.suptitle(f'Epoch {swanlab.config[\"num_epochs\"]} Predictions (Random 6 samples)')\n        \n        for i, idx in enumerate(indices):\n            #",
    "163": "一级标题：Yolo目标检测\n二级标题：概述\n内容：\nYOLO（You Only Look Once）是一种由Joseph Redmon等人提出的目标检测模型，广泛应用于各种计算机视觉任务。YOLO通过将图像分成网格，并在每个网格内预测边界框和类别概率，能够实现实时的目标检测，在许多任务上表现出色。\n\n在这个任务中，我们将使用YOLO模型在COCO128数据集上进行目标检测任务，同时用SwanLab进行监控和可视化。\n\n![yolo](/assets/example-yolo-1.png)\n\nCOCO128 数据集是一个小型的目标检测数据集，来源于广泛使用的 COCO（Common Objects in Context）数据集。COCO128 数据集包含 128 张图像，是 COCO 数据集的一个子集，主要用于快速测试和调试目标检测模型。",
    "164": "一级标题：Yolo目标检测\n二级标题：环境安装\n内容：\n本案例基于`Python>=3.8`，请在您的计算机上安装好Python。 环境依赖：\n\n```txt\nultralytics\nswanlab\n```\n\n快速安装命令：\n\n```bash\npip install ultralytics swanlab\n```\n\n> 本文的代码测试于ultralytics==8.2.18、swanlab==0.3.6",
    "165": "一级标题：Yolo目标检测\n二级标题：完整代码\n内容：\n```python\nfrom ultralytics import YOLO\nfrom swanlab.integration.ultralytics import add_swanlab_callback\n\ndef main():\n    model = YOLO(\"yolov8n.pt\")\n    add_swanlab_callback(model)\n    model.train(data=\"coco128.yaml\", epochs=5, imgsz=640, batch=64)\n\nif __name__ == \"__main__\":\n    main()\n```",
    "166": "一级标题：Yolo目标检测\n二级标题：演示效果\n内容：\n![yolo-2](/assets/example-yolo-2.png)\n\n![yolo-3](/assets/example-yolo-3.png)",
    "167": "一级标题：为 SwanLab 作出贡献\n二级标题：标准开发流程\n内容：\n有兴趣为 SwanLab 做出贡献吗？我们欢迎社区的贡献！本指南讨论`swanlab`的开发工作流和内部结构。\n\n1. 浏览 GitHub 上的[Issues](https://github.com/SwanHubX/SwanLab/issues)，查看你愿意添加的功能或修复的错误，以及它们是否已被\n   Pull Request。\n\n    - 如果没有，请创建一个[新 Issue](https://github.com/SwanHubX/SwanLab/issues/new/choose)——这将帮助项目跟踪功能请求和错误报告，并确保不重复工作。\n\n2. 如果你是第一次为开源项目贡献代码，请转到 [本项目首页](https://github.com/SwanHubX/SwanLab) 并单击右上角的\"Fork\"\n   按钮。这将创建你用于开发的仓库的个人副本。\n\n    - 将 Fork 的项目克隆到你的计算机，并添加指向`swanlab`项目的远程链接：\n\n   ```bash\n   git clone https://github.com/<your-username>/swanlab.git\n   cd swanlab\n   git remote add upstream https://github.com/swanhubx/swanlab.git\n   ```\n\n3. 开发你的贡献\n\n    - 确保您的 Fork 与主存储库同步：\n\n   ```bash\n   git checkout main\n   git pull upstream main\n   ```\n\n    - 创建一个`git`分支，您将在其中发展您的贡献。为分支使用合理的名称，例如：\n\n   ```bash\n   git checkout -b <username>/<short-dash-seperated-feature-description>\n   ```\n\n    - 当你取得进展时，在本地提交你的改动，例如：\n\n   ```bash\n   git add changed-file.py tests/test-changed-file.py\n   git commit -m \"feat(integrations): Add integration with the `awesomepyml` library\"\n   ```\n\n4. 发起贡献：\n\n    - [Github Pull Request](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests)\n    - 当您的贡献准备就绪后，将您的分支推送到 GitHub：\n\n   ```bash\n   git push origin <username>/<short-dash-seperated-feature-description>\n   ```\n\n    - 分支上传后， `GitHub`将打印一个 URL，用于将您的贡献作为拉取请求提交。在浏览器中打开该 URL，为您的拉取请求编写信息丰富的标题和详细描述，然后提交。\n\n    - 请将相关 Issue（现有 Issue 或您创建的 Issue）链接到您的 PR。请参阅 PR 页面的右栏。或者，在 PR\n      描述中提及“修复问题链接” - GitHub 将自动进行链接。\n\n    - 我们将审查您的贡献并提供反馈。要合并审阅者建议的更改，请将编辑提交到您的分支，然后再次推送到分支（无需重新创建拉取请求，它将自动跟踪对分支的修改），例如：\n\n   ```bash\n   git add tests/test-changed-file.py\n   git commit -m \"test(sdk): Add a test case to address reviewer feedback\"\n   git push origin <username>/<short-dash-seperated-feature-description>\n   ```\n\n    - 一旦您的拉取请求被审阅者批准，它将被合并到存储库的主分支中。",
    "168": "一级标题：为 SwanLab 作出贡献\n二级标题：IDE 与插件\n内容：\n1. **使用 VSCode 作为你的开发 IDE**\n\n   SwanLab 仓库已经配好了[VSCode](https://code.visualstudio.com/)的环境、插件与调试脚本（位于`.vscode`\n   文件夹中），使用 VSCode 开发 SwanLab 会有最好的体验。\n\n2. **安装 VSCode 插件（可选）**\n\n   用 VSCode 打开项目，进入 [扩展] ，在搜索框输入“@recommended”，会出现一系列推荐插件，推荐全部安装这些插件。\n\n   ![vscode-recommend](/assets/guide_cloud/community/contributing-code/vscode_recommend.png)",
    "169": "一级标题：为 SwanLab 作出贡献\n二级标题：配置 Python 环境\n内容：\nSwanLab 项目环境需要`python>=3.8`的支持。\n\n必须性的 python 依赖集中记录在项目根目录下的 `requirements.txt`。\n\n同样在项目根目录启动终端，运行以下命令安装依赖：\n\n```Bash\n# swanlab所依赖的包\npip install -r requirements.txt\npip install -r requirements-media.txt\n```\n\n编译、开发、单元测试等工作需要使用以下命令额外安装依赖：\n\n```Bash\n# 编译、单元测试等功能需要使用的包\npip install -r requirements-dev.txt\n```",
    "170": "一级标题：为 SwanLab 作出贡献\n二级标题：调试脚本\n内容：\n1. **VSCode 调试脚本**\n\n在 VSCode-运行和调试 中，项目配置好了一系列调试脚本：\n\n![img](/assets/guide_cloud/community/contributing-code/debug.png)\n\n- **开启一个实验**：运行`test/create_experiment.py`脚本\n\n- **运行当前文件**：使用配置好的 Python 环境运行你选中的文件\n\n- **测试当前文件**：使用 debug 模式测试你选中的文件\n\n- **进行所有单元测试**：运行`test/unit`中的脚本对 swanlab 基础功能进行完整单元测试\n\n- **(跳过云)进行所有单元测试**：运行`test/unit`中的脚本对 swanlab 基础功能进行完整单元测试，但是跳过云测试\n\n- **构建项目**：打包项目为 whl 文件（pip 安装包格式）\n\nPs: 如果你不想使用 VSCode 进行开发，可以前往`.vscode/launch.json`，查看每个调试项对应的命令，了解其配置。",
    "171": "一级标题：为 SwanLab 作出贡献\n二级标题：python 脚本调试\n内容：\n在完成你的改动后，可以将你用于测试的 python 脚本放到根目录或`test`文件夹下，然后通过[VSCode 脚本](#调试脚本)中的\"\n运行当前文件\"来运行你的 Python 测试脚本, 这样你的脚本运行将使用到已改动后的 swanlab。",
    "172": "一级标题：为 SwanLab 作出贡献\n二级标题：单元测试\n内容：\n可以通过[VSCode 脚本](#调试脚本)或者在项目根目录下运行以下命令进行单元测试：\n\n```Bash\nexport PYTHONPATH=. && pytest test/unit\n```\n\n由于 swanlab 涉及与云端的交互，而云端部分是闭源的，所以如果你是第一次贡献代码，最简单的方式是只进行本地测试。\n针对这种情况，请在本地根目录下创建`.env`文件，并填写如下环境变量配置：\n\n```dotenv\nSWANLAB_RUNTIME=test-no-cloud\n```\n\n这样就可以跳过云端测试，只进行本地的部分功能测试。 如果想进行完整的测试，请在`.env`中补充如下信息：\n\n```dotenv\nSWANLAB_RUNTIME=test\nSWANLAB_API_KEY=<你的API KEY>\nSWANLAB_API_HOST=https://swanlab.cn/api\nSWANLAB_WEB_HOST=https://swanlab.cn\n```\n\n*注意：在进行云端版测试时会在您的云端账号下生成一些无用的测试实验数据，需要手动删除*\n\n配置完后即可运行完整测试",
    "173": "一级标题：为SwanLab官方文档做贡献\n二级标题：如何为文档做贡献\n内容：\n很简单！只需要克隆项目、增添或修改Markdown文件、提交他们，再创建一个PR就可以。",
    "174": "一级标题：为SwanLab官方文档做贡献\n二级标题：环境安装\n内容：\n1. 克隆本仓库\n\n```bash\ngit clone https://github.com/SwanHubX/SwanLab-Docs\n```\n\n2. 安装依赖环境\n\n需要提前安装nodejs和npm，详细方法请查询[node官方教程](https://nodejs.org/en/download/package-manager)\n\n使用如下命令安装其他依赖项目\n\n```bash\nnpm add -D vitepress\n```",
    "175": "一级标题：为SwanLab官方文档做贡献\n二级标题：本地运行文档\n内容：\n如果进行本地开发或者预览文档，可在项目根目录运行：\n\n```bash\nnpm run docs:dev\n```\n\n如果要进行完整的编译打包，使用如下命令：\n\n```bash\nnpm run docs:build\nnpm run docs:preview\n```",
    "176": "一级标题：关于我们\n二级标题：公司简介\n内容：\n情感机器（北京）科技有限公司 是一家专注于人工智能和机器学习底层工具研发的高科技企业。公司致力于为AI开发者提供基础开发工具和建立开源开放的技术社区。\n\n![](/assets/emotion-machine.png)\n\n使命：打造AI工具链，赋能全球AI开发者生态。",
    "177": "一级标题：关于我们\n二级标题：公司信息\n内容：\n**公司**：情感机器（北京）科技有限公司  \n**工作地点**：北京市朝阳区关庄路2号院1号楼中关村科技服务大厦2层B205-1室  \n**联系我们**：contact@swanlab.cn  \n**交流群**：[微信](/guide_cloud/community/online-support.md)",
    "178": "一级标题：Github README徽章\n二级标题：徽章\n内容：\n[![](https://raw.githubusercontent.com/SwanHubX/assets/main/badge2.svg)](https://swanlab.cn)\n\n[![](https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg)](https://swanlab.cn)\n\n\n复制下面的代码到您的README.md文件中（二选一）：\n\n```markdown\n[![](https://raw.githubusercontent.com/SwanHubX/assets/main/badge2.svg)](your experiment url)\n[![](https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg)](your experiment url)\n```",
    "179": "一级标题：Github README徽章\n二级标题：更多设计素材\n内容：\n- [Iconfont](https://www.iconfont.cn/search/index?searchType=icon&q=swanlab)\n- [Github](https://github.com/SwanHubX/assets)",
    "180": "一级标题：在线支持\n二级标题：欢迎与我们交流\n内容：\n| 微信公众号 | 微信交流群 |\n| --- | ---  |\n| <div align=\"center\"><img src=\"/assets/wechat_public_account.jpg\" width=300></div> | <div align=\"center\"><img src=\"/assets/wechat-QR-Code.png\" width=300></div> |\n\n| 飞书群 |\n| --- |\n| <div align=\"center\"><img src=\"/assets/feishu-QR-Code.png\" width=300></div> |",
    "181": "一级标题：在线支持\n二级标题：用Github或邮件联系我们\n内容：\n- **GitHub Issues**：[链接](https://github.com/SwanHubX/SwanLab/issues)，反馈使用SwanLab时遇到的错误和问题\n- **电子邮件支持**：<contact@swanlab.cn>",
    "182": "一级标题：在论文中引用SwanLab\n二级标题：BibTeX引用格式\n内容：\n```bibtex\n@software{Zeyilin_SwanLab_2023,\n  author = {Zeyi Lin, Shaohong Chen, Kang Li, Qiushan Jiang, Zirui Cai,  Kaifang Ji and {The SwanLab team}},\n  doi = {10.5281/zenodo.11100550},\n  license = {Apache-2.0},\n  title = {{SwanLab}},\n  url = {https://github.com/swanhubx/swanlab},\n  year = {2023}\n}\n```",
    "183": "一级标题：用配置文件创建实验\n二级标题：swanlab.config载入配置文件\n内容：\n本节将介绍如何使用json、yaml格式的配置文件来创建SwanLab实验。\n\n`swanlab.init`的`config`参数支持传入json或yaml格式的配置文件路径，并将配置文件解析为字典以进行实验创建。\n\n### 使用json文件\n\n下面是一个json格式的配置文件示例：\n\n```json\n{\n    \"epochs\": 20,\n    \"learning-rate\": 0.001,\n}\n```\n\n将配置文件的路径传入config参数，它会把配置文件解析为字典：\n\n```python\nswanlab.init(config=\"swanlab-init-config.json\")\n# 等价于swanlab.init(config={\"epochs\": 20, \"learning-rate\": 0.001})\n```\n\n### 使用yaml文件\n\n下面是一个yaml格式的配置文件示例：\n\n```yaml\nepochs: 20\nlearning-rate: 0.001\n```\n\n将配置文件的路径传入`config`参数，它会把配置文件解析为字典：\n```python\nswanlab.init(config=\"swanlab-init-config.yaml\")\n# 等价于swanlab.init(config={\"epochs\": 20, \"learning-rate\": 0.001})\n```",
    "184": "一级标题：用配置文件创建实验\n二级标题：swanlab.init载入配置文件\n内容：\n`swanlab.init`的`load`参数支持传入json或yaml格式的配置文件路径，并解析配置文件以进行实验创建。\n\n### 使用json文件\n\n下面是一个json格式的配置文件示例：\n\n```json\n{\n    \"project\": \"cat-dog-classification\",\n    \"experiment_name\": \"Resnet50\",\n    \"description\": \"我的第一个人工智能实验\",\n    \"config\":{\n        \"epochs\": 20,\n        \"learning-rate\": 0.001}\n}\n```\n\n将配置文件的路径传入`load`参数，它会解析配置文件以初始化实验：\n\n```python\nswanlab.init(load=\"swanlab-config.json\")\n# 等价于\n# swanlab.init(\n#     project=\"cat-dog-classification\",\n#     experiment_name=\"Resnet50\",\n#     description=\"我的第一个人工智能实验\",\n#     config={\n#         \"epochs\": 20,\n#         \"learning-rate\": 0.001}\n# )\n```\n\n### 使用yaml文件\n\n下面是一个json格式的配置文件示例：\n\n```yaml\nproject: cat-dog-classification\nexperiment_name: Resnet50\ndescription: 我的第一个人工智能实验\nconfig:\n  epochs: 20\n  learning-rate: 0.001\n```\n\n将配置文件的路径传入`load`参数，它会解析配置文件以初始化实验：\n\n```python\nswanlab.init(load=\"swanlab-config.yaml\")\n# 等价于\n# swanlab.init(\n#     project=\"cat-dog-classification\",\n#     experiment_name=\"Resnet50\",\n#     description=\"我的第一个人工智能实验\",\n#     config={\n#         \"epochs\": 20,\n#         \"learning-rate\": 0.001}\n# )\n```",
    "185": "一级标题：用配置文件创建实验\n二级标题：常见问题\n内容：\n### 1. 配置文件命名是固定的吗？\n\n配置文件的命名是自由的，但推荐使用`swanlab-init`和`swanlab-init-config`这两个配置名。\n\n### 2. 配置文件和脚本内的参数之间是什么关系？\n\n脚本内参数的优先级大于配置文件，即脚本内参数会覆盖配置文件参数。\n\n比如，下面有一段yaml配置文件和示例代码片段：\n\n```yaml\nproject: cat-dog-classification\nexperiment_name: Resnet50\ndescription: 我的第一个人工智能实验\nconfig:\n  epochs: 20\n  learning-rate: 0.001\n```\n\n```python\nswanlab.init(\n    experiment_name=\"resnet101\"，\n    config={\"epochs\": 30},\n    load=\"swanlab-init.yaml\"\n)\n```\n\n最终`experiment_name`为resnet101，`config`为{\"epochs\":30}。",
    "186": "一级标题：创建一个实验\n二级标题：如何创建一个SwanLab实验?\n内容：\n创建一个SwanLab实验分为3步：\n1. 初始化SwanLab\n2. 传递一个超参数字典\n3. 在你的训练循环中记录指标\n\n### 1. 初始化SwanLab\n\n`swanlab.init()`的作用是初始化一个SwanLab实验，它将启动后台进程以同步和记录数据。  \n下面的代码片段展示了如何创建一个名为 **cat-dog-classification** 的新SwanLab项目。并为其添加了：\n\n1. **project**：项目名。\n1. **experiment_name**：实验名。实验名为当前实验的标识，以帮助您识别此实验。  \n2. **description**：描述。描述是对实验的详细介绍。\n\n```python\n# 导入SwanLab Python库\nimport swanlab\n\n# 1. 开启一个SwanLab实验\nrun = swanlab.init(\n    project=\"cat-dog-classification\",\n    experiment_name=\"Resnet50\",\n    description=\"我的第一个人工智能实验\",\n)\n```\n\n当你初始化SwanLab时，`swanlab.init()`将返回一个对象。  \n此外，SwanLab会创建一个本地目录（默认名称为“swanlog”），所有日志和文件都保存在其中，并异步传输到 SwanLab 服务器。（该目录也可以被`swanlab watch -l [logdir]`命令打开本地实验看板。）\n\n::: info\n**注意**：如果调用 `swanlab.init` 时该项目已存在，则实验会添加到预先存在的项目中。  \n例如，如果您已经有一个名为`\"cat-dog-classification\"`的项目，那么新的实验会添加到该项目中。\n:::\n\n<br>\n\n### 2. 传递超参数字典\n\n传递超参数字典，例如学习率或模型类型。  \n你在`config`中传入的字典将被保存并用于后续的实验对比与结果查询。\n\n```python\n# 2. 传递一个超参数字典\nswanlab.config={\"epochs\": 20, \"learning_rate\": 1e-4, \"batch_size\": 32, \"model_type\": \"CNN\"}\n```\n\n有关如何配置实验的更多信息，请参阅[设置实验配置](/guide_cloud/experiment_track/set-experiment-config.md)。\n\n<br>\n\n### 3. 在训练循环中记录指标\n在每轮for循环（epoch）中计算准确率与损失值指标，并用`swanlab.log()`将它们记录到SwanLab中。  \n在默认情况下，当您调用`swanlab.log`时，它会创建一个新的step添加到对应指标的历史数据中，规则是新的step=旧的最大step数+1。  \n下面的代码示例展示了如何用`swanlab.log()`记录指标：  \n\n```python\n# 省略了如何设置模型与如何设置数据集的细节\n\n# 设置模型和数据集\nmodel, dataloader = get_model(), get_data()\n\n# 训练循环\nfor epoch in range(swanlab.config.epochs):\n    for batch in dataloader:\n        loss, acc = model.train_step()\n        # 3. 在你的训练循环中记录指标，用于在仪表盘中进行可视化\n        swanlab.log({\"acc\": acc, \"loss\": loss})\n```\n\n<br>\n\n### 完整代码\n\n包含上述代码片段的完整脚本如下：\n\n```python\n# 导入SwanLab Python库\nimport swanlab\n\n# 1. 开启一个SwanLab实验\nrun = swanlab.init(\n    project=\"cat-dog-classification\",\n    experiment_name=\"Resnet50\",\n    description=\"我的第一个人工智能实验\",\n)\n\n# 2. 传递一个超参数字典\nswanlab.config={\"epochs\": 20, \"learning_rate\": 1e-4, \"batch_size\": 32, \"model_type\": \"CNN\"}\n\n# 省略了如何设置模型与如何设置数据集的细节\n# 设置模型和数据集\nmodel, dataloader = get_model(), get_data()\n\n# 训练循环\nfor epoch in range(swanlab.config.epochs):\n    for batch in dataloader:\n        loss, acc = model.train_step()\n        # 3. 在你的训练循环中记录指标，用于在仪表盘中进行可视化\n        swanlab.log({\"acc\": acc, \"loss\": loss})\n```\n\n<br>\n\n### 可视化你的实验\n\n使用SwanLab仪表盘作为管理和可视化人工智能模型结果的一站式节点。  \n可以可视化丰富的交互式图表，例如折线图、图像图表、音频图表、3D点云图表等。  \n有关如何查看实验更多信息，请参阅[查看实验结果](/guide_cloud/experiment_track/view-result.md)。\n\n![](./create-experiment/show.jpg)",
    "187": "一级标题：创建一个实验\n二级标题：最佳实践\n内容：\n下面介绍一下创建实验时可以参考的写法，一个完整的实验创建可以包含下面这四个参数：  \n- `config`：配置。记录你想要用于复现模型的任何内容，比如超参数、模型名称、数据集等。这些内容将显示在仪表盘的“表格视图”与“实验卡片”页中，也可以作为实验比较、筛选、过滤的依据。\n- `project`：项目。项目是一组可以一起比较的实验，它们将在一个统一的仪表盘中显示。\n- `experiment_name`：实验名。定义实验的名称。您在脚本中设置，可以之后在SwanLab应用上编辑。\n- `description`：描述。对实验的介绍文本，记录不同实验之间的差异和灵感。您在脚本中设置，可以之后在SwanLab应用上编辑。\n\n以下代码片段展示了一个最佳实践案例：\n\n```python\nimport swanlab\n\nconfig = dict(\n    learning_rate=1e-4, optimizer=\"Adam\", architecture=\"Transformer\", dataset_id=\"cats-dogs-2024\"\n)\n\nswanlab.init(\n    project=\"cats-dogs-classification\",\n    experiment_name=\"ViT-Adam-1e-4\",\n    description=\"基于ViT模型和1e-4学习率的Adam优化器的猫狗分类实验。\",\n    config=config,\n)\n```\n\n关于创建SwanLab实验时更多可用参数的信息，请参阅API文档中的[swanlab.init](/api/py-init.md)文档。\n\n内容获取失败",
    "188": "一级标题：实验元数据\n二级标题：获取实验元数据需swanlab>=0.3.25\n内容：\n> 获取实验元数据需swanlab>=0.3.25\n\n总有些时候，你想要在代码中获取实验的元数据，比如实验的项目名、ID、实验名、网址等。",
    "189": "一级标题：实验元数据\n二级标题：获取方式\n内容：\n```python\nimport swanlab\n\nrun = swanlab.init(\n    project=\"test-project\",\n    experiment=\"test-exp\",\n)\n\n# 打印出所有元数据\nprint(run.public.json())\n\n# 打印出单个元数据\nprint(run.public.project_name)\nprint(run.public.cloud.experiment_url)\n```",
    "190": "一级标题：实验元数据\n二级标题：`swanlab.init`返回的类`run`会携带`public`属性\n内容：\n`swanlab.init`返回的类`run`会携带`public`属性，替换了之前的`settings`属性，他会返回：\n\n- `project_name`：当前运行的项目名称\n- `version`：当前运行的swanlab版本\n- `run_id`：一个唯一id\n- `swanlog_dir`：swanlab保存文件夹\n- `run_dir`：本次实验的保存文件夹\n- `cloud`：云端环境的相关信息\n    - `available`：是否运行在云端模式，如果不是，下面的属性全部为None\n    - `project_name`：本次运行的项目名称\n    - `project_url`：本次运行在云端项目url\n    - `experiment_name`：本次运行的实验名称\n    - `experiment_url`：本次运行的云端实验url",
    "191": "一级标题：FAQ\n二级标题：登录时，API Key为什么输入不进去？\n内容：\n见此回答：[链接](https://www.zhihu.com/question/720308649/answer/25076837539)",
    "192": "一级标题：FAQ\n二级标题：如何从一个脚本启动多个实验？\n内容：\n在多次创建实验之间增加`swanlab.finish()`即可。\n\n执行了`swanlab.finish()`之后，再次执行`swanlab.init()`就会创建新的实验；\n如果不执行`swanlab.finish()`的情况下，再次执行`swanlab.init()`，将无视此次执行。",
    "193": "一级标题：FAQ\n二级标题：如何将数据上传到私有化部署的SwanLab?\n内容：\n有两种方法可以做到这一点：\n\n::: code-group\n\n```python [方法一]\nswanlab.login(api_key='你的API Key', host='你的私有化部署IP地址')\n```\n\n```bash [方法二]\nswanlab login --host 你的私有化部署IP地址 --api-key 你的API Key\n```\n\n完成登录后，就可以将数据指定上传到私有化部署的SwanLab了。\n:::",
    "194": "一级标题：FAQ\n二级标题：如何在训练时关闭swanlab记录（Debug调试）？\n内容：\n将`swanlab.init`的`mode`参数设置为disabled，就可以不创建实验以及不写入数据。\n\n```python\nswanlab.init(mode='disabled')\n```",
    "195": "一级标题：FAQ\n二级标题：在同一台机器上，有多个人都在使用SwanLab，应该如何配置？\n内容：\n`swanlab.login`登录完成之后，会在该机器上生成一个配置文件记录登录信息，以便下次不用重复登录。但如果有多人使用这一台机器的话，则需要小心日志传递到对方账号上。\n\n**推荐的配置方式有两种：**\n\n**方式一(推荐)**：在代码开头加上`swanlab.login(api_key='你的API Key')`，这样不会将登录配置文件写入到本地，[文档](/api/py-login)\n\n**方式二**：在运行代码前，设置环境变量`SWANLAB_API_KEY=\"你的API Key\"`",
    "196": "一级标题：FAQ\n二级标题：本地的训练已经结束，但SwanLab UI上仍然在运行中，要怎么改变状态？\n内容：\n点击实验名旁边的终止按钮，会将实验状态从“进行中”转为“中断”，并停止接收数据的上传。\n\n![stop](/assets/stop.png)",
    "197": "一级标题：FAQ\n二级标题：如何查看折线图的局部细节？\n内容：\n放大折线图，长按鼠标划过目标的区域，即可放大查看该区域。\n\n![details](/assets/faq-chart-details.png)",
    "198": "一级标题：FAQ\n二级标题：内部指标名\n内容：\n指标名称是指`swanlab.log()`传入字典的key部分。有一部分key在内部被SwanLab用于传递系统硬件指标，所以不太建议使用。\n\n内部指标包括：\n\n- `__swanlab__.xxx`",
    "199": "一级标题：FAQ\n二级标题：实验状态规则\n内容：\n实验一共分为三种状态：完成、运行中与中断。\n\n- **完成**：训练进程自然结束，或手动执行了`swanlab.finish()`。\n- **运行中**：训练进程正在运行，且没有执行`swanlab.finish()`。\n- **中断**：训练进程因为Bug、机器关闭、`Ctrl+C`等异常中断。\n\n有些用户会遇到这样的情况：为什么我的训练进程好像还在进行中，但是SwanLab图表上显示中断？\n\n这是因为SwanLab判定中断有一条隐藏规则，如果训练进程在15分钟以内没有任何日志上传（包含自动收集的系统指标），则判定为中断，这是为了避免训练进程被意外Kill后，无法触达SwanLab SDK中的状态上传逻辑，导致实验永远处于“运行中”状态。\n\n所以如果你的机器出现了网络问题，且时间大于15分钟，就会导致实验状态显示为“中断”。",
    "200": "一级标题：FAQ\n二级标题：命令行记录与截断\n内容：\nSwanLab会记录`swanlab.init()`之后进程中的标准输出流，可以在实验的「日志」选项卡查看。如果一行的命令行输出过长，会被截断，目前的默认限制是`1024`个字符，最大限制是`4096`个字符。\n\n如果你想修改限制，可以使用下面的代码进行修改：\n\n```python\nimport swanlab\n\n# 创建新的设置对象，修改max_log_length参数\nnew_settings = swanlab.Settings(\n    max_log_length=4096,\n)\n\n# 更新全局设置\nswanlab.merge_settings(new_settings)\n\nswanlab.init()\n...\n```",
    "201": "一级标题：FAQ\n二级标题：如何开启实验平滑\n内容：\n找到实验页面的右上角，点击「设置」按钮：\n\n![](./faq/smooth_setting.png)\n\n在右侧拉出的菜单中，找到「平滑」选项，拉动滑动条即可开启平滑：\n\n![](./faq/smooth_button.png)",
    "202": "一级标题：FAQ\n二级标题：如何修改实验“中断”状态\n内容：\n在实验页面，点击状态标签：\n\n![](./faq/exp_header_crash.png)\n\n在弹窗中，选择你想要的状态：\n\n![](./faq/exp_windows_finish.png)",
    "203": "一级标题：FAQ\n二级标题：如何开启断点续训？\n内容：\n参考文档：[resume](/api/py-init#断点续训)",
    "204": "一级标题：结束一个实验\n二级标题：在一般的Python运行环境下\n内容：\n在一般的Python运行环境下，当脚本运行结束时，SwanLab会自动调用`swanlab.finish`来关闭实验，并将运行状态设置为「完成」。这一步无需显式调用。",
    "205": "一级标题：结束一个实验\n二级标题：在一些特殊情况下\n内容：\n但在一些特殊情况下，比如**Jupyter Notebook**中，则需要用`swanlab.finish`来显式关闭实验。\n使用方式也很简单, 在`init`之后执行`finish`即可：\n\n```python (5)\nimport swanlab\n\nswanlab.init()\n...\nswanlab.finish()\n```",
    "206": "一级标题：结束一个实验\n二级标题：FAQ\n内容：\n### 在运行一次Python脚本中，我可以初始化多次实验吗？\n\n可以，但你需要在多次`init`中间加上`finish`，如：\n\n```python\nswanlab.init()\n···\nswanlab.finish()\n···\nswanlab.init()\n```",
    "207": "一级标题：用 Notebook 跟踪实验\n二级标题：在Notebook中安装SwanLab\n内容：\n将 SwanLab 与 Jupyter 结合使用，无需离开Notebook即可获得交互式可视化效果。\n\n![](./jupyter-notebook/swanlab-love-jupyter.jpg)\n\n```bash\n!pip install swanlab -qqq\n```\nps: `-qqq`是用来控制命令执行时的输出信息量的，可选。",
    "208": "一级标题：用 Notebook 跟踪实验\n二级标题：在Notebok中与SwanLab交互\n内容：\n```python\nimport swanlab\n\nswanlab.init()\n...\n# 在Notebook中，需要显式关闭实验\nswanlab.finish()\n```\n\n在用`swanlab.init`初始化实验时，打印信息的最后会出现一个“Display SwanLab Dashboard”按钮：\n\n![](/assets/jupyter-notebook-1.jpg)\n\n点击该按钮，就会在Notebook中嵌入该实验的SwanLab网页：\n\n![](/assets/jupyter-notebook-2.jpg)\n\n现在，你可以在这个嵌入的网页中直接看到训练过程，以及和它交互。",
    "209": "一级标题：限制与性能\n二级标题：优化指标记录\n内容：\n使用 `swanlab.log` 跟踪记录实验指标，记录后，这些指标会生成图表与显示在表格中。当记录的数据量过多时，可能会使网页的访问变慢。\n\n### 建议1：将不同指标的总数保持在1万以下\n\n记录超过10k个不同的指标名，可能会减慢你仪表盘渲染与表格操作速度。\n\n对于媒体数据，尽量将相关的媒体数据记录到相同的指标名称下：\n\n```python\n# ❌ 不推荐的做法\nfor i, img in enumerate(images):\n    swanlab.log({f\"pred_img_{i}\": swanlab.Image(image)})\n\n# ✅ 推荐的做法\nswanlab.log({\"pred_imgs\": [swanlab.Image(image) for image in images]})\n```\n\n<br>\n\n### 建议2：指标宽度保持在1000万以下\n\n指标宽度在以step为横轴的折线图中，指的是step最小值与最大值之间的范围差。\n\n在指标宽度过大时，会影响实验中所有指标的绘图加载时间，导致访问缓慢。\n\n<br>\n\n### 建议3：限制指标的提交频率\n\n选择适合你正在记录的指标的记录频率。在经验上，指标越宽，记录它的频率就越低。\n\n具体来说，我们建议：\n\n- 标量：每个指标 < `50k` 个记录点\n- 媒体数据：每个指标 < `10k` 个记录点\n\n如果你超出这些准则，SwanLab 将继续接受你记录的数据，但页面加载速度可能会很慢。\n\n推荐的记录方法如下代码所示：\n\n```python\n# 比如有1MB次循环\nfor step in range(1000000):\n    ....\n\n    # 每1k次循环提交一次，有效降低指标的提交频次\n    if step % 1000 == 0:\n        swanlab.log({\"scalar\": step})\n```",
    "210": "一级标题：记录自定义3D图表\n二级标题：3D柱状图 bar3d\n内容：\n![bar3d](./py-echarts/bar3d-1.png)\n\n```python\nimport swanlab\nimport pyecharts.options as opts\n\nswanlab.init(project=\"swanlab-echarts-3d-demo\")\n\n# 定义数据\nhours = [\"12a\", \"1a\", \"2a\", \"3a\", \"4a\", \"5a\", \"6a\", \"7a\", \"8a\", \"9a\", \"10a\", \"11a\", \"12p\", \"1p\", \"2p\", \"3p\", \"4p\", \"5p\", \"6p\", \"7p\", \"8p\", \"9p\", \"10p\", \"11p\"]\n\ndays = [\"Saturday\", \"Friday\", \"Thursday\", \"Wednesday\", \"Tuesday\", \"Monday\", \"Sunday\"]\n\ndata = [\n    [0, 0, 5], [0, 1, 1], [0, 2, 0], [0, 3, 0], [0, 4, 0], [0, 5, 0], [0, 6, 0], [0, 7, 0], [0, 8, 0], [0, 9, 0], [0, 10, 0], [0, 11, 2], [0, 12, 4], [0, 13, 1], [0, 14, 1], [0, 15, 3], [0, 16, 4], [0, 17, 6], [0, 18, 4], [0, 19, 4], [0, 20, 3], [0, 21, 3], [0, 22, 2], [0, 23, 5],\n    [1, 0, 7],[1, 1, 0],[1, 2, 0],[1, 3, 0],[1, 4, 0],[1, 5, 0],[1, 6, 0],[1, 7, 0],[1, 8, 0],[1, 9, 0],[1, 10, 5],[1, 11, 2],[1, 12, 2],[1, 13, 6],[1, 14, 9],[1, 15, 11],[1, 16, 6],[1, 17, 7],[1, 18, 8],[1, 19, 12],[1, 20, 5],[1, 21, 5],[1, 22, 7],[1, 23, 2],\n    [2, 0, 1],[2, 1, 1],[2, 2, 0],[2, 3, 0],[2, 4, 0],[2, 5, 0],[2, 6, 0],[2, 7, 0],[2, 8, 0],[2, 9, 0],[2, 10, 3],[2, 11, 2],[2, 12, 1],[2, 13, 9],[2, 14, 8],[2, 15, 10],[2, 16, 6],[2, 17, 5],[2, 18, 5],[2, 19, 5],[2, 20, 7],[2, 21, 4],[2, 22, 2],[2, 23, 4],\n    [3, 0, 7],[3, 1, 3],[3, 2, 0],[3, 3, 0],[3, 4, 0],[3, 5, 0],[3, 6, 0],[3, 7, 0],[3, 8, 1],[3, 9, 0],[3, 10, 5],[3, 11, 4],[3, 12, 7],[3, 13, 14],[3, 14, 13],[3, 15, 12],[3, 16, 9],[3, 17, 5],[3, 18, 5],[3, 19, 10],[3, 20, 6],[3, 21, 4],[3, 22, 4],[3, 23, 1],\n    [4, 0, 1],[4, 1, 3],[4, 2, 0],[4, 3, 0],[4, 4, 0],[4, 5, 1],[4, 6, 0],[4, 7, 0],[4, 8, 0],[4, 9, 2],[4, 10, 4],[4, 11, 4],[4, 12, 2],[4, 13, 4],[4, 14, 4],[4, 15, 14],[4, 16, 12],[4, 17, 1],[4, 18, 8],[4, 19, 5],[4, 20, 3],[4, 21, 7],[4, 22, 3],[4, 23, 0],\n    [5, 0, 2],[5, 1, 1],[5, 2, 0],[5, 3, 3],[5, 4, 0],[5, 5, 0],[5, 6, 0],[5, 7, 0],[5, 8, 2],[5, 9, 0],[5, 10, 4],[5, 11, 1],[5, 12, 5],[5, 13, 10],[5, 14, 5],[5, 15, 7],[5, 16, 11],[5, 17, 6],[5, 18, 0],[5, 19, 5],[5, 20, 3],[5, 21, 4],[5, 22, 2],[5, 23, 0],\n    [6, 0, 1],[6, 1, 0],[6, 2, 0],[6, 3, 0],[6, 4, 0],[6, 5, 0],[6, 6, 0],[6, 7, 0],[6, 8, 0],[6, 9, 0],[6, 10, 1],[6, 11, 0],[6, 12, 2],[6, 13, 1],[6, 14, 3],[6, 15, 4],[6, 16, 0],[6, 17, 0],[6, 18, 0],[6, 19, 0],[6, 20, 1],[6, 21, 2],[6, 22, 2],[6, 23, 6],\n]\ndata = [[d[1], d[0], d[2]] for d in data]\n\n# 创建echarts bar3d对象\nbar3d = swanlab.echarts.Bar3D()\n\n# 设置bar3d数据\nbar3d.add(\n    \"bar3d\",\n    data,\n    xaxis3d_opts=opts.Axis3DOpts(data=hours, type_=\"category\"),\n    yaxis3d_opts=opts.Axis3DOpts(data=days, type_=\"category\"),\n    zaxis3d_opts=opts.Axis3DOpts(data=data, type_=\"value\"),\n)\n\nbar3d.set_global_opts(\n        visualmap_opts=opts.VisualMapOpts(\n            max_=20,\n            range_color=[\n                \"#313695\",\n                \"#4575b4\",\n                \"#74add1\",\n                \"#abd9e9\",\n                \"#e0f3f8\",\n                \"#ffffbf\",\n                \"#fee090\",\n                \"#fdae61\",\n                \"#f46d43\",\n                \"#d73027\",\n                \"#a50026\",\n            ],\n        )\n    )\n\n# 记录到swanlab\nswanlab.log({\"bar3d\": bar3d})\n```",
    "211": "一级标题：记录自定义3D图表\n二级标题：3D散点图 scatter3d\n内容：\n![scatter3d](./py-echarts/scatter3d-1.png)\n\n```python\nimport asyncio\nfrom aiohttp import TCPConnector, ClientSession\nimport swanlab\nimport pyecharts.options as opts\n\nswanlab.init(project=\"swanlab-echarts-3d-demo\")\n\n# 定义数据\nasync def get_json_data(url: str) -> dict:\n    async with ClientSession(connector=TCPConnector(ssl=False)) as session:\n        async with session.get(url=url) as response:\n            return await response.json()\n\n\n# 获取echarts官方示例数据\ndata = asyncio.run(\n    get_json_data(\n        url=\"https://echarts.apache.org/examples/data/asset/data/nutrients.json\"\n    )\n)\n\n# 列名映射\nfield_indices = {\n    \"calcium\": 3,\n    \"calories\": 12,\n    \"carbohydrate\": 8,\n    \"fat\": 10,\n    \"fiber\": 5,\n    \"group\": 1,\n    \"id\": 16,\n    \"monounsat\": 14,\n    \"name\": 0,\n    \"polyunsat\": 15,\n    \"potassium\": 7,\n    \"protein\": 2,\n    \"saturated\": 13,\n    \"sodium\": 4,\n    \"sugars\": 9,\n    \"vitaminc\": 6,\n    \"water\": 11,\n}\n\n# 配置 config\nconfig_xAxis3D = \"protein\"\nconfig_yAxis3D = \"fiber\"\nconfig_zAxis3D = \"sodium\"\nconfig_color = \"fiber\"\nconfig_symbolSize = \"vitaminc\"\n\n# 构造数据\n\"\"\"\n数据结构为[[x, y, z, color, size, index]]\n例子：\n[[19.9, 0.4, 0.385, 0.4, 0.0769, 0],\n[35.8, 2, 0.717, 2, 0.138, 1],\n[23.5, 1.6, 0.78, 1.6, 0.0012, 2], ...]\n\"\"\"\ndata = [\n    [\n        item[field_indices[config_xAxis3D]],\n        item[field_indices[config_yAxis3D]],\n        item[field_indices[config_zAxis3D]],\n        item[field_indices[config_color]],\n        item[field_indices[config_symbolSize]],\n        index,\n    ]\n    for index, item in enumerate(data)\n]\n\n# 创建echarts scatter3d对象\nscatter3d = swanlab.echarts.Scatter3D()\n\n# 设置scatter3d数据\nscatter3d.add(\n    \"scatter3d\",\n    data,\n    xaxis3d_opts=opts.Axis3DOpts(name=config_xAxis3D, type_=\"value\"),\n    yaxis3d_opts=opts.Axis3DOpts(name=config_yAxis3D, type_=\"value\"),\n    zaxis3d_opts=opts.Axis3DOpts(name=config_zAxis3D, type_=\"value\"),\n    grid3d_opts=opts.Grid3DOpts(width=100, height=100, depth=100),\n)\nscatter3d.set_global_opts(\n        visualmap_opts=[\n            opts.VisualMapOpts(\n                type_=\"color\",\n                is_calculable=True,\n                dimension=3,\n                pos_top=\"10\",\n                max_=79 / 2,\n                range_color=[\n                    \"#1710c0\",\n                    \"#0b9df0\",\n                    \"#00fea8\",\n                    \"#00ff0d\",\n                    \"#f5f811\",\n                    \"#f09a09\",\n                    \"#fe0300\",\n                ],\n            ),\n            opts.VisualMapOpts(\n                type_=\"size\",\n                is_calculable=True,\n                dimension=4,\n                pos_bottom=\"10\",\n                max_=2.4 / 2,\n                range_size=[10, 40],\n            ),\n        ]\n    )\n\n# 记录到swanlab\nswanlab.log({\"scatter3d\": scatter3d})\n```",
    "212": "一级标题：记录自定义3D图表\n二级标题：3D折线图 line3d\n内容：\n![line3d](./py-echarts/line3d-1.png)\n\n```python\nimport math\nimport swanlab\nimport pyecharts.options as opts\nfrom pyecharts.faker import Faker\n\n\nswanlab.init(project=\"swanlab-echarts-3d-demo\")\n\n# 构造数据\ndata = []\nfor t in range(0, 25000):\n    _t = t / 1000\n    x = (1 + 0.25 * math.cos(75 * _t)) * math.cos(_t)\n    y = (1 + 0.25 * math.cos(75 * _t)) * math.sin(_t)\n    z = _t + 2.0 * math.sin(75 * _t)\n    data.append([x, y, z])\n\n\n# 创建echarts line3d对象\nline3d = swanlab.echarts.Line3D()\n\n# 设置line3d数据\nline3d.add(\n    \"line3d\",\n    data,\n    xaxis3d_opts=opts.Axis3DOpts(Faker.clock, type_=\"value\"),\n    yaxis3d_opts=opts.Axis3DOpts(Faker.week_en, type_=\"value\"),\n    grid3d_opts=opts.Grid3DOpts(width=100, depth=100),\n)\n\nline3d.set_global_opts(\n        visualmap_opts=opts.VisualMapOpts(\n            max_=30, min_=0, range_color=Faker.visual_color\n        ),\n    )\n\n# 记录到swanlab\nswanlab.log({\"line3d\": line3d})\n```",
    "213": "一级标题：记录自定义3D图表\n二级标题：3D曲面图 3d_surface\n内容：\n![3d_surface](./py-echarts/surface3d-1.png)\n\n```python\nimport math\nimport swanlab\nimport pyecharts.options as opts\nfrom typing import Union\n\nswanlab.init(project=\"swanlab-echarts-3d-demo\")\n\n# 构造数据\ndef float_range(start: int, end: int, step: Union[int, float], round_number: int = 2):\n    \"\"\"\n    浮点数 range\n    :param start: 起始值\n    :param end: 结束值\n    :param step: 步长\n    :param round_number: 精度\n    :return: 返回一个 list\n    \"\"\"\n    temp = []\n    while True:\n        if start < end:\n            temp.append(round(start, round_number))\n            start += step\n        else:\n            break\n    return temp\n\n\ndef surface3d_data():\n    for t0 in float_range(-3, 3, 0.05):\n        y = t0\n        for t1 in float_range(-3, 3, 0.05):\n            x = t1\n            z = math.sin(x**2 + y**2) * x / 3.14\n            yield [x, y, z]\n\n\n# 创建echarts surface3d对象\nsurface3d = swanlab.echarts.Surface3D()\n\n# 设置surface3d数据\nsurface3d.add(\n    \"surface3d\",\n    data=list(surface3d_data()),\n    xaxis3d_opts=opts.Axis3DOpts(type_=\"value\"),\n    yaxis3d_opts=opts.Axis3DOpts(type_=\"value\"),\n    grid3d_opts=opts.Grid3DOpts(width=100, height=40, depth=100),\n)\n\nsurface3d.set_global_opts(\n        visualmap_opts=opts.VisualMapOpts(\n            dimension=2,\n            max_=1,\n            min_=-1,\n            range_color=[\n                \"#313695\",\n                \"#4575b4\",\n                \"#74add1\",\n                \"#abd9e9\",\n                \"#e0f3f8\",\n                \"#ffffbf\",\n                \"#fee090\",\n                \"#fdae61\",\n                \"#f46d43\",\n                \"#d73027\",\n                \"#a50026\",\n            ],\n        )\n    )\n\n# 记录到swanlab\nswanlab.log({\"surface3d\": surface3d})\n```",
    "214": "一级标题：记录自定义图表\n二级标题：折线图 line\n内容：\n![line](./py-echarts/line-1.png)\n\n:::code-group\n\n```python [基础]\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nweek_name_list = [\"周一\", \"周二\", \"周三\", \"周四\", \"周五\", \"周六\", \"周日\"]\nhigh_temperature = [11, 11, 15, 13, 12, 13, 10]\nlow_temperature = [1, -2, 2, 5, 3, 2, 0]\n\n# 创建echarts line对象\nline = swanlab.echarts.Line()\n\n# 设置line的轴\nline.add_xaxis(week_name_list)\n# 设置line的数据\nline.add_yaxis(\"high_temperature\", high_temperature)\nline.add_yaxis(\"low_temperature\", low_temperature)\n\n# 记录到swanlab\nswanlab.log({\"line\": line})\n```\n\n```python [样式-设置不透明度]\n\"\"\"\ndemo: \nhttps://swanlab.cn/@ZeyiLin/swanlab-echarts-demo/runs/trptzejp9037cimxd786e/chart#eDczbzM0-blJ6R3dXSFU=\n\"\"\"\n\nimport swanlab\nimport pyecharts.options as opts\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nweek_name_list = [\"周一\", \"周二\", \"周三\", \"周四\", \"周五\", \"周六\", \"周日\"]\nhigh_temperature = [11, 11, 15, 13, 12, 13, 10]\nlow_temperature = [1, -2, 2, 5, 3, 2, 0]\n\n# 创建echarts line对象\nline = swanlab.echarts.Line()\n# 设置line的轴\nline.add_xaxis(week_name_list)\n# 设置line的数据\nline.add_yaxis(\"high_temperature\", high_temperature)\nline.add_yaxis(\"low_temperature\", low_temperature)\n\n# 设置不透明度为0.5\nline.set_series_opts(areastyle_opts=opts.AreaStyleOpts(opacity=0.5))\n\n# 记录到swanlab\nswanlab.log({\"line_opacity\": line})\n```\n:::",
    "215": "一级标题：记录自定义图表\n二级标题：柱状图 bar\n内容：\n![bar](./py-echarts/bar-1.png)\n\n::: code-group\n\n```python [基础]\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nx = [\"a\", \"b\", \"c\"]\ny = [1, 2, 3]\n\n# 创建echarts bar对象\nbar = swanlab.echarts.Bar()\n\n# 设置x轴数据\nbar.add_xaxis(x)\n# 设置y轴数据\nbar.add_yaxis(\"value\", y)\n\n# 记录到swanlab\nswanlab.log({\"bar\": bar})\n```\n\n```python [水平方向]\n\"\"\"\ndemo:\nhttps://swanlab.cn/@ZeyiLin/swanlab-echarts-demo/runs/rtqyhofvc5080tpmdfxkz/chart#bGw5M2My-ZnRhOGRnWVE=\n\"\"\"\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nx = [\"a\", \"b\", \"c\"]\ny = [1, 2, 3]\n\n# 创建echarts bar对象\nbar = swanlab.echarts.Bar()\n\n# 设置x轴数据\nbar.add_xaxis(x)\n# 设置y轴数据\nbar.add_yaxis(\"value\", y)\n# 翻转\nbar.reversal_axis()\n\n# 记录到swanlab\nswanlab.log({\"bar_horizontal\": bar})\n```\n\n:::",
    "216": "一级标题：记录自定义图表\n二级标题：饼状图 pie\n内容：\n![pie](./py-echarts/pie-1.png)\n\n```python\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nx_data = [\"直接访问\", \"邮件营销\", \"联盟广告\", \"视频广告\", \"搜索引擎\"]\ny_data = [335, 310, 274, 235, 400]\n\n# 组合数据\ndata_pair = [list(z) for z in zip(x_data, y_data)]\ndata_pair.sort(key=lambda x: x[1])\n\n# 创建echarts pie对象\npie = swanlab.echarts.Pie()\n\n# 设置x轴数据并配置标签显示\npie.add(\n    \"访问来源\", \n    data_pair,\n    # 配置标签显示\n    label_opts={\n        \"formatter\": \"{b}: {d}%\",  # 显示百分比\n        \"position\": \"outside\"  # 标签位置\n    }\n)\n\n# 记录到swanlab\nswanlab.log({\"pie\": pie})\n```",
    "217": "一级标题：记录自定义图表\n二级标题：热力图 heatmap\n内容：\n![heatmap](./py-echarts/heatmap-1.png)\n\n:::code-group\n\n```python [基础]\nimport swanlab\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nhours = [\"12a\", \"1a\", \"2a\", \"3a\", \"4a\", \"5a\", \"6a\", \"7a\", \"8a\", \"9a\", \"10a\", \"11a\", \"12p\", \"1p\", \"2p\", \"3p\", \"4p\", \"5p\", \"6p\", \"7p\", \"8p\", \"9p\", \"10p\", \"11p\"]\n\ndays = [\"Saturday\", \"Friday\", \"Thursday\", \"Wednesday\", \"Tuesday\", \"Monday\", \"Sunday\"]\n\ndata = [\n    [0, 0, 5], [0, 1, 1], [0, 2, 0], [0, 3, 0], [0, 4, 0], [0, 5, 0], [0, 6, 0], [0, 7, 0], [0, 8, 0], [0, 9, 0], [0, 10, 0], [0, 11, 2], [0, 12, 4], [0, 13, 1], [0, 14, 1], [0, 15, 3], [0, 16, 4], [0, 17, 6], [0, 18, 4], [0, 19, 4], [0, 20, 3], [0, 21, 3], [0, 22, 2], [0, 23, 5],\n    [1, 0, 7],[1, 1, 0],[1, 2, 0],[1, 3, 0],[1, 4, 0],[1, 5, 0],[1, 6, 0],[1, 7, 0],[1, 8, 0],[1, 9, 0],[1, 10, 5],[1, 11, 2],[1, 12, 2],[1, 13, 6],[1, 14, 9],[1, 15, 11],[1, 16, 6],[1, 17, 7],[1, 18, 8],[1, 19, 12],[1, 20, 5],[1, 21, 5],[1, 22, 7],[1, 23, 2],\n    [2, 0, 1],[2, 1, 1],[2, 2, 0],[2, 3, 0],[2, 4, 0],[2, 5, 0],[2, 6, 0],[2, 7, 0],[2, 8, 0],[2, 9, 0],[2, 10, 3],[2, 11, 2],[2, 12, 1],[2, 13, 9],[2, 14, 8],[2, 15, 10],[2, 16, 6],[2, 17, 5],[2, 18, 5],[2, 19, 5],[2, 20, 7],[2, 21, 4],[2, 22, 2],[2, 23, 4],\n    [3, 0, 7],[3, 1, 3],[3, 2, 0],[3, 3, 0],[3, 4, 0],[3, 5, 0],[3, 6, 0],[3, 7, 0],[3, 8, 1],[3, 9, 0],[3, 10, 5],[3, 11, 4],[3, 12, 7],[3, 13, 14],[3, 14, 13],[3, 15, 12],[3, 16, 9],[3, 17, 5],[3, 18, 5],[3, 19, 10],[3, 20, 6],[3, 21, 4],[3, 22, 4],[3, 23, 1],\n    [4, 0, 1],[4, 1, 3],[4, 2, 0],[4, 3, 0],[4, 4, 0],[4, 5, 1],[4, 6, 0],[4, 7, 0],[4, 8, 0],[4, 9, 2],[4, 10, 4],[4, 11, 4],[4, 12, 2],[4, 13, 4],[4, 14, 4],[4, 15, 14],[4, 16, 12],[4, 17, 1],[4, 18, 8],[4, 19, 5],[4, 20, 3],[4, 21, 7],[4, 22, 3],[4, 23, 0],\n    [5, 0, 2],[5, 1, 1],[5, 2, 0],[5, 3, 3],[5, 4, 0],[5, 5, 0],[5, 6, 0],[5, 7, 0],[5, 8, 2],[5, 9, 0],[5, 10, 4],[5, 11, 1],[5, 12, 5],[5, 13, 10],[5, 14, 5],[5, 15, 7],[5, 16, 11],[5, 17, 6],[5, 18, 0],[5, 19, 5],[5, 20, 3],[5, 21, 4],[5, 22, 2],[5, 23, 0],\n    [6, 0, 1],[6, 1, 0],[6, 2, 0],[6, 3, 0],[6, 4, 0],[6, 5, 0],[6, 6, 0],[6, 7, 0],[6, 8, 0],[6, 9, 0],[6, 10, 1],[6, 11, 0],[6, 12, 2],[6, 13, 1],[6, 14, 3],[6, 15, 4],[6, 16, 0],[6, 17, 0],[6, 18, 0],[6, 19, 0],[6, 20, 1],[6, 21, 2],[6, 22, 2],[6, 23, 6],\n]\ndata = [[d[1], d[0], d[2] or \"-\"] for d in data]\n\n# 创建echarts heatmap对象\nheatmap = swanlab.echarts.HeatMap()\n\n# 设置x轴数据并配置标签显示\nheatmap.add_xaxis(hours)\nheatmap.add_yaxis(\n  \"Punch Card\", \n  days,\n  data,\n)\n\n# 记录到swanlab\nswanlab.log({\"heatmap\": heatmap})\n```\n\n```python [设置颜色映射范围]\n\"\"\"\ndemo:\nhttps://swanlab.cn/@ZeyiLin/swanlab-echarts-demo/runs/c1wm57rkfnwkyz7kaat8a/chart#OWJ5bWJl-c2M5bDFFc2I=\n\"\"\"\"\n\nimport swanlab\nfrom pyecharts import options as opts\n\nswanlab.init(project=\"echarts-test\")\n\n# 定义数据\nhours = [\"12a\", \"1a\", \"2a\", \"3a\", \"4a\", \"5a\", \"6a\", \"7a\", \"8a\", \"9a\", \"10a\", \"11a\", \"12p\", \"1p\", \"2p\", \"3p\", \"4p\", \"5p\", \"6p\", \"7p\", \"8p\", \"9p\", \"10p\", \"11p\"]\n\ndays = [\"Saturday\", \"Friday\", \"Thursday\", \"Wednesday\", \"Tuesday\", \"Monday\", \"Sunday\"]\n\ndata = [\n    [0, 0, 5], [0, 1, 1], [0, 2, 0], [0, 3, 0], [0, 4, 0], [0, 5, 0], [0, 6, 0], [0, 7, 0], [0, 8, 0], [0, 9, 0], [0, 10, 0], [0, 11, 2], [0, 12, 4], [0, 13, 1], [0, 14, 1], [0, 15, 3], [0, 16, 4], [0, 17, 6], [0, 18, 4], [0, 19, 4], [0, 20, 3], [0, 21, 3], [0, 22, 2], [0, 23, 5],\n    [1, 0, 7],[1, 1, 0],[1, 2, 0],[1, 3, 0],[1, 4, 0],[1, 5, 0],[1, 6, 0],[1, 7, 0],[1, 8, 0],[1, 9, 0],[1, 10, 5],[1, 11, 2],[1, 12, 2],[1, 13, 6],[1, 14, 9],[1, 15, 11],[1, 16, 6],[1, 17, 7],[1, 18, 8],[1, 19, 12],[1, 20, 5],[1, 21, 5],[1, 22, 7],[1, 23, 2],\n    [2, 0, 1],[2, 1, 1],[2, 2, 0],[2, 3, 0],[2, 4, 0],[2, 5, 0],[2, 6, 0],[2, 7, 0],[2, 8, 0],[2, 9, 0],[2, 10, 3],[2, 11, 2],[2, 12, 1],[2, 13, 9],[2, 14, 8],[2, 15, 10],[2, 16, 6],[2, 17, 5],[2, 18, 5],[2, 19, 5],[2, 20, 7],[2, 21, 4],[2, 22, 2],[2, 23, 4],\n    [3, 0, 7],[3, 1, 3],[3, 2, 0],[3, 3, 0],[3, 4, 0],[3, 5, 0],[3, 6, 0],[3, 7, 0],[3, 8, 1],[3, 9, 0],[3, 10, 5],[3, 11, 4],[3, 12, 7],[3, 13, 14],[3, 14, 13],[3, 15, 12],[3, 16, 9],[3, 17, 5],[3, 18, 5],[3, 19, 10],[3, 20, 6],[3, 21, 4],[3, 22, 4],[3, 23, 1],\n    [4, 0, 1],[4, 1, 3],[4, 2, 0],[4, 3, 0],[4, 4, 0],[4, 5, 1],[4, 6, 0],[4, 7, 0],[4, 8, 0],[4, 9, 2],[4, 10, 4],[4, 11, 4],[4, 12, 2],[4, 13, 4],[4, 14, 4],[",
    "218": "一级标题：记录实验指标\n二级标题：记录标量指标\n内容：\n在训练循环中，将指标名和数据组成一个键值对字典，传递给 `swanlab.log()` 完成1次指标的记录：\n\n```python\nfor epoch in range(num_epochs):\n    for data, ground_truth in dataloader:\n        predict = model(data)\n        loss = loss_fn(predict, ground_truth)\n        # 记录指标，指标名为loss\n        swanlab.log({\"loss\": loss})\n```\n\n在 `swanlab.log` 记录时，会根据指标名，将`{指标名: 指标}`字典汇总到一个统一位置存储。\n\n⚠️需要注意的是，`swanlab.log({key: value})`中的value必须是`int` / `float` / `BaseType`这三种类型（如果传入的是`str`类型，会先尝试转为`float`，如果转换失败就会报错），其中`BaseType`类型主要是多媒体数据，详情请看[记录多媒体数据](/guide_cloud/experiment_track/log-media.md)。\n\n在每次记录时，会为该次记录赋予一个 `step`。在默认情况下，`step` 为0开始，并在你每一次在同一个指标名下记录时，`step` 等于该指标名历史记录的最大 `step` + 1，例如：\n\n```python\nimport swanlab\nswanlab.init()\n\n...\n\nswanlab.log({\"loss\": loss, \"acc\": acc})  \n# 此次记录中，loss的step为0, acc的step为0\n\nswanlab.log({\"loss\": loss, \"iter\": iter})  \n# 此次记录中，loss的step为1, iter的step为0, acc的step为0\n\nswanlab.log({\"loss\": loss, \"iter\": iter})  \n# 此次记录中，loss的step为2, iter的step为1, acc的step为0\n```",
    "219": "一级标题：记录实验指标\n二级标题：指标分组\n内容：\n在脚本中可以通过指标名的前缀（以“/”为分隔）进行图表分组，例如 `train/loss` 会被分到名为“train”的分组、`val/loss` 会被分到名为“val”的分组：\n\n```python\n# 分到train组\nswanlab.log({\"train/loss\": loss})\nswanlab.log({\"train/batch_cost\": batch_cost})\n\n# 分到val组\nswanlab.log({\"val/acc\": acc})\n```",
    "220": "一级标题：记录实验指标\n二级标题：指定记录的step\n内容：\n在一些指标的记录频率不一致，但希望它们的step可以对齐时，可以通过设置 `swanlab.log` 的 `step` 参数实现对齐：\n\n```python\nfor iter, (data, ground_truth) in enumerate(train_dataloader):\n    predict = model(data)\n    train_loss = loss_fn(predict, ground_truth)\n    swanlab.log({\"train/loss\": loss}, step=iter)\n\n    # 测试部分\n    if iter % 1000 == 0:\n        acc = val_trainer(model)\n        swanlab.log({\"val/acc\": acc}, step=iter)\n```\n\n需要注意的是，同一个指标名不允许出现2个相同的step的数据，一旦出现，SwanLab将保留先记录的数据，抛弃后记录的数据。",
    "221": "一级标题：记录实验指标\n二级标题：打印指标\n内容：\n也许你希望在训练循环中打印指标，可以通过 `print_to_console` 参数控制是否将指标打印到控制台（以`dict`的形式）：\n\n```python\nswanlab.log({\"acc\": acc}, print_to_console=True)\n```\n\n或者：\n\n```python\nprint(swanlab.log({\"acc\": acc}))\n```",
    "222": "一级标题：记录实验指标\n二级标题：自动记录环境信息\n内容：\nSwanLab在实验期间自动记录以下信息：\n\n- **命令行输出**：标准输出流和标准错误流被自动记录，并显示在实验页面的“日志”选项卡中。\n- **实验环境**：记录包括操作系统、硬件配置、Python解释器路径、运行目录、Python库依赖等在内的数十项的环境信息。\n- **训练时间**：记录训练开始时间和总时长。",
    "223": "一级标题：记录媒体数据\n二级标题：图像\n内容：\n`swanlab.Image` 支持记录多种图像类型，包括 numpy、PIL、Tensor、读取文件等。[API文档](/api/py-Image)。\n\n![](/assets/media-image-1.jpg)\n\n### 1.1 记录 Array 型图像\n\nArray型包括numpy和tensor。直接将 Array 传入 `swanlab.Image`，它将根据类型自动做相应处理：\n\n- 如果是 `numpy.ndarray`：SwanLab 会使用 pillow (PIL) 对其进行读取 。\n- 如果是 `tensor`：SwanLab 会使用 `torchvision` 的 `make_grid`函数做转换，然后使用 pillow 对其进行读取。\n\n示例代码：\n\n```python\nimage = swanlab.Image(image_array, caption=\"左图: 输入, 右图: 输出\")\nswanlab.log({\"examples\": image})\n```\n\n### 1.2 记录 PIL 型图像\n\n直接传入 `swanlab.Image`：\n\n```python\nimage = PIL.Image.fromarray(image_array)\nswanlab.log({\"examples\": image})\n```\n\n### 1.3 记录文件图像\n\n提供文件路径给 `swanlab.Image`：\n\n```python\nimage = swanlab.Image(\"myimage.jpg\")\nswanlab.log({\"example\": image})\n```\n\n### 1.4 记录 Matplotlib\n\n将 `matplotlib.pyplot` 的 `plt` 对象传入 `swanlab.Image`：\n\n```python\nimport matplotlib.pyplot as plt\n\n# 数据\nx = [1, 2, 3, 4, 5]\ny = [2, 3, 5, 7, 11]\n# 创建折线图\nplt.plot(x, y)\n# 添加标题和标签\nplt.title(\"Examples\")\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")\n\nswanlab.log({\"example\": swanlab.Image(plt)})\n```\n\n### 1.5 单步记录多个图像\n\n单步记录多个图像即在一次 `swanlab.log` 中，传递一个由 `swanlab.Image` 类型对象组成的列表。\n\n```python\n# 创建一个空列表\nimage_list = []\nfor i in range(3):\n    random_image = np.random.randint(low=0, high=256, size=(100, 100, 3))\n    image = swanlab.Image(random_image, caption=f\"随机图像{i}\")\n    # 将 swanlab.Image 类型对象添加到列表中\n    image_list.append(image)\n\nswanlab.log({\"examples\": image_list})\n```\n\n关于图像的更多细节，可参考[API文档](/api/py-Image)",
    "224": "一级标题：记录媒体数据\n二级标题：音频\n内容：\n[API文档](/api/py-Audio)\n\n![](/assets/media-audio-1.jpg)\n\n### 2.1 记录 Array 型音频\n\n```python\naudio = swanlab.Audio(np_array, sample_rate=44100, caption=\"white_noise\")\nswanlab.log({\"white_noise\": audio})\n```\n\n### 2.2 记录音频文件\n\n```python\nswanlab.log({\"white_noise\": swanlab.Audio(\"white_noise.wav\")})\n```\n\n### 2.3 单步记录多个音频\n\n```python\nexamples = []\nfor i in range(3):\n    white_noise = np.random.randn(100000)\n    audio = swanlab.Audio(white_noise, caption=\"audio_{i}\")\n    # 列表中添加swanlab.Audio类型对象\n    examples.append(audio)\n\nrun.log({\"examples\": examples})\n```",
    "225": "一级标题：记录媒体数据\n二级标题：文本\n内容：\n[API文档](/api/py-Text)\n\n### 3.1 记录字符串\n\n```python\nswanlab.log({\"text\": swanlab.Text(\"A example text.\")})\n```\n\n### 3.2 单步记录多个文本\n\n```python\n# 创建一个空列表\ntext_list = []\nfor i in range(3):\n    text = swanlab.Text(\"A example text.\", caption=f\"{i}\")\n    text_list.append(text)\n\nswanlab.log({\"examples\": text_list})\n```\n\n![alt text](/assets/log-media-text.png)",
    "226": "一级标题：记录媒体数据\n二级标题：3D点云\n内容：\n![](/zh/api/py-object3d/demo.png)\n\n请参考此文档：[API-Oject3D](/api/py-object3d)",
    "227": "一级标题：记录媒体数据\n二级标题：生物化学分子\n内容：\n![](/assets/molecule.gif)\n\n请参考此文档：[API-Molecule](/api/py-molecule)",
    "228": "一级标题：记录媒体数据\n二级标题：Q&A\n内容：\n### 1. caption参数有什么作用？\n\n每一个媒体类型都会有1个`caption`参数，它的作用是对该媒体数据的文字描述，比如对于图像：\n\n```python\napple_image = swanlab.Image(data, caption=\"苹果\")\nswanlab.log({\"im\": apple_image})\n```\n<img src=\"/assets/log-media-image.png\" width=400, height=400>\n\n### 2. 想要媒体数据和epoch数同步，怎么办？\n\n在用swanlab.log记录媒体数据时，指定`step`参数为epoch数即可。\n\n```python\nfor epoch in epochs:\n    ···\n    swanlab.log({\"im\": sw_image}, step=epoch)\n```",
    "229": "一级标题：恢复实验/断点续训\n二级标题：使用场景\n内容：\n:::warning 使用场景\n1. **断点续训：** 之前的训练进程断了，基于checkpoint继续训练时，希望实验图表能和之前的swanlab实验续上，而非创建1个新swanlab实验\n2. **补充图表：** 训练和评估分为了两个进程，但希望评估和训练记录在同一个swanlab实验中\n3. **更新超参：** config中有一些参数填写有误，希望更新config参数\n:::",
    "230": "一级标题：恢复实验/断点续训\n二级标题：基本用法\n内容：\n恢复实验主要依赖两个参数，`resume`和`id`：\n\n```python\nswanlab.init(\n    project=\"<project>\",\n    workspace=\"<workspace>\",\n    resume=True,\n    id=\"<exp_id>\",  # id必须为21位字符串\n)\n```\n\n`resume`参数控制了实验恢复的行为，有以下几种选择：\n\n- `must`：如果项目下存在id对应的实验，则会resume该实验，否则将报错\n- `allow`：如果项目下存在id对应的实验，则会resume该实验，否则将创建一个新的实验。\n- `never`：传递 id 参数将会报错；否则会创建一个新的实验。(即不开启resume的效果)\n- `True`：即`allow`\n- `False`：即`never`\n\n`实验id`是实验的唯一标识，可以在实验的「环境」选项卡或URL中找到，必须为1个21位字符串：\n\n![](./resume-experiment/exp_id.png)\n\n或者打开一个实验，在其URL结构中的`<exp_id>`部分就是实验id：\n\n```\nhttps://swanlab.cn/@<username>/<project>/runs/<exp_id>/...\n```\n\n其中`<exp_id>`就是实验id。",
    "231": "一级标题：恢复实验/断点续训\n二级标题：示例\n内容：\n```python\nimport swanlab\n\nrun = swanlab.init(project=\"resume_test\")\nswanlab.log({\"loss\": 2, \"acc\":0.4})\n# 完成实验\nrun.finish()\n\n# 恢复实验\nrun = swanlab.init(project=\"resume_test\", resume=True, id=run.id)\nswanlab.log({\"loss\": 0.2, \"acc\": 0.9})\n```",
    "232": "一级标题：邮件/第三方通知\n二级标题：邮件通知\n内容：\nSwanLab支持通过邮件或第三方通知的方式，在实验结束/发生错误时发送通知。\n\n![](../../plugin/notification-email/logo.jpg)\n\n- [邮件通知](/plugin/notification-email.md)\n- [飞书通知](/plugin/notification-lark.md)\n- [钉钉通知](/plugin/notification-dingtalk.md)\n- [企业微信通知](/plugin/notification-wxwork.md)\n- [Discord通知](/plugin/notification-discord.md)\n- [Slack通知](/plugin/notification-slack.md)",
    "233": "一级标题：设置实验配置\n二级标题：设置实验配置\n内容：\n`config` 通常在训练脚本的开头定义。当然，不同的人工智能工作流可能会有所不同，因此 `config` 也支持在脚本的不同位置定义，以满足灵活的需求。\n\n以下部分概述了定义实验配置的不同场景。",
    "234": "一级标题：设置实验配置\n二级标题：在init中设置\n内容：\n下面的代码片段演示了如何使用Python字典定义 `config`，以及如何在初始化SwanLab实验时将该字典作为参数传递：\n\n```python\nimport swanlab\n\n# 定义一个config字典\nconfig = {\n  \"hidden_layer_sizes\": [64, 128],\n  \"activation\": \"ELU\",\n  \"dropout\": 0.5,\n  \"num_classes\": 10,\n  \"optimizer\": \"Adam\",\n  \"batch_normalization\": True,\n  \"seq_length\": 100,\n}\n\n# 在你初始化SwanLab时传递config字典\nrun = swanlab.init(project=\"config_example\", config=config)\n```\n\n访问 `config` 中的值与在Python中访问其他字典的方式类似：\n\n- 用键名作为索引访问值\n  ```python\n  hidden_layer_sizes = swanlab.config[\"hidden_layer_sizes\"]\n  ```\n- 用 `get()` 方法访问值\n  ```python\n  activation = swanlab.config.get[\"activation\"]\n  ```\n- 用点号访问值\n  ```python\n  dropout = swanlab.config.dropout\n  ```",
    "235": "一级标题：设置实验配置\n二级标题：用argparse设置\n内容：\n你可以用 `argparse` 对象设置 `config`。`argparse` 是Python标准库（Python >= 3.2）中的一个非常强大的模块，用于从命令行接口（CLI）解析程序参数。这个模块让开发者能够轻松地编写用户友好的命令行界面。\n\n可以直接传递 `argparse` 对象设置 `config`：\n\n```python\nimport argparse\nimport swanlab\n\n# 初始化Argparse\nparser = argparse.ArgumentParser()\nparser.add_argument('--epochs', default=20)\nparser.add_argument('--learning-rate', default=0.001)\nargs = parser.parse_args()\n\nswanlab.init(config=args)\n```\n\n等同于 `swanlab.init(config={\"epochs\": 20, \"learning-rate\": 0.001})`",
    "236": "一级标题：设置实验配置\n二级标题：在脚本的不同位置设置\n内容：\n你可以在整个脚本的不同位置向 `config` 对象添加更多参数。\n\n下面的代码片段展示了如何向 `config` 对象添加新的键值对：\n\n```python\nimport swanlab\n\n# 定义一个config字典\nconfig = {\n  \"hidden_layer_sizes\": [64, 128],\n  \"activation\": \"ELU\",\n  \"dropout\": 0.5,\n  # ... 其他配置项\n}\n\n# 在你初始化SwanLab时传递config字典\nrun = swanlab.init(project=\"config_example\", config=config)\n\n# 在你初始化SwanLab之后，更新config\nswanlab.config[\"dropout\"] = 0.8\nswanlab.config.epochs = 20\nswanlab.config.set[\"batch_size\", 32]\n```",
    "237": "一级标题：设置实验配置\n二级标题：用配置文件设置\n内容：\n可以用json和yaml配置文件初始化 `config`，详情请查看[用配置文件创建实验](/guide_cloud/experiment_track/create-experiment-by-configfile)。",
    "238": "一级标题：设置实验Tag\n二级标题：常规标签\n内容：\n实验Tag可以快速标记本次实验所使用的**方法、数据集、模型、超参数、Git仓库等**，以及在未来可以用于分组和过滤实验。\n\n设置好的Tag会在实验名的下方出现：\n\n![](./set-experiment-tag/example.png)\n\n**方法一：编程设置**\n\n你可以使用`swanlab.init`中的`tags`参数，来设置实验的Tag（标签）。\n\n```python\nswanlab.init(\n    tags=[\"tag1\", \"tag2\"],\n)\n```\n\n**方法二：GUI设置**\n\n在网页中，找到实验的顶部区域，点击「添加标签」按钮，即可开始编辑标签：\n\n![](./set-experiment-tag/gui-setting.png)",
    "239": "一级标题：设置实验Tag\n二级标题：Git标签\n内容：\n支持识别标签中的Github、Gitee的仓库链接，呈现一个特殊样式，并可以点击跳转。\n\n```python\nswanlab.init(\n    tags=[\n        \"https://github.com/SwanHubX/SwanLab\",\n        \"https://gitee.com/SwanHubX/SwanLab\",\n    ],\n)\n```\n\n![](./set-experiment-tag/git-tag.png)",
    "240": "一级标题：设置实验Tag\n二级标题：Arxiv标签\n内容：\n支持识别标签中的Arxiv链接，呈现一个特殊样式，并可以点击跳转。\n\n```python\nswanlab.init(\n    tags=[\"https://arxiv.org/abs/1706.03762\"],\n)\n```\n\n![](./set-experiment-tag/arxiv-tag.png)",
    "241": "一级标题：设置实验Tag\n二级标题：AI开源社区标签\n内容：\n支持识别标签中的AI开源社区链接（[HuggingFace](https://huggingface.co/)、[魔搭社区](https://www.modelscope.cn/)、[魔乐社区](https://www.modelers.cn/)），呈现一个特殊样式，并可以点击跳转。\n\n```python\nswanlab.init(\n    tags=[\n        \"https://huggingface.co/deepseek-ai/DeepSeek-R1-0528\",\n        \"https://modelscope.cn/models/deepseek-ai/DeepSeek-R1-0528\",\n        \"https://modelers.cn/models/Modelers_Park/DeepSeek-R1-0528-Qwen3-8B\",\n    ],\n)\n```\n\n![](./set-experiment-tag/ai-community-tag.png)",
    "242": "一级标题：设置实验Tag\n二级标题：算力标签\n内容：\n支持识别标签中的算力卡品牌（nvidia、ascend、apple）。\n\n```python\nswanlab.init(\n    tags=[\"nvidia\", \"ascend\", \"apple\"],\n)\n```\n\n![](./set-experiment-tag/power-tag.png)",
    "243": "一级标题：在内网计算节点访问SwanLab Cloud\n二级标题：开启代理转发网络\n内容：\n我们可以通过使用SSH代理转发来实现让计算节点也能连接上 [SwanLab Cloud](https://swanlab.cn/)。\n\n> 确保你的计算节点能通过SSH连接上跳板机\n\n在计算节点上执行以下命令连接到跳板机：\n\n```bash\nssh -D {port} {user}@{ip}\n```\n\n- `port` 参数为用于代理转发的端口，例如 `2015`\n- `user` 和 `ip` 参数为跳板机服务器对应的用户名和内网IP地址\n\n例如：`ssh -D 2015 hello@192.168.31.10`\n\n连接到跳板机成功后，即在对应的端口开启了一个SOCKS代理通道，那么可以直接在终端设置环境变量来配置代理，例如：\n\n```bash\nexport http_proxy=socks5://127.0.0.1:{port} https_proxy=socks5://127.0.0.1:{port}\n```\n\n> 注意将对应的 `port` 更换为自己设置的端口，协议为 [socks5](https://en.wikipedia.org/wiki/SOCKS)\n\n配置成功后可以使用以下命令测试是否正确连接到公网:\n\n```bash\ncurl ipinfo.io\n```\n\n配置成功后就可以愉快地使用SwanLab云端版了🥳。\n\n注意SSH连接不能断开，关闭终端会话会导致连接断开，那么可以使用 [tmux](https://github.com/tmux/tmux/wiki) 将SSH连接命令放置在后台。\n\n```bash\ntmux\n# 在tmux中执行SSH连接命令\nssh -D {port} {user}@{ip}\n```\n\n新开终端会话必须重新配置环境变量，当然可以将上述导入环境变量的命令写入 `.bashrc` 文件中实现每次开启新终端会话时自动写入环境变量。例如：\n```bash\necho \"export http_proxy=socks5://127.0.0.1:{port}\" >> ~/.bashrc\necho \"export https_proxy=socks5://127.0.0.1:{port}\" >> ~/.bashrc\n```\n> 注意将 `{port}` 替换为自己设置的端口",
    "244": "一级标题：在内网计算节点访问SwanLab Cloud\n二级标题：实现原理\n内容：\n上述实现借助于 [SSH 动态转发](https://en.wikipedia.org/wiki/Port_forwarding#Dynamic_port_forwarding)功能，SSH 动态端口转发将 SSH 服务器变成 SOCKS 代理服务器，您计算机上的应用程序可以将其用作连接远程服务器的中介。\n\n> **注意：**程序必须支持 SOCKS 类型的代理，您才能使用动态端口转发从该应用程序路由流量。",
    "245": "一级标题：系统硬件监控\n二级标题：系统硬件监控\n内容：\nSwanLab在跟踪实验的过程中，会**自动监控**机器的硬件资源情况，并记录到 **「系统」图表** 当中。当前支持的硬件列表：\n\n| 硬件 | 信息记录 | 资源监控 | 脚本 |\n| --- | --- | --- | --- |\n| 英伟达GPU | ✅ | ✅ | [nvidia.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/gpu/nvidia.py) |\n| 昇腾NPU | ✅ | ✅ | [ascend.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/npu/ascend.py) |\n| 寒武纪MLU | ✅ | ✅ | [cambricon.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/mlu/cambricon.py) |\n| 昆仑芯XPU | ✅ | ✅ | [kunlunxin.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/xpu/kunlunxin.py) |\n| 摩尔线程GPU | ✅ | ✅ | [moorethread.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/gpu/moorethread.py) |\n| 沐曦GPU | ✅ | ✅ | [metax.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/gpu/metax.py) |\n| 海光DCU | ✅ | ✅ | [hygon.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/dcu/hygon.py) |\n| CPU | ✅ | ✅ | [cpu.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/cpu.py) |\n| 内存 | ✅ | ✅ | [memory.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/memory.py) |\n| 硬盘 | ✅ | ✅ | [disk.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/disk.py) |\n| 网络 | ✅ | ✅ | [network.py](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/run/metadata/hardware/network.py) |\n\n[[toc]]",
    "246": "一级标题：系统硬件监控\n二级标题：系统监控指标详解\n内容：\nSwanLab 支持在当前实验运行的机器上自动监控硬件资源情况，并为每个指标生成图表，统一展示在 **「系统」图表** 选项卡中。\n\n![](./system-monitor/head.png)\n\n**采集策略与频率**：SwanLab根据当前实验的持续运行时间，自动调整硬件数据采集的频率，以平衡数据粒度与系统性能，采集频率分为以下几档：\n\n| 已采集数据点数 | 采集频率 |\n|   :---:   |   :---:   |\n| 0~10    | 10 秒/次 |\n| 10~50   | 30 秒/次 |\n| 50+     | 60 秒/次 |\n\nSwanLab 采集的硬件资源情况涵盖了GPU、NPU、CPU、系统内存、硬盘IO以及网络情况等多个与训练过程相关的指标。以下详细介绍每个部分的监控内容及其在图表展示中的意义。",
    "247": "一级标题：系统硬件监控\n二级标题：GPU（NVIDIA）\n内容：\n![](./system-monitor/nvidia.png)\n\n> 在多卡机器上，每个GPU的资源情况都会单独记录，最终在图表中展示多条图线。\n\n| 指标 | 描述 |  \n|--------|------------|  \n| GPU Memory Allocated (%) | **GPU 显存使用率**，表示此GPU的显存占用百分比。|\n| GPU Memory Allocated (MB) | **GPU 显存使用率**，表示此GPU的显存占用量，以MB为单位。该指标对应图表的纵坐标上限为所有GPU中的最大总显存。|\n| GPU Utilization (%) | **GPU 利用率**，表示此GPU的计算资源占用百分比。|\n| GPU Temperature (℃) | **GPU 温度**，表示此GPU的温度，以摄氏度为单位。|\n| GPU Power Usage (W) | **GPU 功耗**，表示此GPU的功耗，以瓦特为单位。|\n| GPU Time Spent Accessing Memory (%) | **GPU 内存访问时间**，表示此GPU在执行任务时，花费在访问 GPU 内存（显存）上的时间百分比。|",
    "248": "一级标题：系统硬件监控\n二级标题：NPU（Ascend）\n内容：\n![](./system-monitor/ascend.png)\n\n> 在多卡机器上，每个NPU的资源情况都会单独记录，最终在图表中展示多条图线。\n\n| 指标 | 描述 |  \n|--------|------------|  \n| NPU Utilization (%) | **NPU 利用率**，表示此NPU的计算资源占用百分比。|\n| NPU Memory Allocated (MB) | **NPU 显存使用率**，表示此GPU的显存占用量，以MB为单位。该指标对应图表的纵坐标上限为所有GPU中的最大总显存。|\n| NPU Memory Allocated (%) | **NPU 显存使用率**，表示此NPU的显存占用百分比。|\n| NPU Temperature (℃) | **NPU 温度**，表示此NPU的温度，以摄氏度为单位。|\n| NPU Power (W) | **NPU 功率**，表示此NPU的功率，以瓦特为单位。|",
    "249": "一级标题：系统硬件监控\n二级标题：MLU（寒武纪）\n内容：\n![](./system-monitor/cambricon.png)\n\n> 在多卡机器上，每个MLU的资源情况都会单独记录，最终在图表中展示多条图线。\n\n| 指标 | 描述 |  \n|--------|------------|  \n| MLU Utilization (%) | **MLU 利用率**，表示此MLU的计算资源占用百分比。|\n| MLU Memory Allocated (MB) | **MLU 显存使用率**，表示此MLU的显存占用量，以MB为单位。该指标对应图表的纵坐标上限为所有MLU中的最大总显存。|\n| MLU Memory Allocated (%) | **MLU 显存使用率**，表示此MLU的显存占用百分比。|\n| MLU Temperature (℃) | **MLU 温度**，表示此MLU的温度，以摄氏度为单位。|\n| MLU Power (W) | **MLU 功率**，表示此MLU的功率，以瓦特为单位。|",
    "250": "一级标题：系统硬件监控\n二级标题：XPU（昆仑芯）\n内容：\n![](./system-monitor/kunlunxin.png)\n\n> 在多卡机器上，每个XPU的资源情况都会单独记录，最终在图表中展示多条图线。\n\n| 指标 | 描述 |  \n|--------|------------|  \n| XPU Utilization (%) | **XPU 利用率**，表示此XPU的计算资源占用百分比。|\n| XPU Memory Allocated (MB) | **XPU 显存使用率**，表示此XPU的显存占用量，以MB为单位。该指标对应图表的纵坐标上限为所有XPU中的最大总显存。|\n| XPU Memory Allocated (%) | **XPU 显存使用率**，表示此XPU的显存占用百分比。|\n| XPU Temperature (℃) | **XPU 温度**，表示此XPU的温度，以摄氏度为单位。|\n| XPU Power (W) | **XPU 功率**，表示此XPU的功率，以瓦特为单位。|",
    "251": "一级标题：系统硬件监控\n二级标题：GPU（摩尔线程）\n内容：\n![](./system-monitor/moorethread.png)\n\n> 在多卡机器上，每个摩尔线程GPU的资源情况都会单独记录，最终在图表中展示多条图线。\n\n| 指标 | 描述 |  \n|--------|------------|  \n| GPU Utilization (%) | **GPU 利用率**，表示此GPU的计算资源占用百分比。|\n| GPU Memory Allocated (MB) | **GPU 显存使用率**，表示此GPU的显存占用量，以MB为单位。该指标对应图表的纵坐标上限为所有GPU中的最大总显存。|\n| GPU Memory Allocated (%) | **GPU 显存使用率**，表示此GPU的显存占用百分比。|\n| GPU Temperature (℃) | **GPU 温度**，表示此GPU的温度，以摄氏度为单位。|\n| GPU Power (W) | **GPU 功率**，表示此GPU的功率，以瓦特为单位。|",
    "252": "一级标题：系统硬件监控\n二级标题：GPU（沐曦）\n内容：\n![](./system-monitor/metax.png)\n\n> 在多卡机器上，每个沐曦GPU的资源情况都会单独记录，最终在图表中展示多条图线。\n\n| 指标 | 描述 |     \n|--------|------------|  \n| GPU Utilization (%) | **GPU 利用率**，表示此GPU的计算资源占用百分比。|\n| GPU Memory Allocated (MB) | **GPU 显存使用率**，表示此GPU的显存占用量，以MB为单位。该指标对应图表的纵坐标上限为所有GPU中的最大总显存。|\n| GPU Memory Allocated (%) | **GPU 显存使用率**，表示此GPU的显存占用百分比。|\n| GPU Temperature (℃) | **GPU 温度**，表示此GPU的温度，以摄氏度为单位。|\n| GPU Power (W) | **GPU 功率**，表示此GPU的功率，以瓦特为单位。|",
    "253": "一级标题：系统硬件监控\n二级标题：DCU（海光）\n内容：\n> 在多卡机器上，每个海光DCU的资源情况都会单独记录，最终在图表中展示多条图线。\n\n| 指标 | 描述 |     \n|--------|------------|  \n| GPU Utilization (%) | **GPU 利用率**，表示此GPU的计算资源占用百分比。|\n| GPU Memory Allocated (MB) | **GPU 显存使用率**，表示此GPU的显存占用量，以MB为单位。该指标对应图表的纵坐标上限为所有GPU中的最大总显存。|\n| GPU Memory Allocated (%) | **GPU 显存使用率**，表示此GPU的显存占用百分比。|\n| GPU Temperature (℃) | **GPU 温度**，表示此GPU的温度，以摄氏度为单位。|\n| GPU Power (W) | **GPU 功率**，表示此GPU的功率，以瓦特为单位。|",
    "254": "一级标题：系统硬件监控\n二级标题：CPU\n内容：\n| 指标 | 描述 |  \n|--------|------------|  \n| CPU Utilization (%) | **CPU 利用率**，表示此CPU的计算资源占用百分比。|\n| Process CPU Threads | **CPU 线程数**，表示当前运行的实验所使用的CPU总线程数。|",
    "255": "一级标题：系统硬件监控\n二级标题：内存\n内容：\n| 指标 | 描述 |  \n|--------|------------|  \n| System Memory Utilization (%) | **系统内存使用率**，表示当前系统的内存占用百分比。|\n| Process Memory In Use (non-swap) (MB) | **进程占用内存**，当前进程实际占用的物理内存量（不包含交换区），直观反映实验运行时的内存消耗。|\n| Process Memory Utilization (MB) | **进程分配内存**，当前进程分配的内存量（包含交换区），不一定是实际使用的内存量。|\n| Process Memory Available （non-swap） (MB) | **进程可用内存**，当前进程可用的物理内存量（不包含交换区），即当前进程可以使用的内存量。|",
    "256": "一级标题：系统硬件监控\n二级标题：硬盘\n内容：\n| 指标 | 描述 |  \n|--------|------------|  \n| Disk IO Utilization (MB) | **硬盘I/O**，表示硬盘的读写速度，以MB/s为单位。读速率和写速率会在图表中作为两条图线，分开展示。|\n| Disk Utilization (%) | **硬盘使用情况**，表示当前系统盘的使用率，以百分比为单位。|\n\n在Linux平台，取根目录`/`的使用率；若操作系统为Windows，则取系统盘（通常是`C:`）的使用率。",
    "257": "一级标题：系统硬件监控\n二级标题：网络\n内容：\n| 指标 | 描述 |  \n|--------|------------|  \n| Network Traffic (KB) | **网络I/O**，表示网络的读写速度，以KB/s为单位。接收速率和发送速率会在图表中作为两条图线，分开展示。|\n\n> 表示网络的读写速度，以KB/s为单位。接收速率和发送速率会在图表中作为两条图线，分开展示。",
    "258": "一级标题：查看实验结果\n二级标题：云端同步\n内容：\n无论您在哪里训练模型 —— **自己的电脑、实验室的服务器集群、还是云上的实例**，我们都能轻松收集与汇总您的训练数据，并且随时随地访问训练进展，哪怕是在手机上。\n\n您也无需花时间将终端的输出截图或粘贴到Excel，也无需管理来自不同计算机的Tensorboard文件，用SwanLab就能轻松搞定。\n\n![](./view-result/cloud.jpg)",
    "259": "一级标题：查看实验结果\n二级标题：移动端看实验\n内容：\n你一定遇到过，实验正在training，但你不在电脑旁边 —— 也许在运动、在通勤、或者刚刚起床，十分想瞄一眼实验的进展和结果。这个时候，手机+SwanLab，会是绝佳组合。[查看详情](../general/app.md)\n\n![](../general/app/android.png)",
    "260": "一级标题：查看实验结果\n二级标题：表格视图\n内容：\n通过表格视图比较每次训练实验，看看哪些超参数发生了变化。  \n表格视图默认会将数据以`[实验名]-[元信息]-[配置]-[指标]`的顺序排序。\n\n![view-result](/assets/view-result-1.jpg)",
    "261": "一级标题：查看实验结果\n二级标题：图表对比视图\n内容：\n通过**图表对比视图**可以将每个实验的图表进行整合，生成一个多实验对比图表视图。  \n在多实验图表当中，可以清晰地对比不同实验在同一个指标下的变化情况与性能差异。\n\n![chart-comparison](/assets/chart-comparison.jpg)",
    "262": "一级标题：查看实验结果\n二级标题：日志\n内容：\n在实验开始到结束，SwanLab会记录下从`swanlab.init`到实验结束的终端输出，并记录在实验的「日志」选项卡，可以随时查看、复制与下载。我们也支持通过搜索找到关键信息。\n\n![logging](/assets/logging.jpg)",
    "263": "一级标题：查看实验结果\n二级标题：环境\n内容：\n在实验开始后，SwanLab会记录下训练相关的环境参数，包括：\n\n- **基础数据**：运行时间、主机名、操作系统、Python版本、Python解释器、运行目录、命令行、Git仓库URL、Git分支、Git提交、日志文件目录、SwanLab版本\n- **系统硬件**：CPU核心数、内存大小、GPU数量、GPU型号、GPU显存\n- **Python库**：运行环境下的所有Python库\n\n![environment](/assets/environment.jpg)",
    "264": "一级标题：什么是实验跟踪\n二级标题：实验跟踪的定义\n内容：\n**实验跟踪** 是指在机器学习模型开发过程中，记录每个实验从开始到结束的**超参数、指标、硬件、环境、日志**等数据，并在UI界面进行**组织**和**呈现**的过程。实验跟踪的目的是帮助研究人员更有效地**管理**和**分析**实验结果，以便更好地理解模型性能的变化，进而优化模型开发过程。\n\n::: warning 🤔简单来说\n实验跟踪的作用可以理解为，在进行机器学习实验时，记录下实验的各个关键信息，**为后续模型的进化提供“弹药”**。\n:::\n\n![](./what-is-experiment-track/overview.jpg)",
    "265": "一级标题：什么是实验跟踪\n二级标题：实验跟踪的重要性\n内容：\n与**实验跟踪**息息相关的，是**可视化**、**可复现性**、**实验比较**以及**团队协作**。\n\n1. **📊 可视化**: 通过UI界面对实验跟踪数据进行可视化，可以让训练师**直观地看到实验每一步**的结果，**分析指标走势**，判断哪些**变化**导致了模型效果的提升，从而**整体性地提升模型迭代效率**。\n\n![](./what-is-experiment-track/visualization.jpg)\n\n<br>\n\n2. **♻️ 可复现性**: 实验从跑通到可用，再到SOTA，往往需要经历**大量试验**，而一些非常好的结果可能出现在中前期。但如果没有实验跟踪和可视化，训练师难以记住这些结果，从而导致大量优秀的实验结果**记不清细节或被遗忘**。而通过SwanLab的实验跟踪和可视化功能，可以帮助训练师随时**回顾**这些结果，大大提高了可复现性与整体效率。\n\n![](./what-is-experiment-track/card.jpg)\n\n<br>\n\n3. **🆚 实验比较**: 训练师可以通过SwanLab**轻松地比较**多组实验结果，分析哪些变化导致了性能提升，从而**快速找到最优的训练策略**。\n\n![](./what-is-experiment-track/table.jpg)\n\n<br>\n\n4. **👥 团队协作**: 通过SwanLab的**实验分享、团队空间、多人协同**实验等功能，无缝地共享训练进展和心得经验，打通团队成员之间的信息孤岛，**提高团队协作效率**。",
    "266": "一级标题：SwanLab是如何进行实验跟踪的？\n二级标题：SwanLab实验跟踪流程\n内容：\n**SwanLab**帮助你只需使用几行代码，便可以跟踪机器学习实验，并在交互式仪表板中查看与比较结果。跟踪流程：\n\n1. 创建SwanLab实验。\n2. 将超参数字典（例如学习率或模型类型）存储到您的配置中 (swanlab.config)。\n3. 在训练循环中随时间记录指标 (swanlab.log)，例如准确性acc和损失loss。\n\n下面的伪代码演示了常见的**SwanLab实验跟踪工作流**：\n\n```python\n# 1. 创建1个SwanLab实验\nswanlab.init(project=\"my-project-name\")\n\n# 2. 存储模型的输入或超参数\nswanlab.config.learning_rate = 0.01\n\n# 这里写模型的训练代码\n...\n\n# 3. 记录随时间变化的指标以可视化表现\nswanlab.log({\"loss\": loss})\n```",
    "267": "一级标题：如何开始？\n二级标题：开始使用SwanLab实验跟踪\n内容：\n探索以下资源以了解SwanLab实验跟踪：\n\n- 阅读[快速开始](/guide_cloud/general/quick-start)\n- 探索本章以了解如何：\n  - [创建一个实验](/guide_cloud/experiment_track/create-experiment)\n  - [配置实验](/guide_cloud/experiment_track/set-experiment-config.md)\n  - [记录指标](/guide_cloud/experiment_track/log-experiment-metric.md)\n  - [查看实验结果](/guide_cloud/experiment_track/view-result.md)\n- 在[API文档](/api/api-index)中探索SwanLab Python 库。",
    "268": "一级标题：在手机上使用SwanLab\n二级标题：安卓\n内容：\n流程示例图如下所示，以Chrome浏览器为例：\n\n![alt text](/zh/guide_cloud/general/app/android.png)\n\n1. 在你的手机浏览器上，访问[swanlab.cn](https://swanlab.cn)\n2. 点击右上角三个点按钮后，在菜单中点击 **「添加到主屏幕」**\n3. 在弹窗中，选择 **「安装」** 或 **「创建快捷方式」** 均可\n4. 回到桌面，现在你在主屏幕上就可以找到 **SwanLab\"APP\"** 了！",
    "269": "一级标题：在手机上使用SwanLab\n二级标题：iOS\n内容：\n流程示例图如下所示，以Safari浏览器为例：\n\n![alt text](/zh/guide_cloud/general/app/ios.png)\n\n1. 在你的手机浏览器上，访问[swanlab.cn](https://swanlab.cn)\n2. 点击底部中间的分享按钮后，在菜单中点击 **「添加到主屏幕」**\n3. 在弹窗中，编辑应用名称，点击右上角的 **「添加」**\n4. 回到桌面，现在你在主屏幕上就可以找到 **SwanLab\"APP\"** 了！",
    "270": "一级标题：更新日志\n二级标题：更新指南\n内容：\n::: warning 更新指南\n升级到最新版：`pip install -U swanlab`  \nGithub: https://github.com/SwanHubX/SwanLab\n:::",
    "271": "一级标题：更新日志\n二级标题：v0.6.5 - 2025.7.5\n内容：\n**🚀新增功能**\n- 支持**resume断点续训**\n- 支持小折线图局部放大\n- 支持配置单个折线图平滑\n\n**⚙️优化**\n- 大幅改进了图像图表放大后的交互效果\n\n**🔌集成**\n- 🤗集成[accelerate](https://github.com/huggingface/accelerate)框架，[文档](/guide_cloud/integration/integration-huggingface-accelerate.md)增强分布式训练中的实验记录体验；\n- 集成[ROLL](https://github.com/alibaba/ROLL)框架，[文档](/guide_cloud/integration/integration-roll.md)增强分布式训练中的实验记录体验；\n- 集成[Ray](https://github.com/ray-project/ray)框架，[文档](/guide_cloud/integration/integration-ray.md)增强分布式训练中的实验记录体验；\n\n**🔌插件**\n- 新增`LogdirFileWriter`插件，支持将文件写入到日志文件夹\n\n**生态**\n- 阿里云计算巢服务上架：[指引](/zh/guide_cloud/self_host/alibabacloud-computenest.md)",
    "272": "一级标题：更新日志\n二级标题：v0.6.4 - 2025.6.18\n内容：\n**🚀新增功能**\n- 新增与[AREAL](https://github.com/inclusionAI/AReaL)框架的集成，[PR](https://github.com/inclusionAI/AReaL/pull/98)\n- 支持鼠标Hover到侧边栏实验时，高亮相应曲线\n- 支持跨组对比折线图\n- 启用渐进式图表渲染，提高页面加载速度\n- 支持设置实验名裁剪规则\n\n**⚙️修复**\n- 修复了`local`模式下，日志文件无法正确`sync`和`watch`的问题",
    "273": "一级标题：更新日志\n二级标题：v0.6.3 - 2025.6.12\n内容：\n**🚀新增功能**\n- 新增`swnalab.echarts.table`，支持创建表格图表\n- 昇腾/沐曦/海光/寒武纪/昆仑芯 硬件监控 增加显存（MB）记录\n- `swanlab sync`支持一次多日志上传\n- 工作区增加`公开/私有`筛选\n- 表格视图增加`最新/最大/最小值`切换模块",
    "274": "一级标题：更新日志\n二级标题：v0.6.2 - 2025.6.9\n内容：\n**🚀新增功能**\n- 新增`swanlab sync`命令，支持将本地日志同步到SwanLab云端/私有化部署端\n- 支持在本地存储完整的实验日志文件",
    "275": "一级标题：更新日志\n二级标题：v0.6.1 - 2025.6.5\n内容：\n**🚀新增功能**\n- 鼠标放到表头，可以显示缩略的名称了\n- 表格视图增加「展开子表」功能\n- 硬件监控支持海光DCU\n- 硬件监控支持获取昇腾NPU的功耗信息\n\n**🤔优化**\n- 优化了HuggigngFace accelerate框架的集成\n- 默认不再打印重复step log warning",
    "276": "一级标题：更新日志\n二级标题：v0.6.0 - 2025.6.1\n内容：\n**🚀新增功能**\n- 支持 **图表自由拖拽**\n- 支持ECharts自定义图表，增加包括柱状图、饼状图、直方图在内的20+图表类型\n- 硬件监控已支持 **沐曦** 显卡\n- 集成 [PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP) 框架",
    "277": "一级标题：更新日志\n二级标题：v0.5.9 - 2025.5.25\n内容：\n**🚀新增功能**\n-  📒 日志记录支持了标准错误流，EvalScope / PyTorch Lightning等这些框架的日志记录体验大幅提升\n-  💻 硬件监控已支持 **摩尔线程** 显卡\n-  🔐 新增运行命令记录的安全防护功能，API Key将被自动隐藏\n-  ⚙️ 设置新增「默认空间」和「默认可见性」配置，可以指定你的项目默认创建在哪个组织下啦！",
    "278": "一级标题：更新日志\n二级标题：v0.5.8 - 2025.5.13\n内容：\n**🚀新增功能**\n\n- 新增**实验Tag**功能\n- 新增折线图 **Log Scale** 功能\n- 新增 **实验分组拖拽** 功能\n- 新增实验卡片中**配置**与**指标**表格下载功能\n- 新增[开放接口](/zh/api/py-openapi.md)，支持通过API获取SwanLab数据\n- 大幅优化了指标传输性能，提升上千指标的传输速度\n- 集成`paddlenlp`框架\n\n**🤔优化**\n- 优化了个人主页的一系列交互\n\n**生态**\n- 腾讯云云应用上架：[指引](/zh/guide_cloud/self_host/tencentcloud-app.md)",
    "279": "一级标题：更新日志\n二级标题：v0.5.6 - 2025.4.23\n内容：\n**🚀新增功能**\n\n- 折线图支持**图表配置**功能，本次更新支持配置图表的X、Y轴范围；主标题；X、Y轴标题\n- 图表搜索支持**正则表达式**\n- SwanLab私有化部署版，已支持离线激活验证\n- 支持**昆仑芯XPU**的环境记录与硬件监控\n- 适配对使用`uv`环境下的pip环境记录\n- 环境记录支持记录**Linux发行版**（如Ubuntu、CentOS、Kylin等）\n\n**🤔优化**\n- 修复了侧边栏一键隐藏实验的一些问题",
    "280": "一级标题：更新日志\n二级标题：v0.5.5 - 2025.4.7\n内容：\n**🚀新增功能**\n- 新增`swanlab.Molecule`数据类型，支持生物化学分子可视化，为AlphaFold等AI4Science训练任务提供更好的训练体验\n- 实验表格，现在支持记忆你的排序、筛选、列拖拽了！\n- 支持了寒武纪MLU的温度和功率指标记录\n- 新增SWANLAB_PROJ、SWANLAB_WORKSPACE、SWANLAB_EXP_NAME三个环境变量\n- 环境中支持显示寒武纪MLU Logo\n\n**🌍生态**\n- 大模型评估框架[EvalScope](https://github.com/modelscope/evalscope) 已集成SwanLab！：https://github.com/modelscope/evalscope/pull/453\n\n**🤔优化**\n- 优化了网页加载性能",
    "281": "一级标题：更新日志\n二级标题：v0.5.4 - 2025.3.31\n内容：\n**🚀新增功能**\n- 新增`swanlab.Settings`方法，支持更精细化的实验行为控制，进一步增强开放性\n- 支持了寒武纪MLU的硬件记录和资源监控\n- 昇腾NPU的硬件记录支持记录CANN版本\n- 英伟达GPU的硬件记录支持记录GPU架构和cuda核心数\n- 英伟达GPU的硬件监控支持记录“GPU 访问内存所花费的时间百分比”\n- 「个人主页」支持显示你所在的「组织」\n- 「概览」页支持编辑\"项目描述\"文本\n\n**🤔优化**\n- 修复了sync_wandb的一些问题\n- 修复了Obejct3D类的一些问题\n- 优化「常规」设置样式\n- 大幅优化了打开项目的性能\n\n**🔌插件**\n- 官方插件增加Slack通知、Discord通知，进一步打通海外生态",
    "282": "一级标题：更新日志\n二级标题：v0.5.3 - 2025.3.20\n内容：\n![swanlab x huggingface](./changelog/hf.png)\n\n**🚀新增功能**\n- SwanLab已正式加入 **🤗HuggingFace生态**！Transformers 4.50.0版本开始 正式将SwanLab集成为实验跟踪工具，在TrainingArguments中加入`report_to=\"swanlab\"`即可开始跟踪训练。\n- 新增了`swanlab.Object3D`，支持记录三维点云，[文档](/api/py-object3d)\n- 硬件监控支持了 GPU显存（MB）、磁盘利用率、网络上下行 的记录\n\n**优化**\n- 修复了一些问题",
    "283": "一级标题：更新日志\n二级标题：v0.5.0 - 2025.3.12\n内容：\n![logo](../self_host/docker-deploy/swanlab-docker.jpg)\n\n**🎉🎉SwanLab私有化部署（社区版）现已重磅发布！！**[部署文档](/guide_cloud/self_host/docker-deploy.md)\n\n**🚀新增功能**\n- `swanlab.init`新增参数`callbacks`，支持在初始化时注册回调函数，以支持各式各样的自定义插件类\n- 新增`swanlab.register_callback()`，支持在`init`外部注册回调函数，[文档](/api/py-register-callback.html)\n- `swanlab.login()`升级，新增`host`、`web_host`、`save`参数，适配了私有化部署服务的特性，同时支持不将用户登录凭证写入本地，以适应共用服务器场景。[文档](/zh/api/py-login.md)\n- `swanlab login`升级，新增`host`、`web_host`、`api-key`参数，[文档](/zh/api/cli-swanlab-login.md)\n- 新增支持使用`swanlab.sync_mlflow()`将MLFlow项目同步到SwanLab，[文档](/guide_cloud/integration/integration-mlflow.md)\n\n**🤔优化**\n- 我们大幅优化了sdk架构，提升了sdk在大量metric场景下的性能\n- 实验侧边栏可以拉伸了！\n- 实验页面右上角增加了「Git代码」按钮，一键跳转到对应的仓库\n\n**🔌插件**：\n- 新增**通知类插件**，支持在训练结束时使用**邮件、飞书、钉钉、企业微信**进行通知\n- 新增**记录类插件**，支持在训练过程中将元数据、配置、指标写入到**本地CSV文件**",
    "284": "一级标题：更新日志\n二级标题：v0.4.12 - 2025.3.8\n内容：\n**优化**\n- 修复了一些问题",
    "285": "一级标题：更新日志\n二级标题：v0.4.11 - 2025.3.5\n内容：\n**优化**\n- 修复了部分版本W&B格式转换报错的问题\n- 修复了一些交互问题",
    "286": "一级标题：更新日志\n二级标题：v0.4.10 - 2025.3.4\n内容：\n**🚀新增功能**\n- 新增了和[DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio)的集成, [文档](/guide_cloud/integration/integration-diffsynth-studio.md)\n- 新增支持转换 **MLFlow** 实验到 SwanLab，[文档](/guide_cloud/integration/integration-mlflow.md)\n- 新增**项目描述**，支持给你的项目记一些简短的笔记\n\n**优化**\n- 修复了在OpenEuler系统上无法正确记录CPU型号的问题",
    "287": "一级标题：更新日志\n二级标题：v0.4.9 - 2025.2.28\n内容：\n**🚀新增功能**\n- 新增了`移动实验`功能\n- 对一些集成Callback类增加了`update_config`方法\n- `run`新增`get_url()`和`get_project_url()`方法，支持获取实验和项目的URL\n\n**优化**\n- 修复了在部分Linux系统上CPU品牌获取不到的问题",
    "288": "一级标题：更新日志\n二级标题：v0.4.8 - 2025.2.16\n内容：\n**🚀新增功能**\n- 新增了和Modelscope Swift的集成，[文档](/guide_cloud/integration/integration-swift.md)\n- 新增了`添加分组`和`移动图表到其他分组`功能\n\n**优化**\n- 修复了sdk的一些问题",
    "289": "一级标题：更新日志\n二级标题：v0.4.7 - 2025.2.11\n内容：\n**🚀新增功能**\n- `swanlab.log`支持了参数`print_to_console`，开启后可以将`swanlab.log`的`key`、`value`以字典的形式打印到终端\n- `swanlab.init`支持了对`name`、`notes`参数的适配，等价于`experiment_name`和`description`",
    "290": "一级标题：更新日志\n二级标题：v0.4.6 - 2025.2.3\n内容：\n**🚀新增功能**\n- 新增与LLM强化学习框架[verl](https://github.com/volcengine/verl)的集成，[文档](/guide_cloud/integration/integration-verl.md)\n- `swanlab.log`支持了嵌套字典传入\n\n**优化**\n- 优化了在PyTorch Lightning框架下的分布式训练优化",
    "291": "一级标题：更新日志\n二级标题：v0.4.5 - 2025.1.22\n内容：\n**🚀新增功能**\n- 新增`swanlab.sync_tensorboardX()`和`swanlab.sync_tensorboard_torch()`：支持使用TensorboardX或PyTorch.utils.tensorboard跟踪实验时，同步指标到SwanLab\n\n**优化**\n- 优化了`sync_wandb()`的代码兼容性",
    "292": "一级标题：更新日志\n二级标题：v0.4.3 - 2025.1.17\n内容：\n**🚀新增功能**\n- 新增`swanlab.sync_wandb()`：支持使用Weights&Biases跟踪实验时，同步指标到SwanLab，[文档](/guide_cloud/integration/integration-wandb.md)\n- 新增在使用框架集成时，配置项将记录所使用的框架\n\n**优化**\n- 改进了表格视图的交互，增加了行列拖拽、筛选、排序交互\n- 大幅优化了工作区加载的性能\n- 大幅优化了日志渲染的性能\n- 改进了在未登录的计算机上，执行`swanlab.init()`的交互\n- 修复了一些已知问题",
    "293": "一级标题：更新日志\n二级标题：元旦节更新\n内容：\n**🚀新增功能**\n- 升级了图表平滑，网页刷新后状态将仍然保留\n- 更新了图表大小修改，现在可以通过拖拽图表的右下角来改变大小\n\n**⚙️问题修复**\n- 修复了没有实验时，项目设置不显示删除的bug",
    "294": "一级标题：更新日志\n二级标题：v0.4.2 - 2024.12.24\n内容：\n**🚀新增功能**\n- 新增密码登录\n- 新增项目设置页\n\n**优化**\n- 修复在一些设备上运行硬件监控会warning的问题",
    "295": "一级标题：更新日志\n二级标题：v0.4.0 - 2024.12.15\n内容：\n🎉万众期待的硬件监控功能（云端版）已经上线，支持**CPU、NPU、GPU**的系统级信息监控：\n\n- **CPU**：利用率、线程数\n- **内存**：利用率、进程利用率、可用内存\n- **Nvidia GPU**：利用率、显存分配、温度、功耗\n- **Ascend NPU**：利用率、HBM分配、温度\n\n更多信息的监控已经在路上！\n\nby Cunyue",
    "296": "一级标题：更新日志\n二级标题：v0.3.28 - 2024.12.6\n内容：\n> 🍥公告：硬件监控功能即将推出！\n\n**🚀新增功能**\n- 新增与LightGBM的集成\n- 新增与XGBoost的集成\n\n**优化**\n- 提高了对日志记录时单行长度的限制\n- 改善了部分性能，为0.4.0版本做准备",
    "297": "一级标题：更新日志\n二级标题：v0.3.27 - 2024.11.26\n内容：\n**🚀新增功能**\n- 新增华为昇腾NPU显卡检测\n- 新增与青云基石智算(Coreshub)的集成",
    "298": "一级标题：更新日志\n二级标题：新UI上线！\n内容：\n![alt text](/assets/new-homepage.png)\n\n**🚀我们改进了什么**\n- 从用户体验出发，上线全新的官网和UI界面\n- 上线个人/组织主页\n- 增加「黑夜模式」\n- 全面优化的「新手快速开始」，增加了框架集成和案例\n- 优化「图表对比视图」的实验选择逻辑",
    "299": "一级标题：更新日志\n二级标题：v0.3.25 - 2024.11.11\n内容：\n**🚀新增功能**\n- 🎉[VSCode插件](https://marketplace.visualstudio.com/items?itemName=SwanLab.swanlab&ssr=false#overview)已上线\n- 新增与Keras框架的集成\n- 新增`run.public`方法，支持获取实验的项目名、实验名、链接等信息，[#732](https://github.com/S",
    "300": "一级标题：团队使用SwanLab\n二级标题：创建组织\n内容：\n在主页的左上方，点击“创建组织”按钮，填写组织名、组织ID等信息，即可完成组织创建。\n\n<div align=\"center\">\n<img src=\"/assets/organization-create.jpg\" width=\"400\">\n</div>",
    "301": "一级标题：团队使用SwanLab\n二级标题：邀请成员\n内容：\n<div align=\"center\">\n<img src=\"./organization/invite.png\">\n</div>\n\n在组织空间下，点击「设置」-「常规」，在「成员」栏下，点击「邀请成员」按钮，将邀请链接分享给要加入组织的成员。\n\n<div align=\"center\">\n<img src=\"./organization/join.png\">\n</div>\n\n成员点击邀请链接，提交申请后，经管理员审核通过，即可完成加入。",
    "302": "一级标题：团队使用SwanLab\n二级标题：将实验上传到组织空间\n内容：\n在默认情况下（即不设置`workspace`参数），你的项目会被上传到个人空间下。  \n想要上传到组织空间下，则将`swanlab.init`的`workspace`参数设置为组织的组织名（不是组织昵称）即可。\n\n```python\nimport swanlab\n\nswanlab.init(\n    workspace=\"[组织名username]\"\n)\n```\n\n如果组织里的多个人想要在一个项目下协作，则只需要将`swanlab.init`的`project`参数设置为同一个即可。",
    "303": "一级标题：快速开始\n二级标题：1. 安装SwanLab\n内容：\n使用 [pip](https://pip.pypa.io/en/stable/) 在Python3环境的计算机上安装swanlab库。\n\n打开命令行，输入：\n\n```bash\npip install swanlab\n```\n\n按下回车，等待片刻完成安装。\n\n> 如果遇到安装速度慢的问题，可以指定国内源安装：  \n> `pip install swanlab -i https://mirrors.cernet.edu.cn/pypi/web/simple`",
    "304": "一级标题：快速开始\n二级标题：2. 登录账号\n内容：\n> 如果你还没有SwanLab账号，请在 [官网](https://swanlab.cn) 免费注册。\n\n打开命令行，输入：\n\n```bash\nswanlab login\n```\n\n当你看到如下提示时：\n\n```bash\nswanlab: Logging into swanlab cloud.\nswanlab: You can find your API key at: https://swanlab.cn/settings\nswanlab: Paste an API key from your profile and hit enter, or press 'CTRL-C' to quit:\n```\n\n在[用户设置](https://swanlab.cn/settings)页面复制您的 **API Key**，粘贴后按下回车（你不会看到粘贴后的API Key，请放心这是正常的），即可完成登录。之后无需再次登录。\n\n::: info\n\n如果你的计算机不太支持`swanlab login`的登录方式，也可以使用python脚本登录：\n\n```python\nimport swanlab\nswanlab.login(api_key=\"你的API Key\", save=True)\n```\n\n:::",
    "305": "一级标题：快速开始\n二级标题：3. 开启一个实验并跟踪超参数\n内容：\n在Python脚本中，我们用`swanlab.init`创建一个SwanLab实验，并向`config`参数传递将一个包含超参数键值对的字典：\n\n```python\nimport swanlab\n\nrun = swanlab.init(\n    # 设置项目\n    project=\"my-project\",\n    # 跟踪超参数与实验元数据\n    config={\n        \"learning_rate\": 0.01,\n        \"epochs\": 10,\n    },\n)\n```\n\n`run`是SwanLab的基本组成部分，你将经常使用它来记录与跟踪实验指标。",
    "306": "一级标题：快速开始\n二级标题：4. 记录实验指标\n内容：\n在Python脚本中，用`swanlab.log`记录实验指标（比如准确率acc和损失值loss）。\n\n用法是将一个包含指标的字典传递给`swanlab.log`：\n\n```python\nswanlab.log({\"accuracy\": acc, \"loss\": loss})\n```",
    "307": "一级标题：快速开始\n二级标题：5. 完整代码，在线查看可视化看板\n内容：\n我们将上面的步骤整合为下面所示的完整代码：\n\n```python (5,25)\nimport swanlab\nimport random\n\n# 初始化SwanLab\nrun = swanlab.init(\n    # 设置项目\n    project=\"my-project\",\n    # 跟踪超参数与实验元数据\n    config={\n        \"learning_rate\": 0.01,\n        \"epochs\": 10,\n    },\n)\n\nprint(f\"学习率为{run.config.learning_rate}\")\n\noffset = random.random() / 5\n\n# 模拟训练过程\nfor epoch in range(2, run.config.epochs):\n    acc = 1 - 2**-epoch - random.random() / epoch - offset\n    loss = 2**-epoch + random.random() / epoch + offset\n    print(f\"epoch={epoch}, accuracy={acc}, loss={loss}\")\n    # 记录指标\n    swanlab.log({\"accuracy\": acc, \"loss\": loss})\n```\n\n运行代码，访问[SwanLab](https://swanlab.cn)，查看在每个训练步骤中，你使用SwanLab记录的指标（准确率和损失值）的改进情况。\n\n![quick-start-1](./quick_start/line-chart.png)",
    "308": "一级标题：快速开始\n二级标题：下一步是什么\n内容：\n1. 查看SwanLab如何[记录多媒体内容](/guide_cloud/experiment_track/log-media)（图片、音频、文本、...）\n1. 查看SwanLab记录[MNIST手写体识别](/examples/mnist.md)的案例\n2. 查看与其他框架的[集成](/guide_cloud/integration/integration-pytorch-lightning.md)\n3. 查看如何通过SwanLab与[团队协作](/guide_cloud/general/organization.md)",
    "309": "一级标题：快速开始\n二级标题：常见问题\n内容：\n### 1. 在哪里可以找到我的API Key？\n\n登陆SwanLab网站后，API Key将显示在[用户设置](https://swanlab.cn/settings)页面上。\n\n### 2. 我可以离线使用SwanLab吗？\n\n可以，具体流程请查看[自托管部分](/guide_cloud/self_host/docker-deploy.md)。",
    "310": "一级标题：欢迎使用SwanLab\n二级标题：在线演示\n内容：\n| [ResNet50 猫狗分类][demo-cats-dogs] | [Yolov8-COCO128 目标检测][demo-yolo] |\n| :--------: | :--------: |\n| [![][demo-cats-dogs-image]][demo-cats-dogs] | [![][demo-yolo-image]][demo-yolo] |\n| 跟踪一个简单的 ResNet50 模型在猫狗数据集上训练的图像分类任务。 | 使用 Yolov8 在 COCO128 数据集上进行目标检测任务，跟踪训练超参数和指标。 |\n\n| [Qwen2 指令微调][demo-qwen2-sft] | [LSTM Google 股票预测][demo-google-stock] |\n| :--------: | :--------: |\n| [![][demo-qwen2-sft-image]][demo-qwen2-sft] | [![][demo-google-stock-image]][demo-google-stock] |\n| 跟踪 Qwen2 大语言模型的指令微调训练，完成简单的指令遵循。 | 使用简单的 LSTM 模型在 Google 股价数据集上训练，实现对未来股价的预测。 |\n\n| [ResNeXt101 音频分类][demo-audio-classification] | [Qwen2-VL COCO数据集微调][demo-qwen2-vl] |\n| :--------: | :--------: |\n| [![][demo-audio-classification-image]][demo-audio-classification] | [![][demo-qwen2-vl-image]][demo-qwen2-vl] |\n| 从ResNet到ResNeXt在音频分类任务上的渐进式实验过程 | 基于Qwen2-VL多模态大模型，在COCO2014数据集上进行Lora微调。 |\n\n| [EasyR1 多模态LLM RL训练][demo-easyr1-rl] | [Qwen2.5-0.5B GRPO训练][demo-qwen2-grpo] |\n| :--------: | :--------: |\n| [![][demo-easyr1-rl-image]][demo-easyr1-rl] | [![][demo-qwen2-grpo-image]][demo-qwen2-grpo] |\n| 使用EasyR1框架进行多模态LLM RL训练 | 基于Qwen2.5-0.5B模型在GSM8k数据集上进行GRPO训练 |\n\n视频Demo：\n\n<video controls src=\"./what_is_swanlab/demo.mp4\"></video>",
    "311": "一级标题：欢迎使用SwanLab\n二级标题：SwanLab能做什么？\n内容：\n**1. 📊 实验指标与超参数跟踪**: 极简的代码嵌入您的机器学习 pipeline，跟踪记录训练关键指标\n\n- ☁️ 支持**云端**使用（类似Weights & Biases），随时随地查看训练进展。[手机看实验的方法](https://docs.swanlab.cn/guide_cloud/general/app.html)\n- 🌸 **可视化训练过程**: 通过UI界面对实验跟踪数据进行可视化，可以让训练师直观地看到实验每一步的结果，分析指标走势，判断哪些变化导致了模型效果的提升，从而整体性地提升模型迭代效率。\n- 📝 **超参数记录**、**指标总结**、**表格分析**\n- **支持的元数据类型**：标量指标、图像、音频、文本、3D点云、生物化学",
    "312": "一级标题：集成框架一览\n二级标题：基础框架\n内容：\n- [PyTorch](/guide_cloud/integration/integration-pytorch.html)\n- [MindSpore](/guide_cloud/integration/integration-ascend.html)\n- [Keras](/guide_cloud/integration/integration-keras.html)",
    "313": "一级标题：集成框架一览\n二级标题：专有/微调框架\n内容：\n- [PyTorch Lightning](/guide_cloud/integration/integration-pytorch-lightning.html)\n- [HuggingFace Transformers](/guide_cloud/integration/integration-huggingface-transformers.html)\n- [LLaMA Factory](/guide_cloud/integration/integration-llama-factory.html)\n- [Modelscope Swift](/guide_cloud/integration/integration-swift.html)\n- [DiffSynth-Studio](/guide_cloud/integration/integration-diffsynth-studio.html)\n- [Sentence Transformers](/guide_cloud/integration/integration-sentence-transformers.html)\n- [PaddleNLP](/guide_cloud/integration/integration-paddlenlp.html)\n- [OpenMind](https://modelers.cn/docs/zh/openmind-library/1.0.0/basic_tutorial/finetune/finetune_pt.html#%E8%AE%AD%E7%BB%83%E7%9B%91%E6%8E%A7)\n- [Torchtune](/guide_cloud/integration/integration-pytorch-torchtune.html)\n- [XTuner](/guide_cloud/integration/integration-xtuner.html)\n- [MMEngine](/guide_cloud/integration/integration-mmengine.html)\n- [FastAI](/guide_cloud/integration/integration-fastai.html)\n- [LightGBM](/guide_cloud/integration/integration-lightgbm.html)\n- [XGBoost](/guide_cloud/integration/integration-xgboost.html)",
    "314": "一级标题：集成框架一览\n二级标题：评估框架\n内容：\n- [EvalScope](/guide_cloud/integration/integration-evalscope.html)",
    "315": "一级标题：集成框架一览\n二级标题：计算机视觉\n内容：\n- [Ultralytics](/guide_cloud/integration/integration-ultralytics.html)\n- [MMDetection](/guide_cloud/integration/integration-mmdetection.html)\n- [MMSegmentation](/guide_cloud/integration/integration-mmsegmentation.html)\n- [PaddleDetection](/guide_cloud/integration/integration-paddledetection.html)\n- [PaddleYOLO](/guide_cloud/integration/integration-paddleyolo.html)",
    "316": "一级标题：集成框架一览\n二级标题：强化学习\n内容：\n- [Stable Baseline3](/guide_cloud/integration/integration-sb3.html)\n- [veRL](/guide_cloud/integration/integration-verl.html)\n- [HuggingFace trl](/guide_cloud/integration/integration-huggingface-trl.html)\n- [EasyR1](/guide_cloud/integration/integration-easyr1.html)\n- [AReaL](/guide_cloud/integration/integration-areal.html)\n- [ROLL](/guide_cloud/integration/integration-roll.html)",
    "317": "一级标题：集成框架一览\n二级标题：其他框架\n内容：\n- [Tensorboard](/guide_cloud/integration/integration-tensorboard.html)\n- [Weights&Biases](/guide_cloud/integration/integration-wandb.html)\n- [MLFlow](/guide_cloud/integration/integration-mlflow.html)\n- [HuggingFace Accelerate](/guide_cloud/integration/integration-huggingface-accelerate.html)\n- [Ray](/guide_cloud/integration/integration-ray.html)\n- [Hydra](/guide_cloud/integration/integration-hydra.html)\n- [Omegaconf](/guide_cloud/integration/integration-omegaconf.html)\n- [OpenAI](/guide_cloud/integration/integration-openai.html)\n- [ZhipuAI](/guide_cloud/integration/integration-zhipuai.html)",
    "318": "一级标题：将SwanLab集成到你的库\n二级标题：补充Requirements\n内容：\n在开始之前，请决定是否在您的库的依赖项中要求 SwanLab：\n\n### 1.1 将swanlab作为依赖项\n\n```plaintext\ntorch==2.5.0\n...\nswanlab==0.4.*\n```\n\n### 1.2 将swanlab作为可选安装\n\n有两种设置swanlab成为可选安装的方法。\n\n1. 在代码中使用try-except语句，当用户没有安装swanlab时，抛出错误。\n\n```python\ntry:\n    import swanlab\nexcept ImportError:\n    raise ImportError(\n        \"You are trying to use swanlab which is not currently installed.\"\n        \"Please install it using pip install swanlab\"\n    )\n```\n\n2. 如果你要构建Python包，请将`swanlab`作为可选依赖项添加到`pyproject.toml`文件中：\n\n```toml\n[project]\nname = \"my_awesome_lib\"\nversion = \"0.1.0\"\ndependencies = [\n    \"torch\",\n    \"transformers\"\n]\n\n[project.optional-dependencies]\ndev = [\n    \"swanlab\"\n]\n```",
    "319": "一级标题：将SwanLab集成到你的库\n二级标题：用户登录\n内容：\n您的用户有几种方法可以登录SwanLab：\n\n::: code-group\n\n```bash [命令行]\nswanlab login\n```\n\n```python [Python]\nimport swanlab\nswanlab.login()\n```\n\n```bash [环境变量(Bash)]\nexport SWANLAB_API_KEY=$YOUR_API_KEY\n```\n\n```python [环境变量(Python)]\nimport os\nos.environ[\"SWANLAB_API_KEY\"] = \"zxcv1234...\"\n```\n\n:::\n\n如果用户是第一次使用`swanlab`而没有遵循上述任何步骤，则当您的脚本调用`swanlab.init`时，系统会自动提示他们登录。",
    "320": "一级标题：将SwanLab集成到你的库\n二级标题：启动SwanLab实验\n内容：\n实验是SwanLab的计算单元。通常，你可以为每个实验创建一个`Experiment`对象，并使用`swanlab.init`方法启动实验。\n\n### 3.1 初始化实验\n\n初始化SwanLab，并在您的代码种启动实验：\n\n```python\nswanlab.init()\n```\n\n你可以为这个实验提供项目名、实验名、工作空间等参数：\n\n```python\nswanlab.init(\n    project=\"my_project\",\n    experiment_name=\"my_experiment\",\n    workspace=\"my_workspace\",\n    )\n```\n\n::: warning 最好把 swanlab.init 放在哪里？\n\n您的库应该尽早创建SwanLab实验，因为SwanLab会自动收集控制台中的任何输出，这将使得调试更加容易。\n\n:::\n\n### 3.2 配置三种启动模式\n\n你可以通过`mode`参数来配置SwanLab的启动模式：\n\n::: code-group\n\n```python [云端模式]\nswanlab.init(\n    mode=\"cloud\",  # 默认模式\n    )\n```\n\n```python [本地模式]\nswanlab.init(\n    mode=\"local\",\n    )\n```\n\n```python [禁用模式]\nswanlab.init(\n    mode=\"disabled\",\n    )\n```\n\n:::\n\n- **云端模式**：默认模式。SwanLab会将实验数据上传到一个web服务器（SwanLab官方云或您自行部署的私有云）。\n- **本地模式**：SwanLab不会将实验数据上传到云端，但会记录一个特殊的`swanlog`目录，可以被`dashboard`插件打开进行可视化。\n- **禁用模式**：SwanLab不会收集任何数据，代码执行到`swanlab`相关代码时将不做任何处理。\n\n### 3.3 定义实验超参数/配置\n\n使用swanlab实验配置(config)，您可以在创建SwanLab实验时提供有关您的模型、数据集等的元数据。您可以使用这些信息来比较不同的实验并快速了解主要差异。\n\n您可以记录的典型配置参数包括：\n\n- 模型名称、版本、架构参数等\n- 数据集名称、版本、训练/测试数据数等。\n- 训练参数，例如学习率、批量大小、优化器等。\n\n以下代码片段显示了如何记录配置：\n\n```python\nconfig = {\"learning_rate\": 0.001, ...}\nswanlab.init(..., config=config)\n```\n\n**更新配置**：\n\n使用`swanlab.config.update`方法来更新配置。在定义config字典后获取参数时，用此方法更新config字典非常方便。\n\n例如，你可能希望在实例化模型后，添加模型的参数：\n\n```python\nswanlab.config.update({\"model_params\": \"1.5B\"})\n```",
    "321": "一级标题：将SwanLab集成到你的库\n二级标题：记录数据到SwanLab\n内容：\n创建一个字典，其中key是指标的名称，value是指标的值。将此字典对象传递给`swanlab.log`：\n\n::: code-group\n\n```python [记录一组指标]\nmetrics = {\"loss\": 0.5, \"accuracy\": 0.8}\nswanlab.log(metrics)\n```\n\n```python [循环记录指标]\nfor epoch in range(NUM_EPOCHS):\n    for input, ground_truth in data:\n        prediction = model(input)\n        loss = loss_fn(prediction, ground_truth)\n        metrics = { \"loss\": loss }\n        swanlab.log(metrics)\n```\n\n:::\n\n如果您有很多指标，则可以在指标名称中使用前缀（如 `train/...` 和 `val/...`）。在 UI 中，SwanLab将自动对它们进行分组，来隔离不同门类的图表数据：\n\n```python\nmetrics = {\n    \"train/loss\": 0.5,\n    \"train/accuracy\": 0.8,\n    \"val/loss\": 0.6,\n    \"val/accuracy\": 0.7,\n}\nswanlab.log(metrics)\n```\n\n有关`swanlab.log`的更多信息，请参阅[记录指标](../experiment_track/log-experiment-metric)章节。",
    "322": "一级标题：将SwanLab集成到你的库\n二级标题：高级集成\n内容：\n您还可以在以下集成中查看高级 SwanLab 集成的形态：\n\n- [HuggingFace Transformers](../integration/integration-huggingface-transformers.md)\n- [PyTorch Lightning](../integration/integration-pytorch-lightning.md)",
    "323": "一级标题：AREAL\n二级标题：AReaL 介绍\n内容：\n[AReaL](https://github.com/inclusionAI/AReaL)（Ant Reasoning RL）是由蚂蚁研究院强化学习实验室（RL Lab） 开发的一套开源 、完全异步的强化学习训练系统， 适用于大型推理模型。该系统基于开源项目 [RealHF](https://github.com/openpsi-project/ReaLHF) 致力于开源，提供训练细节、数据以及复现结果所需的基础设施，并提供模型本身。\n\n<img src=\"./areal/logo.png\" width=\"200\">",
    "324": "一级标题：AREAL\n二级标题：AReaL 与 SwanLab 集成\n内容：\nAReaL项目已集成SwanLab，指引可见此文档：[Areal - monitoring-the-training-process](https://inclusionai.github.io/AReaL/tutorial/quickstart.html#monitoring-the-training-process)",
    "325": "一级标题：Argparse\n二级标题：argparse模块介绍\n内容：\n`argparse` 是 Python 标准库中的一个模块，用于解析命令行参数和选项。通过 argparse，开发者可以轻松地编写用户友好的命令行接口，定义命令行参数的名称、类型、默认值、帮助信息等。",
    "326": "一级标题：Argparse\n二级标题：与swanlab的集成\n内容：\n`argparse` 与swanlab的集成非常简单，直接将创建好的argparse对象传递给swanlab.config，即可记录为超参数：\n\n```python\nimport argparse\nimport swanlab\n\n# 初始化Argparse\nparser = argparse.ArgumentParser()\nparser.add_argument('--epochs', default=20)\nparser.add_argument('--lr', default=0.001)\nargs = parser.parse_args()\n\nswanlab.init(config=args)\n```\n运行案例：\n```bash\npython main.py --epochs 100 --lr 1e-4\n```\n\n![alt text](/assets/ig-argparse.png)",
    "327": "一级标题：Ascend NPU & MindSpore\n二级标题：简介\n内容：\n本案例使用实现的IMDB数据集情感分类任务。并使用SwanLab跟踪模型训练进展。",
    "328": "一级标题：Ascend NPU & MindSpore\n二级标题：任务介绍\n内容：\nIMDB情感分类任务是一种自然语言处理任务，旨在分析IMDB（Internet Movie Database）电影评论中的文本内容，以判断评论的情感倾向，通常分为正面（Positive）和负面（Negative）两类。该任务广泛用于研究情感分析技术，尤其是在监督学习和深度学习领域。\n\n数据集中通常包含预处理好的评论文本及其对应的情感标签，每条评论均标注为正面或负面。如下图：\n\n![data_image](/assets/guide_cloud/integration/ascend/data_image.png)\n\nLSTM（Long Short-Term Memory）是一种改进的循环神经网络，专为处理和预测序列数据中的长距离依赖而设计。与传统RNN相比，LSTM通过引入**记忆单元**和**门机制**，能够有效缓解梯度消失和梯度爆炸问题，使其在长序列数据的建模中表现优异。使用LSTM能轻松完成IMDB的语言情感分类任务。关于LSTM的具体原理建议参考[大神博客](https://blog.csdn.net/zhaojc1995/article/details/80572098)\n\n![lstm](/assets/guide_cloud/integration/ascend/lstm.png)\n\n本代码参考[MindSpore官方文档](https://www.mindspore.cn/tutorials/zh-CN/r2.4.1/nlp/sentiment_analysis.html#%E6%95%B0%E6%8D%AE%E9%9B%86%E9%A2%84%E5%A4%84%E7%90%86)，进行整理并简化了部分实现.",
    "329": "一级标题：Ascend NPU & MindSpore\n二级标题：环境安装\n内容：\n### 克隆项目\n\n附上[github项目链接](https://github.com/ShaohonChen/mindspore_imdb_train.git)和下载命令\n\n```bash\ngit clone https://github.com/ShaohonChen/mindspore_imdb_train.git\n```\n\n如果访问不了github可在本博客后文找到[代码章节](#代码章节)\n\n推荐还是用github ;)\n\n### CPU环境安装\n\n可以在CPU环境下安装MindSpore，虽然看起来没有Pytorch那么好用，但实际上文档还是写的很细的，真的很细，看得出华为工程师的严谨orz。配合sheng腾卡使用的话是非常有潜力的框架（MAC死活打不出sheng字）。\n\n官方安装文档[link](https://www.mindspore.cn/install/)\n\n也可以直接使用如下命令安装：\n\n```bash\npip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/2.4.1/MindSpore/unified/x86_64/mindspore-2.4.1-cp311-cp311-linux_x86_64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.tuna.tsinghua.edu.cn/simple\n```\n\n验证安装成功命令：\n\n```bash\npython -c \"import mindspore;mindspore.set_context(device_target='CPU');mindspore.run_check()\"\n```\n\n如果输出如下信息说明MindSpore安装成功了：\n\n```bash\nMindSpore version: 2.4.1\nThe result of multiplication calculation is correct, MindSpore has been installed on platform [CPU] successfully!\n```\n\n### 华为Ascend NPU显卡环境安装\n\n由于华为Ascend环境安装较为复杂，建议参考[MindSpore安装教程和踩坑记录](///)教程完成MindSpore环境安装。下面简述MindSpore安装过程\n\n>本博客写的时间是2024年12月6日，安装的版本是**MindSpore2.4.1**，因为感觉MindSpore变动会比较大特意记录一下时间和版本。\n\n#### 驱动安装&验证\n\n首先得确定有NPU卡和NPU相关驱动，驱动是**8.0.RC3.beta1**，如果没安装可以参考[CANN官方安装教程](https://www.hiascend.com/document/detail/zh/canncommercial/80RC3/softwareinst/instg/instg_0000.html?Mode=PmIns&OS=Ubuntu&Software=cannToolKit)\n\n完成安装后检测方法是运行\n\n```bash\nnpu-smi info\n```\n\n可以看到如下信息的话就表示驱动已经安装完成了。\n\n![npu-smi](/assets/guide_cloud/integration/ascend/a_mask.png)\n\n#### 安装MindSpore\n\n个人比较推荐使用conda安装，这样环境比较好管理，自动安装的依赖项也比较多\n\n首先需要安装前置依赖的包：\n\n```bash\npip install sympy\npip install \"numpy>=1.20.0,<2.0.0\"\npip install /usr/local/Ascend/ascend-toolkit/latest/lib64/te-*-py3-none-any.whl\npip install /usr/local/Ascend/ascend-toolkit/latest/lib64/hccl-*-py3-none-any.whl\n```\n\n如果本地下载比较慢可以使用带国内源版本的命令\n\n```bash\npip install sympy -i https://mirrors.cernet.edu.cn/pypi/web/simple\npip install \"numpy>=1.20.0,<2.0.0\" -i https://mirrors.cernet.edu.cn/pypi/web/simple\npip install /usr/local/Ascend/ascend-toolkit/latest/lib64/te-*-py3-none-any.whl -i https://mirrors.cernet.edu.cn/pypi/web/simple\npip install /usr/local/Ascend/ascend-toolkit/latest/lib64/hccl-*-py3-none-any.whl  -i https://mirrors.cernet.edu.cn/pypi/web/simple\n```\n\nconda安装MindSpore方法如下：\n\n```bash\nconda install mindspore=2.4.1 -c mindspore -c conda-forge\n```\n\n因为某些众所周知的原因，有时候conda源会失效，反应出来就是conda安装mindspore时会进度一直为0%，如下图：\n\n![condainstallfailed](/assets/guide_cloud/integration/ascend/b.png)\n\n可以使用如下方法指定国内源：\n\n```bash\nconda install mindspore=2.4.1 -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/MindSpore/ -c conda-forge\n```\n\npip安装MindSpore命令如下：\n\n```bash\npip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/2.4.1/MindSpore/unified/aarch64/mindspore-2.4.1-cp311-cp311-linux_aarch64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.tuna.tsinghua.edu.cn/simple\n```\n\n安装完成后可以使用如下命令进行测试\n\n```bash\npython -c \"import mindspore;mindspore.set_context(device_target='Ascend');mindspore.run_check()\"\n```\n\n如果这步出现报错可以参考本文后面[环境安装疑难杂症](#环境安装疑难杂症)章节\n\n出现版本号信息和计算验证便意味着安装成功\n\n```bash\nMindSpore version:  2.4.1\nThe result of multiplication calculation is correct, MindSpore has been installed on platform [Ascend] successfully!\n```\n\n也附上官方安装教程链接[mindspore官方安装教程](https://www.mindspore.cn/install)，注意本教程使用的是[Mindspore 2.4.1](https://www.mindspore.cn/versions#2.4.1)，建议环境与本教程保持一致。\n\n此外本教程使用[SwanLab](https://swanlab.cn)进行训练过程跟踪，SwanLab支持对Ascend系列NPU进行硬件识别和跟踪。\n\n### 记得安装SwanLab ;)\n\n安装方法：\n\n```bash\npip install swanlab\n```",
    "330": "一级标题：Ascend NPU & MindSpore\n二级标题：数据集&词编码文件准备\n内容：\n### 数据集准备\n\nLinux使用如下命令完成下载+解压\n\n```bash\nwget -P ./data/ https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\ntar -xzvf data/aclImdb_v1.tar.gz -C data/\n```\n\n如果下载太慢可以使用[华为云提供的国内链接](https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/aclImdb_v1.tar.gz)下载。并且在`./data/`目录下解压。\n\n> 如果解压不了tar.gz推荐安装[7zip解压器](https://www.7-zip.org/)，开源且通用的解压器\n\n### 词编码器准备\n\n使用如下命令下载+解压词编码器文件\n\n```bash\nwget -P ./embedding/ https://nlp.stanford.edu/data/glove.6B.zip\nunzip embedding/glove.6B.zip -d embedding/\n```\n\n如果下载太慢可以使用[华为云提供的国内链接](https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/glove.6B.zip)下载。并且在`./embedding/`目录下解压。",
    "331": "一级标题：Ascend NPU & MindSpore\n二级标题：开始训练\n内容：\n使用如下命令开始训练\n\n```\npython train.py\n```\n\n可是这\n\n> 如果提示登录swanlab，可以参考[如何登录SwanLab](https://docs.swanlab.cn/guide_cloud/general/quick-start.html#_2-%E7%99%BB%E5%BD%95%E8%B4%A6%E5%8F%B7)，这样将能够使用**云上看版**随时查看训练过程与结果。\n\n完成设置便可以在云上实时看到训练进展，我的实验记录可参考[完整实验记录](https://swanlab.cn/@ShaohonChen/Ascend_IMDB_CLS/charts)\n\n![log_img](/assets/guide_cloud/integration/ascend/log_img.png)\n\n并且附上其他脚本与在线实验记录：\n\n| 内容  | 训练命令  | 实验log  |\n|--------|--------|--------|\n| 基线 | `python train.py configs/baseline.json` | [log](https://swanlab.cn/@ShaohonChen/Ascend_IMDB_CLS/runs/qhl47nxl23tc4oycr6pmg/chart) |\n| CPU运行 | `python train.py configs/baseline.json CPU` | [log](https://swanlab.cn/@ShaohonChen/Ascend_IMDB_CLS/runs/s60wuicmwaitxe2v401ry/chart) |\n| 双层LSTM | `python train.py configs/two_layer.json` | [log](https://swanlab.cn/@ShaohonChen/Ascend_IMDB_CLS/runs/ydrgxvnqhjfrimzdj3oh4/chart) |\n| 小batch数 | `python train.py configs/small_batch.json` | [log](https://swanlab.cn/@ShaohonChen/Ascend_IMDB_CLS/runs/uovjgenfzcnxrl9gup900/chart) |\n| 隐藏层加大 | `python train.py configs/large_hs.json` | [log](https://swanlab.cn/@ShaohonChen/Ascend_IMDB_CLS/runs/eki6pa1him482w4jcc7gn/chart) |\n| 学习率加大 | `python train.py configs/large_hs.json` | [log](https://swanlab.cn/@ShaohonChen/Ascend_IMDB_CLS/runs/if3o10o6nf3am87f4ou62/chart) |\n\n相关超参数和最终结果可在[图标视图查看](https://swanlab.cn/@ShaohonChen/Ascend_IMDB_CLS/overview)\n\n![log_table](/assets/guide_cloud/integration/ascend/log_table.png)\n\n> PS: 观察了下日志，发现还是训练量不足，应该增大些训练量（40-50epoch比较合适）",
    "332": "一级标题：Ascend NPU & MindSpore\n二级标题：代码章节\n内容：\n如果访问不了github也提供一段测试代码，不过就是没法使用其他超参数了T_T\n\n```python\n# 读取训练参数+初始化日志记录\nimport os\nimport sys\nimport json\nimport mindspore as ms\nimport swanlab\n\n# ms.set_context(device_target=\"CPU\") # 使用CPU\nms.set_context(device_target=\"Ascend\")  # 使用NPU\n\nargs={  # 超参数\n    \"hidden_size\": 256,\n    \"output_size\": 1,\n    \"num_layers\": 2,\n    \"lr\": 0.001,\n    \"num_epochs\": 10,\n    \"batch_size\": 64,\n    \"report_interval\": 10\n}\n\nexp_name = \"baseline\"\nswanlab.init(project=\"Ascend_IMDB_CLS\", experiment_name=exp_name, config=args)\n\n\n# 构造数据集\nimport mindspore.dataset as ds\n\n\nclass IMDBData:\n    label_map = {\"pos\": 1, \"neg\": 0}\n\n    def __init__(self, path, mode=\"train\"):\n        self.docs, self.labels = [], []\n        for label in self.label_map.keys():\n            doc_dir = os.path.join(path, mode, label)\n            doc_list = os.listdir(doc_dir)\n            for fname in doc_list:\n                with open(os.path.join(doc_dir, fname)) as f:\n                    doc = f.read()\n                    doc = doc.lower().split()\n                    self.docs.append(doc)\n                    self.labels.append([self.label_map[label]])\n\n    def __getitem__(self, idx):\n        return self.docs[idx], self.labels[idx]\n\n    def __len__(self):\n        return len(self.docs)\n\n\nimdb_path = \"data/aclImdb\"\nimdb_train = ds.GeneratorDataset(\n    IMDBData(imdb_path, \"train\"), column_names=[\"text\", \"label\"], shuffle=True\n)\nimdb_test = ds.GeneratorDataset(\n    IMDBData(imdb_path, \"test\"), column_names=[\"text\", \"label\"], shuffle=False\n)\n\n# 构造embedding词表\nimport numpy as np\n\n\ndef load_glove(glove_path):\n    embeddings = []\n    tokens = []\n    with open(os.path.join(glove_path, \"glove.6B.100d.txt\"), encoding=\"utf-8\") as gf:\n        for glove in gf:\n            word, embedding = glove.split(maxsplit=1)\n            tokens.append(word)\n            embeddings.append(np.fromstring(embedding, dtype=np.float32, sep=\" \"))\n    # 添加 <unk>, <pad> 两个特殊占位符对应的embedding\n    embeddings.append(np.random.rand(100))\n    embeddings.append(np.zeros((100,), np.float32))\n\n    vocab = ds.text.Vocab.from_list(\n        tokens, special_tokens=[\"<unk>\", \"<pad>\"], special_first=False\n    )\n    embeddings = np.array(embeddings).astype(np.float32)\n    return vocab, embeddings\n\n\nvocab, embeddings = load_glove(\"./embedding\")\nprint(f\"VOCAB SIZE: {len(vocab.vocab())}\")\n\n# 数据预处理\nimport mindspore as ms\n\nlookup_op = ds.text.Lookup(vocab, unknown_token=\"<unk>\")\npad_op = ds.transforms.PadEnd([500], pad_value=vocab.tokens_to_ids(\"<pad>\"))\ntype_cast_op = ds.transforms.TypeCast(ms.float32)\n\nimdb_train = imdb_train.map(operations=[lookup_op, pad_op], input_columns=[\"text\"])\nimdb_train = imdb_train.map(operations=[type_cast_op], input_columns=[\"label\"])\n\nimdb_test = imdb_test.map(operations=[lookup_op, pad_op], input_columns=[\"text\"])\nimdb_test = imdb_test.map(operations=[type_cast_op], input_columns=[\"label\"])\n\nimdb_train, imdb_valid = imdb_train.split([0.7, 0.3])\n\nprint(f\"TRAIN SET SIZE: {len(imdb_train)}\")\nprint(f\"VALID SET SIZE: {len(imdb_valid)}\")\nprint(f\"TEST SET SIZE: {len(imdb_test)}\")\n\nimdb_train = imdb_train.batch(args[\"batch_size\"], drop_remainder=True)\nimdb_valid = imdb_valid.batch(args[\"batch_size\"], drop_remainder=True)\n\n\n# LSTM分类器实现\nimport math\nimport mindspore as ms\nimport mindspore.nn as nn\nimport mindspore.ops as ops\nfrom mindspore.common.initializer import Uniform, HeUniform\n\n\nclass LSTM_CLS(nn.Cell):\n    def __init__(self, embeddings, hidden_dim, output_dim, n_layers, pad_idx):\n        super().__init__()\n        vocab_size, embedding_dim = embeddings.shape\n        self.embedding = nn.Embedding(\n            vocab_size,\n            embedding_dim,\n            embedding_table=ms.Tensor(embeddings),\n            padding_idx=pad_idx,\n        )\n        self.rnn = nn.LSTM(\n            embedding_dim, hidden_dim, num_layers=n_layers, batch_first=True\n        )\n        weight_init = HeUniform(math.sqrt(5))\n        bias_init = Uniform(1 / math.sqrt(hidden_dim * 2))\n        self.fc = nn.Dense(\n            hidden_dim, output_dim, weight_init=weight_init, bias_init=bias_init\n        )\n\n    def construct(self, inputs):\n        embedded = self.embedding(inputs)\n        _, (hidden, _) = self.rnn(embedded)\n        hidden = hidden[-1, :, :]\n        output = self.fc(hidden)\n        return output\n\n\nmodel = LSTM_CLS(\n    embeddings,\n    args[\"hidden_size\"],\n    args[\"output_size\"],\n    args[\"num_layers\"],\n    vocab.tokens_to_ids(\"<pad>\"),\n)\n\n# 损失函数与优化器\nloss_fn = nn.BCEWithLogitsLoss(reduction=\"mean\")\noptimizer = nn.Adam(model.trainable_params(), learning_rate=args[\"lr\"])\n\n# 训练过程实现\nfrom tqdm import tqdm\nimport time\n\n\ndef forward_fn(data, label):\n    logits = model(data)\n    loss = loss_fn(logits, label)\n    return loss\n\n\ngrad_fn = ms.value_and_grad(forward_fn, None, optimizer.parameters)\n\n\ndef train_step(data, label):\n    loss, grads = grad_fn(data, label)\n    optimizer(grads)\n    return loss\n\n\ndef train_one_epoch(model, train_dataset, epoch=0):\n    model.set_train()\n    total = train_dataset.get_dataset_size()\n    step_total = 0\n    last_time = time",
    "333": "一级标题：DiffSynth Studio\n二级标题：介绍\n内容：\n[DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio) 是 [ModelScope](https://modelscope.cn/) 推出的一个开源的扩散模型引擎，专注于图像与视频的风格迁移与生成任务。它通过优化架构设计（如文本编码器、UNet、VAE 等组件），在保持与开源社区模型兼容性的同时，显著提升计算性能，为用户提供高效、灵活的创作工具。\n\nDiffSynth Studio 支持多种扩散模型，包括 Wan-Video、StepVideo、HunyuanVideo、CogVideoX、FLUX、ExVideo、Kolors、Stable Diffusion 3 等。\n\n![](./diffsynth/logo.jpg)\n\n你可以使用DiffSynth Studio快速进行Diffusion模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n[[toc]]",
    "334": "一级标题：DiffSynth Studio\n二级标题：准备工作\n内容：\n**1. 克隆仓库并安装环境**\n\n```bash\ngit clone https://github.com/modelscope/DiffSynth-Studio.git\ncd DiffSynth-Studio\npip install -e .\npip install swanlab\npip install lightning lightning_fabric\n```\n\n**2. 准备数据集**\n\nDiffSynth Studio 的数据集需要按下面的格式进行构建，比如将图像数据存放在`data/dog`目录下：\n\n```bash\ndata/dog/\n└── train\n    ├── 00.jpg\n    ├── 01.jpg\n    ├── 02.jpg\n    ├── 03.jpg\n    ├── 04.jpg\n    └── metadata.csv\n```\n\n`metadata.csv` 文件需要按下面的格式进行构建：\n\n```csv\nfile_name,text\n00.jpg,一只小狗\n01.jpg,一只小狗\n02.jpg,一只小狗\n03.jpg,一只小狗\n04.jpg,一只小狗\n```\n\n> 这里有一份整理好格式的火影忍者数据集，[百度云](https://pan.baidu.com/s/1kPvkTV6gy2xWFRpyXRX0Yw?pwd=2p6h)，供参考与测试\n\n**3. 准备模型**\n\n这里以Kolors模型为例，下载模型权重和VAE权重：\n\n```bash\nmodelscope download --model=Kwai-Kolors/Kolors --local_dir models/kolors/Kolors\nmodelscope download --model=AI-ModelScope/sdxl-vae-fp16-fix --local_dir models/kolors/sdxl-vae-fp16-fix\n```",
    "335": "一级标题：DiffSynth Studio\n二级标题：设置SwanLab参数\n内容：\n在运行训练脚本时，添加`--use_swanlab`，即可将训练过程记录到SwanLab平台。\n\n如果你需要离线记录，可以添加`--swanlab_mode \"local\"`。\n\n```bash\nCUDA_VISIBLE_DEVICES=\"0\" python examples/train/kolors/train_kolors_lora.py \\\n...\n--use_swanlab \\  # [!code ++]\n--swanlab_mode \"cloud\"  # [!code ++]\n```",
    "336": "一级标题：DiffSynth Studio\n二级标题：开启训练\n内容：\n使用下面的命令即可开启训练，并使用SwanLab记录超参数、训练日志、loss曲线等信息：\n\n```bash {11,12}\nCUDA_VISIBLE_DEVICES=\"0\" python examples/train/kolors/train_kolors_lora.py \\\n--pretrained_unet_path models/kolors/Kolors/unet/diffusion_pytorch_model.safetensors \\\n--pretrained_text_encoder_path models/kolors/Kolors/text_encoder \\\n--pretrained_fp16_vae_path models/kolors/sdxl-vae-fp16-fix/diffusion_pytorch_model.safetensors \\\n--dataset_path data/dog \\\n--output_path ./models \\\n--max_epochs 10 \\\n--center_crop \\\n--use_gradient_checkpointing \\\n--precision \"16-mixed\" \\\n--use_swanlab \\\n--swanlab_mode \"cloud\"\n```\n\n![](./diffsynth/ui-1.png)\n\n![](./diffsynth/ui-2.png)",
    "337": "一级标题：DiffSynth Studio\n二级标题：补充\n内容：\n如果你想要自定义SwanLab的项目名、实验名等参数，可以：\n\n**1. 文生图任务**\n\n在`DiffSynth-Studio/diffsynth/trainers/text_to_image.py`文件中，找到`swanlab_logger`变量的位置，修改`project`和`name`参数：\n\n```python {6-7}\nif args.use_swanlab:\n    from swanlab.integration.pytorch_lightning import SwanLabLogger\n    swanlab_config = {\"UPPERFRAMEWORK\": \"DiffSynth-Studio\"}\n    swanlab_config.update(vars(args))\n    swanlab_logger = SwanLabLogger(\n        project=\"diffsynth_studio\", \n        name=\"diffsynth_studio\",\n        config=swanlab_config,\n        mode=args.swanlab_mode,\n        logdir=args.output_path,\n    )\n    logger = [swanlab_logger]\n```\n\n**2. Wan-Video文生视频任务**\n\n在`DiffSynth-Studio/examples/wanvideo/train_wan_t2v.py`文件中，找到`swanlab_logger`变量的位置，修改`project`和`name`参数：\n\n```python {6-7}\nif args.use_swanlab:\n    from swanlab.integration.pytorch_lightning import SwanLabLogger\n    swanlab_config = {\"UPPERFRAMEWORK\": \"DiffSynth-Studio\"}\n    swanlab_config.update(vars(args))\n    swanlab_logger = SwanLabLogger(\n        project=\"wan\", \n        name=\"wan\",\n        config=swanlab_config,\n        mode=args.swanlab_mode,\n        logdir=args.output_path,\n    )\n    logger = [swanlab_logger]\n```",
    "338": "一级标题：EasyR1\n二级标题：准备工作\n内容：\n在执行下面的命令之前，请先确保你的环境中已经安装了Python>=3.9，CUDA和PyTorch。\n\n```bash\ngit clone https://github.com/hiyouga/EasyR1.git\ncd EasyR1\npip install -e .\npip install git+https://github.com/hiyouga/MathRuler.git\npip install swanlab\n```\n\n:::warning 注意\n\nEasyR1的依赖中有flash-attn，直接安装非常慢，请在[flash-attention预编译包](https://github.com/Dao-AILab/flash-attention/releases)中找到对应Python与CUDA版本的包，下载并安装。\n\n:::",
    "339": "一级标题：EasyR1\n二级标题：训练Qwen2.5-7b数学模型\n内容：\n在`EasyR1`目录下，执行下面的命令，即可使用GRPO训练Qwen2.5-7b数学模型，并使用SwanLab进行跟踪与可视化：\n\n```bash\nbash examples/run_qwen2_5_7b_math_swanlab.sh\n```\n\n![](./easyr1/qwen_math.png)\n\n当然，这里我们可以剖析一下，由于EasyR1是原始 veRL 项目的一个干净分叉，所以继承了[veRL与SwanLab的集成](/guide_cloud/integration/integration-verl.md)。所以这里我们来看`run_qwen2_5_7b_math_swanlab.sh`文件：\n\n```sh {10}\nset -x\n\nexport VLLM_ATTENTION_BACKEND=XFORMERS\n\nMODEL_PATH=Qwen/Qwen2.5-7B-Instruct  # replace it with your local file path\n\npython3 -m verl.trainer.main \\\n    config=examples/grpo_example.yaml \\\n    worker.actor.model.model_path=${MODEL_PATH} \\\n    trainer.logger=['console','swanlab'] \\\n    trainer.n_gpus_per_node=4\n```\n\n只需要在`python3 -m verl.trainer.main`参数中加入一行`trainer.logger=['console','swanlab']`，即可使用SwanLab进行跟踪与可视化。",
    "340": "一级标题：EasyR1\n二级标题：训练Qwen2.5-VL-7b多模态模型\n内容：\n在`EasyR1`目录下，执行下面的命令，即可使用GRPO训练Qwen2.5-VL-7b多模态模型，并使用SwanLab进行跟踪与可视化：\n\n```bash\nbash examples/run_qwen2_5_vl_7b_geo_swanlab.sh\n```",
    "341": "一级标题：EasyR1\n二级标题：每轮评估时记录生成文本\n内容：\n如果你希望在每轮评估（val）时将生成的文本记录到SwanLab中，只需在命令行钟增加一行`val_generations_to_log=1`即可：\n\n```bash {6}\npython3 -m verl.trainer.main \\\n    config=examples/grpo_example.yaml \\\n    worker.actor.model.model_path=${MODEL_PATH} \\\n    trainer.logger=['console','swanlab'] \\\n    trainer.n_gpus_per_node=4 \\\n    val_generations_to_log=1\n```",
    "342": "一级标题：EasyR1\n二级标题：写在最后\n内容：\nEasyR1 是 [LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory) 作者 [hiyouga](https://github.com/hiyouga) 的全新开源项目，一个适用于多模态大模型的强化学习框架。感谢 [hiyouga](https://github.com/hiyouga) 为全球开源生态的贡献，SwanLab也将继续与AI开发者同行。",
    "343": "一级标题：EvalScope\n二级标题：简介\n内容：\n[EvalScope](https://github.com/modelscope/evalscope) 是 [ModelScope](https://modelscope.cn/) 的官方模型评估和基准测试框架，专为满足各种评估需求而设计。它支持各种模型类型，包括大型语言模型、多模态模型、Embedding模型、Reranker模型和 CLIP 模型。\n\n![evalscope-logo](./evalscope/logo.png)\n\n该框架支持多种评估场景，如端到端的RAG评估、竞技场模式和推理性能测试。它内置了MMLU、CMMLU、C-Eval和GSM8K等基准和指标。与 [ms-swift](https://github.com/modelscope/ms-swift) 训练框架无缝集成，EvalScope实现了单击评估，为模型训练和评估提供全面支持 🚀。\n\n现在，你可以使用 EvalScope 评估LLM性能，同时使用SwanLab方便地跟踪、对比、可视化。\n\n[Demo](https://swanlab.cn/@ShaohonChen/perf_benchmark/overview)",
    "344": "一级标题：EvalScope\n二级标题：准备工作\n内容：\n安装下面的环境：\n\n```bash\npip install evalscope\npip install swanlab\n```\n\n如果你需要扩展evalscope的更多功能，可以按需安装：\n\n```bash\npip install -e '.[opencompass]'   # Install OpenCompass backend\npip install -e '.[vlmeval]'       # Install VLMEvalKit backend\npip install -e '.[rag]'           # Install RAGEval backend\npip install -e '.[perf]'          # Install Perf dependencies\npip install -e '.[app]'           # Install visualization dependencies\npip install -e '.[all]'           # Install all backends (Native, OpenCompass, VLMEvalKit, RAGEval)\n```",
    "345": "一级标题：EvalScope\n二级标题：Qwen模型推理性能压测\n内容：\n如果你希望评估`Qwen2.5-0.5B-Instruct`在[openqa格式默认数据集](https://www.modelscope.cn/datasets/AI-ModelScope/HC3-Chinese)上的表现，同时使用`SwanLab`观测性能，可以运行下面的命令：\n\n```bash {5,6}\nexport CUDA_VISIBLE_DEVICES=0\nevalscope perf \\\n --model Qwen/Qwen2.5-0.5B-Instruct \\\n --dataset openqa \\\n --number 20 \\\n --parallel 2 \\\n --limit 5 \\\n --swanlab-api-key '你的API Key' \\\n --name 'qwen2.5-openqa' \\\n --temperature 0.9 \\\n --api local\n```\n\n其中`swanlab-api-key`是你的SwanLab API Key，`name`是实验名。  \n如果你希望设置自定义项目名，可以去往`EvalScope`源码的 `evalscope/perf/benchmark.py` 的 `statistic_benchmark_metric_worker`函数，找到swanlab部分，修改`project`参数。\n\n**可视化效果案例：**\n\n![](./evalscope/show.png)",
    "346": "一级标题：EvalScope\n二级标题：上传到私有化部署版\n内容：\n如果你希望将评估结果上传到私有化部署版，可以先在命令行登录到私有化部署版。比如你的部署地址是`http://localhost:8000`，可以运行：\n\n```bash\nswanlab login --host http://localhost:8000\n```\n\n完成登录后，再运行`evalscope`的命令，就可以将评估结果上传到私有化部署版了。\n```",
    "347": "一级标题：fastai\n二级标题：fastai介绍\n内容：\n[fastai](https://github.com/fastai/fastai) 是一个基于 PyTorch 的高层次深度学习库，旨在使现代深度学习的应用更加容易和高效。它提供了一个简单的 API，使用户能够快速构建、训练和评估复杂的模型，而无需深入了解底层细节。\n\n你可以使用fastai快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。",
    "348": "一级标题：fastai\n二级标题：引入SwanLabCallback\n内容：\n```python\nfrom swanlab.integration.fastai import SwanLabCallback\n```\n**SwanLabCallback**是适配于fastai的日志记录类。  \n\n**SwanLabCallback**可以定义的参数有：\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。\n- 你也可以在外部通过`swanlab.init`创建项目，集成会将实验记录到你在外部创建的项目中。",
    "349": "一级标题：fastai\n二级标题：传入训练器\n内容：\n```python\nfrom fastai.vision.all import *\nfrom swanlab.integration.fastai import SwanLabCallback\n\n...\n\n# 定义模型\nlearn = vision_learner(...)\n\n# 添加SwanLabCallback\nlearn.fit_one_cycle(5, cbs=SwanLabCallback)\n```",
    "350": "一级标题：fastai\n二级标题：案例-宠物分类\n内容：\n```python (2,16)\nfrom fastai.vision.all import *\nfrom swanlab.integration.fastai import SwanLabCallback\n\n# 加载数据\npath = untar_data(URLs.PETS)\ndls = ImageDataLoaders.from_name_re(\n    path, get_image_files(path / \"images\"), pat=r\"([^/]+)_\\d+.jpg$\", item_tfms=Resize(224)\n)\n\n# 定义模型\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\n\n# 添加SwanLabCallback\nlearn.fit_one_cycle(\n    5,\n    cbs=SwanLabCallback(\n        project=\"fastai-swanlab-integration-test\",\n        experiment_name=\"super-test\",\n        description=\"Test fastai integration with swanlab\",\n        logdir=\"./logs\",\n    ),\n)\n```",
    "351": "一级标题：HuggingFace Accelerate\n二级标题：介绍\n内容：\nHuggingFace 的 [accelerate](https://huggingface.co/docs/accelerate/index) 是一个简化和优化深度学习模型训练与推理的开源库。\n\n> 🚀在几乎任何设备和分布式配置上启动、训练和使用PyTorch模型的简单方法，支持自动混合精度(包括fp8)，以及易于配置的FSDP和DeepSpeed\n\n它提供了高效的分布式训练和推理的工具，使开发者能够更轻松地在不同硬件设备上部署和加速模型。通过简单的几行代码改动，就可以轻松将现有的训练代码集成进 `torch_xla` 和 `torch.distributed` 这类平台，而无需为复杂的分布式计算架构烦恼，从而提升工作效率和模型性能。\n\n![hf-accelerate-image](./huggingface_accelerate/logo.png)\n\n你可以使用`accelerate`快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n> `accelerate`>=1.8.0 的版本，已官方集成了swanlab  \n> 如果你的版本低于1.8.0，请使用 **SwanLabTracker集成**",
    "352": "一级标题：HuggingFace Accelerate\n二级标题：两行代码完成集成\n内容：\n```python {4,9}\nfrom accelerate import Accelerator\n\n# 告诉 Accelerator 对象使用 swanlab 进行日志记录\naccelerator = Accelerator(log_with=\"swanlab\")\n\n# 初始化您的 swanlab 实验，传递 swanlab 参数和任何配置信息\naccelerator.init_trackers(\n    ...\n    init_kwargs={\"swanlab\": {\"experiment_name\": \"hello_world\"}}\n    )\n```\n\n::: warning 补充信息\n1. swanlab项目名由`accelerator.init_trackers`的`project_name`参数指定\n2. 向`init_kwargs`传递的`swanlab`字典，key-value和`swanlab.init`的参数完全一致（除了project）。\n:::\n\n最小能跑代码：\n\n```python {4,10}\nfrom accelerate import Accelerator\n\n# Tell the Accelerator object to log with swanlab\naccelerator = Accelerator(log_with=\"swanlab\")\n\n# Initialise your swanlab experiment, passing swanlab parameters and any config information\naccelerator.init_trackers(\n    project_name=\"accelerator\",\n    config={\"dropout\": 0.1, \"learning_rate\": 1e-2},\n    init_kwargs={\"swanlab\": {\"experiment_name\": \"hello_world\"}}\n    )\n\nfor i in range(100):\n    # Log to swanlab by calling `accelerator.log`, `step` is optional\n    accelerator.log({\"train_loss\": 1.12, \"valid_loss\": 0.8}, step=i+1)\n\n# Make sure that the swanlab tracker finishes correctly\naccelerator.end_training()\n```",
    "353": "一级标题：HuggingFace Accelerate\n二级标题：SwanLabTracker集成\n内容：\n如果你使用的是`accelerate<1.8.0`的版本，则可以使用SwanLabCallback集成。\n\n### 2.1 引入\n\n```bash\nfrom swanlab.integration.accelerate import SwanLabTracke\n```\n\n### 2.2 在初始化accelerate时指定日志记录器\n\n```python (1,7,9,12)\nfrom swanlab.integration.accelerate import SwanLabTracker\nfrom accelerate import Accelerator\n\n...\n\n# 创建SwanLab日志记录器\ntracker = SwanLabTracker(\"YOUR_SMART_PROJECT_NAME\")\n# 传入Accelerator\naccelerator = Accelerator(log_with=tracker)\n\n# 初始化所有日志记录器\naccelerator.init_trackers(\"YOUR_SMART_PROJECT_NAME\", config=config)\n\n# training code\n...\n```\n\n- 虽然上面的代码两次设定了项目名，实际上只有第一个项目名设置才起了作用\n\n- 显式调用`init_trackers`来初始化所有日志记录是`accelerate`的机制，第二次设置的项目名是当有多个日志记录器时,初始化内置的日志记录器的情况下才会用到。\n\n### 2.3 完整案例代码\n\n下面是一个使用accelerate进行cifar10分类，并使用SwanLab进行日志跟踪的案例：\n\n```python (10,45,46,47,71,90)\nimport torch\nimport torch.utils\nimport torch.utils.data\nimport torch.utils.data.dataloader\nimport torchvision\nfrom torchvision.models import resnet18, ResNet18_Weights\nfrom accelerate import Accelerator\nfrom accelerate.logging import get_logger\nimport swanlab\nfrom swanlab.integration.accelerate import SwanLabTracker\n\n\ndef main():\n    # hyperparameters\n    config = {\n        \"num_epoch\": 5,\n        \"batch_num\": 16,\n        \"learning_rate\": 1e-3,\n    }\n\n    # Download the raw CIFAR-10 data.\n    transform = torchvision.transforms.Compose(\n        [\n            torchvision.transforms.ToTensor(),\n            torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n        ]\n    )\n    train_data = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n    test_data = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n    BATCH_SIZE = config[\"batch_num\"]\n    my_training_dataloader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n    my_testing_dataloader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n\n    # Using resnet18 model, make simple changes to fit the data set\n    my_model = resnet18(weights=ResNet18_Weights.DEFAULT)\n    my_model.conv1 = torch.nn.Conv2d(my_model.conv1.in_channels, my_model.conv1.out_channels, 3, 1, 1)\n    my_model.maxpool = torch.nn.Identity()\n    my_model.fc = torch.nn.Linear(my_model.fc.in_features, 10)\n\n    # Criterion and optimizer\n    criterion = torch.nn.CrossEntropyLoss()\n    my_optimizer = torch.optim.SGD(my_model.parameters(), lr=config[\"learning_rate\"], momentum=0.9)\n\n    # Init accelerate with swanlab tracker\n    tracker = SwanLabTracker(\"CIFAR10_TRAING\")\n    accelerator = Accelerator(log_with=tracker)\n    accelerator.init_trackers(\"CIFAR10_TRAING\", config=config)\n    my_model, my_optimizer, my_training_dataloader, my_testing_dataloader = accelerator.prepare(\n        my_model, my_optimizer, my_training_dataloader, my_testing_dataloader\n    )\n    device = accelerator.device\n    my_model.to(device)\n\n    # Get logger\n    logger = get_logger(__name__)\n\n    # Begin training\n\n    for ep in range(config[\"num_epoch\"]):\n        # train model\n        if accelerator.is_local_main_process:\n            print(f\"begin epoch {ep} training...\")\n        step = 0\n        for stp, data in enumerate(my_training_dataloader):\n            my_optimizer.zero_grad()\n            inputs, targets = data\n            outputs = my_model(inputs)\n            loss = criterion(outputs, targets)\n            accelerator.backward(loss)\n            my_optimizer.step()\n            accelerator.log({\"training_loss\": loss, \"epoch_num\": ep})\n            if accelerator.is_local_main_process:\n                print(f\"train epoch {ep} [{stp}/{len(my_training_dataloader)}] | train loss {loss}\")\n\n        # eval model\n        if accelerator.is_local_main_process:\n            print(f\"begin epoch {ep} evaluating...\")\n        with torch.no_grad():\n            total_acc_num = 0\n            for stp, (inputs, targets) in enumerate(my_testing_dataloader):\n                predictions = my_model(inputs)\n                predictions = torch.argmax(predictions, dim=-1)\n                # Gather all predictions and targets\n                all_predictions, all_targets = accelerator.gather_for_metrics((predictions, targets))\n                acc_num = (all_predictions.long() == all_targets.long()).sum()\n                total_acc_num += acc_num\n                if accelerator.is_local_main_process:\n                    print(f\"eval epoch {ep} [{stp}/{len(my_testing_dataloader)}] | val acc {acc_num/len(all_targets)}\")\n\n            accelerator.log({\"eval acc\": total_acc_num / len(my_testing_dataloader.dataset)})\n\n    accelerator.wait_for_everyone()\n    accelerator.save_model(my_model, \"cifar_cls.pth\")\n\n    accelerator.end_training()\n\n\nif __name__ == \"__main__\":\n    main()\n```",
    "354": "一级标题：HuggingFace Transformers\n二级标题：1. 一行代码完成集成\n内容：\n只需要在你的训练代码中，找到`TrainingArguments`部分，添加`report_to=\"swanlab\"`参数，即可完成集成。\n\n```python\nfrom transformers import TrainingArguments, Trainer\n\nargs = TrainingArguments(\n    ...,\n    report_to=\"swanlab\" # [!code ++]\n\n)\n\ntrainer = Trainer(..., args=args)\n```\n\n如果你想要设定一下实验名，以区分每次训练，可以设置`run_name`参数：\n\n```python\nargs = TrainingArguments(\n    ...,\n    report_to=\"swanlab\",\n    run_name=\"great_try_1\", # [!code ++]\n)\n```",
    "355": "一级标题：HuggingFace Transformers\n二级标题：2. 自定义项目/工作空间\n内容：\n默认下，项目名会使用你运行代码的`目录名`，实验名等于`output_dir`。\n\n如果你想自定义项目名或工作空间，可以设置`SWANLAB_PROJECT`和`SWANLAB_WORKSPACE`环境变量：\n\n::: code-group\n\n```python\nimport os  # [!code ++]\n\nos.environ[\"SWANLAB_PROJECT\"]=\"qwen2-sft\"  # [!code ++]\nos.environ[\"SWANLAB_WORKSPACE\"]=\"EmotionMachine\"  # [!code ++]\n\n...\n\nfrom transformers import TrainingArguments, Trainer\n\nargs = TrainingArguments(\n    ...,\n    report_to=\"swanlab\",\n    run_name=\"great_try_1\",\n)\n\ntrainer = Trainer(..., args=args)\n```\n\n```bash [Command Line（Linux/MacOS）]\nexport SWANLAB_PROJECT=\"qwen2-sft\"\nexport SWANLAB_WORKSPACE=\"EmotionMachine\"\n```\n\n```bash [Command Line（Windows）]\nset SWANLAB_PROJECT=\"qwen2-sft\"\nset SWANLAB_WORKSPACE=\"EmotionMachine\"\n```\n\n:::",
    "356": "一级标题：HuggingFace Transformers\n二级标题：3. 案例代码：Bert文本分类\n内容：\n```python\nimport evaluate\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\n\ndataset = load_dataset(\"yelp_review_full\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n\nmetric = evaluate.load(\"accuracy\")\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\n\ntraining_args = TrainingArguments(\n    output_dir=\"test_trainer\",\n    num_train_epochs=3,\n    logging_steps=50,\n    report_to=\"swanlab\", # [!code ++]\n    run_name=\"bert_train\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n```",
    "357": "一级标题：HuggingFace Transformers\n二级标题：4. SwanLabCallback集成\n内容：\n如果你使用的是`Transformers<4.50.0`的版本，或者你希望更灵活地控制SwanLab的行为，则可以使用SwanLabCallback集成。\n\n### 4.1 引入SwanLabCallback\n\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\n```\n\n**SwanLabCallback**是适配于Transformers的日志记录类。\n\n**SwanLabCallback**可以定义的参数有：\n\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。\n- 你也可以在外部通过`swanlab.init`创建项目，集成会将实验记录到你在外部创建的项目中。\n\n### 4.2 传入Trainer\n\n```python (1,7,12)\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom transformers import Trainer, TrainingArguments\n\n...\n\n# 实例化SwanLabCallback\nswanlab_callback = SwanLabCallback(project=\"hf-visualization\")\n\ntrainer = Trainer(\n    ...\n    # 传入callbacks参数\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n```\n\n### 4.3 完整案例代码\n\n```python (4,41,50)\nimport evaluate\nimport numpy as np\nimport swanlab\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom datasets import load_dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\n\ndataset = load_dataset(\"yelp_review_full\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n\nmetric = evaluate.load(\"accuracy\")\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\n\ntraining_args = TrainingArguments(\n    output_dir=\"test_trainer\",\n    # 如果只需要用SwanLab跟踪实验，则将report_to参数设置为”none“\n    report_to=\"none\",\n    num_train_epochs=3,\n    logging_steps=50,\n)\n\n# 实例化SwanLabCallback\nswanlab_callback = SwanLabCallback(experiment_name=\"TransformersTest\")\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n    # 传入callbacks参数\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n```\n\n### 4.4 GUI效果展示\n\n超参数自动记录：\n\n![ig-hf-transformers-gui-1](./huggingface_transformers/card.jpg)\n\n指标记录：\n\n![ig-hf-transformers-gui-2](./huggingface_transformers/chart.jpg)\n\n### 4.5 拓展：增加更多回调\n\n试想一个场景，你希望在每个epoch结束时，让模型推理测试样例，并用swanlab记录推理的结果，那么你可以创建一个继承自`SwanLabCallback`的新类，增加或重构生命周期函数。比如：\n\n```python\nclass NLPSwanLabCallback(SwanLabCallback):    \n    def on_epoch_end(self, args, state, control, **kwargs):\n        test_text_list = [\"example1\", \"example2\"]\n        log_text_list = []\n        for text in test_text_list:\n            result = model(text)\n            log_text_list.append(swanlab.Text(result))\n            \n        swanlab.log({\"Prediction\": test_text_list}, step=state.global_step)\n```\n\n上面是一个在NLP任务下的新回调类，增加了`on_epoch_end`函数，它会在`transformers`训练的每个epoch结束时执行。\n\n查看全部的Transformers生命周期回调函数：[链接](https://github.com/huggingface/transformers/blob/main/src/transformers/trainer_callback.py#L311)",
    "358": "一级标题：HuggingFace Transformers\n二级标题：5. 环境变量\n内容：\n参考：[HuggingFace Docs: transformers.integrations.SwanLabCallback](https://huggingface.co/docs/transformers/main/en/main_classes/callback#transformers.integrations.SwanLabCallback)",
    "359": "一级标题：HuggingFace Trl\n二级标题：TRL简介\n内容：\nTRL (Transformers Reinforcement Learning，用强化学习训练Transformers模型) 是一个领先的Python库，旨在通过监督微调（SFT）、近端策略优化（PPO）和直接偏好优化（DPO）等先进技术，对基础模型进行训练后优化。TRL 建立在 🤗 Transformers 生态系统之上，支持多种模型架构和模态，并且能够在各种硬件配置上进行扩展。\n\n![logo](./huggingface_trl/logo.png)\n\n你可以使用Trl快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n[Demo](https://swanlab.cn/@ZeyiLin/trl-visualization/runs/q1uf2r4wmao7iomc5z1ff/overview)\n\n> `transformers>=4.50.0` 的版本，已官方集成了SwanLab  \n> 如果你的版本低于4.50.0，请使用[SwanLabCallback集成](#_5-使用swanlabcallback)。",
    "360": "一级标题：HuggingFace Trl\n二级标题：一行代码集成\n内容：\n只需要在你的训练代码中，找到HF的`Config`部分（比如`SFTConfig`、`GRPOConfig`等），添加`report_to=\"swanlab\"`参数，即可完成集成。\n\n```python\nfrom trl import SFTConfig, SFTTrainer\n\nargs = SFTConfig(\n    ...,\n    report_to=\"swanlab\" # [!code ++]\n)\n\ntrainer = Trainer(..., args=args)\n```",
    "361": "一级标题：HuggingFace Trl\n二级标题：自定义项目名\n内容：\n默认下，项目名会使用你运行代码的`目录名`。\n\n如果你想自定义项目名，可以设置`SWANLAB_PROJECT`环境变量：\n\n::: code-group\n\n```python\nimport os\nos.environ[\"SWANLAB_PROJECT\"]=\"qwen2-sft\"\n```\n\n```bash [Command Line（Linux/MacOS）]\nexport SWANLAB_PROJECT=\"qwen2-sft\"\n```\n\n```bash [Command Line（Windows）]\nset SWANLAB_PROJECT=\"qwen2-sft\"\n```\n\n:::",
    "362": "一级标题：HuggingFace Trl\n二级标题：案例代码\n内容：\n使用Qwen2.5-0.5B-Instruct模型，使用Capybara数据集进行SFT训练：\n\n```python\nfrom trl import SFTConfig, SFTTrainer\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"trl-lib/Capybara\", split=\"train\")\n\ntraining_args = SFTConfig(\n    output_dir=\"Qwen/Qwen2.5-0.5B-SFT\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    num_train_epochs=1,\n    logging_steps=20,\n    learning_rate=2e-5,\n    report_to=\"swanlab\", # [!code ++]\n    )\n\ntrainer = SFTTrainer(\n    args=training_args,\n    model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n    train_dataset=dataset,\n)\n\ntrainer.train()\n```\n\nDPO、GRPO、PPO等同理，只需要将`report_to=\"swanlab\"`传入对应的`Config`即可。",
    "363": "一级标题：HuggingFace Trl\n二级标题：GUI效果展示\n内容：\n**超参数自动记录：**\n\n![ig-hf-trl-gui-1](./huggingface_trl/ig-hf-trl-gui-1.png)\n\n**指标记录：**\n\n![ig-hf-trl-gui-2](./huggingface_trl/ig-hf-trl-gui-2.png)",
    "364": "一级标题：HuggingFace Trl\n二级标题：使用SwanLabCallback\n内容：\n如果你使用的是`Transformers<4.50.0`的版本，或者你希望更灵活地控制SwanLab的行为，则可以使用SwanLabCallback集成。",
    "365": "一级标题：HuggingFace Trl\n二级标题：引入SwanLabCallback\n内容：\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\n```\n\n**SwanLabCallback**是适配于Transformers的日志记录类。\n\n**SwanLabCallback**可以定义的参数有：\n\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。\n- 你也可以在外部通过`swanlab.init`创建项目，集成会将实验记录到你在外部创建的项目中。",
    "366": "一级标题：HuggingFace Trl\n二级标题：传入Trainer\n内容：\n```python (1,7,12)\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom trl import SFTConfig, SFTTrainer\n\n...\n\n# 实例化SwanLabCallback\nswanlab_callback = SwanLabCallback(project=\"trl-visualization\")\n\ntrainer = SFTTrainer(\n    ...\n    # 传入callbacks参数\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n```",
    "367": "一级标题：HuggingFace Trl\n二级标题：完整案例代码\n内容：\n使用Qwen2.5-0.5B-Instruct模型，使用Capybara数据集进行SFT训练：\n\n```python (3,7,26)\nfrom trl import SFTConfig, SFTTrainer\nfrom datasets import load_dataset\nfrom swanlab.integration.transformers import SwanLabCallback\n\ndataset = load_dataset(\"trl-lib/Capybara\", split=\"train\")\n\nswanlab_callback = SwanLabCallback(\n    project=\"trl-visualization\",\n    experiment_name=\"Qwen2.5-0.5B-SFT\",\n    description=\"测试使用trl框架sft训练\"\n)\n\ntraining_args = SFTConfig(\n    output_dir=\"Qwen/Qwen2.5-0.5B-SFT\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    num_train_epochs=1,\n    logging_steps=20,\n    learning_rate=2e-5,\n    report_to=\"none\",\n    )\n\ntrainer = SFTTrainer(\n    args=training_args,\n    model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n    train_dataset=dataset,\n)\n\ntrainer.train()\n```\n\nDPO、GRPO、PPO等同理，只需要将`SwanLabCallback`传入对应的`Trainer`即可。\n\n环境变量内容：\n参考：[HuggingFace Docs: transformers.integrations.SwanLabCallback](https://huggingface.co/docs/transformers/main/en/main_classes/callback#transformers.integrations.SwanLabCallback)",
    "368": "一级标题：Hydra\n二级标题：跟踪指标\n内容：\n你可以继续使用 Hydra 进行配置管理，同时使用SwanLab的强大功能。\n\n假设你的hydra配置文件为`configs/defaults.yaml`，则添加几行：\n\n```yaml\nswanlab:\n  project: \"my-project\"\n```\n\n在训练脚本中，将配置文件中的`project`传入：\n\n```python\nimport swanlab\nimport hydra\n\n@hydra.main(config_path=\"configs/\", config_name=\"defaults\")\ndef run_experiment(cfg):\n    run = swanlab.init(project=cfg.swanlab.project)\n    ...\n    swanlab.log({\"loss\": loss})\n```",
    "369": "一级标题：Hydra\n二级标题：跟踪超参数\n内容：\nHydra使用[omegaconf](https://omegaconf.readthedocs.io/en/2.1_branch/)作为与配置字典交互的默认方式。\n\n可以直接将OmegaConf的字典传递给`swanlab.config`：\n\n```python\n@hydra.main(config_path=\"configs/\", config_name=\"defaults\")\ndef run_experiment(cfg):\n    run = swanlab.init(project=cfg.swanlab.project,\n                       config=cfg,\n    )\n    ...\n    swanlab.log({\"loss\": loss})\n    model = Model(**swanlab.config.model.configs)\n```\n\n如果传递`cfg`时出现意外的结果，那么可以先转换`omegaconf.DictConfig`为原始类型：\n\n```python\n@hydra.main(config_path=\"configs/\", config_name=\"defaults\")\ndef run_experiment(cfg):\n    run = swanlab.init(project=cfg.swanlab.project,\n                       config=omegaconf.OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True)\n    ...\n    swanlab.log({\"loss\": loss})\n    model = Model(**swanlab.config.model.configs)\n```",
    "370": "一级标题：Keras\n二级标题：Keras 简介\n内容：\n[Keras](https://keras.io/) 是一个用 Python 编写的高级神经网络 API，最初由 François Chollet 创建，并于 2017 年合并到 TensorFlow 中，但依然可以作为一个独立的框架使用。它是一个开源的深度学习框架，运行在 TensorFlow、Theano 或 Microsoft Cognitive Toolkit (CNTK) 等深度学习后端之上。\n\n![keras-image](/assets/ig-keras-1.png)\n\n你可以使用Keras快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n[在线演示](https://swanlab.cn/@ZeyiLin/keras_mnist/runs/9gzx3m1ga2q2xb6t6ekxb/chart)",
    "371": "一级标题：Keras\n二级标题：引入SwanLabLogger\n内容：\n```python\nfrom swanlab.integration.keras import SwanLabLogger\n```",
    "372": "一级标题：Keras\n二级标题：与model.fit配合\n内容：\n首先初始化SwanLab：\n\n```python\nswanlab.init(\n    project=\"keras_mnist\",\n    experiment_name=\"mnist_example\",\n    description=\"Keras MNIST Example\"\n    )\n```\n\n然后，在`model.fit`的`callbacks`参数中添加`SwanLabLogger`，即可完成集成：\n\n```python\nmodel.fit(..., callbacks=[SwanLabLogger()])\n```",
    "373": "一级标题：Keras\n二级标题：案例-MNIST\n内容：\n```python\nfrom swanlab.integration.keras import SwanLabLogger\nimport tensorflow as tf\nimport swanlab\n\n# Initialize SwanLab\nswanlab.init(\n    project=\"keras_mnist\",\n    experiment_name=\"mnist_example\",\n    description=\"Keras MNIST Example\"\n    )\n\n# Load and preprocess MNIST data\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\nx_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n\n# Build a simple CNN model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Conv2D(64, 3, activation='relu'),\n    tf.keras.layers.MaxPooling2D(),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(10, activation='softmax')\n])\n\n# Compile the model\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Train the model with SwanLabLogger\nmodel.fit(\n    x_train, \n    y_train,\n    epochs=5,\n    validation_data=(x_test, y_test),\n    callbacks=[SwanLabLogger()]\n)\n```\n\n效果演示：\n\n![keras-image](/assets/ig-keras-2.png)\n\n[在线演示](https://swanlab.cn/@ZeyiLin/keras_mnist/runs/9gzx3m1ga2q2xb6t6ekxb/chart)",
    "374": "一级标题：LightGBM\n二级标题：引入SwanLabCallback\n内容：\n```python\nfrom swanlab.integration.lightgbm import SwanLabCallback\n```\nSwanLabCallback是适配于LightGBM的日志记录类。",
    "375": "一级标题：LightGBM\n二级标题：初始化SwanLab\n内容：\n```python\nswanlab.init(\n    project=\"lightgbm-example\", \n    experiment_name=\"breast-cancer-classification\"\n)\n```",
    "376": "一级标题：LightGBM\n二级标题：传入`lgb.train`\n内容：\n```python\nimport lightgbm as lgb\n\ngbm = lgb.train(\n    ...\n    callbacks=[SwanLabCallback()]\n)\n```",
    "377": "一级标题：LightGBM\n二级标题：完整测试代码\n内容：\n```python\nimport lightgbm as lgb\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport swanlab\nfrom swanlab.integration.lightgbm import SwanLabCallback\n\n# Step 1: 初始化swanlab\nswanlab.init(project=\"lightgbm-example\", experiment_name=\"breast-cancer-classification\")\n\n# Step 2: 加载数据集\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Step 3: 分割数据集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Step 4: 创建LightGBM数据集\ntrain_data = lgb.Dataset(X_train, label=y_train)\ntest_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n\n# Step 5: 设置参数\nparams = {\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'boosting_type': 'gbdt',\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9\n}\n\n# Step 6: 使用swanlab callback训练模型\nnum_round = 100\ngbm = lgb.train(\n    params,\n    train_data,\n    num_round,\n    valid_sets=[test_data],\n    callbacks=[SwanLabCallback()]\n)\n\n# Step 7: 预测\ny_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\ny_pred_binary = [1 if p >= 0.5 else 0 for p in y_pred]\n\n# Step 8: 评估模型\naccuracy = accuracy_score(y_test, y_pred_binary)\nprint(f\"模型准确率: {accuracy:.4f}\")\nswanlab.log({\"accuracy\": accuracy})\n\n# Step 9: 保存模型\ngbm.save_model('lightgbm_model.txt')\n\n# Step 10: 加载模型并预测\nbst_loaded = lgb.Booster(model_file='lightgbm_model.txt')\ny_pred_loaded = bst_loaded.predict(X_test)\ny_pred_binary_loaded = [1 if p >= 0.5 else 0 for p in y_pred_loaded]\n\n# Step 11: 评估加载模型\naccuracy_loaded = accuracy_score(y_test, y_pred_binary_loaded)\nprint(f\"加载模型后的准确率: {accuracy_loaded:.4f}\")\nswanlab.log({\"accuracy_loaded\": accuracy_loaded})\n\n# Step 12: 结束swanlab实验\nswanlab.finish()\n```",
    "378": "一级标题：LLaMA Factory\n二级标题：0. 前言\n内容：\n我们非常高兴地宣布**SwanLab**与[LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory)建立合作伙伴关系，致力于为中国训练者提供优质、高效的大模型训练体验。\n\n现在你使用新版本的LLaMA Factory启动训练前，可以在WebUI的「SwanLab configurations」（中文：SwanLab参数设置）卡片中勾选「Use SwanLab」，就可以通过SwanLab强大的训练看板进行这一次大模型微调的跟踪、记录与可视化。\n\nLLaMA Factory 是一个用于微调大语言模型 (LLM) 的开源工具包，它提供了一个统一且高效的框架，支持 100 多个 LLM （包括Qwen、LLaMA、ChatGLM、Mistral等）的微调，涵盖了各种训练方法、数据集和先进算法。\n\n大语言模型的微调是一个上手门槛颇高的工作，LLaMA Factory通过提供用户友好的 Web UI 和命令行界面，结合其统一且高效的框架，大幅降低了大模型从微调到测试评估的上手门槛。\n\n为了提供用户更好的大模型微调过程监控与日志记录体验，我们与LLaMA Factory团队合作开展了两项举措：利用SwanLab增强LLaMA Factory的实验监控能力，以及在SwanLab中记录 LLaMA Factory的专属超参数。\n\n\n> LLaMA Factory：https://github.com/hiyouga/LLaMA-Factory  \n> SwanLab：https://swanlab.cn  \n> SwanLab开源仓库：https://github.com/SwanHubX/SwanLab  \n> 实验过程：https://swanlab.cn/@ZeyiLin/llamafactory/runs/y79f9ri9jr1mkoh24a7g8/chart\n\n我们将以使用LLaMA Factory + SwanLab可视化微调Qwen2.5为案例。",
    "379": "一级标题：LLaMA Factory\n二级标题：1. 安装环境\n内容：\n首先，你需要确保你拥有Python3.8以上环境与Git工具，然后克隆仓库：\n\n```shellscript\ngit clone --depth 1 https://github.com/hiyouga/LLaMA-Factory\n```\n\n安装相关环境：\n\n```shellscript\ncd LLaMA-Factory\npip install -e \".[torch,metrics,swanlab]\"\n```\n\n> 如果你是昇腾NPU用户，可以访问：[华为NPU适配](https://llamafactory.readthedocs.io/zh-cn/latest/advanced/npu.html) 查看昇腾NPU版安装教程。",
    "380": "一级标题：LLaMA Factory\n二级标题：2. 使用LLaMA Board开启训练\n内容：\nLLaMA Board是基于Gradio的可视化微调界面，你可以通过下面的代码启动LLaMA Board：\n\n```shellscript\nllamafactory-cli webui\n```\n\n提示：LLaMA Factory默认的模型/数据集下载源是HuggingFace，如果你所在的网络环境对与HuggingFace下载并不友好，可以在启动LLaMA Board之前，将下载源设置为魔搭社区或魔乐社区：\n\n```shellscript\n# 下载源改为魔搭社区\nexport USE_MODELSCOPE_HUB=1 # Windows 使用 `set USE_MODELSCOPE_HUB=1`\n\n# 下载源改为魔乐社区\nexport USE_OPENMIND_HUB=1 # Windows 使用 `set USE_OPENMIND_HUB=1`\n```\n\n执行 llamafactory-cli webui 之后，你可以在浏览器看到下面的UI界面。本案例选择Qwen2-1.5B-instruct作为模型，alpaca\\_zh\\_demo作为数据集：\n\n在页面的下方，你会看到一个「SwanLab参数设置」的卡片，展开后，你就可以配置SwanLab的项目名、实验名、工作区、API 密钥以及模式等参数。\n\n> 如果你是第一次使用SwanLab，还需要在 swanlab.cn 注册一个账号获取专属的API密钥。\n\n我们勾&#x9009;**「使用SwanLab」：**\n\n现在，点&#x51FB;**「开始」按钮**，就可以开启微调：\n\n在完成载入模型、载入数据集，正式开启微调后，我们可以在命令行界面找到SwanLab部分：\n\n点击箭头对应的实验链接，就可以在**浏览器**中打开SwanLab实验跟踪看板：\n\n在「卡片」栏下的「配置」表中，第一个就会是LLamaFactory，标识了这次训练的使用框架。",
    "381": "一级标题：LLaMA Factory\n二级标题：3. 使用命令行开启训练\n内容：\nLLaMA Factory还支持通过yaml配置文件，在命令行中进行微调。\n\n我们编辑LLaMA Factory项目目录下的 **examples/train\\_lora/qwen2vl\\_lora\\_sft.yaml** 文件，在文件尾部增加：\n\n```yaml\n...\n\n### swanlab\nuse_swanlab: true\nswanlab_project: llamafactory\nswanlab_run_name: Qwen2-VL-7B-Instruct\n```\n\n然后运行：\n\n```shellscript\nllamafactory-cli train examples/train_lora/qwen2vl_lora_sft.yaml\n```\n\n在完成载入模型、载入数据集，正式开启微调后，与LLaMA Board一样，可以在命令行界面找到SwanLab部分，通过实验链接访问SwanLab实验看板。\n\n致敬 LLaMA Factory 团队，感谢他们为开源社区提供了这么一个优秀的模型训练工具。随着我们的继续合作，敬请期待SwanLab工具为大模型训练师提供更深入、强大的实验跟踪功能。",
    "382": "一级标题：LLaMA Factory\n二级标题：4.附录：支持的参数\n内容：\n```yaml\n# swanlab\nuse_swanlab: true\nswanlab_project: your_project_name\nswanlab_run_name: your_experiment_name\nswanlab_workspace: your_workspace\nswanlab_mode: your_mode\nswanlab_api_key: your_api_key\n```\n\n> 更多可见：[LLaMA Factory - Github](https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md#%E5%AE%89%E8%A3%85-llama-factory) 中的`SwanLabArguments`类。",
    "383": "一级标题：MLFlow\n二级标题：MLFlow简介\n内容：\n[MLFlow](https://github.com/mlflow/mlflow) 是一个开源的机器学习生命周期管理平台，由 Databricks 创建并维护。它旨在帮助数据科学家和机器学习工程师更高效地管理机器学习项目的整个生命周期，包括实验跟踪、模型管理、模型部署和协作。MLflow 的设计是模块化的，可以与任何机器学习库、框架或工具集成。\n\n![mlflow](./mlflow/logo.png)\n\n:::warning 其他工具的同步教程\n\n- [TensorBoard](/guide_cloud/integration/integration-tensorboard.md)\n- [Weights & Biases](/guide_cloud/integration/integration-wandb.md)\n:::\n\n你可以用两种方式将MLflow上的项目同步到SwanLab：\n\n1. **同步跟踪**：如果你现在的项目使用了mlflow进行实验跟踪，你可以使用`swanlab.sync_mlflow()`命令，在运行训练脚本时同步记录指标到SwanLab。\n2. **转换已存在的项目**：如果你想要将mlflow上的项目复制到SwanLab，你可以使用`swanlab convert`，将mlflow上已存在的项目转换成SwanLab项目。\n\n::: info\n在当前版本暂仅支持转换标量图表。\n:::\n\n[[toc]]",
    "384": "一级标题：MLFlow\n二级标题：同步跟踪\n内容：\n### 1.1 添加sync_mlflow命令\n\n在你的代码执行`mlflow.start_run()`之前的任何位置，添加一行`swanlab.sync()`命令，即可在运行训练脚本时同步记录指标到SwanLab。\n\n```python\nimport swanlab\n\nswanlab.sync_mlflow()\n\n...\n\nmlflow.start_run()\n```\n\n在上述这种代码写法中，`mlflow.start_run()`的同时会初始化swanlab，项目名、实验名和配置和`mlflow.start_run()`中的`experiment_name`、`run_name`、`log_param`一致，因此你不需要再手动初始化swanlab。\n\n### 1.2 另一种写法\n\n另一种用法是先手动初始化swanlab，再运行mlflow的代码。\n\n```python\nimport swanlab\n\nswanlab.init(...)\nswanlab.sync_mlflow()\n```\n\n在这种写法中，项目名、实验名、配置和`swanlab.init()`中的`project`、`experiment_name`、`config`一致，而后续`mlflow.start_run()`中的`experiment_name`、`run_name`会被忽略，`config`会更新进`swanlab.config`中。\n\n### 1.3 测试代码\n\n```python\nimport mlflow\nimport random\nimport swanlab\n\nswanlab.sync_mlflow()\n\nmlflow.set_experiment(\"mlflow_sync_test\")\n\nwith mlflow.start_run(run_name=\"test_run\"):\n    mlflow.log_param(\"learning_rate\", 0.01)\n    mlflow.log_params({\"batch_size\": 32, \"epochs\": 10})\n    \n    for epoch in range(10):\n        acc = 1 - 2 ** -epoch - random.random() / epoch\n        loss = 2 ** -epoch + random.random() / epoch\n        mlflow.log_metric(\"accuracy\", acc, step=epoch)\n        mlflow.log_metric(\"loss\", loss, step=epoch)\n        \n        mlflow.log_metrics({\n            \"precision\": acc * 0.9,\n            \"recall\": acc * 0.8\n        }, step=epoch)\n```",
    "385": "一级标题：MLFlow\n二级标题：转换已经存在的项目\n内容：\n### 2.1 准备工作\n\n**（必须）mlflow服务的url链接**\n\n首先，需要记下mlflow服务的**url链接**，如`http://127.0.0.1:5000`。\n\n> 如果还没有启动mlflow服务，那么需要使用`mlflow ui`命令启动服务，并记下url链接。\n\n**（可选）实验ID**\n\n如果你只想转换其中的一组实验，那么在下图所示的地方，记下该实验ID。\n\n![](./mlflow/ui-1.png)\n\n### 2.2 方式一：命令行转换\n\n转换命令行：\n\n```bash\nswanlab convert -t mlflow --mlflow-url <MLFLOW_URL> --mlflow-exp <MLFLOW_EXPERIMENT_ID>\n```\n\n支持的参数如下：\n\n- `-t`: 转换类型，可选wandb、tensorboard和mlflow。\n- `-p`: SwanLab项目名。\n- `-w`: SwanLab工作空间名。\n- `--cloud`: (bool) 是否上传模式为\"cloud\"，默认为True\n- `-l`: logdir路径。\n- `--mlflow-url`: mlflow服务的url链接。\n- `--mlflow-exp`: mlflow实验ID。\n\n如果不填写`--mlflow-exp`，则会将指定项目下的全部实验进行转换；如果填写，则只转换指定的实验组。\n\n### 2.3 方式二：代码内转换\n\n```python\nfrom swanlab.converter import MLFLowConverter\n\nmlflow_converter = MLFLowConverter(project=\"mlflow_converter\")\n# mlflow_exp可选\nmlflow_converter.run(tracking_uri=\"http://127.0.0.1:5000\", experiment=\"1\")\n```\n\n效果与命令行转换一致。\n\n`MLFLowConverter`支持的参数：\n\n- `project`: SwanLab项目名。\n- `workspace`: SwanLab工作空间名。\n- `cloud`: (bool) 是否上传模式为\"cloud\"，默认为True。\n- `logdir`: wandb Run（项目下的某一个实验）的id。",
    "386": "一级标题：MMDetection\n二级标题：教程\n内容：\n:::info 教程\n[mmdetection如何使用swanlab远程查看训练日志](https://zhuanlan.zhihu.com/p/699058426)\n:::\n\n<div align=\"center\">\n<img src=\"/assets/integration-mmdetection.png\" width=600>\n</div>\n\n[MMdetection](https://github.com/open-mmlab/mmdetection) 是一个由 [OpenMMLab](https://openmmlab.com/) 社区开发的深度学习训练框架，建立在 PyTorch 深度学习框架之上，旨在为研究人员和工程师提供一个高效、灵活、易于扩展的目标检测平台。MMDetection 支持多种主流的目标检测方法，并提供了大量预训练模型和丰富的配置选项，使得在目标检测任务中的应用和开发变得更加便捷。\n\n<div align=\"center\">\n<img src=\"/assets/integration-mmdetection-intro.png\">\n</div>\n\n可以通过修改MMDetection的配置文件来使用SwanLab作为实验记录工具。",
    "387": "一级标题：MMDetection\n二级标题：在配置文件中指定SwanLab作为VisBackend\n内容：\n确保你安装了SwanLab，或者使用`pip install -U swanlab`安装最新版。\n\n将如下内容添加到mmdetection的config文件中, 其中`init_kwargs`中填入的参数字典与`swanlab.init`的规则一致:\n\n```python\n# swanlab visualizer\ncustom_imports = dict(  # 引入SwanLab作为日志记录器，对于部分不支持custom_imports的项目可以直接初始化SwanlabVisBackend并加入vis_backends\n    imports=[\"swanlab.integration.mmengine\"], allow_failed_imports=False\n)\n\nvis_backends = [\n    dict(\n        type=\"SwanlabVisBackend\",\n        init_kwargs={ # swanlab.init 参数\n            \"project\": \"swanlab-mmengine\",\n            \"experiment_name\": \"Your exp\",  # 实验名称\n            \"description\": \"Note whatever you want\",  # 实验的描述信息\n        },\n    ),\n]\n\nvisualizer = dict(\n    type=\"Visualizer\",\n    vis_backends=vis_backends,\n    name=\"visualizer\",\n)\n```\n\n> 有关其他引入方法和更灵活的配置，可以参考[MMEngine接入SwanLab](https://docs.swanlab.cn/zh/guide_cloud/integration/integration-mmengine.html)",
    "388": "一级标题：MMDetection\n二级标题：使用案例：MMDetection训练faster-rcnn\n内容：\n首先克隆[MMDetction](https://github.com/open-mmlab/mmdetection)项目到本地。\n\n然后在faster-rnn对应的config文件（`configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py`）的最后增加下面的代码：\n\n```python\n_base_ = [\n    '../_base_/models/faster-rcnn_r50_fpn.py',\n    '../_base_/datasets/coco_detection.py',\n    '../_base_/schedules/schedule_1x.py', '../_base_/default_runtime.py'\n]\n\n# swanlab\ncustom_imports = dict(  # 引入SwanLab作为日志记录器\n    imports=[\"swanlab.integration.mmengine\"], allow_failed_imports=False\n)\nvis_backends = [\n    dict(type=\"LocalVisBackend\"),\n    dict(\n        type=\"SwanlabVisBackend\",\n        init_kwargs={  # swanlab.init 参数\n            \"project\": \"MMDetection\",  # 项目名称\n            \"experiment_name\": \"faster-rcnn\",  # 实验名称\n            \"description\": \"faster-rcnn r50 fpn 1x coco\",  # 实验的描述信息\n        },\n    ),\n]\nvisualizer = dict(\n    type=\"DetLocalVisualizer\", vis_backends=vis_backends, name=\"visualizer\"\n)\n```\n\n**然后开启训练即可**：\n\n```bash\npython tools/train.py configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py\n```\n\n![ig-mmengine-1](/assets/ig-mmengine-1.png)\n\n**在swanlab中远程查看训练日志**：\n\n![ig-mmengine-2](/assets/ig-mmengine-2.png)",
    "389": "一级标题：MMEngine\n二级标题：MMEngine简介\n内容：\n[MMEngine](https://github.com/open-mmlab/mmengine) 是一个由 [OpenMMLab](https://openmmlab.com/) 社区开发的深度学习训练框架，专为深度学习研究和开发而设计。MMEngine 提供了一种高效、灵活且用户友好的方式来构建、训练和测试深度学习模型，尤其是在计算机视觉领域。它的目标是简化研究人员和开发者在深度学习项目中的工作流程，并提高其开发效率。\n\n<div align=\"center\">\n<img src=\"/assets/integration-mmengine.jpeg\" width=440>\n</div>\n\nMMEngine 为 OpenMMLab 算法库实现了下一代训练架构，为 OpenMMLab 中的 30 多个算法库提供了统一的执行基础。其核心组件包括训练引擎、评估引擎和模块管理。\n\nSwanLab将专为MMEngine设计的`SwanlabVisBackend`集成到MMEngine中，可用于记录训练、评估指标、记录实验配置、记录图像等。\n\n::: warning MM生态的其他集成\n\n- [MMPretrain](/zh/guide_cloud/integration/integration-mmpretrain.md)\n- [MMDetection](/zh/guide_cloud/integration/integration-mmdetection.md)\n- [MMSegmentation](/zh/guide_cloud/integration/integration-mmsegmentation.md)\n- [XTuner](/zh/guide_cloud/integration/integration-xtuner.md)\n\n:::",
    "390": "一级标题：MMEngine\n二级标题：MMEngine系列框架兼容性说明\n内容：\n使用mmengine的框架都可以使用如下方法引入SwanLab，比如MM官方框架 [mmdetection](https://docs.swanlab.cn/zh/guide_cloud/integration/integration-mmdetection.html)，[mmsegmentation](https://docs.swanlab.cn/zh/guide_cloud/integration/integration-mmsegmentation.html)等，以及[自己基于mmengine实现的训练框架](https://mmengine.readthedocs.io/zh-cn/latest/get_started/15_minutes.html)。\n\n> 可以在[OpenMMLab官方GitHub账号](https://github.com/open-mmlab)下查看有哪些优秀框架。\n\n部分框架比如[Xtuner](https://github.com/InternLM/xtuner)项目，其没有完全兼容mmengine，需要做一些简单改动，可以前往[SwanLab的Xtuner集成](https://docs.swanlab.cn/zh/guide_cloud/integration/integration-xtuner.html)查看如何在Xtuner中使用SwanLab。\n\nmmengine有两种引入SwanLab进行实验可视化跟踪的方法：",
    "391": "一级标题：MMEngine\n二级标题：使用方法一：训练脚本传入visualizer，开始训练\n内容：\n:::info\n可以参考[mmengine15分钟教程](https://mmengine.readthedocs.io/zh-cn/latest/get_started/15_minutes.html)将自己的训练代码适配mmengine\n:::\n\n确保你安装了SwanLab，或者使用`pip install -U swanlab`安装最新版。\n\n如果你按照官方案例使用了mmengine作为你的训练框架。只需在训练脚本中进行如下改动：\n1. 在初始化`visualizer`时加入SwanlabVisBackend\n2. 初始化`runner`传入`visualizer`即可：\n\n```python (10,20)\nfrom mmengine.visualization import Visualizer\nfrom mmengine.runner import Runner\n\nfrom swanlab.integration.mmengine import SwanlabVisBackend\n...\n# 初始化SwanLab\nswanlab_vis_backend = SwanlabVisBackend(init_kwargs={})# init args can be found in https://docs.swanlab.cn/zh/guide_cloud/integration/integration-mmengine.html\n# 初始化mmegine的Visulizer，并引入SwanLab作为Visual Backend\nvisualizer = Visualizer(\n    vis_backends=swanlab_vis_backend\n)  \n\n# 构建mmengine的Runner\nrunner = Runner(\n    model,\n    work_dir='runs/gan/',\n    train_dataloader=train_dataloader,\n    train_cfg=train_cfg,\n    optim_wrapper=opt_wrapper_dict,\n    visualizer=visualizer,\n)\n\n# 开始训练\nrunner.train()\n```\n\n如果希望像平常使用swanlab那样指定实验名等信息，可以在实例化SwanlabVisBackend时在init_kwargs中指定参数，具体参考[init api](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/sdk.py#L71)，不过不像使用`swanlab.init`那样直接作为参数传入，而是需要构建字典。\n\n下面列举了两者在交互上的不同：\n\n直接使用`swanlab.init`:\n\n```python\nrun = swanlab.init(\n    project=\"cat-dog-classification\",\n    experiment_name=\"Resnet50\",\n    description=\"我的第一个人工智能实验\",\n)\n```\n\n使用`SwanlabVisBackend`，则是以字典的形式传入`init`的参数:\n\n```python\nswanlab_vis_backend = SwanlabVisBackend(\n    init_kwargs={\n        \"project\": \"cat-dog-classification\",\n        \"experiment_name\": \"Resnet50\",\n        \"description\": \"我的第一个人工智能实验\",\n    }\n)\n```",
    "392": "一级标题：MMEngine\n二级标题：使用方法二：config文件引入SwanlabVisBackend\n内容：\n:::info\n此方法对于大多数基于mmengine的训练框架都是适用的\n:::\n\n将如下内容添加到mm系列框架的任意config文件中, 其中`init_kwargs`中填入的参数字典与`swanlab.init`的规则一致:\n\n```python\n# swanlab visualizer\ncustom_imports = dict(  # 引入SwanLab作为日志记录器，对于部分不支持custom_imports的项目可以直接初始化SwanlabVisBackend并加入vis_backends\n    imports=[\"swanlab.integration.mmengine\"], allow_failed_imports=False\n)\n\nvis_backends = [\n    dict(\n        type=\"SwanlabVisBackend\",\n        init_kwargs={ # swanlab.init 参数\n            \"project\": \"swanlab-mmengine\",\n            \"experiment_name\": \"Your exp\",  # 实验名称\n            \"description\": \"Note whatever you want\",  # 实验的描述信息\n        },\n    ),\n]\n\nvisualizer = dict(\n    type=\"Visualizer\",\n    vis_backends=vis_backends,\n    name=\"visualizer\",\n)\n```\n\n可以使用如下代码测试config文件是否能够成功引入SwanLab，将上面的config文件保存为`my_swanlab_config.py`，创建一个`test_config.py`写入如下代码并运行：\n\n```python\nfrom mmengine.config import Config\nimport mmengine\n\nprint(mmengine.__version__)\ncfg = Config.fromfile(\n    \"my_swanlab_config.py\"\n)\n\nfrom mmengine.registry import VISUALIZERS\n\ncustom_vis = VISUALIZERS.build(cfg.visualizer)\nprint(custom_vis)\n\n```\n\n如果看到终端打印出类似如下信息，则表示成功引入了swanlab：\n\n```console\nMMEngine Version: 0.10.4\nSwanLab Version: 0.3.11\n<mmengine.visualization.visualizer.Visualizer object at 0x7f7cf15b1e20>\n```",
    "393": "一级标题：MMEngine\n二级标题：案例：MMEngine训练ResNet-50\n内容：\n:::info 参考MMEngine官方15分钟上手教程\n[15 分钟上手 MMENGINE](https://mmengine.readthedocs.io/zh-cn/latest/get_started/15_minutes.html)\n:::\n\n按照[MMEngine官方教程](https://mmengine.readthedocs.io/zh-cn/latest/get_started/installation.html)安装MMEngine。\n\n这里将安装环境的命令抄录下来，强烈建议按照官方文档安装，以环境为python3.11，CUDA12.1为例。\n\n```sh\n# with cuda12.1 or you can find torch version you want at pytorch.org\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\npip install -U openmim\nmim install mmengine\npip install swanlab\n```\n\n使用如下代码构建ResNet-50网络并引入Cifar10数据集开始训练\n\n```python\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.optim import SGD\nfrom torch.utils.data import DataLoader\n\nfrom mmengine.evaluator import BaseMetric\nfrom mmengine.model import BaseModel\nfrom mmengine.runner import Runner\nfrom mmengine.visualization import Visualizer\n\nfrom swanlab.integration.mmengine import SwanlabVisBackend\n\n\nclass MMResNet50(BaseModel):\n    def __init__(self):\n        super().__init__()\n        self.resnet = torchvision.models.resnet50()\n\n    def forward(self, imgs, labels, mode):\n        x = self.resnet(imgs)\n        if mode == \"loss\":\n            return {\"loss\": F.cross_entropy(x, labels)}\n        elif mode == \"predict\":\n            return x, labels\n\n\nclass Accuracy(BaseMetric):\n    def process(self, data_batch, data_samples):\n        score, gt = data_samples\n        self.results.append(\n            {\n                \"batch_size\": len(gt),\n                \"correct\": (score.argmax(dim=1) == gt).sum().cpu(),\n            }\n        )\n\n    def compute_metrics(self, results):\n        total_correct = sum(item[\"correct\"] for item in results)\n        total_size = sum(item[\"batch_size\"] for item in results)\n        return dict(accuracy=100 * total_correct / total_size)\n\n\nnorm_cfg = dict(mean=[0.491, 0.482, 0.447], std=[0.202, 0.199, 0.201])\ntrain_dataloader = DataLoader(\n    batch_size=32,\n    shuffle=True,\n    dataset=torchvision.datasets.CIFAR10(\n        \"data/cifar10\",\n        train=True,\n        download=True,\n        transform=transforms.Compose(\n            [\n                transforms.RandomCrop(32, padding=4),\n                transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                transforms.Normalize(**norm_cfg),\n            ]\n        ),\n    ),\n)\n\nval_dataloader = DataLoader(\n    batch_size=32,\n    shuffle=False,\n    dataset=torchvision.datasets.CIFAR10(\n        \"data/cifar10\",\n        train=False,\n        download=True,\n        transform=transforms.Compose(\n            [transforms.ToTensor(), transforms.Normalize(**norm_cfg)]\n        ),\n    ),\n)\n\nvisualizer = Visualizer(\n    vis_backends=SwanlabVisBackend(init_kwargs={})\n)  # init args can be found in https://docs.swanlab.cn/zh/guide_cloud/integration/integration-mmengine.html\n\nrunner = Runner(\n    model=MMResNet50(),\n    work_dir=\"./work_dir\",\n    train_dataloader=train_dataloader,\n    optim_wrapper=dict(optimizer=dict(type=SGD, lr=0.001, momentum=0.9)),\n    train_cfg=dict(by_epoch=True, max_epochs=5, val_interval=1),\n    val_dataloader=val_dataloader,\n    val_cfg=dict(),\n    val_evaluator=dict(type=Accuracy),\n    visualizer=visualizer,\n)\nrunner.train()\n\n```\n\n可以在[公开训练图表](https://swanlab.cn/@ShaohonChen/cifar10_with_resnet50/runs/f8znz8vj06huv6rm7j5a8/chart)查看到上脚本的训练结果。\n\n<div align=\"center\">\n<img src=\"/assets/integration-mmegine-train.png\" width=600>\n</div>",
    "394": "一级标题：MMPretrain\n二级标题：简介\n内容：\n[MMPretrain](https://github.com/open-mmlab/mmpretrain) 是 [OpenMMLab](https://openmmlab.com/) 旗下的一个开源预训练模型库，专注于为计算机视觉任务提供高效、易用的预训练模型。\n\n<div align=\"center\">\n<img src=\"/assets/integration-mmpretrain.jpg\" width=440>\n</div>\n\n基于 PyTorch 构建，MMPretrain 旨在帮助研究人员和开发人员快速应用和评估预训练模型，从而提升下游任务的性能和效率。该库包含了多种预训练模型，如 ResNet、Vision Transformer（ViT）和 Swin Transformer 等，这些模型经过大规模数据集的训练，能够直接用于图像分类、目标检测和分割等任务。此外，MMPretrain 提供了灵活的配置系统和丰富的接口，用户可以方便地进行模型的加载、微调和评估。详细的文档和教程使得用户能够快速上手和应用，适用于学术研究和工业实践中的各种场景。通过使用 MMPretrain，用户可以显著减少模型训练时间，专注于模型优化和应用创新。",
    "395": "一级标题：MMPretrain\n二级标题：使用SwanLab作为实验记录工具\n内容：\n可以通过修改MMPretrain的配置文件来使用SwanLab作为实验记录工具。\n\n确保你安装了SwanLab，或者使用`pip install -U swanlab`安装最新版。\n\n将如下内容添加到所使用的mmpretrain的config文件中, 其中`init_kwargs`中填入的参数字典与`swanlab.init`的规则一致:\n\n```python\n...\n# swanlab visualizer\ncustom_imports = dict(  # 引入SwanLab作为日志记录器，对于部分不支持custom_imports的项目可以直接初始化SwanlabVisBackend并加入vis_backends\n    imports=[\"swanlab.integration.mmengine\"], allow_failed_imports=False\n)\n\nvis_backends = [\n    dict(\n        type=\"SwanlabVisBackend\",\n        init_kwargs={ # swanlab.init 参数\n            \"project\": \"swanlab-mmengine\",\n            \"experiment_name\": \"Your exp\",  # 实验名称\n            \"description\": \"Note whatever you want\",  # 实验的描述信息\n        },\n    ),\n]\n\nvisualizer = dict(\n    type=\"Visualizer\",\n    vis_backends=vis_backends,\n    name=\"visualizer\",\n)\n...\n```\n\n> 有关其他引入方法和更灵活的配置，可以参考[MMEngine接入SwanLab](https://docs.swanlab.cn/zh/guide_cloud/integration/integration-mmengine.html)",
    "396": "一级标题：MMSegmentation\n二级标题：简介\n内容：\n[MMSegmentation](https://github.com/open-mmlab/mmengine) 是一个由 [OpenMMLab](https://openmmlab.com/) 社区开发的深度学习训练框架，基于 PyTorch 构建，旨在为研究人员和开发人员提供便捷高效的图像分割解决方案。\n\n<div align=\"center\">\n<img src=\"/assets/integration-mmsegmentation.png\" width=440>\n</div>\n\n该工具箱采用模块化设计，提供多种预训练模型如 U-Net、DeepLabV3 和 PSPNet 等，支持语义分割、实例分割和全景分割任务。MMSegmentation 内置强大的数据处理功能和多种分割性能评价指标，如 mIoU 和 Dice 系数，能够全面评估模型性能。其灵活的配置系统允许用户快速进行实验配置和参数调整。\n\n<div align=\"center\">\n<img src=\"/assets/integration-mmsegmentation-demo.gif\">\n</div>\n\nMMSegmentation 提供详细的文档和示例，帮助用户快速上手，并支持分布式训练和模型加速推理。该工具箱广泛应用于医学图像分割、遥感图像分割和自动驾驶等领域。\n\n可以通过修改MMSegmentation的配置文件来使用SwanLab作为实验记录工具。",
    "397": "一级标题：MMSegmentation\n二级标题：在配置文件中指定\n内容：\n确保你安装了SwanLab，或者使用`pip install -U swanlab`安装最新版。\n\n将如下内容添加到所使用的mmsegmentation的config文件中, 其中`init_kwargs`中填入的参数字典与`swanlab.init`的规则一致:\n\n```python\n...\n# swanlab visualizer\ncustom_imports = dict(  # 引入SwanLab作为日志记录器，对于部分不支持custom_imports的项目可以直接初始化SwanlabVisBackend并加入vis_backends\n    imports=[\"swanlab.integration.mmengine\"], allow_failed_imports=False\n)\n\nvis_backends = [\n    dict(\n        type=\"SwanlabVisBackend\",\n        init_kwargs={ # swanlab.init 参数\n            \"project\": \"swanlab-mmengine\",\n            \"experiment_name\": \"Your exp\",  # 实验名称\n            \"description\": \"Note whatever you want\",  # 实验的描述信息\n        },\n    ),\n]\n\nvisualizer = dict(\n    type=\"Visualizer\",\n    vis_backends=vis_backends,\n    name=\"visualizer\",\n)\n...\n```\n\n> 有关其他引入方法和更灵活的配置，可以参考[MMEngine接入SwanLab](https://docs.swanlab.cn/zh/guide_cloud/integration/integration-mmengine.html)",
    "398": "一级标题：Omegaconf\n二级标题：OmegaConf 与 swanlab 的集成\n内容：\nOmegaConf 是一个用于处理配置的 Python 库，尤其适用于需要灵活配置和配置合并的场景。\nOmegaConf 与swanlab的集成非常简单，直接将`omegaconf`对象传递给`swanlab.config`，即可记录为超参数：\n\n```python\nfrom omegaconf import OmegaConf\nimport swanlab\n\ncfg = OmegaConf.load(\"config.yaml\")\nswanlab.init(config=cfg,)\n```\n\n如果传递`cfg`时出现意外的结果，那么可以先转换`omegaconf.DictConfig`为原始类型：\n\n```python\nfrom omegaconf import OmegaConf\nimport swanlab\n\ncfg = OmegaConf.load(\"config.yaml\")\nswanlab.init(config=OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True))\n\n```",
    "399": "一级标题：OpenAI\n二级标题：引入autolog\n内容：\n```python\nfrom swanlab.integration.openai import autolog\n```\n`autolog`是一个为openai适配的过程记录类，能够自动记录你的openai交互的过程。",
    "400": "一级标题：OpenAI\n二级标题：传入参数\n内容：\n```python\nautolog(init={\"project\":\"openai_autologging\", \"experiment_name\":\"chatgpt4.0\"})\n```\n这里给`init`传入的参数与`swanlab.init`的参数形式完全一致。",
    "401": "一级标题：OpenAI\n二级标题：自动记录\n内容：\n由于`openai`在1.0.0版本以后，采用了和先前不一样的API设计，所以下面分为两个版本：\n### openai>=1.0.0\n需要将`client=openai.OpenAI()`替换为`client=autolog.client`。\n```python\nfrom swanlab.integration.openai import autolog\n\nautolog(init=dict(experiment_name=\"openai_autologging\"))\nclient = autolog.client\n\n# chat_completion\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-0125\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n        {\"role\": \"user\", \"content\": \"Who won the world series in 2015?\"},\n    ],\n)\n\n# text_completion\nresponse2 = client.completions.create(model=\"gpt-3.5-turbo-instruct\", prompt=\"Write a song for jesus.\")\n```\n### openai<=0.28.0\n```python\nimport openai\nfrom swanlab.integration.openai import autolog\n\nautolog(init=dict(experiment_name=\"openai_logging123\"))\n\nchat_request_kwargs = dict(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers\"},\n        {\"role\": \"user\", \"content\": \"Where was it played?\"},\n    ],\n)\nresponse = openai.ChatCompletion.create(**chat_request_kwargs)\n```",
    "402": "一级标题：PaddleDetection\n二级标题：介绍\n内容：\n[PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection) 是百度基于其深度学习框架 PaddlePaddle 开发的一个端到端的目标检测开发工具包。它支持对象检测、实例分割、多对象跟踪和实时多人关键点检测，旨在帮助开发者更高效地进行目标检测模型的开发和训练。\n\n![PaddleDetection](/assets/ig-paddledetection-1.png)\n\n你可以使用PaddleDetection快速进行目标检测模型训练，同时使用SwanLab进行实验跟踪与可视化。",
    "403": "一级标题：PaddleDetection\n二级标题：1. 引入SwanLabCallback\n内容：\n首先在你clone的PaddleDetection项目中，找到`ppdet/engine/callbacks.py`文件，在代码的底部添加如下代码：\n\n```python\nclass SwanLabCallback(Callback):\n    def __init__(self, model):\n        super(SwanLabCallback, self).__init__(model)\n\n        try:\n            import swanlab\n            self.swanlab = swanlab\n        except Exception as e:\n            logger.error('swanlab not found, please install swanlab. '\n                         'Use: `pip install swanlab`.')\n            raise e\n\n        self.swanlab_params = {k[8:]: v for k, v in model.cfg.items() if k.startswith(\"swanlab_\")}\n\n        self._run = None\n        if dist.get_world_size() < 2 or dist.get_rank() == 0:\n            _ = self.run\n            self.run.config.update(self.model.cfg)\n\n        self.best_ap = -1000.\n        self.fps = []\n\n    @property\n    def run(self):\n        if self._run is None:\n            self._run = self.swanlab.get_run() or self.swanlab.init(**self.swanlab_params)\n        return self._run\n\n    def on_step_end(self, status):\n        if dist.get_world_size() < 2 or dist.get_rank() == 0 and status['mode'] == 'train':\n            training_status = status['training_staus'].get()\n            batch_time = status['batch_time']\n            data_time = status['data_time']\n            batch_size = self.model.cfg['{}Reader'.format(status['mode'].capitalize())]['batch_size']\n\n            ips = float(batch_size) / float(batch_time.avg)\n            metrics = {\n                \"train/\" + k: float(v) for k, v in training_status.items()\n            }\n            metrics.update({\n                \"train/ips\": ips,\n                \"train/data_cost\": float(data_time.avg),\n                \"train/batch_cost\": float(batch_time.avg)\n            })\n\n            self.fps.append(ips)\n            self.run.log(metrics)\n\n    def on_epoch_end(self, status):\n        if dist.get_world_size() < 2 or dist.get_rank() == 0:\n            mode = status['mode']\n            epoch_id = status['epoch_id']\n            \n            if mode == 'train':\n                fps = sum(self.fps) / len(self.fps)\n                self.fps = []\n\n                end_epoch = self.model.cfg.epoch\n                if (epoch_id + 1) % self.model.cfg.snapshot_epoch == 0 or epoch_id == end_epoch - 1:\n                    save_name = str(epoch_id) if epoch_id != end_epoch - 1 else \"model_final\"\n                    tags = [\"latest\", f\"epoch_{epoch_id}\"]\n            \n            elif mode == 'eval':\n                fps = status['sample_num'] / status['cost_time']\n\n                merged_dict = {\n                    f\"eval/{key}-mAP\": map_value[0]\n                    for metric in self.model._metrics\n                    for key, map_value in metric.get_results().items()\n                }\n                merged_dict.update({\n                    \"epoch\": status[\"epoch_id\"],\n                    \"eval/fps\": fps\n                })\n\n                self.run.log(merged_dict)\n\n                if status.get('save_best_model'):\n                    for metric in self.model._metrics:\n                        map_res = metric.get_results()\n                        key = next((k for k in ['bbox', 'keypoint', 'mask'] if k in map_res), None)\n                        \n                        if not key:\n                            logger.warning(\"Evaluation results empty, this may be due to \"\n                                           \"training iterations being too few or not \"\n                                           \"loading the correct weights.\")\n                            return\n                        \n                        if map_res[key][0] >= self.best_ap:\n                            self.best_ap = map_res[key][0]\n                            save_name = 'best_model'\n                            tags = [\"best\", f\"epoch_{epoch_id}\"]\n\n    def on_train_end(self, status):\n        self.run.finish()\n```",
    "404": "一级标题：PaddleDetection\n二级标题：2. 修改trainer代码\n内容：\n在`ppdet/engine/trainer.py`文件中，在`from .callbacks import`那一行添加SwanLabCallback：\n\n```python\nfrom .callbacks import Callback, ComposeCallback, LogPrinter, Checkpointer, WiferFaceEval, VisualDLWriter, SniperProposalsGenerator, WandbCallback, SemiCheckpointer, SemiLogPrinter, SwanLabCallback\n```\n\n接着，我们找到`Trainer`类的`__init_callbacks`方法，在`if self.mode == 'train':`下添加如下代码：\n\n```python\nif self.cfg.get('use_swanlab', False) or 'swanlab' in self.cfg:\n    self._callbacks.append(SwanLabCallback(self))\n```\n\n至此，你已经完成了SwanLab与PaddleYolo的集成！接下来，只需要在训练的配置文件中添加`use_swanlab: True`，即可开始可视化跟踪训练。",
    "405": "一级标题：PaddleDetection\n二级标题：3. 修改配置文件\n内容：\n我们以`yolov3_mobilenet_v1_roadsign`为例。\n\n在`configs/yolov3/yolov3_mobilenet_v1_roadsign.yml`文件中，在下面添加如下代码：\n\n```yaml\nuse_swanlab: true\nswanlab_project: PaddleYOLO # 可选\nswanlab_experiment_name: yolov3_mobilenet_v1_roadsign # 可选\nswanlab_description: 对PaddleYOLO的一次训练测试 # 可选\n# swanlab_workspace: swanhub # 组织名，可选\n```",
    "406": "一级标题：PaddleDetection\n二级标题：4. 开始训练\n内容：\n```bash\npython -u tools/train.py -c configs/yolov3/yolov3_mobilenet_v1_roadsign.yml --eval\n```\n\n在训练过程中，即可看到整个训练过程的日志，以及训练结束后自动生成的可视化图表。",
    "407": "一级标题：PaddleNLP\n二级标题：简介\n内容：\n[PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP) 是一款基于飞桨深度学习框架的大语言模型(LLM)开发套件，支持在多种硬件上进行高效的大模型训练、无损压缩以及高性能推理。PaddleNLP 具备简单易用和性能极致的特点，致力于助力开发者实现高效的大模型产业级应用。\n\n![paddlenlp-image](./paddlenlp/logo.png)\n\n你可以使用`PaddleNLP`快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。",
    "408": "一级标题：PaddleNLP\n二级标题：引入SwanLabCallback\n内容：\n```python\nfrom swanlab.integration.paddlenlp import SwanLabCallback\n```\n\n**SwanLabCallback**是适配于PaddleNLP的日志记录类。\n\n**SwanLabCallback**可以定义的参数有：\n\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。\n- 你也可以在外部通过`swanlab.init`创建项目，集成会将实验记录到你在外部创建的项目中。",
    "409": "一级标题：PaddleNLP\n二级标题：传入Trainer\n内容：\n```python (1,7,12)\nfrom swanlab.integration.paddlenlp import SwanLabCallback\nfrom paddlenlp.trainer import  TrainingArguments, Trainer\n\n...\n\n# 实例化SwanLabCallback\nswanlab_callback = SwanLabCallback(project=\"paddlenlp-demo\")\n\ntrainer = Trainer(\n    ...\n    # 传入callbacks参数\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n```",
    "410": "一级标题：PaddleNLP\n二级标题：完整案例代码\n内容：\n> 需要能连接上HuggingFace服务器下载数据集。\n\n```python {8,19,28}\n\"\"\"\n测试于：\npip install paddlepaddle-gpu==3.0.0 -i https://www.paddlepaddle.org.cn/packages/stable/cu126/\npip install paddlenlp==3.0.0b4\n\"\"\"\nfrom paddlenlp.trl import SFTConfig, SFTTrainer\nfrom datasets import load_dataset\nfrom swanlab.integration.paddlenlp import SwanLabCallback\n\ndataset = load_dataset(\"ZHUI/alpaca_demo\", split=\"train\")\n\ntraining_args = SFTConfig(\n    output_dir=\"Qwen/Qwen2.5-0.5B-SFT\",\n    device=\"gpu\",\n    per_device_train_batch_size=1,\n    logging_steps=20\n    )\n\nswanlab_callback = SwanLabCallback(\n    project=\"Qwen2.5-0.5B-SFT-paddlenlp\",\n    experiment_name=\"Qwen2.5-0.5B\",\n)\n\ntrainer = SFTTrainer(\n    args=training_args,\n    model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n    train_dataset=dataset,\n    callbacks=[swanlab_callback],\n)\ntrainer.train()\n```",
    "411": "一级标题：PaddleNLP\n二级标题：GUI效果展示\n内容：\n超参数自动记录：\n\n![ig-paddlenlp-gui-1](./paddlenlp/config.png)\n\n指标记录：\n\n![ig-paddlenlp-gui-2](./paddlenlp/chart.png)",
    "412": "一级标题：PaddleNLP\n二级标题：拓展：增加更多回调\n内容：\n试想一个场景，你希望在每个epoch结束时，让模型推理测试样例，并用swanlab记录推理的结果，那么你可以创建一个继承自`SwanLabCallback`的新类，增加或重构生命周期函数。比如：\n\n```python\nclass NLPSwanLabCallback(SwanLabCallback):    \n    def on_epoch_end(self, args, state, control, **kwargs):\n        test_text_list = [\"example1\", \"example2\"]\n        log_text_list = []\n        for text in test_text_list:\n            result = model(text)\n            log_text_list.append(swanlab.Text(result))\n            \n        swanlab.log({\"Prediction\": test_text_list}, step=state.global_step)\n```\n\n上面是一个在NLP任务下的新回调类，增加了`on_epoch_end`函数，它会在`transformers`训练的每个epoch结束时执行。",
    "413": "一级标题：PaddleYolo\n二级标题：介绍\n内容：\n[PaddleYolo](https://github.com/PaddlePaddle/PaddleYOLO) 是飞桨（PaddlePaddle）框架下的一个目标检测库，主要用于图像和视频中的物体检测。PaddleYOLO包含YOLO系列模型的相关代码，支持YOLOv3、PP-YOLO、PP-YOLOv2、PP-YOLOE、PP-YOLOE+、RT-DETR、YOLOX、YOLOv5、YOLOv6、YOLOv7、YOLOv8、YOLOv5u、YOLOv7u、YOLOv6Lite、RTMDet等模型\n\n你可以使用PaddleYolo快速进行目标检测模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n[Demo](https://swanlab.cn/@ZeyiLin/PaddleYOLO/runs/10zy8zickn2062kubch34/chart)",
    "414": "一级标题：PaddleYolo\n二级标题：引入SwanLabCallback\n内容：\n首先在你clone的PaddleYolo项目中，找到`ppdet/engine/callbacks.py`文件，在代码的底部添加如下代码：\n\n```python\nclass SwanLabCallback(Callback):\n    def __init__(self, model):\n        super(SwanLabCallback, self).__init__(model)\n\n        try:\n            import swanlab\n            self.swanlab = swanlab\n        except Exception as e:\n            logger.error('swanlab not found, please install swanlab. '\n                         'Use: `pip install swanlab`.')\n            raise e\n\n        self.swanlab_params = {k[8:]: v for k, v in model.cfg.items() if k.startswith(\"swanlab_\")}\n\n        self._run = None\n        if dist.get_world_size() < 2 or dist.get_rank() == 0:\n            _ = self.run\n            self.run.config.update(self.model.cfg)\n\n        self.best_ap = -1000.\n        self.fps = []\n\n    @property\n    def run(self):\n        if self._run is None:\n            self._run = self.swanlab.get_run() or self.swanlab.init(**self.swanlab_params)\n        return self._run\n\n    def on_step_end(self, status):\n        if dist.get_world_size() < 2 or dist.get_rank() == 0 and status['mode'] == 'train':\n            training_status = status['training_staus'].get()\n            batch_time = status['batch_time']\n            data_time = status['data_time']\n            batch_size = self.model.cfg['{}Reader'.format(status['mode'].capitalize())]['batch_size']\n\n            ips = float(batch_size) / float(batch_time.avg)\n            metrics = {\n                \"train/\" + k: float(v) for k, v in training_status.items()\n            }\n            metrics.update({\n                \"train/ips\": ips,\n                \"train/data_cost\": float(data_time.avg),\n                \"train/batch_cost\": float(batch_time.avg)\n            })\n\n            self.fps.append(ips)\n            self.run.log(metrics)\n\n    def on_epoch_end(self, status):\n        if dist.get_world_size() < 2 or dist.get_rank() == 0:\n            mode = status['mode']\n            epoch_id = status['epoch_id']\n            \n            if mode == 'train':\n                fps = sum(self.fps) / len(self.fps)\n                self.fps = []\n\n                end_epoch = self.model.cfg.epoch\n                if (epoch_id + 1) % self.model.cfg.snapshot_epoch == 0 or epoch_id == end_epoch - 1:\n                    save_name = str(epoch_id) if epoch_id != end_epoch - 1 else \"model_final\"\n                    tags = [\"latest\", f\"epoch_{epoch_id}\"]\n            \n            elif mode == 'eval':\n                fps = status['sample_num'] / status['cost_time']\n\n                merged_dict = {\n                    f\"eval/{key}-mAP\": map_value[0]\n                    for metric in self.model._metrics\n                    for key, map_value in metric.get_results().items()\n                }\n                merged_dict.update({\n                    \"epoch\": status[\"epoch_id\"],\n                    \"eval/fps\": fps\n                })\n\n                self.run.log(merged_dict)\n\n                if status.get('save_best_model'):\n                    for metric in self.model._metrics:\n                        map_res = metric.get_results()\n                        key = next((k for k in ['bbox', 'keypoint', 'mask'] if k in map_res), None)\n                        \n                        if not key:\n                            logger.warning(\"Evaluation results empty, this may be due to \"\n                                           \"training iterations being too few or not \"\n                                           \"loading the correct weights.\")\n                            return\n                        \n                        if map_res[key][0] >= self.best_ap:\n                            self.best_ap = map_res[key][0]\n                            save_name = 'best_model'\n                            tags = [\"best\", f\"epoch_{epoch_id}\"]\n\n    def on_train_end(self, status):\n        self.run.finish()\n```",
    "415": "一级标题：PaddleYolo\n二级标题：修改trainer代码\n内容：\n在`ppdet/engine/trainer.py`文件中，在`from .callbacks import`那一行添加SwanLabCallback：\n\n```python\nfrom .callbacks import Callback, ComposeCallback, LogPrinter, Checkpointer, VisualDLWriter, WandbCallback, SwanLabCallback\n```\n\n接着，我们找到`Trainer`类的`__init_callbacks`方法，在`if self.mode == 'train':`下添加如下代码：\n\n```python\nif self.cfg.get('use_swanlab', False) or 'swanlab' in self.cfg:\n    self._callbacks.append(SwanLabCallback(self))\n```\n\n至此，你已经完成了SwanLab与PaddleYolo的集成！接下来，只需要在训练的配置文件中添加`use_swanlab: True`，即可开始可视化跟踪训练。",
    "416": "一级标题：PaddleYolo\n二级标题：修改配置文件\n内容：\n我们以`yolov3_mobilenet_v1_roadsign`为例。\n\n在`configs/yolov3/yolov3_mobilenet_v1_roadsign.yml`文件中，在下面添加如下代码：\n\n```yaml\nuse_swanlab: true\nswanlab_project: PaddleYOLO # 可选\nswanlab_experiment_name: yolov3_mobilenet_v1_roadsign # 可选\nswanlab_description: 对PaddleYOLO的一次训练测试 # 可选\n# swanlab_workspace: swanhub # 组织名，可选\n```",
    "417": "一级标题：PaddleYolo\n二级标题：开始训练\n内容：\n```bash\npython -u tools/train.py -c configs/yolov3/yolov3_mobilenet_v1_roadsign.yml --eval\n```\n\n在训练过程中，即可看到整个训练过程的日志，以及训练结束后自动生成的可视化图表。\n\n![paddleyolo-image](/assets/ig-paddleyolo.png)",
    "418": "一级标题：PyTorch Lightning\n二级标题：PyTorch Lightning 简介\n内容：\nPyTorch Lightning是一个开源的机器学习库，它建立在 PyTorch 之上，旨在帮助研究人员和开发者更加方便地进行深度学习模型的研发。Lightning 的设计理念是将模型训练中的繁琐代码（如设备管理、分布式训练等）与研究代码（模型架构、数据处理等）分离，从而使研究人员可以专注于研究本身，而不是底层的工程细节。\n![pytorch-lightning-image](/assets/ig-pytorch-lightning.png)\n你可以使用PyTorch Lightning快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。",
    "419": "一级标题：PyTorch Lightning\n二级标题：引入SwanLabLogger\n内容：\n```python\nfrom swanlab.integration.pytorch_lightning import SwanLabLogger\n```\n**SwanLabLogger**是适配于PyTorch Lightning的日志记录类。\n**SwanLabLogger**可以定义的参数有：\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。\n- 你也可以在外部通过`swanlab.init`创建项目，集成会将实验记录到你在外部创建的项目中。",
    "420": "一级标题：PyTorch Lightning\n二级标题：传入Trainer\n内容：\n```python (6,11)\nimport pytorch_lightning as pl\n\n...\n\n# 实例化SwanLabLogger\nswanlab_logger = SwanLabLogger(project=\"lightning-visualization\")\n\ntrainer = pl.Trainer(\n    ...\n    # 传入callbacks参数\n    logger=swanlab_logger,\n)\n\ntrainer.fit(...)\n```",
    "421": "一级标题：PyTorch Lightning\n二级标题：完整案例代码\n内容：\n```python (1,65,70)\nfrom swanlab.integration.pytorch_lightning import SwanLabLogger\n\nimport importlib.util\nimport os\n\nimport pytorch_lightning as pl\nfrom torch import nn, optim, utils\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\n\n# define any number of nn.Modules (or use your current ones)\nencoder = nn.Sequential(nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 3))\ndecoder = nn.Sequential(nn.Linear(3, 128), nn.ReLU(), nn.Linear(128, 28 * 28))\n\n\n# define the LightningModule\nclass LitAutoEncoder(pl.LightningModule):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def training_step(self, batch, batch_idx):\n        # training_step defines the train loop.\n        # it is independent of forward\n        x, y = batch\n        x = x.view(x.size(0), -1)\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        loss = nn.functional.mse_loss(x_hat, x)\n        # Logging to TensorBoard (if installed) by default\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        # test_step defines the test loop.\n        # it is independent of forward\n        x, y = batch\n        x = x.view(x.size(0), -1)\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        loss = nn.functional.mse_loss(x_hat, x)\n        # Logging to TensorBoard (if installed) by default\n        self.log(\"test_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n        return optimizer\n\n\n# init the autoencoder\nautoencoder = LitAutoEncoder(encoder, decoder)\n\n\n# setup data\ndataset = MNIST(os.getcwd(), train=True, download=True, transform=ToTensor())\ntrain_dataset, val_dataset = utils.data.random_split(dataset, [55000, 5000])\ntest_dataset = MNIST(os.getcwd(), train=False, download=True, transform=ToTensor())\n\ntrain_loader = utils.data.DataLoader(train_dataset)\nval_loader = utils.data.DataLoader(val_dataset)\ntest_loader = utils.data.DataLoader(test_dataset)\n\nswanlab_logger = SwanLabLogger(\n    project=\"swanlab_example\",\n    experiment_name=\"example_experiment\",\n)\n\ntrainer = pl.Trainer(limit_train_batches=100, max_epochs=5, logger=swanlab_logger)\n\n\ntrainer.fit(model=autoencoder, train_dataloaders=train_loader, val_dataloaders=val_loader)\ntrainer.test(dataloaders=test_loader)\n```",
    "422": "一级标题：Torchtune\n二级标题：修改配置文件，引入SwanLabLogger\n内容：\n我们以使用`torchtune`微调Google的`gemma-2b`模型为例。\n\ntorchtune在微调一个模型时，需要训练者先准备一个配置文件，如用QLoRA微调Gemma-2b模型：[2B_qlora_single_device.yaml](https://github.com/pytorch/torchtune/blob/main/recipes/configs/gemma/2B_qlora_single_device.yaml)。\n\n下载后，编辑这个配置文件。我们在文件中找到下面的代码段：\n\n```yaml\n# Logging\nmetric_logger:\n  _component_: torchtune.utils.metric_logging.DiskLogger\n  log_dir: ${output_dir}\n```\n\n将该代码段替换为：\n\n```yaml\n# Logging\nmetric_logger:\n  _component_: swanlab.integration.torchtune.SwanLabLogger\n  project: \"gemma-fintune\"\n  experiment_name: \"gemma-2b\"\n  log_dir: ${output_dir}\n```\n\n其中，`_component_`对应的`swanlab.integration.torchtune.SwanLabLogger`是适配于PyTorch torchtune的日志记录类。而`project`、`experiment_name`等则是创建SwanLab项目传入的参数，支持传入的参数与[swanlab.init](/zh/api/py-init.html)规则一致。",
    "423": "一级标题：Torchtune\n二级标题：开始训练\n内容：\n```bash\ntune run lora_finetune_single_device --config 2B_qlora_single_device.yaml\n```\n\n```\n一级标题：PyTorch\n二级标题：PyTorch简介\n内容：\n在学术研究者当中，[PyTorch](https://pytorch.org/) 是最流行的 Python 深度学习框架。\n你可以使用PyTorch进行深度学习模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n::: warning Pytorch生态的其他集成\n- [Lightning](/guide_cloud/integration/integration-pytorch-lightning.md)\n- [Torchtune](/guide_cloud/integration/integration-pytorch-torchtune.md)\n:::\n\n二级标题：记录Tensor图像\n内容：\n你可以将带有图像数据的PyTorch `Tensors`传递给`swanlab.Image`，`swanlab.Image`将使用`torchvision`把它们转换成图像：\n```python\nimage_tensors = ...  # shape为[B, C, H, W]的Tensor图像\nswanlab.log({\"examples\": [swanlab.Image(im) for im in image_tensors]})\n```\n```",
    "424": "一级标题：Ray\n二级标题：Ray 简介\n内容：\nRay 是一个分布式计算框架，专为大规模并行任务和强化学习应用设计。它由加州大学伯克利分校的研究团队开发，旨在简化构建高性能、可扩展的分布式应用程序的过程。Ray 支持 Python 和 Java，并且可以轻松集成到现有的机器学习、数据处理和强化学习工作流中。\n\n![ray](./ray/logo.png)\n\nSwanLab 支持 Ray 的实验记录，通过 `SwanLabLoggerCallback` 可以方便地记录实验指标和超参数。",
    "425": "一级标题：Ray\n二级标题：引入SwanLabCallback\n内容：\n```python\nfrom swanlab.integration.ray import SwanLabLoggerCallback\n```\n\n`SwanLabLoggerCallback` 是适配于 `Ray` 的日志记录类。\n\n`SwanLabLoggerCallback`可以定义的参数有：\n- `project`: 项目名称\n- `workspace`: 工作空间名称\n- 其他和`swanlab.init`一致的参数",
    "426": "一级标题：Ray\n二级标题：与`tune.Tuner`集成\n内容：\n```python\ntuner = tune.Tuner(\n    ...\n    run_config=tune.RunConfig(\n        callbacks=[SwanLabLoggerCallback(project=\"Ray_Project\")],\n    ),\n)\n```",
    "427": "一级标题：Ray\n二级标题：完整案例\n内容：\n```python\nimport random\nfrom ray import tune\nfrom swanlab.integration.ray import SwanLabLoggerCallback\n\ndef train_func(config):\n    offset = random.random() / 5\n    for epoch in range(2, config[\"epochs\"]):\n        acc = 1 - (2 + config[\"lr\"]) ** -epoch - random.random() / epoch - offset\n        loss = (2 + config[\"lr\"]) ** -epoch + random.random() / epoch + offset\n        tune.report({\"acc\": acc, \"loss\": loss})\n\n\ntuner = tune.Tuner(\n    train_func,\n    param_space={\n        \"lr\": tune.grid_search([0.001, 0.01, 0.1, 1.0]),\n        \"epochs\": 10,\n    },\n    run_config=tune.RunConfig(\n        callbacks=[SwanLabLoggerCallback(project=\"Ray_Project\")],\n    ),\n)\nresults = tuner.fit()\n```\n\n![ray-tune](./ray/demo.png)",
    "428": "一级标题：ROLL\n二级标题：简介\n内容：\nROLL 是一个高效且用户友好的强化学习库，专为利用大规模 GPU 资源的大型语言模型 (LLM) 而设计。它显著提升了 LLM 在人类偏好对齐、复杂推理和多轮代理交互等关键领域的性能。\n\nROLL 利用 Ray 的多角色分布式架构实现灵活的资源分配和异构任务调度，并集成 Megatron-Core、SGLang 和 vLLM 等尖端技术来加速模型训练和推理。\n\n![ROLL](./roll/logo.png)",
    "429": "一级标题：ROLL\n二级标题：使用SwanLab\n内容：\n在ROLL中使用SwanLab非常简单，只需要设置一些参数即可，详情参考 [agentic_pipeline_config.yaml](https://github.com/alibaba/ROLL/blob/main/tests/pipeline/agentic_pipeline_config.yaml) 中的`track_with: swanlab`部分。",
    "430": "一级标题：Stable-Baseline3\n二级标题：简介\n内容：\nStable Baselines3 (SB3) 是一个强化学习的开源库，基于 PyTorch 框架构建。它是 Stable Baselines 项目的继任者，旨在提供一组可靠且经过良好测试的RL算法实现，便于研究和应用。StableBaseline3主要被应用于机器人控制、游戏AI、自动驾驶、金融交易等领域。\n![sb3](/assets/ig-sb3.png)\n你可以使用sb3快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。",
    "431": "一级标题：Stable-Baseline3\n二级标题：引入SwanLabCallback\n内容：\n```python\nfrom swanlab.integration.sb3 import SwanLabCallback\n```\n**SwanLabCallback**是适配于 Stable Baselines3 的日志记录类。\n**SwanLabCallback**可以定义的参数有：\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。\n- 你也可以在外部通过`swanlab.init`创建项目，集成会将实验记录到你在外部创建的项目中。",
    "432": "一级标题：Stable-Baseline3\n二级标题：传入model.learn\n内容：\n```python (1,7)\nfrom swanlab.integration.sb3 import SwanLabCallback\n\n...\n\nmodel.learn(\n    ...\n    callback=SwanLabCallback(),\n)\n```\n在`model.learn`的`callback`参数传入`SwanLabCallback`实例，即可开始跟踪。",
    "433": "一级标题：Stable-Baseline3\n二级标题：完整案例代码\n内容：\n下面是一个PPO模型的简单训练案例，使用SwanLab做训练可视化和监控：\n```python (6,31)\nimport gymnasium as gym\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nimport swanlab\nfrom swanlab.integration.sb3 import SwanLabCallback\n\n\nconfig = {\n    \"policy_type\": \"MlpPolicy\",\n    \"total_timesteps\": 25000,\n    \"env_name\": \"CartPole-v1\",\n}\n\n\ndef make_env():\n    env = gym.make(config[\"env_name\"], render_mode=\"rgb_array\")\n    env = Monitor(env)\n    return env\n\n\nenv = DummyVecEnv([make_env])\nmodel = PPO(\n    config[\"policy_type\"],\n    env,\n    verbose=1,\n)\n\nmodel.learn(\n    total_timesteps=config[\"total_timesteps\"],\n    callback=SwanLabCallback(\n        project=\"PPO\",\n        experiment_name=\"MlpPolicy\",\n        verbose=2,\n    ),\n)\n\nswanlab.finish()\n\n```",
    "434": "一级标题：Sentence Transformers\n二级标题：引入SwanLabCallback\n内容：\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\n```\n\n**SwanLabCallback**是适配于HuggingFace系列工具（Transformers等）的日志记录类。\n\n**SwanLabCallback**可以定义的参数有：\n\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。\n- 你也可以在外部通过`swanlab.init`创建项目，集成会将实验记录到你在外部创建的项目中。",
    "435": "一级标题：Sentence Transformers\n二级标题：传入Trainer\n内容：\n```python (1,7,12)\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom sentence_transformers import SentenceTransformer, SentenceTransformerTrainer\n\n...\n\n# 实例化SwanLabCallback\nswanlab_callback = SwanLabCallback(project=\"hf-visualization\")\n\ntrainer = SentenceTransformerTrainer(\n    ...\n    # 传入callbacks参数\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n```",
    "436": "一级标题：Sentence Transformers\n二级标题：完整案例代码\n内容：\n```python (4,12,19)\nfrom datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer, SentenceTransformerTrainer\nfrom sentence_transformers.losses import MultipleNegativesRankingLoss\nfrom swanlab.integration.transformers import SwanLabCallback\n\nmodel = SentenceTransformer(\"bert-base-uncased\")\n\ntrain_dataset = load_dataset(\"sentence-transformers/all-nli\", \"pair\", split=\"train[:10000]\")\neval_dataset = load_dataset(\"sentence-transformers/all-nli\", \"triplet\", split=\"dev\")\nmnrl_loss = MultipleNegativesRankingLoss(model)\n\nswanlab_callback = SwanLabCallback(project=\"sentence-transformers\", experiment_name=\"bert-all-nli\")\n\ntrainer = SentenceTransformerTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    loss=mnrl_loss,\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n```",
    "437": "一级标题：Modelscope Swift\n二级标题：介绍\n内容：\n> SwanLab已经与Swift官方集成，见：[#3142](https://github.com/modelscope/ms-swift/pull/3142)  \n> 可视化在线Demo：[swift-robot](https://swanlab.cn/@ZeyiLin/swift-robot/runs/9lc9rmmwm4hh7ay1vkzd7/chart)\n\n[Modelscope魔搭社区](https://modelscope.cn/) 的 [Swift](https://github.com/modelscope/swift) 是一个集模型训练、微调、推理、部署于一体的框架。\n\n![logo](./swift/logo.png)\n\n🍲 **ms-swift** 是 ModelScope 社区提供的官方框架，用于微调和部署大型语言模型和多模态大型模型。它目前支持 **450+** 大型模型和 **150+** 多模态大型模型的训练（预训练、微调、人工对齐）、推理、评估、量化和部署。\n\n🍔 此外，ms-swift 还采用了**最新的训练技术**，包括 **LoRA、QLoRA、Llama-Pro、LongLoRA、GaLore、Q-GaLore、LoRA+、LISA、DoRA、FourierFt、ReFT、UnSloth 和 Liger 等轻量级技术**，以及 **DPO、GRPO、RM、PPO、KTO、CPO、SimPO 和 ORPO** 等人工对齐训练方法。\n\nms-swift 支持使用 vLLM 和 LMDeploy 加速推理、评估和部署模块，并支持使用 GPTQ、AWQ 和 BNB 等技术进行模型量化。此外，ms-swift 还提供了基于 Gradio 的 Web UI 和丰富的最佳实践。\n\n你可以使用Swift快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n[[toc]]",
    "438": "一级标题：Modelscope Swift\n二级标题：安装ms-swift和swanlab\n内容：\n安装ms-swift（>=3.1.1）：\n\n```bash\npip install ms-swift\n```\n\n安装swanlab：\n\n```bash\npip install swanlab\n```",
    "439": "一级标题：Modelscope Swift\n二级标题：CLI微调\n内容：\n你只需要在ms-swift的CLI中添加`--report_to`和`--swanlab_project`两个参数，即可使用SwanLab进行实验跟踪与可视化：\n\n```bash\nswift sft \\\n    ...\n    --report_to swanlab \\  # [!code ++]\n    --swanlab_project swift-robot \\  # [!code ++]\n    ...\n```\n\n下面是在swift官方的CLI微调案例，中结合SwanLab的示例（见代码最后）：\n\n```bash {29-30}\n# 22GB\nCUDA_VISIBLE_DEVICES=0 \\\nswift sft \\\n    --model Qwen/Qwen2.5-7B-Instruct \\\n    --train_type lora \\\n    --dataset 'AI-ModelScope/alpaca-gpt4-data-zh#500' \\\n              'AI-ModelScope/alpaca-gpt4-data-en#500' \\\n              'swift/self-cognition#500' \\\n    --torch_dtype bfloat16 \\\n    --num_train_epochs 1 \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --learning_rate 1e-4 \\\n    --lora_rank 8 \\\n    --lora_alpha 32 \\\n    --target_modules all-linear \\\n    --gradient_accumulation_steps 16 \\\n    --eval_steps 50 \\\n    --save_steps 50 \\\n    --save_total_limit 5 \\\n    --logging_steps 5 \\\n    --max_length 2048 \\\n    --output_dir output \\\n    --system 'You are a helpful assistant.' \\\n    --warmup_ratio 0.05 \\\n    --dataloader_num_workers 4 \\\n    --model_author swift \\\n    --model_name swift-robot \\\n    --report_to swanlab \\\n    --swanlab_project swift-robot\n```\n\n运行指令后，就可以在SwanLab看到训练过程：\n\n![](./swift/dashboard-1.png)\n\n支持的完整参数：\n\n- `swanlab_token`: SwanLab的api-key\n- `swanlab_project`: swanlab的project\n- `swanlab_workspace`: 默认为None，会使用api-key对应的username\n- `swanlab_exp_name`: 实验名，可以为空，为空时默认传入--output_dir的值\n- `swanlab_mode`: 可选cloud和local，云模式或者本地模式",
    "440": "一级标题：Modelscope Swift\n二级标题：WebUI微调\n内容：\nSwift不仅支持CLI微调，还为开发者提供非常方便的**WebUI（网页端）**的微调界面。你同样可以在WebUI当中启动SwanLab跟踪实验。\n\n启动WebUI方式：\n\n```bash\nswift web-ui\n```\n\n启动后，会自动打开浏览器，显示微调界面（或者访问 `http://localhost:7860/` ）：\n\n![ig-swift-2](./swift/dashboard-2.png)\n\n在下方的「训练记录」模块中，在`训练记录方式`部分选择`swanlab`：\n\n![ig-swift-3](./swift/webui-1.png)\n\n你还可以在「训练记录」模块的其他填写更细致的swanlab参数，包括：\n\n- `swanlab_token`: SwanLab的api-key\n- `swanlab_project`: swanlab的project\n- `swanlab_workspace`: 默认为None，会使用api-key对应的username\n- `swanlab_exp_name`: 实验名，可以为空，为空时默认传入--output_dir的值\n- `swanlab_mode`: 可选cloud和local，云模式或者本地模式\n\n然后，点击「🚀开始训练」按钮，即可启动训练，并使用SwanLab跟踪实验：\n\n![ig-swift-4](./swift/webui-2.png)",
    "441": "一级标题：Modelscope Swift\n二级标题：Python代码微调\n内容：\n**3.1 引入SwanLabCallback**\n\n因为`Swift`的`trainer`集成自`transformers`，所以可以直接使用`swanlab`与`huggingface`集成的`SwanLabCallback`：\n\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\n```\n\nSwanLabCallback可以定义的参数有：\n\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。 你也可以在外部通过swanlab.init创建项目，集成会将实验记录到你在外部创建的项目中。\n\n**3.2 引入Trainer**\n\n```python {1,7,11}\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom swift import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\n···\n\n#实例化SwanLabCallback\nswanlab_callback = SwanLabCallback(project=\"swift-visualization\")\n\ntrainer = Seq2SeqTrainer(\n    ...\n    callbacks=[swanlab_callback],\n    )\n\ntrainer.train()\n```\n\n**3.3 使用SwanLabCallback**\n\n> Lora微调一个Qwen2-0.5B模型\n\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom swift import Seq2SeqTrainer, Seq2SeqTrainingArguments\nfrom swift.llm import get_model_tokenizer, load_dataset, get_template, EncodePreprocessor\nfrom swift.utils import get_logger, find_all_linears, get_model_parameter_info, plot_images, seed_everything\nfrom swift.tuners import Swift, LoraConfig\nfrom swift.trainers import Seq2SeqTrainer, Seq2SeqTrainingArguments\nfrom functools import partial\nimport os\n\nlogger = get_logger()\nseed_everything(42)\n\n# Hyperparameters for training\n# model\nmodel_id_or_path = 'Qwen/Qwen2.5-3B-Instruct'  # model_id or model_path\nsystem = 'You are a helpful assistant.'\noutput_dir = 'output'\n\n# dataset\ndataset = ['AI-ModelScope/alpaca-gpt4-data-zh#500', 'AI-ModelScope/alpaca-gpt4-data-en#500',\n           'swift/self-cognition#500']  # dataset_id or dataset_path\ndata_seed = 42\nmax_length = 2048\nsplit_dataset_ratio = 0.01  # Split validation set\nnum_proc = 4  # The number of processes for data loading.\n# The following two parameters are used to override the placeholders in the self-cognition dataset.\nmodel_name = ['小黄', 'Xiao Huang']  # The Chinese name and English name of the model\nmodel_author = ['魔搭', 'ModelScope']  # The Chinese name and English name of the model author\n\n# lora\nlora_rank = 8\nlora_alpha = 32\n\n# training_args\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=output_dir,\n    learning_rate=1e-4,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_checkpointing=True,\n    weight_decay=0.1,\n    lr_scheduler_type='cosine',\n    warmup_ratio=0.05,\n    logging_first_step=True,\n    save_strategy='steps',\n    save_steps=50,\n    eval_strategy='steps',\n    eval_steps=50,\n    gradient_accumulation_steps=16,\n    num_train_epochs=1,\n    metric_for_best_model='loss',\n    save_total_limit=5,\n    logging_steps=5,\n    dataloader_num_workers=1,\n    data_seed=data_seed,\n)\n\noutput_dir = os.path.abspath(os.path.expanduser(output_dir))\nlogger.info(f'output_dir: {output_dir}')\n\n# Obtain the model and template, and add a trainable Lora layer on the model.\nmodel, tokenizer = get_model_tokenizer(model_id_or_path)\nlogger.info(f'model_info: {model.model_info}')\ntemplate = get_template(model.model_meta.template, tokenizer, default_system=system, max_length=max_length)\ntemplate.set_mode('train')\n\ntarget_modules = find_all_linears(model)\nlora_config = LoraConfig(task_type='CAUSAL_LM', r=lora_rank, lora_alpha=lora_alpha,\n                         target_modules=target_modules)\nmodel = Swift.prepare_model(model, lora_config)\nlogger.info(f'lora_config: {lora_config}')\n\n# Print model structure and trainable parameters.\nlogger.info(f'model: {model}')\nmodel_parameter_info = get_model_parameter_info(model)\nlogger.info(f'model_parameter_info: {model_parameter_info}')\n\n# Download and load the dataset, split it into a training set and a validation set,\n# and encode the text data into tokens.\ntrain_dataset, val_dataset = load_dataset(dataset, split_dataset_ratio=split_dataset_ratio, num_proc=num_proc,\n        model_name=model_name, model_author=model_author, seed=data_seed)\n\nlogger.info(f'train_dataset: {train_dataset}')\nlogger.info(f'val_dataset: {val_dataset}')\nlogger.info(f'train_dataset[0]: {train_dataset[0]}')\n\ntrain_dataset = EncodePreprocessor(template=template)(train_dataset, num_proc=num_proc)\nval_dataset = EncodePreprocessor(template=template)(val_dataset, num_proc=num_proc)\nlogger.info(f'encoded_train_dataset[0]: {train_dataset[0]}')\n\n# Print a sample\ntemplate.print_inputs(train_dataset[0])\n\n# Get the trainer and start the training.\nmodel.enable_input_require_grads()  # Compatible with gradient checkpointing\n\nswanlab_callback = SwanLabCallback(\n    project=\"swift-visualization\",\n    experiment_name=\"lora-qwen2-0.5b\",\n    description=\"Lora微调一个Qwen2-0.5B模型\"\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    data_collator=template.data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    template=template,\n    callbacks=[swanlab_callback],\n)\ntrainer.train()\n\nlast_model_checkpoint = trainer.state.last_model_checkpoint\nlogger.info(f'last_model_checkpoint: {last_model_checkpoint}')\n```\n\n运行可视化结果：\n\n![ig-swift-5](./swift/dashboard-3.png)",
    "442": "一级标题：Tensorboard\n二级标题：Tensorboard介绍\n内容：\n[TensorBoard](https://github.com/tensorflow/tensorboard) 是 Google TensorFlow 提供的一个可视化工具，用于帮助理解、调试和优化机器学习模型。它通过图形界面展示训练过程中的各种指标和数据，让开发者更直观地了解模型的性能和行为。\n\n![TensorBoard](/assets/ig-tensorboard.png)\n\n:::warning 其他工具的同步教程\n\n- [Wandb](/guide_cloud/integration/integration-wandb.md)\n- [MLFlow](/guide_cloud/integration/integration-mlflow.md)\n:::\n\n你可以用两种方式将使用Tensorboard跟踪的项目同步到SwanLab：\n\n- **同步跟踪**：如果你现在的项目使用了Tensorboard进行实验跟踪，你可以使用`swanlab.sync_tensorboardX()`或`swanlab.sync_tensorboard_torch()`命令，在运行训练脚本时同步记录指标到SwanLab。\n- **转换已存在的项目**：如果你想要将Tensorboard上的项目复制到SwanLab，你可以使用`swanlab convert`，将存放TFevent文件的目录转换成SwanLab项目。\n\n::: info\n在当前版本暂仅支持转换标量和图像图表。\n:::\n\n[[toc]]",
    "443": "一级标题：Tensorboard\n二级标题：同步跟踪\n内容：\n## 1. 同步跟踪\n\n### 1.1 TensorboardX: 添加sync_tensorboardX命令\n\n如果你使用的是TensorboardX，可以在代码执行`tensorboardX.SummaryWriter()`之前的任何位置，添加一行`swanlab.sync_tensorboardX()`命令，即可在训练时同步记录指标到SwanLab。\n\n```python\nimport swanlab\nfrom tensorboardX import SummaryWriter\n\nswanlab.sync_tensorboardX()\n\nwriter = SummaryWriter(log_dir='./runs')\n```\n\n### 1.2 PyTorch: 添加sync_tensorboard_torch命令\n\n如果你使用的是PyTorch自带的tensorboard，那么可以在代码执行`torch.utils.tensorboard.SummaryWriter()`之前的任何位置，添加一行`swanlab.sync_tensorboard_torch()`命令，即可在训练时同步记录指标到SwanLab。\n\n```python\nimport swanlab\nimport torch\n\nswanlab.sync_tensorboard_torch()\n\nwriter = torch.utils.tensorboard.SummaryWriter(log_dir='./runs')\n```\n\n### 1.3 另一种写法\n\n你也可以先手动初始化swanlab，再运行tensorboard的代码。\n\n::: code-group\n\n```python [TensorboardX]\nimport swanlab\nfrom tensorboardX import SummaryWriter\n\nswanlab.init(...)\nswanlab.sync_tensorboardX()\n\n...\n\nwriter = SummaryWriter(log_dir='./runs')\n```\n\n```python [PyTorch]\nimport swanlab\nfrom torch.utils.tensorboard import SummaryWriter\n\nswanlab.init(...)\nswanlab.sync_tensorboard_torch()\n\n...\n\nwriter = SummaryWriter(log_dir='./runs')\n```\n:::\n\n### 1.4 测试代码\n\n::: code-group\n\n```python [TensorboardX]\nimport swanlab\nfrom tensorboardX import SummaryWriter\n\nswanlab.sync_tensorboardX()\n\nwriter = SummaryWriter(log_dir='./runs')\n\nepochs = 10\noffset = random.random() / 5\nfor epoch in range(2, epochs):\n  acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n  loss = 2 ** -epoch + random.random() / epoch + offset\n\n  writer.add_scalar(\"acc\", acc, epoch)\n  writer.add_scalar(\"loss\", loss, epoch)\n```\n\n```python [PyTorch]\nimport swanlab\nfrom torch.utils.tensorboard import SummaryWriter\n\nswanlab.sync_tensorboard_torch()\n\nwriter = SummaryWriter(log_dir='./runs')\n\nepochs = 10\noffset = random.random() / 5\nfor epoch in range(2, epochs):\n  acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n  loss = 2 ** -epoch + random.random() / epoch + offset\n\n  writer.add_scalar(\"acc\", acc, epoch)\n  writer.add_scalar(\"loss\", loss, epoch)\n```\n\n:::",
    "444": "一级标题：Tensorboard\n二级标题：转换已存在的项目\n内容：\n## 2. 转换已存在的项目\n\n### 2.1 方式一：命令行转换\n\n```bash\nswanlab convert -t tensorboard --tb_logdir [TFEVENT_LOGDIR]\n```\n\n支持的参数如下：\n\n- `-t`: 转换类型，可选wandb、tensorboard和mlflow。\n- `-p`: SwanLab项目名。\n- `-w`: SwanLab工作空间名。\n- `--cloud`: (bool) 是否上传模式为\"cloud\"，默认为True\n- `-l`: logdir路径。\n- `--tb_logdir`: Tensorboard日志文件路径。\n\n这里的`[TFEVENT_LOGDIR]`是指你先前用Tensorboard记录实验时，生成的日志文件路径。\n\nSwanLab Converter将会自动检测文件路径及其子目录下的`tfevent`文件（默认子目录深度为3），并为每个`tfevent`文件创建一个SwanLab实验。\n\n### 2.2 方式二：代码内转换\n\n```python\nfrom swanlab.converter import TFBConverter\n\ntfb_converter = TFBConverter(convert_dir=\"[TFEVENT_LOGDIR]\")\ntfb_converter.run()\n```\n\n效果与命令行转换一致。\n\n### 2.3 参数列表\n\n| 参数 | 对应CLI参数       | 描述                  | \n| ---- | ---------- | --------------------- | \n| convert_dir    | -      | Tfevent文件路径       | \n| project    | -p, --project      | SwanLab项目名       |\n| workspace  | -w, --workspace      | SwanLab工作空间名 |\n| cloud    | --cloud      | 是否使用云端版，默认为True       | \n| logdir    | -l, --logdir      | SwanLab日志文件保存路径       | \n\n例子：\n\n```python\nfrom swanlab.converter import TFBConverter\n\ntfb_converter = TFBConverter(\n    convert_dir=\"./runs\",\n    project=\"Tensorboard-Converter\",\n    workspace=\"SwanLab\",\n    logdir=\"./logs\",\n    )\ntfb_converter.run()\n```\n\n与之作用相同的CLI：\n```bash\nswanlab convert -t tensorboard --tb_logdir ./runs -p Tensorboard-Converter -w SwanLab -l ./logs\n```\n\n执行上面的脚本，将会在`SwanLab`空间下，创建一个名为`Tensorboard-Converter`的项目，将`./runs`目录下tfevent文件创建为一个个swanlab实验，并将swanlab运行时产生的日志保存在`./logs`目录下。",
    "445": "一级标题：Ultralytics\n二级标题：Ultralytics YOLOv8 简介\n内容：\n[Ultralytics](https://github.com/ultralytics/ultralytics) YOLOv8 是一款尖端、最先进的 （SOTA） 模型，它建立在以前 YOLO 版本的成功基础上，并引入了新功能和改进，以进一步提高性能和灵活性。YOLOv8 设计为快速、准确且易于使用，使其成为各种对象检测和跟踪、实例分割、图像分类和姿态估计任务的绝佳选择。\n\n你可以使用Ultralytics快速进行计算机视觉模型训练，同时使用SwanLab进行实验跟踪与可视化。\n\n下面介绍两种引入SwanLab的方式：  \n1. `add_swanlab_callback`：无需修改源码，适用于单卡训练场景\n2. `return_swanlab_callback`：需要修改源码，适用于单卡以及多卡DDP训练场景",
    "446": "一级标题：Ultralytics\n二级标题：引入add_swanlab_callback\n内容：\n```python\nfrom swanlab.integration.ultralytics import add_swanlab_callback\n```\n\n`add_swanlab_callback`的作用是为Ultralytics模型添加回调函数，以在模型训练的各个生命周期执行SwanLab记录。",
    "447": "一级标题：Ultralytics\n二级标题：代码案例\n内容：\n下面是使用yolov8n模型在coco数据集上的训练，只需将model传入`add_swanlab_callback`函数，即可完成与SwanLab的集成。\n\n```python {9}\nfrom ultralytics import YOLO\nfrom swanlab.integration.ultralytics import add_swanlab_callback\n\n\nif __name__ == \"__main__\":\n    model = YOLO(\"yolov8n.yaml\")\n    model.load()\n    # 添加swanlab回调\n    add_swanlab_callback(model)\n\n    model.train(\n        data=\"./coco128.yaml\",\n        epochs=3, \n        imgsz=320,\n    )\n```\n\n如果需要自定义SwanLab的项目、实验名等参数，则可以在`add_swanlab_callback`中添加：\n\n```python\nadd_swanlab_callback(\n    model,\n    project=\"ultralytics\",\n    experiment_name=\"yolov8n\",\n    description=\"yolov8n在coco128数据集上的训练。\",\n    mode=\"local\",\n    )\n```",
    "448": "一级标题：Ultralytics\n二级标题：多卡训练/DDP训练\n内容：\n> swanlab>=0.3.7\n\n在Ultralytics多卡训练的场景下，由于启动训练的方式与单卡完全不同，所以需要用一种不同的方式接入SwanLab回调。\n\n这是一个ultralytics开启DDP训练的样例代码：\n\n```python\nfrom ultralytics import YOLO\n\nif __name__ == \"__main__\":\n    model = YOLO(\"yolov8n.pt\")\n\n    model.train(\n        data=\"./coco128.yaml\",\n        epochs=3, \n        imgsz=320,\n        # 开启DDP\n        device=[0,1],\n    )\n```\n\n我们需要修改ultralytics的源码，去到`ultralytics/utils/callbacks/base.py`，找到`add_integration_callbacks`函数，添加下面的三行代码：\n\n```python (15,16,18)\ndef add_integration_callbacks(instance):\n    ...\n    \n    # Load training callbacks\n    if \"Trainer\" in instance.__class__.__name__:\n        from .clearml import callbacks as clear_cb\n        from .comet import callbacks as comet_cb\n        from .dvc import callbacks as dvc_cb\n        from .mlflow import callbacks as mlflow_cb\n        from .neptune import callbacks as neptune_cb\n        from .raytune import callbacks as tune_cb\n        from .tensorboard import callbacks as tb_cb\n        from .wb import callbacks as wb_cb\n\n        from swanlab.integration.ultralytics import return_swanlab_callback\n        sw_cb = return_swanlab_callback()\n\n        callbacks_list.extend([..., sw_cb])\n```\n\n然后运行，就可以在ddp下正常跟踪实验了。\n\n如果需要自定义SwanLab的项目、实验名等参数，则可以在`return_swanlab_callback`中添加：\n\n```python\nreturn_swanlab_callback(\n    model,\n    project=\"ultralytics\",\n    experiment_name=\"yolov8n\",\n    description=\"yolov8n在coco128数据集上的训练。\",\n    mode=\"local\",\n    )\n```\n\n:::warning ps\n1. 写入源码之后，之后运行就不需要在训练脚本中增加`add_swanlab_callback`了。\n2. 项目名由model.train()的project参数定义，实验名由name参数定义。\n:::",
    "449": "一级标题：Ultralytics\n二级标题：代码案例\n内容：\n```python\nfrom ultralytics import YOLO\n\nif __name__ == \"__main__\":\n    model = YOLO(\"yolov8n.pt\")\n\n    model.train(\n        data=\"./coco128.yaml\",\n        epochs=3, \n        imgsz=320,\n        # 开启DDP\n        device=[0,1,2,3],\n        # 可以通过project参数设置SwanLab的project，name参数设置SwanLab的experiment_name\n        project=\"Ultralytics\",\n        name=\"yolov8n\"\n    )\n```",
    "450": "一级标题：Unsloth\n二级标题：引入SwanLabCallback\n内容：\n```markdown\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\n```\n\nSwanLabCallback是适配于Transformers的日志记录类。\n\nSwanLabCallback可以定义的参数有：\n\n- project、experiment_name、description 等与 swanlab.init 效果一致的参数, 用于SwanLab项目的初始化。\n- 你也可以在外部通过swanlab.init创建项目，集成会将实验记录到你在外部创建的项目中。\n```",
    "451": "一级标题：Unsloth\n二级标题：传入Trainer\n内容：\n```markdown\n```python {1,7,12}\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom trl import GRPOTrainer\n\n...\n\n# 实例化SwanLabCallback\nswanlab_callback = SwanLabCallback(project=\"unsloth-example\")\n\ntrainer = GRPOTrainer(\n    ...\n    # 传入callbacks参数\n    callbacks=[swanlab_callback],\n)\n\ntrainer.train()\n```\n```",
    "452": "一级标题：Unsloth\n二级标题：与Unsloth结合的案例模板\n内容：\n```markdown\n```python\nfrom swanlab.integration.transformers import SwanLabCallback\nfrom unsloth import FastLanguageModel, PatchFastRL\n\nPatchFastRL(\"GRPO\", FastLanguageModel)  # 对 TRL 进行补丁处理\nfrom trl import GRPOConfig, GRPOTrainer, ModelConfig, TrlParser\n\n...\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n...\n) \n\n# PEFT 模型\nmodel = FastLanguageModel.get_peft_model(\n...\n)\n\n# 实例化SwanLabCallback\nswanlab_callback = SwanLabCallback(\n  project=\"trl_integration\",\n  experiment_name=\"qwen2.5-sft\",\n  description=\"测试swanlab和trl的集成\",\n  config={\"framework\": \"🤗TRL\"},\n)\n\n# 定义GRPOTrainer\ntrainer = GRPOTrainer(\n    ...\n    # 传入callbacks参数\n    callbacks=[swanlab_callback],\n)\n\n#开启训练！\ntrainer.train()\n```\n```",
    "453": "一级标题：verl\n二级标题：verl 简介\n内容：\n[verl](https://github.com/volcengine/verl) 是一个灵活、高效且可用于生产环境的强化学习（RL）训练框架，专为大型语言模型（LLMs）的后训练设计。它由字节跳动火山引擎团队开源，是 [HybridFlow](https://arxiv.org/abs/2409.19256) 论文的开源实现。\n\n<div style=\"text-align: center;\">\n    <img src=\"./verl/verl_logo.svg\" alt=\"verl_logo\" style=\"width: 70%;\">\n</div>\n\n**verl 具有以下特点，使其灵活且易于使用：**\n\n1. **易于扩展的多样化 RL 算法**：Hybrid 编程模型结合了单控制器和多控制器范式的优点，能够灵活表示并高效执行复杂的后训练数据流。用户只需几行代码即可构建 RL 数据流。\n\n2. **与现有 LLM 基础设施无缝集成的模块化 API**：通过解耦计算和数据依赖，verl 能够与现有的 LLM 框架（如 PyTorch FSDP、Megatron-LM 和 vLLM）无缝集成。此外，用户可以轻松扩展到其他 LLM 训练和推理框架。\n\n3. **灵活的设备映射和并行化**：支持将模型灵活地映射到不同的 GPU 组上，以实现高效的资源利用，并在不同规模的集群上具有良好的扩展性。\n\n4. **与流行的 HuggingFace 模型轻松集成**：verl 能够方便地与 HuggingFace 模型进行集成。\n\n**verl 也具有以下优势，使其运行速度快：**\n\n1. **最先进的吞吐量**：通过无缝集成现有的 SOTA LLM 训练和推理框架，verl 实现了高生成和训练吞吐量。\n\n2. **基于 3D-HybridEngine 的高效 Actor 模型重分片**：消除了内存冗余，并显著减少了在训练和生成阶段之间切换时的通信开销。\n\n更多信息可参考如下链接\n\n> * verl GitHub仓库链接: [https://github.com/volcengine/verl](https://github.com/volcengine/verl)\n> * 官方文档: [https://verl.readthedocs.io/en/latest/index.html](https://verl.readthedocs.io/en/latest/index.html)\n> * HybridFlow论文地址: [https://arxiv.org/pdf/2409.19256v2](https://arxiv.org/pdf/2409.19256v2)\n\n\n你可以使用verl快速进行大模型强化学习训练，同时使用SwanLab进行实验跟踪与可视化。",
    "454": "一级标题：verl\n二级标题：环境安装\n内容：\n需要环境：\n\n* Python: Version >= 3.9\n\n* CUDA: Version >= 12.1\n\n参考verl官方文档安装：[https://verl.readthedocs.io/en/latest/start/install.html](https://verl.readthedocs.io/en/latest/start/install.html)\n\n以及需要额外安装SwanLab\n\n```bash\npip install -U swanlab\n```",
    "455": "一级标题：verl\n二级标题：使用方法\n内容：\n以verl官方文档的[Post-train a LLM using PPO with GSM8K dataset](https://verl.readthedocs.io/en/latest/start/quickstart.html)为例。\n\n你仅需要通过在实验的启动命令中，增加`trainer.logger=['swanlab']`，即可选择swanlab进行实验跟踪。\n\n**完整的测试命令如下：**\n\n```bash {4}\nPYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo \\\n data.train_files=$HOME/data/gsm8k/train.parquet \\\n data.val_files=$HOME/data/gsm8k/test.parquet \\\n trainer.logger=['console','swanlab'] \\\n data.train_batch_size=256 \\\n data.val_batch_size=1312 \\\n data.max_prompt_length=512 \\\n data.max_response_length=256 \\\n actor_rollout_ref.model.path=Qwen/Qwen2.5-0.5B-Instruct \\\n actor_rollout_ref.actor.optim.lr=1e-6 \\\n actor_rollout_ref.actor.ppo_mini_batch_size=64 \\\n actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 \\\n actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=8 \\\n actor_rollout_ref.rollout.tensor_model_parallel_size=1 \\\n actor_rollout_ref.rollout.gpu_memory_utilization=0.4 \\\n actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=4 \\\n critic.optim.lr=1e-5 \\\n critic.model.path=Qwen/Qwen2.5-0.5B-Instruct \\\n critic.ppo_micro_batch_size_per_gpu=4 \\\n algorithm.kl_ctrl.kl_coef=0.001 \\\n trainer.val_before_train=False \\\n trainer.default_hdfs_dir=null \\\n trainer.n_gpus_per_node=1 \\\n trainer.nnodes=1 \\\n trainer.save_freq=10 \\\n trainer.test_freq=10 \\\n trainer.total_epochs=15 2>&1 | tee verl_demo.log\n```\n\n:::info\n如果你需要设置项目和实验名，可以设置`trainer.project_name`和`trainer.experiment_name`。  \n如：\n```bash\nPYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo \\\n ...\n trainer.project_name=\"verl_demo\" \\\n trainer.experiment_name=\"ppo\" \\\n ...\n```\n:::\n\n如果启动训练时你还未登陆SwanLab，会出现如下提示。\n\n![select](./verl/select.png)\n\n选择**1、2**则为使用云端跟踪模式，选择后根据引导输入官网的API即可实现在线跟踪。可以在线查看训练跟踪结果。选择**3**则不上传训练数据，采用离线跟踪。\n\n\n当然，你也可以通过[环境变量](/api/environment-variable)的方式登陆或者设置跟踪模式：\n\n```bash\nexport SWANLAB_API_KEY=<你的登陆API>           # 设置在线跟踪模式API\nexport SWANLAB_LOG_DIR=<设置本地日志存储路径>    # 设置本地日志存储路径\nexport SWANLAB_MODE=<设置SwanLab的运行模式>     # 包含四种模式：cloud云端跟踪模式（默认）、cloud-only仅云端跟踪本地不保存文件、local本地跟踪模式、disabled完全不记录用于debug\n```",
    "456": "一级标题：verl\n二级标题：查看训练日志\n内容：\n完成登陆后会显示如下登陆信息：\n\n![track](./verl/track.png)\n\n运行进程，即可在[SwanLab官网](https://swanlab.cn)上查看训练日志：\n\n![remote](./verl/remote.png)\n\n更多使用方法可以参考[SwanLab查看使用结果](https://docs.swanlab.cn/guide_cloud/experiment_track/view-result.html)\n\n---\n\n如果你使用本地看板模式，则可以通过如下命令打开本地看板\n\n```bash\nswanlab watch\n```\n\n更多详细可以参考[SwanLab离线看板模式](https://docs.swanlab.cn/guide_cloud/self_host/offline-board.html)\n\n服务器设置端口号可以查看[离线看板端口号](https://docs.swanlab.cn/api/cli-swanlab-watch.html#%E8%AE%BE%E7%BD%AEip%E5%92%8C%E7%AB%AF%E5%8F%A3%E5%8F%B7)",
    "457": "一级标题：verl\n二级标题：每轮评估时记录生成文本\n内容：\n如果你希望在每轮评估（val）时将生成的文本记录到SwanLab中，只需在命令行钟增加一行`trainer.log_val_generations=1`即可：\n\n```bash {5}\nPYTHONUNBUFFERED=1 python3 -m verl.trainer.main_ppo \\\n data.train_files=$HOME/data/gsm8k/train.parquet \\\n data.val_files=$HOME/data/gsm8k/test.parquet \\\n trainer.logger=['console','swanlab'] \\\n trainer.log_val_generations=1 \\\n ...\n```\n\n> 如果你希望每轮评估时生成多条结果，如10条，那么修改`trainer.log_val_generations=10`即可",
    "458": "一级标题：Weights & Biases\n二级标题：简介\n内容：\nWeights & Biases (Wandb) 是一个用于机器学习和深度学习项目的实验跟踪、模型优化和协作平台。W&B 提供了强大的工具来记录和可视化实验结果，帮助数据科学家和研究人员更好地管理和分享他们的工作。\n\n![wandb](/assets/ig-wandb.png)\n\n:::warning 其他工具的同步教程\n\n- [TensorBoard](/guide_cloud/integration/integration-tensorboard.md)\n- [MLFlow](/guide_cloud/integration/integration-mlflow.md)\n:::\n\n你可以用两种方式将Wandb上的项目同步到SwanLab：\n\n1. **同步跟踪**：如果你现在的项目使用了wandb进行实验跟踪，你可以使用`swanlab.sync_wandb()`命令，在运行训练脚本时同步记录指标到SwanLab。\n2. **转换已存在的项目**：如果你想要将wandb上的项目复制到SwanLab，你可以使用`swanlab convert`，将Wandb上已存在的项目转换成SwanLab项目。\n\n::: info\n在当前版本暂仅支持转换标量图表。\n:::\n\n[[toc]]",
    "459": "一级标题：Weights & Biases\n二级标题：同步跟踪\n内容：\n### 1.1 添加sync_wandb命令\n\n在你的代码执行`wandb.init()`之前的任何位置，添加一行`swanlab.sync()`命令，即可在训练时同步wandb的指标到SwanLab。\n\n```python\nimport swanlab\n\nswanlab.sync_wandb()\n\n...\n\nwandb.init()\n```\n\n在上述这种代码写法中，`wandb.init()`的同时会初始化swanlab，项目名、实验名和配置和`wandb.init()`中的`project`、`name`、`config`一致，因此你不需要再手动初始化swanlab。\n\n:::info\n\n**`sync_wandb`支持设置两个参数：**\n\n- `mode`: swanlab的记录模式，支持cloud、local和disabled三种模式。\n- `wandb_run`: 如果此参数设置为**False**，则不会将数据上传到wandb，等同于设置wandb.init(mode=\"offline\")\n\n:::\n\n### 1.2 另一种写法\n\n另一种用法是先手动初始化swanlab，再运行wandb的代码。\n\n```python\nimport swanlab\n\nswanlab.init(...)\nswanlab.sync_wandb()\n\n...\n\nwandb.init()\n```\n\n在这种写法中，项目名、实验名、配置和`swanlab.init()`中的`project`、`experiment_name`、`config`一致，而后续`wandb.init()`中的`project`、`name`会被忽略，`config`会更新进`swanlab.config`中。\n\n### 1.3 测试代码\n\n```python\nimport wandb\nimport random\nimport swanlab\n\nswanlab.sync_wandb()\n# swanlab.init(project=\"sync_wandb\")\n\nwandb.init(\n  project=\"test\",\n  config={\"a\": 1, \"b\": 2},\n  name=\"test\",\n  )\n\nepochs = 10\noffset = random.random() / 5\nfor epoch in range(2, epochs):\n  acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n  loss = 2 ** -epoch + random.random() / epoch + offset\n\n  wandb.log({\"acc\": acc, \"loss\": loss})\n```\n\n![alt text](/assets/ig-wandb-4.png)",
    "460": "一级标题：Weights & Biases\n二级标题：转换已存在的项目\n内容：\n### 2.1 找到你在wandb.ai上的projecy、entity和runid\n\nprojecy、entity和runid是转换所需要的（runid是可选的）。  \nproject和entity的位置：\n![alt text](/assets/ig-wandb-2.png)\n\nrunid的位置：\n\n![alt text](/assets/ig-wandb-3.png)\n\n### 2.2 方式一：命令行转换\n\n首先，需要确保当前环境下，你已登录了wandb，并有权限访问目标项目。\n\n转换命令行：\n\n```bash\nswanlab convert -t wandb --wb-project [WANDB_PROJECT_NAME] --wb-entity [WANDB_ENTITY]\n```\n\n支持的参数如下：\n\n- `-t`: 转换类型，可选wandb与tensorboard。\n- `-p`: SwanLab项目名。\n- `-w`: SwanLab工作空间名。\n- `--mode`: (str) 选择模式，默认为\"cloud\"，可选 [\"cloud\", \"local\", \"offline\", \"disabled\"]\n- `-l`: logdir路径。\n- `--wb-project`：待转换的wandb项目名。\n- `--wb-entity`：wandb项目所在的空间名。\n- `--wb-runid`: wandb Run（项目下的某一个实验）的id。\n\n如果不填写`--wb-runid`，则会将指定项目下的全部Run进行转换；如果填写，则只转换指定的Run。\n\n---\n\n**异步转换方法（先将数据下载到本地，再上传到swanlab）**\n\n1. 数据下载到本地：\n\n```bash\nswanlab convert --mode 'offline' -t wandb --wb-project [WANDB_PROJECT_NAME] --wb-entity [WANDB_ENTITY]\n```\n\n2. 上传到swanlab：\n\n```bash\nswanlab sync [日志文件夹路径]\n```\n\n[swanlab sync文档](/zh/api/cli-swanlab-sync.md)\n\n### 2.3 方式二：代码内转换\n\n```python\nfrom swanlab.converter import WandbConverter\n\nwb_converter = WandbConverter()\n# wb_runid可选\nwb_converter.run(wb_project=\"WANDB_PROJECT_NAME\", wb_entity=\"WANDB_USERNAME\")\n```\n\n效果与命令行转换一致。\n\n`WandbConverter`支持的参数：\n\n- `project`: SwanLab项目名。\n- `workspace`: SwanLab工作空间名。\n- `mode`: (str) 选择模式，默认为\"cloud\"，可选 [\"cloud\", \"local\", \"offline\", \"disabled\"]\n- `logdir`: logdir路径。\n\n`WandbConverter.run`支持的参数：\n\n- `wb_project`: wandb项目名。\n- `wb_entity`: wandb项目所在的空间名。\n- `wb_runid`: wandb Run（项目下的某一个实验）的id。\n\n**异步转换方法（先将数据下载到本地，再上传到swanlab）**\n\n1. 数据下载到本地：\n\n```python\nfrom swanlab.converter import WandbConverter\n\nwb_converter = WandbConverter(mode=\"offline\")\n# wb_runid可选\nwb_converter.run(wb_project=\"WANDB_PROJECT_NAME\", wb_entity=\"WANDB_USERNAME\")\n```\n\n2. 上传到swanlab：\n\n```bash\nswanlab sync [日志文件夹路径]\n```\n\n[swanlab sync文档](/zh/api/cli-swanlab-sync.md)",
    "461": "一级标题：XGBoost\n二级标题：介绍\n内容：\nXGBoost（eXtreme Gradient Boosting）是一种高效、灵活且广泛使用的梯度提升框架，由陈天奇在2014年提出。它基于决策树算法，通过集成多个弱学习器（通常是决策树）来构建一个强大的预测模型。XGBoost在各种机器学习竞赛和实际应用中表现出色，尤其是在分类、回归和排序任务中。\n\n![xgboost](/zh/guide_cloud/integration/xgboost/logo.png)\n\n你可以使用XGBoost快速进行模型训练，同时使用SwanLab进行实验跟踪与可视化。",
    "462": "一级标题：XGBoost\n二级标题：引入SwanLabCallback\n内容：\n```python\nfrom swanlab.integration.xgboost import SwanLabCallback\n```\n\nSwanLabCallback是适配于XGBoost的日志记录类。",
    "463": "一级标题：XGBoost\n二级标题：初始化SwanLab\n内容：\n```python\nswanlab.init(\n    project=\"xgboost-example\", \n)\n```",
    "464": "一级标题：XGBoost\n二级标题：传入`xgb.train`\n内容：\n```python\nimport xgboost as xgb\n\nbst = xgb.train(\n    ...\n    callbacks=[SwanLabCallback()]\n)\n```",
    "465": "一级标题：XGBoost\n二级标题：完整测试代码\n内容：\n```python\nimport xgboost as xgb\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nimport swanlab\nfrom swanlab.integration.xgboost import SwanLabCallback\n\n# 初始化swanlab\nswanlab.init(\n    project=\"xgboost-breast-cancer\",\n    config={\n        \"learning_rate\": 0.1,\n        \"max_depth\": 3,\n        \"subsample\": 0.8,\n        \"colsample_bytree\": 0.8,\n        \"num_round\": 100\n    }\n)\n\n# 加载数据集\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# 将数据集分为训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 转换为DMatrix格式，这是XGBoost的内部数据格式\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n\n# 设置参数\nparams = {\n    'objective': 'binary:logistic',  # 二分类任务\n    'max_depth': 3,                  # 树的最大深度\n    'eta': 0.1,                      # 学习率\n    'subsample': 0.8,                # 样本采样比例\n    'colsample_bytree': 0.8,         # 特征采样比例\n    'eval_metric': 'logloss'         # 评估指标\n}\n\n# 训练模型\nnum_round = 100  # 迭代次数\nbst = xgb.train(\n    params, \n    dtrain, \n    num_round,\n    evals=[(dtrain, 'train'), (dtest, 'test')], \n    callbacks=[SwanLabCallback()]\n)\n\n# 进行预测\ny_pred = bst.predict(dtest)\ny_pred_binary = [round(value) for value in y_pred]  # 将概率转换为二分类结果\n\n# 评估模型\naccuracy = accuracy_score(y_test, y_pred_binary)\nprint(f\"Accuracy: {accuracy:.4f}\")\n\n# 打印分类报告\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred_binary, target_names=data.target_names))\n\n# 保存模型\nbst.save_model('xgboost_model.model')\n\n# 结束swanlab会话\nswanlab.finish()\n```\n```",
    "466": "一级标题：Xtuner\n二级标题：简介\n内容：\n[XTuner](https://github.com/InternLM/xtuner) 是一个高效、灵活、全能的轻量化大模型微调工具库。\n\n<div align=\"center\">\n<img src=\"/assets/integration-xtuner.png\" width=440>\n</div>\n\nXtuner支持与书生·浦语（InternLM）、Llama等多款开源大模型的适配，可执行增量预训练、指令微调、工具类指令微调等任务类型。硬件要求上，在Tesla T4、A100等传统数据中心之外，开发者最低使用消费级显卡便可进行训练，实现大模型特定需求能力。\n\n<div align=\"center\">\n<img src=\"/assets/integration-xtuner-intro.png\">\n</div>",
    "467": "一级标题：Xtuner\n二级标题：使用SwanLab可视化跟踪Xtuner微调进展\n内容：\n打开要训练的配置文件（比如[qwen1_5_7b_chat_full_alpaca_e3.py](https://github.com/InternLM/xtuner/blob/main/xtuner/configs/qwen/qwen1_5/qwen1_5_7b_chat/qwen1_5_7b_chat_full_alpaca_e3.py)）），找到`visualizer`参数的位置，将它替换成：\n\n```python\n# set visualizer\nfrom mmengine.visualization import Visualizer\nfrom swanlab.integration.mmengine import SwanlabVisBackend\n\nvisualizer = dict(type=Visualizer, vis_backends=[dict(type=SwanlabVisBackend)])\n```\n\n然后照样运行微调命令，即可实现SwanLab实验跟踪：\n\n```bash\nxtuner train qwen1_5_7b_chat_full_alpaca_e3.py\n```\n\n---\n\n如果希望像平常使用SwanLab那样指定项目名、实验名等信息，可以在实例化`SwanlabVisBackend`时在`init_kwargs`参数中指定，可以参考 [swanlab init](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/data/sdk.py#L71) 查看可配置的参数。\n\n通过以字典的形式传入`init_kwargs`，该参数最终会传给 `swanlab.init` 方法，下面举了个指定项目名称的案例。\n\n```python (5)\nvisualizer = dict(\n  type=Visualizer,\n  vis_backends=[dict(\n        type=SwanlabVisBackend,\n        init_kwargs=dict(project='toy-example', experiment_name='Qwen'),\n    )])\n```\n\n有关MM系列的其他引入方法和更灵活的配置，可以参考[MMEngine接入SwanLab](https://docs.swanlab.cn/zh/guide_cloud/integration/integration-mmengine.html)。",
    "468": "一级标题：ZhipuAI\n二级标题：引入autolog\n内容：\n```python\nfrom swanlab.integration.zhipuai import autolog\n```\nautolog是一个为zhipuai适配的过程记录类，能够自动记录你的zhipuai交互的过程。",
    "469": "一级标题：ZhipuAI\n二级标题：传入参数\n内容：\n```python\nautolog(init=dict(project=\"zhipuai_logging\"))\n```\n这里给`init`传入的参数与`swanlab.init`的参数形式完全一致。",
    "470": "一级标题：ZhipuAI\n二级标题：自动记录\n内容：\n```python\nfrom swanlab.integration.zhipuai import autolog\n\nautolog(init=dict(project=\"zhipuai_logging\"))\nclient = autolog.client\n\nresponse = client.chat.completions.create(\n    model=\"glm-4\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"作为一名营销专家，请为我的产品创作一个吸引人的slogan\"},\n        {\"role\": \"assistant\", \"content\": \"当然，为了创作一个吸引人的slogan，请告诉我一些关于您产品的信息\"},\n        {\"role\": \"user\", \"content\": \"智谱AI开放平台\"},\n        {\"role\": \"assistant\", \"content\": \"智启未来，谱绘无限一智谱AI，让创新触手可及!\"},\n        {\"role\": \"user\", \"content\": \"创造一个更精准、吸引人的slogan\"},\n    ],\n)\n\nresponse2 = client.chat.completions.create(\n    model=\"glm-4\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"谁获得了NBA2015年的总冠军\"},\n    ],\n)\n```",
    "471": "一级标题：阿里云计算巢应用部署\n二级标题：关于第三方部署\n内容：\n:::warning 关于第三方部署\n\n第三方部署是由社区贡献的部署方式，官方不保证能实时同步最新版本。\n\n:::\n\n目前 SwanLab 社区版本已上线阿里云计算巢服务市场，欢迎各位训练师通过阿里云一键部署使用~",
    "472": "一级标题：阿里云计算巢应用部署\n二级标题：前提条件\n内容：\n部署 SwanLab 社区版服务实例，需要对部分阿里云资源进行访问和创建操作。因此您的账号需要包含如下资源的权限。\n**说明**：当您的账号是RAM账号时，才需要添加此权限。\n\n| 权限策略名称                          | 备注                         |\n|---------------------------------|----------------------------|\n| AliyunECSFullAccess             | 管理云服务器服务（ECS）的权限           |\n| AliyunVPCFullAccess             | 管理专有网络（VPC）的权限             |\n| AliyunROSFullAccess             | 管理资源编排服务（ROS）的权限           |\n| AliyunComputeNestUserFullAccess | 管理计算巢服务（ComputeNest）的用户侧权限 |",
    "473": "一级标题：阿里云计算巢应用部署\n二级标题：计费说明\n内容：\nSwanLab社区版在计算巢部署的费用主要涉及：\n\n- 所选vCPU与内存规格\n- 系统盘类型及容量\n- 公网带宽",
    "474": "一级标题：阿里云计算巢应用部署\n二级标题：部署流程\n内容：\n1. 访问计算巢SwanLab社区版[部署链接](https://computenest.console.aliyun.com/service/instance/create/cn-hangzhou?type=user&ServiceId=service-cb2da57160444c3ebdbf)\n，按提示填写部署参数：\n<img src=\"./alibabacloud-computenest/deploy_service_instance.jpg\" width=\"800\"/>\n\n2. 参数填写完成后可以看到对应询价明细，确认参数后点击**下一步：确认订单**。\n\n3. 确认订单完成后同意服务协议并点击**立即创建**进入部署阶段。\n\n4. 等待部署完成后就可以开始使用服务，进入服务实例详情点击服务地址。\n   <img src=\"./alibabacloud-computenest/get_service_instance.jpg\" width=\"800\"/>\n\n5. 访问服务地址注册账号并使用SwanLab服务。\n   <img src=\"./alibabacloud-computenest/swanlab_service.jpg\" width=\"800\"/>\n\n```\n一级标题：使用Docker进行部署\n二级标题：先决条件\n内容：\n> 在安装 SwanLab 之前，请确保您的机器满足以下最低系统要求：\n>\n> - CPU >= 2核\n> - 内存 >= 4GB\n> - 存储空间 >= 20GB\n\nSwanLab 私有化部署版，需要使用 **Docker Compose** 进行安装与部署（暂不支持K8S部署），请根据你的操作系统，对表下面的表格选择正确的Docker及compose版本。\n\n**如果你已经安装了Docker，请跳过这一步。**\n\n| 操作系统               | 软件                                                   | 解释                                                                                                                                                                                                                                                                                                                                                 |\n| ---------------------- | ------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| macOS 10.14 或更高版本 | Docker Desktop                                         | 将 Docker 虚拟机 (VM) 设置为至少使用 2 个虚拟 CPU (vCPU) 和 8 GB 初始内存。否则，安装可能会失败。有关更多信息，请参阅[Mac 版 Docker Desktop 安装指南](https://docs.docker.com/desktop/install/mac-install/)。                                                                                                                                        |\n| Windows（启用了WSL 2） | Docker Desktop                                         | 我们建议将源代码和其他与 Linux 容器绑定的数据存储在 Linux 文件系统中，而不是 Windows 文件系统中。有关更多信息，请参阅 [Windows上使用WSL安装Linux](https://learn.microsoft.com/zh-cn/windows/wsl/install) 与 [在 Windows 上使用 WSL 2 后端的 Docker Desktop 安装指南](https://docs.docker.com/desktop/setup/install/windows-install/#wsl-2-backend)。 |\n| Linux                  | Docker 19.03 或更高版本 Docker Compose 1.28 或更高版本 | 有关如何安装Docker和Docker Compose 的更多信息，请参阅[Docker 安装指南](https://docs.docker.com/engine/install/)和[Docker Compose 安装指南](https://docs.docker.com/compose/install/)。                                                                                                                                                               |\n\n> 如果你还未安装Docker，可以运行我们提供的[安装脚本](https://docs.docker.com/desktop/install/mac-install/)。\n\n---\n\n**端口说明**\n\n如果你将SwanLab部署在服务器上，并希望能够远程访问与实验记录，那么请确保服务器开放以下两个端口：\n\n| 端口号 | 是否可配置 | 用途说明                                                      |\n| ------ | ---------- | ------------------------------------------------------------- |\n| 8000   | 是         | 网关服务端口，可用于接收外部请求，建议在公网环境中设置为 `80` |\n| 9000   | 否         | MinIO 签名端口，用于对象存储访问，端口固定不可修改            |\n\n> 由于网关服务端口（默认为`8000`）支持在部署前后修改，所以请确保你开放的是最终修改的端口。\n```\n\n```\n一级标题：使用Docker进行部署\n二级标题：1. 克隆仓库\n内容：\n使用Git克隆`self-hosted`仓库：\n\n```bash\ngit clone https://github.com/SwanHubX/self-hosted.git\ncd self-hosted\n```\n```\n\n```\n一级标题：使用Docker进行部署\n二级标题：2. 一键脚本安装\n内容：\n> 如果你使用的是Windows系统，请确保已安装并开启 WSL2 和 Docker Desktop\n> <img src=\"./docker-deploy/wsl-dockerinfo.png\" width=\"600\"/>\n\n> 在WSL2的文件系统中执行 `.sh` 安装脚本\n> <img src=\"./docker-deploy/wsl-bash.png\" width=\"600\"/>\n\n默认的安装脚本在`docker/install.sh`，直接执行即可一键安装所有需要的容器以及执行初始化配置。\n\n```bash\ncd ./docker\n./install.sh\n```\n\n默认脚本链接的镜像源在中国，所以中国地区的下载速度非常快！\n\n如果你需要使用 [DockerHub](https://hub.docker.com/) 作为镜像源，则可以使用下面的脚本进行安装：\n\n```bash\n./install-dockerhub.sh\n```\n```\n\n```\n一级标题：使用Docker进行部署\n二级标题：3. 激活主账号\n内容：\nSwanLab社区版默认会使用`8000`端口，如果你使用的是默认配置，那么可以直接访问：`http://localhost:8000`，就可以访问到SwanLab社区版。\n\n> 也有可能社区版部署在了其他端口，请打开 Docker Desktop，找到`traefik`容器旁边的port映射，比如`64703:80`，那么你应该访问`http://localhost:64703`。\n\n![](./docker-deploy/create-account.png)\n\n现在，你需要激活你的主账号。激活需要1个License，个人使用可以免费在[SwanLab官网](https://swanlab.cn)申请一个，位置在 「设置」-「账户与许可证」。\n\n:::warning 离线验证\n\n在私有化部署 > `v1.1`的版本中，支持在离线环境下验证License。\n\n:::\n\n![](./docker-deploy/apply-license.png)\n\n拿到License后，回到激活页面，填写用户名、密码、确认密码和License，点击激活即可完成创建。\n\n![](./docker-deploy/quick-start.png)\n```\n\n```\n一级标题：使用Docker进行部署\n二级标题：4. 开始你的第一个实验\n内容：\n在Python SDK完成登录：\n\n```bash\nswanlab login --host <IP地址>\n```\n\n> 如果你之前登录过swanlab，想要重新登录，请使用：\n> `swanlab login --host <IP地址> --relogin`。\n\n按回车，填写API Key，完成登录。之后你的SwanLab实验将会默认传到私有化部署的SwanLab上。\n\n---\n\n测试脚本：\n\n```bash\nimport swanlab\nimport random\n\n# 创建一个SwanLab项目\nswanlab.init(\n    # 设置项目名\n    project=\"my-awesome-project\",\n\n    # 设置超参数\n    config={\n        \"learning_rate\": 0.02,\n        \"architecture\": \"CNN\",\n        \"dataset\": \"CIFAR-100\",\n        \"epochs\": 10\n    }\n)\n\n# 模拟一次训练\nepochs = 10\noffset = random.random() / 5\nfor epoch in range(2, epochs):\n  acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n  loss = 2 ** -epoch + random.random() / epoch + offset\n\n  # 记录训练指标\n  swanlab.log({\"acc\": acc, \"loss\": loss})\n\n# [可选] 完成训练，这在notebook环境中是必要的\nswanlab.finish()\n```\n\n运行后在网页查看实验：\n\n![](./docker-deploy/test-experiment.png)\n```\n\n```\n一级标题：使用Docker进行部署\n二级标题：升级版本\n内容：\n如果你想要将你本地的私有化部署版本升级到最新版，请使用下面的命令：\n\n```bash\n# 在你之前本地部署的 self-hosted 项目目录下\ncd ./docker\n./upgrade.sh\n```\n\n升级完成的命令行样式：\n\n![](./docker-deploy/upgrade.png)\n```",
    "475": "一级标题：团队/企业版\n二级标题：私有化个人版（免费）\n内容：\n私有化个人版（免费）目前支持现在公有云版的绝大部分功能，但不支持多人协作、创建组织、权限控制、统计看板等高级功能。",
    "476": "一级标题：团队/企业版\n二级标题：团队版/企业版/多租户云版需求\n内容：\n对 团队版/企业版/多租户云版 有需求的伙伴，欢迎联系我们：[contact@swanlab.cn](mailto:contact@swanlab.cn)，并备注您的公司/机构与职位。",
    "477": "一级标题：常见问题\n二级标题：如何修改端口？\n内容：\nSwanLab 自托管版本基于 [Docker](https://www.docker.com/) 部署，默认情况下使用 `8000` 端口，修改自托管服务默认访问端口实际上是修改 **swanlab-traefik** 容器的映射端口，分为以下两种情况：\n\n### 部署前修改\n\n安装脚本提供有一些配置可选项，包括数据存储位置和映射的端口，我们通过修改脚本启动参数来实现修改端口。\n\n- 执行 `install.sh` 安装脚本后，命令行会提示配置可选项，可以交互式输入对应的参数。在命令行输出 `2. Use the default port  (8000)? (y/n):` 后输入 `n`，然后会提示 `Enter a custom port:`，输入对应的端口号即可，例如 `80` 。\n\n```bash\n❯ bash install.sh\n🤩 Docker is installed, so let's get started.\n🧐 Checking if Docker is running...\n\n1. Use the default path  (./data)? (y/n):\n   The selected path is: ./data\n2. Use the default port  (8000)? (y/n):\n```\n\n- 启动脚本时添加参数，安装脚本提供有命令行参数 `-p` 可以用于修改端口，例如： `./install.sh -p 80`。\n\n> 更多命令行参数详见：[通过 Docker 部署](https://github.com/SwanHubX/self-hosted/tree/main/docker)\n\n### 部署后修改\n\n如果需要 SwanLab 服务部署完成后需要修改访问端口，则需要修改生成的 `docker-compose.yaml` 配置文件。\n\n在脚本执行的位置找到 `swanlab/` 目录，执行 `cd swanlab/` 后进入到 `swanlab` 目录下找到对应的 `docker-compose.yaml` 配置文件，然后修改 `traefik` 容器对应的端口 `ports`，如下所示：\n\n```yaml\n  traefik:\n    <<: *common\n    image: ccr.ccs.tencentyun.com/self-hosted/traefik:v3.0\n    container_name: swanlab-traefik\n    ports:\n      - \"8000:80\" # [!code --]\n      - \"80:80\" # [!code ++]\n```\n\n> 上面将访问端口修改为了 `80`\n\n修改完成后执行 `docker compose up -d` 重启容器，重启完成后即可通过 `http://{ip}:80` 访问",
    "478": "一级标题：常见问题\n二级标题：上传媒体文件报错怎么办\n内容：\n当你使用`swanlab.log`记录媒体文件，如图像、音频时，发现报错，如：\n\n```bash\nswanlab: Upload error: An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The Access Key Id you provided does not exist in our records.\n```\n\n请检查你的服务器是否开放了`9000`端口，如果未开放，请在服务器防火墙/安全组中开放`9000`端口。\n\n根据提供的 Markdown 内容，以下是按主题分块整理的内容：",
    "479": "一级标题：离线看板接口文档\n二级标题：接口 1：获取项目详情\n内容：\n- **URL**：`/api/v1/project`\n- **方法**：`GET`\n- **接口说明**：获取当前 Swanlab 实例中加载的项目及其所有实验信息。\n\n### 响应示例\n\n```json\n{\n  \"code\": 0,\n  \"message\": \"success\",\n  \"data\":{\n    \"id\": 1,\n    \"name\": \"llamafactory\",\n    \"experiments\": [\n        {\n        \"id\": 1,\n        \"name\": \"Qwen2.5-7B/20250321-1130-16bed2e2\",\n        \"run_id\": \"run-20250321_125806-a3b1799d\",\n        \"status\": 0,\n        \"config\": { ... },\n        \"create_time\": \"2025-03-21T04:58:06.387383+00:00\"\n        },\n        ...\n    ]\n  }\n}\n```\n\n### 字段说明\n\n- `id` / `name`：项目唯一标识与名称。\n- `experiments`：该项目下的所有实验信息。\n- `logdir`：日志文件存储路径。\n- `charts`：图表数量。\n- `pinned_opened` / `hidden_opened`：控制面板的默认展开状态。",
    "480": "一级标题：离线看板接口文档\n二级标题：接口 2：获取单个实验详情\n内容：\n- **URL**：`/api/v1/experiment/<experiment_id>`\n- **方法**：`GET`\n- **示例**：`/api/v1/experiment/1`\n- **接口说明**：获取指定实验的详细配置信息与系统环境。\n\n### 响应示例\n\n```json\n{\n  \"code\": 0,\n  \"message\": \"success\",\n  \"data\": {\n    \"id\": 1,\n    \"run_id\": \"run-20250321_125806-a3b1799d\",\n    \"name\": \"Qwen2.5-7B/20250321-1130-16bed2e2\",\n    \"config\": { ... },\n    \"system\": {\n      \"cpu\": { \"brand\": \"Intel...\", \"cores\": 104 },\n      \"gpu\": {\n        \"nvidia\": {\n          \"type\": [\"NVIDIA A100-PCIE-40GB\", ...],\n          \"memory\": [40, 40, 40, 40],\n          \"cuda\": \"11.6\"\n        }\n      },\n      \"os\": \"Linux...\",\n      \"python\": \"3.10.14\",\n      \"command\": \"/path/to/cli config.yaml\",\n      \"swanlab\": {\n        \"version\": \"0.5.2\",\n        \"logdir\": \"/path/to/logs\",\n        \"_monitor\": 3\n      }\n    }\n  }\n}\n```\n\n### 字段说明\n\n- `config`：实验的完整参数配置。\n- `system`：运行时主机的系统信息，包括 CPU、GPU、Python 版本、命令等。\n- `run_id`：实验的唯一标识符，通常与日志文件关联。",
    "481": "一级标题：离线看板接口文档\n二级标题：接口 3：获取实验图表信息\n内容：\n- **URL**：`/api/v1/experiment/<experiment_id>/chart`\n- **方法**：`GET`\n- **示例**：`/api/v1/experiment/1/chart`\n- **接口说明**：获取指定实验的所有图表定义和元信息（如 loss 曲线、学习率曲线等）。\n\n### 响应示例\n\n```json\n{\n  \"code\": 0,\n  \"message\": \"success\",\n  \"data\": {\n    \"charts\": [\n      {\n        \"id\": 1,\n        \"name\": \"train/loss\",\n        \"type\": \"line\",\n        \"reference\": \"step\",\n        \"source\": [\"train/loss\"],\n        \"multi\": false\n      },\n      ...\n    ],\n    \"namespaces\": [\n      {\n        \"id\": 1,\n        \"name\": \"train\",\n        \"opened\": 1,\n        \"charts\": [1, 3, 5, 7]\n      }\n    ]\n  }\n}\n```\n\n### 字段说明\n\n- `charts`：图表定义列表，包括图表名称、类型、数据来源等。\n- `namespaces`：图表命名空间，用于分类展示。\n- `reference`：图表的 X 轴参考，5982 `step`（训练步数）。",
    "482": "一级标题：离线看板接口文档\n二级标题：接口 4：获取指标数据\n内容：\n- **URL**：`/api/v1/experiment/<experiment_id>/tag/<namespace>/<metric_name>`\n- **方法**：`GET`\n- **示例**：`/api/v1/experiment/1/tag/train/loss`\n- **接口说明**：获取指定实验中某个具体指标的历史数据（如 loss、accuracy 等）。\n\n### 响应示例\n\n```json\n{\n  \"code\": 0,\n  \"message\": \"success\",\n  \"data\": {\n    \"sum\": 207,\n    \"max\": 1.7614,\n    \"min\": 0.8499,\n    \"experiment_id\": 1,\n    \"list\": [\n      {\n        \"index\": 1,\n        \"data\": 1.6858,\n        \"create_time\": \"2025-03-21T04:58:32.095272+00:00\"\n      },\n      ...,\n      {\n        \"index\": 207,\n        \"data\": 1.1845,\n        \"create_time\": \"2025-03-21T06:05:16.716693+00:00\",\n        \"_last\": true\n      }\n    ]\n  }\n}\n```\n\n### 字段说明\n\n- `sum`：数据总条数。\n- `max` / `min`：指标最大值与最小值。\n- `experiment_id`：所属实验 ID。\n- `list`：具体数据项，每项包括：\n  - `index`：数据点序号。\n  - `data`：具体数值。\n  - `create_time`：记录时间。\n  - `_last`：是否为最后一个数据点（仅最后一条为 true）。",
    "483": "一级标题：离线看板接口文档\n二级标题：接口 5：获取实验最新日志\n内容：\n- **URL**：`/api/v1/experiment/<experiment_id>/recent_log`\n- **方法**：`GET`\n- **示例**：`/api/v1/experiment/1/recent_log`\n- **接口说明**：获取指定实验最新的日志输出。包括 Swanlab 自身日志信息和用户自定义的输出。\n\n### 响应示例\n\n```json\n{\n  \"code\": 0,\n  \"message\": \"success\",\n  \"data\": {\n    \"recent\": [\n      \"swanlab:\",\n      \"{'loss':\"\n    ],\n    \"logs\": [\n      \"swanlab: Tracking run with swanlab version 0.5.2\",\n      \"swanlab: Run data will be saved locally in /data/project/...\",\n      \"{'loss': 1.6858, 'grad_norm': ..., 'epoch': 0.02, ...}\",\n      \"...\"\n    ]\n  }\n}\n```\n\n### 字段说明\n\n- `recent`：最新日志段落，通常用于快速预览。\n- `logs`：日志输出列表，包含 swanlab 系统日志和运行中的配置、输出数据。",
    "484": "一级标题：离线看板接口文档\n二级标题：接口 6：获取实验状态信息\n内容：\n- **URL**：`/api/v1/experiment/<experiment_id>/status`\n- **方法**：`GET`\n- **示例**：`/api/v1/experiment/1/status`\n- **接口说明**：获取指定实验的最新状态、更新时间、图表结构等信息。\n\n### 响应示例\n\n```json\n{\n  \"code\": 0,\n  \"message\": \"success\",\n  \"data\": {\n    \"status\": 0,\n    \"update_time\": \"2025-03-21T04:58:06.387487+00:00\",\n    \"finish_time\": null,\n    \"charts\": {\n      \"charts\": [\n        {\n          \"id\": 1,\n          \"name\": \"train/loss\",\n          \"type\": \"line\",\n          \"reference\": \"step\",\n          \"status\": 0,\n          \"source\": [\"train/loss\"],\n          \"multi\": false,\n          \"source_map\": {\"train/loss\": 1}\n        },\n        {\n          \"id\": 3,\n          \"name\": \"train/grad_norm\",\n          \"type\": \"line\",\n          \"reference\": \"step\",\n          \"status\": 0,\n          \"source\": [\"train/grad_norm\"],\n          \"multi\": false,\n          \"source_map\": {\"train/grad_norm\": 1}\n        },\n        {\n          \"id\": 5,\n          \"name\": \"train/learning_rate\",\n          \"type\": \"line\",\n          \"reference\": \"step\",\n          \"status\": 0,\n          \"source\": [\"train/learning_rate\"],\n          \"multi\": false,\n          \"source_map\": {\"train/learning_rate\": 1}\n        },\n        ...\n      ],\n      \"namespaces\": [\n        {\n          \"id\": 1,\n          \"name\": \"train\",\n          \"opened\": 1,\n          \"charts\": [1, 3, 5, 7, 9, 11]\n        }\n      ]\n    }\n  }\n}\n```\n\n### 字段说明\n\n- `status`：实验当前状态，整型（如 0 表示运行中）。\n- `update_time`：实验状态最近更新时间。\n- `finish_time`：实验完成时间，未完成为 `null`。\n- `charts`：实验中的图表结构信息。\n  - `charts`：图表定义数组，字段与 `/chart` 接口一致。\n  - `namespaces`：图表命名空间，标识图表分类与分组。",
    "485": "一级标题：离线看板接口文档\n二级标题：接口 7：获取实验指标汇总\n内容：\n- **URL**：`/api/v1/experiment/<experiment_id>/summary`\n- **方法**：`GET`\n- **示例**：`/api/v1/experiment/1/summary`\n- **接口说明**：获取指定实验在当前状态下的各项关键指标的最新值汇总。\n\n### 响应示例\n\n```json\n{\n  \"code\": 0,\n  \"message\": \"success\",\n  \"data\": {\n    \"summaries\": [\n      { \"key\": \"train/loss\", \"value\": 1.1845 },\n      { \"key\": \"train/grad_norm\", \"value\": 1.0172306299209595 },\n      { \"key\": \"train/learning_rate\", \"value\": 0.000037463413651718303 },\n      { \"key\": \"train/epoch\", \"value\": 3.288 },\n      { \"key\": \"train/num_input_tokens_seen\", \"value\": 597776 },\n      { \"key\": \"train/global_step\", \"value\": 207 }\n    ]\n  }\n}\n```\n\n### 字段说明\n\n- `summaries`：包含多个指标的汇总值，每项包括：\n  - `key`：指标名称（如 `train/loss`）。\n  - `value`：该指标当前最新值。",
    "486": "一级标题：离线看板\n二级标题：简介\n内容：\n离线看板是SwanLab的历史功能，现阶段仅做简单维护，不再更新。\n\n如果您有私有化部署的需求，推荐使用[Docker版](/guide_cloud/self_host/docker-deploy)。\n\n离线看板是一种使用模式接近`tensorboard`的轻量级离线web看板。\n\nGithub：https://github.com/SwanHubX/SwanLab-Dashboard",
    "487": "一级标题：离线看板\n二级标题：安装\n内容：\n> 在swanlab>=0.5.0版本后，不再自带离线看板，需要使用dashboard扩展安装。\n\n使用离线看板，需要安装`swanlab`的`dashboard`扩展：\n\n```bash\npip install swanlab[dashboard]\n```",
    "488": "一级标题：离线看板\n二级标题：离线实验跟踪\n内容：\n在`swanlab.init`中设置`logdir`和`mode`这两个参数，即可离线跟踪实验：\n\n```python\n...\n\nswanlab.init(\n  logdir='./logs',\n  mode=\"local\",\n)\n\n...\n```\n\n- 参数`mode`设置为`local`，关闭将实验同步到云端\n- 参数`logdir`的设置是可选的，它的作用是指定了SwanLab日志文件的保存位置（默认保存在`swanlog`文件夹下）\n  - 日志文件会在跟踪实验的过程中被创建和更新，离线看板的启动也将基于这些日志文件\n\n其他部分和云端使用完全一致。",
    "489": "一级标题：离线看板\n二级标题：开启离线看板\n内容：\n打开终端，使用下面的指令，开启一个SwanLab仪表板:\n\n```bash\nswanlab watch ./logs\n```\n\n> 谐音：用swanlab看 ./logs 里的文件\n\n运行完成后，将启动一个后端服务，SwanLab会给你1个本地的URL链接（默认是http://127.0.0.1:5092）\n\n访问该链接，就可以在浏览器用离线看板查看实验了。\n\n[如何设置端口号和IP](/api/cli-swanlab-watch.md#设置ip和端口号)",
    "490": "一级标题：纯离线环境部署\n二级标题：部署流程\n内容：\n## 部署流程\n\n### 1. 下载镜像\n\n由于私有化版 [SwanLab](https://github.com/SwanHubX/self-hosted) 基于 Docker 部署，因此我们需要先在一台联网的机器上提前下载好所有镜像。\n\n> [!NOTE]\n>\n> 注意需要在相同 CPU 架构的服务器上下载镜像。比如你的服务器为 AMD64 架构，那么也需要在 AMD64 架构的服务器上拉取镜像，不能在 MacBook 这类采用 ARM64 架构的电脑上下载镜像。\n\n找到一台联网的电脑，确保其安装有 [Docker](https://docs.docker.com/engine/install/)，然后执行 [pull-images.sh](https://github.com/SwanHubX/self-hosted/blob/main/scripts/pull-images.sh) 脚本下载镜像包。执行完成后会得到一个 `swanlab_images.tar` 的压缩包。\n\n::: details pull-images.sh 脚本详情\n\n```shell\n#!/bin/bash\n\n# 定义要下载的镜像列表\nimages=(\n  \"ccr.ccs.tencentyun.com/self-hosted/traefik:v3.0\"\n  \"ccr.ccs.tencentyun.com/self-hosted/postgres:16.1\"\n  \"ccr.ccs.tencentyun.com/self-hosted/redis-stack-server:7.2.0-v15\"\n  \"ccr.ccs.tencentyun.com/self-hosted/clickhouse:24.3\"\n  \"ccr.ccs.tencentyun.com/self-hosted/logrotate:v1\"\n  \"ccr.ccs.tencentyun.com/self-hosted/fluent-bit:3.0\"\n  \"ccr.ccs.tencentyun.com/self-hosted/minio:RELEASE.2025-02-28T09-55-16Z\"\n  \"ccr.ccs.tencentyun.com/self-hosted/minio-mc:RELEASE.2025-04-08T15-39-49Z\"\n  \"ccr.ccs.tencentyun.com/self-hosted/swanlab-server:v1.1.1\"\n  \"ccr.ccs.tencentyun.com/self-hosted/swanlab-house:v1.1\"\n  \"ccr.ccs.tencentyun.com/self-hosted/swanlab-cloud:v1.1\"\n  \"ccr.ccs.tencentyun.com/self-hosted/swanlab-next:v1.1\"\n)\n\n# 下载镜像\nfor image in \"${images[@]}\"; do\n  docker pull \"$image\"\ndone\n\n# 保存镜像到文件\necho \"正在打包所有镜像到 swanlab_images.tar...\"\ndocker save -o ./swanlab_images.tar \"${images[@]}\"\n\necho \"所有镜像都打包至 swanlab_images.tar，可直接上传该文件到目标服务器!\"\n```\n\n:::\n\n###  2. 上传镜像到目标服务器\n\n可以使用 [sftp](https://www.ssh.com/academy/ssh/sftp-ssh-file-transfer-protocol) 等命令。例如：\n\n- 首先连接到服务器\n\n```bash\n$ sftp username@remote_host\n```\n\n- 上传文件\n\n```sftp\n> put swanlab_images.tar swanlab_images.tar\n```\n\n> [!TIP]\n>\n> 借助 [Termius](https://termius.com/) 这类 SSH 工具可以更方便地向服务器上传下载文件\n\n### 3. 加载镜像\n\n> [!NOTE]  \n>\n> 需求确保服务器上安装有 [Docker](https://docs.docker.com/engine/install/)\n\n将镜像上传到目标服务器之后，需要加载镜像，命令如下：\n\n```bash\n$ docker load -i swanlab_images.tar\n```\n\n等待加载成功后，可以通过命令 `docker images` 查看镜像列表。\n\n```bash\n(base) root@swanlab:~# docker images\nREPOSITORY                                              TAG                            IMAGE ID       CREATED         SIZE\nccr.ccs.tencentyun.com/self-hosted/swanlab-server       v1.1.1                         a2b992161a68   8 days ago      1.46GB\nccr.ccs.tencentyun.com/self-hosted/swanlab-next         v1.1                           7a33e5b1afc5   3 weeks ago     265MB\nccr.ccs.tencentyun.com/self-hosted/swanlab-cloud        v1.1                           0bc15f138d79   3 weeks ago     53.3MB\nccr.ccs.tencentyun.com/self-hosted/swanlab-house        v1.1                           007b252f5b6c   3 weeks ago     48.5MB\nccr.ccs.tencentyun.com/self-hosted/minio-mc             RELEASE.2025-04-08T15-39-49Z   f33e36a42eec   5 weeks ago     84.1MB\nccr.ccs.tencentyun.com/self-hosted/clickhouse           24.3                           6ffc1e932ef1   2 months ago    942MB\nccr.ccs.tencentyun.com/self-hosted/fluent-bit           3.0                            97e65b999a4d   2 months ago    84.9MB\nccr.ccs.tencentyun.com/self-hosted/traefik              v3.0                           0f62db80c71d   2 months ago    190MB\nccr.ccs.tencentyun.com/self-hosted/minio                RELEASE.2025-02-28T09-55-16Z   377fe6127f60   2 months ago    180MB\nccr.ccs.tencentyun.com/self-hosted/redis-stack-server   7.2.0-v15                      110cc99f3057   3 months ago    520MB\nccr.ccs.tencentyun.com/self-hosted/postgres             16.1                           86414087c100   16 months ago   425MB\nccr.ccs.tencentyun.com/self-hosted/logrotate            v1                             e07b32a4bfda   6 years ago     45.6MB\n```\n\n### 4. 安装 SwanLab 服务\n\n在完成镜像载入之后，需要使用安装脚本完成服务安装并启动。\n\n首先在一台有网络的计算机上，使用 Git 克隆仓库到本地目录：\n\n```bash\n$ git clone https://github.com/SwanHubX/self-hosted.git\n```\n\n然后，将 `self-hosted` 文件夹上传到目标服务器。\n\n---\n\n在目标服务器，进入 `self-hosted` 目录，执行脚本 `./docker/install.sh` 用于安装，安装成功会看到以下标志：\n\n```bash\n$ ./docker/install.sh\n\n...\n   _____                    _           _     \n  / ____|                  | |         | |    \n | (_____      ____ _ _ __ | |     __ _| |__  \n  \\___ \\ \\ /\\ / / _` | '_ \\| |    / _` | '_ \\ \n  ____) \\ V  V / (_| | | | | |___| (_| | |_) |\n |_____/ \\_/\\_/ \\__,_|_| |_|______\\__,_|_.__/ \n                                              \n Self-Hosted Docker v1.1 - @SwanLab\n\n🎉 Wow, the installation is complete. Everything is perfect.\n🥰 Congratulations, self-hosted SwanLab can be accessed using {IP}:8000\n```\n\n> [!TIP]\n>\n> 默认脚本使用的镜像源在中国，所以中国地区不需要担心网络问题\n>\n> 如果你需要使用 [DockerHub](https://hub.docker.com/) 作为镜像源，可以使用下面的脚本进行安装：\n>\n> ```bash\n> $ ./docker/install-dockerhub.sh\n> ```\n\n脚本执行成功后，将会在当前目录下创建一个 `swanlab/` 目录，并在目录下生成两个文件：\n\n- `docker-compose.yaml`：用于 Docker Compose 的配置文件\n- `.env`：对应的密钥文件，保存数据库对应的初始化密码\n\n在 `swanlab` 目录下执行 `docker compose ps -a` 可以查看所有容器的运行状态：\n\n```bash\n$ docker compose ps -a                                                                                                                                             \nNAME                 IMAGE                                                                   COMMAND                  SERVICE          CREATED          STATUS                    PORTS\nswanlab-clickhouse   ccr.ccs.tencentyun.com/self-hosted/clickhouse:24.3                      \"/entrypoint.sh\"         clickhouse       22 minutes ago   Up 22 minutes (healthy)   8123/tcp, 9000/tcp, 9009/tcp\nswanlab-cloud        ccr.ccs.tencentyun.com/self-hosted/swanlab-cloud:v1                     \"/docker-entrypoint.…\"   swanlab-cloud    22 minutes ago   Up 21 minutes             80/tcp\nswanlab-fluentbit    ccr.ccs.tencentyun.com/self-hosted/fluent-bit:3.0                       \"/fluent-bit/bin/flu…\"   fluent-bit       22 minutes ago   Up 22 minutes             2020/tcp\nswanlab-house        ccr.ccs.tencentyun.com/self-hosted/swanlab-house:v1                     \"./app\"                  swanlab-house    22 minutes ago   Up 21 minutes (healthy)   3000/tcp\nswanlab-logrotate    ccr.ccs.tencentyun.com/self-hosted/logrotate:v1                         \"/sbin/tini -- /usr/…\"   logrotate        22 minutes ago   Up 22 minutes             \nswanlab-minio        ccr.ccs.tencentyun.com/self-hosted/minio:RELEASE.2025-02-28T09-55-16Z   \"/usr/bin/docker-ent…\"   minio            22 minutes ago   Up 22 minutes (healthy)   9000/tcp\nswanlab-next         ccr.ccs.tencentyun.com/self-hosted/swanlab-next:v1                      \"docker-entrypoint.s…\"   swanlab-next     22 minutes ago   Up 21 minutes             3000/tcp\nswanlab-postgres     ccr.ccs.tencentyun.com/self-hosted/postgres:16.1                        \"docker-entrypoint.s…\"   postgres         22 minutes ago   Up 22 minutes (healthy)   5432/tcp\nswanlab-redis        ccr.ccs.tencentyun.com/self-hosted/redis-stack-server:7.2.0-v15         \"/entrypoint.sh\"         redis            22 minutes ago   Up 22 minutes (healthy)   6379/tcp\nswanlab-server       ccr.ccs.tencentyun.com/self-hosted/swanlab-server:v1                    \"docker-entrypoint.s…\"   swanlab-server   22 minutes ago   Up 21 minutes (healthy)   3000/tcp\nswanlab-traefik      ccr.ccs.tencentyun.com/self-hosted/traefik:v3.0                         \"/entrypoint.sh trae…\"   traefik          22 minutes ago   Up 22 minutes (healthy)   0.0.0.0:8000->80/tcp, [::]:8000->80/tcp\n```\n\n通过执行 `docker compose logs <container_name>` 可以查看每个容器的日志。\n\n### 5. 访问 SwanLab\n\n安装成功后，可以通过 `http://localhost:8000` （默认端口为8000）直接打开网站。第一次打开需要激活主账户，流程见[文档](https://docs.swanlab.cn/guide_cloud/self_host/docker-deploy.html#_3-%E6%BF%80%E6%B4%BB%E4%B8%BB%E8%B4%A6%E5%8F%B7)。\n\n### 6. 升级 SwanLab\n\n如果你希望升级私有化部署版，那么回到联网的机器上，同步github上最新的`self-hosted`仓库，然后执行升级脚本：\n\n```bash\n$ ./docker/upgrade.sh\n```\n\n将升级后的镜像导出到目标服务器，载入镜像以覆盖之前的镜像。\n\n同时，将新同步的`self-hosted` 文件夹也上传到目标服务器（⚠️注意：不要覆盖存储原先私有化部署数据的文件夹）。\n\n然后在离线机器上，进入`self-hosted`目录，执行`./docker/upgrade.sh`进行升级。\n\n```bash\ncd self-hosted\n./docker/upgrade.sh\n```\n\n脚本运行完成后即完成升级。",
    "491": "一级标题：远程访问离线看板\n二级标题：准备工作\n内容：\n- `记下远程端IP`：比如你使用的是云服务器，那么它自带的公网IP（形如8.141.192.68）就是你之后本机访问实验看板的IP；如果你使用的是局域网服务器，那么则记下它的局域网IP。\n- `开放端口`：首先需要检查一下远程端的安全组/防火墙，比如你希望实验看板所用的端口为`5092`，那么需要检查服务器是否开放了该端口。\n\n> 可使用telnet <服务器IP> <端口号>命令查看linux服务器端口是否开放",
    "492": "一级标题：远程访问离线看板\n二级标题：在远程端设定实验看板的IP与端口\n内容：\n我们需要在远程端（跑训练所在的机器）运行实验看板服务。\n\n在swanlab watch命令中，可设置的参数主要有`-p`和`-h`：\n\n| API         | 描述                                     | 例子                                             |\n|-------------|------------------------------------------|--------------------------------------------------|\n| `-p, --port`| 设置实验看板Web服务运行的端口，默认为5092。 | `swanlab watch -p 8080`：将实验看板Web服务设置为8080端口 |\n| `-h, --host`| 设置实验看板Web服务运行的IP地址，默认为127.0.0.1。 | `swanlab watch -h 0.0.0.0`：将实验看板Web服务的IP地址设置为0.0.0.0 |\n\n\n一般远程访问实验看板需要将`-h`设置为`0.0.0.0`，`-p`的设置则根据你的需求。这里我们将端口设置为`8080`：\n\n```shell\nswanlab watch -h 0.0.0.0 -p 8080\n```\n\n运行上面的命令，得到：\n\n![image](/assets/self-host_im.jpg)",
    "493": "一级标题：远程访问离线看板\n二级标题：本机访问实验看板\n内容：\n这时我们在本机端打开浏览器，访问`远程端IP地址:端口号`。\n\n比如我的远程服务器的公网IP是`8.146.xxx.71`，端口号设置为`8080`，那么在浏览器就访问`8.146.xxx.71:8080`。",
    "494": "一级标题：腾讯云应用部署\n二级标题：关于第三方部署\n内容：\n:::warning 关于第三方部署\n\n第三方部署是由社区贡献的部署方式，官方不保证能实时同步最新版本。\n\n:::\n\n目前 SwanLab 自托管版本已上线腾讯云应用市场，欢迎各位训练师通过腾讯云开箱使用~\n\n![](./tencentcloud-app/head.png)\n\n- [SwanLab 腾讯云应用](https://app.cloud.tencent.com/detail/SPU_BHEEJEJCDD1984)",
    "495": "一级标题：腾讯云应用部署\n二级标题：先决条件\n内容：\n1. 首先需要一个腾讯云账号，并确保账号拥有 **安装云应用的权限**，参考链接：[腾讯云应用购买安装指引](https://cloud.tencent.com/document/product/1689/113848)\n\n2. 在 [腾讯云控制台-私有网络](https://console.cloud.tencent.com/vpc/vpc) 中，创建一个默认的 `VPC`（Vitual Private Cloud， 虚拟私有云），为云应用提供目标网络，\n目前支持的地域如下：\n    - 境内：南京; 北京; 广州; 成都; 上海; 重庆; 成都\n    - 境外：中国香港; 新加坡; 硅谷; 圣保罗; 法兰克福\n\n<img src=\"./tencentcloud-app/setup-vpc.png\" width=\"600\"/>\n\n以`南京`区域为例，CIDR与子网可以按需修改，必填项只有`名称`、`子网名称`与`可用区`\n\n<img src=\"./tencentcloud-app/setup-vpc-option.png\" width=\"600\"/>",
    "496": "一级标题：腾讯云应用部署\n二级标题：安装教程\n内容：\n1. 进入 [SwanLab 腾讯云应用](https://app.cloud.tencent.com/detail/SPU_BHEEJEJCDD1984) 页面，\n勾选 `我已阅读并同意《腾讯云云应用通用商品用户协议》`，并点击 `安装应用`，跳转到控制台界面\n\n<img src=\"./tencentcloud-app/intro.png\" width=\"800\"/>\n\n2. 在控制台界面，只需要配置 `目标网络`、`云服务器类型` 以及 `数据盘大小` 三项云资源设置：\n<img src=\"./tencentcloud-app/resource-option.png\" width=\"800\"/>\n\n各云资源代表的含义如下：\n\n| 配置项 | 说明 | 配置要求 |\n| ---- | ---- | ---- |\n| 目标网络 | 云服务托管地域 | 可以根据之前创建 `VPC` 的地域进行选择 |\n| 云服务器类型 | 云服务器实例配置 | 最低配置：<br>- CPU: ≥ 4 核<br>- 内存：≥ 8GB<br>- 系统存储空间：默认 40GB |\n| 数据盘大小 | 记录实验数据的硬盘大小 | 默认为 `100GB`，最低 `40GB` |\n\n云资源配置完成之后，点击 `下一步：确定资源`\n\n<img src=\"./tencentcloud-app/resource-confirm.png\" width=\"800\"/>\n\n3. 接着进入`确认订单信息`信息界面，腾讯云会根据上一步选用的云资源整理账单费用，此时需要确保腾讯云账号中有一定的余额。确认订单无误后，点击`允许服务角色调用其他云服务接口`，并点击 `下一步：安装应用`\n\n\n<img src=\"./tencentcloud-app/resource-setupapp.png\" width=\"800\"/>\n\n4. 接下来进入应用安装界面，需要等待所有资源创建并启动，需要等待 5 分钟左右\n\n<img src=\"./tencentcloud-app/app-setup.png\" width=\"600\"/>\n\n5. 完成之后，即可在腾讯云控制台界面看到已创建完成的云应用，点击 `打开应用`，即可使用自托管版的SwanLab\n\n<img src=\"./tencentcloud-app/open-app.png\" width=\"800\"/>\n\n:::info 提示\n\n在应用创建完成后，如果立即打开应用，可能会看到 404 页面，这是因为云服务器实例创建后需要执行一些容器初始化操作，稍等 1~2 分钟再打开即可。\n\n:::",
    "497": "一级标题：腾讯云应用部署\n二级标题：激活主账号\n内容：\n现在，你可以在腾讯云上使用自托管版本的 SwanLab\n\n<img src=\"./tencentcloud-app/swanlab-hello.png\" width=\"600\"/>\n\n个人使用可以免费在 [SwanLab官网](https://swanlab.cn) 申请一个License，位置在 「设置」-「账户与许可证」。\n\n<img src=\"./tencentcloud-app/swanlab-license-1.png\" width=\"600\"/>\n\n<img src=\"./tencentcloud-app/swanlab-license-2.png\" width=\"600\"/>\n\n输入账号、密码、License 后即可激活自托管版的 SwanLab\n\n<img src=\"./tencentcloud-app/swanlab-main.png\" width=\"600\"/>",
    "498": "一级标题：腾讯云应用部署\n二级标题：启动实验\n内容：\n在Python SDK完成登录：\n\n```bash\nswanlab login --host <IP地址>\n```\n\n> 如果你之前登录过swanlab，想要重新登录，请使用：  \n> `swanlab login --host <IP地址> --relogin`。\n\n按回车，填写API Key，完成登录。之后你的SwanLab实验将会默认传到私有化部署的SwanLab上。\n\n---\n\n测试脚本：\n\n```bash\nimport swanlab\nimport random\n\n# 创建一个SwanLab项目\nswanlab.init(\n    # 设置项目名\n    project=\"my-awesome-project\",\n    \n    # 设置超参数\n    config={\n        \"learning_rate\": 0.02,\n        \"architecture\": \"CNN\",\n        \"dataset\": \"CIFAR-100\",\n        \"epochs\": 10\n    }\n)\n\n# 模拟一次训练\nepochs = 10\noffset = random.random() / 5\nfor epoch in range(2, epochs):\n  acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n  loss = 2 ** -epoch + random.random() / epoch + offset\n\n  # 记录训练指标\n  swanlab.log({\"acc\": acc, \"loss\": loss})\n\n# [可选] 完成训练，这在notebook环境中是必要的\nswanlab.finish()\n```\n\n运行后，可在网页查看实验\n\n<img src=\"./tencentcloud-app/swanlab-dashboard.png\" width=\"600\"/>\n\n:::info 提示\n\n如您不再需要使用，请及时在 [腾讯云应用控制台](https://console.cloud.tencent.com/app) 中销毁应用，避免继续计费。\n\n:::",
    "499": "一级标题：版本对照表\n二级标题：版本对照表\n内容：\n| 版本 | 发布时间 | 主要更新 | 兼容的Python包版本 |\n| --- | --- | --- | --- |\n| v1.2 | 25-05-30 | 同步发布时间的云端版更新 |  <=0.6.4 |\n| v1.1 | 25-04-27 | 新增License离线验证功能；<br> 同步发布时间的云端版更新 |  <=0.6.4 |\n| v1.0 | 25-03-12 | 初始版本 |  <=0.5.5 |\n\n[升级版本](/guide_cloud/self_host/docker-deploy.html#升级版本)",
    "500": "一级标题：版本对照表\n二级标题：v1.2 (2025.5.30)\n内容：\n- **Feature**: 上线折线图创建和编辑功能，配置图表功能增加数据源选择功能，支持单张图表显示不同的指标\n- **Feature**: 支持在实验添加Tag标签\n- **Feature**: 支持折线图Log Scale；支持分组拖拽；增加swanlab.OpenApi开放接口\n- **Feature**: 新增「默认空间」和「默认可见性」配置，用于指定项目默认创建在对应的组织下\n- **Optimize**: 优化大量指标上传导致部分数据丢失的问题\n- **Optimize**: 大幅优化指标上传的性能问题\n- **BugFix**: 修复实验无法自动关闭的问题\n```\n\n根据提供的Markdown内容，以下是按主题分块整理的结果：",
    "501": "一级标题：制作你的自定义插件\n二级标题：插件介绍\n内容：\n很开心，在`swanlab>=0.5.0`之后，我们正式开启了插件时代！\n\n插件是SwanLab诞生之初我们便一直探讨的话题，这不仅是增强SwanLab的功能与开放性，更是一种全新的视角来看待SwanLab ——\n\nSwanLab不只是1个训练跟踪工具与实验管理平台，同时可以是一个训练过程中的**数据核心**（比如Chrome core），`swanlab.init`与`swanlab.log`被赋予不同的意义。\n\n---\n\n我们将SwanLab的插件模式定义为三种类型：\n\n- **`Python库插件`**：SwanLab Python库中的回调类（Callback）。通过往SwanLab的生命周期阶段（比如`on_init`、`on_run`、`on_stop`等）注入代码的方式，来实现插件功能。\n- **`开放API插件`**：基于SwanLab平台提供的开放API，通过调用API进行组合的方式，来实现插件功能。\n- **`GUI插件`**：基于SwanLab平台开放的前端API，实现对图表、表格等组件的定制化。\n\n::: warning 👋 支持情况\n目前我们支持的插件类型为`Python库插件`，下面我将重点介绍如何制作你的`Python库插件`。\n:::",
    "502": "一级标题：制作你的自定义插件\n二级标题：认识SwanKitCallback类\n内容：\n> 仓库：[swanlab-toolkit](https://github.com/swanhubx/swanlab-toolkit)\n\n`SwanKitCallback`类是SwanLab的回调类，所有插件都必须继承自该类。\n\n```python\nfrom swankit.callback import SwanKitCallback\n```\n\n`SwanKitCallback`类中定义了所有SwanLab的生命周期阶段，你只需要重写你感兴趣的生命周期阶段即可：\n\n常用的生命周期阶段有：\n\n- `on_init`：初始化阶段，执行`swanlab.init`时调用\n- `before_init_experiment`：在初始化`SwanLabRun`之前调用\n- `on_run`：当`SwanLabRun`初始化完毕时调用\n- `on_log`：每次执行`swanlab.log`时调用\n- `on_stop`：停止阶段，当SwanLab停止时调用\n\n更多的生命周期阶段，请参考：[SwanKitCallback](https://github.com/SwanHubX/SwanLab-Toolkit/blob/main/swankit/callback/__init__.py)",
    "503": "一级标题：制作你的自定义插件\n二级标题：实现一个简单的插件\n内容：\n下面以1个案例为例，介绍如何实现一个插件。\n\n```python\nclass MyPlugin(SwanKitCallback):\n    def on_init(self, proj_name: str, workspace: str, logdir: str = None, *args, **kwargs):\n        print(f\"插件初始化: {proj_name} {workspace} {logdir}\")\n\n    def on_stop(self, error: str = None, *args, **kwargs):\n        print(f\"插件停止: {error}\")\n\n    def __str__(self):\n        return \"MyPlugin\"\n```\n\n这个插件实现的功能非常简单，就是在`swanlab.init()`调用时打印1条消息，在进程停止或`swanlab.finish()`调用时打印1条消息。\n\n而在SwanLab中使用着这个插件非常简单，只需要在`swanlab.init()`的`callbacks`参数中传入插件实例即可。\n\n```python {14,16}\nfrom swankit.callback import SwanKitCallback\nimport swanlab\n\nclass MyPlugin(SwanKitCallback):\n    def on_init(self, proj_name: str, workspace: str, logdir: str = None, *args, **kwargs):\n        print(f\"插件初始化: {proj_name} {workspace} {logdir}\")\n\n    def on_stop(self, error: str = None, *args, **kwargs):\n        print(f\"插件停止: {error}\")\n\n    def __str__(self):\n        return \"MyPlugin\"\n\nmy_plugin = MyPlugin()\n\nswanlab.init(callbacks=[my_plugin])\n```\n\n执行上述代码，你会在控制台看到\n\n![image](./custom-plugin/print.png)",
    "504": "一级标题：制作你的自定义插件\n二级标题：案例：指标打印与告警\n内容：\n我们来实现一个插件，这个插件的功能是打印指标，并当指标`acc`大于0.9时，打印1条消息，并发送告警。\n\n### 1. 定义插件\n\n> 在`SwanKitCallback`类中，定义了`on_log`方法，每次执行`swanlab.log`时都会调用该方法。\n\n```python\nclass ThresholdPlugin(SwanKitCallback):\n    def __init__(self, key: str, threshold: float = 0.9):\n        self.key = key\n        self.threshold = threshold\n\n    def on_log(self, data: dict, step: Optional[int] = None, *args, **kwargs):\n        print(f\"data: {data} step: {step}\")\n        if data[self.key] > self.threshold:\n            print(f\"{self.key} > {self.threshold} !!\")\n```\n\n### 2. 使用插件\n\n```python\nfrom swankit.callback import SwanKitCallback\nfrom typing import Optional\nimport swanlab\nimport random\n\nclass ThresholdPlugin(SwanKitCallback):\n    def __init__(self, key: str, threshold: float = 0.9):\n        self.key = key\n        self.threshold = threshold\n\n    def on_log(self, data: dict, step: Optional[int] = None, *args, **kwargs):\n        print(f\"data: {data} step: {step}\")\n        if data[self.key] > self.threshold:\n            print(f\"{self.key} > {self.threshold} !!\")\n\n    def __str__(self):\n        return \"ThresholdPlugin\"\n\nthreshold_plugin = ThresholdPlugin(key=\"acc\", threshold=0.9)\nswanlab.init(callbacks=[threshold_plugin])\n\n# 模拟一次训练\nepochs = 10\noffset = random.random() / 5\nfor epoch in range(2, epochs):\n  acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n  loss = 2 ** -epoch + random.random() / epoch + offset\n\n  # 记录训练指标\n  swanlab.log({\"acc\": acc, \"loss\": loss}, step=epoch)\n```\n\n执行上述代码，你会在控制台看到\n\n![image](./custom-plugin/threshold.png)",
    "505": "一级标题：制作你的自定义插件\n二级标题：学习更多插件\n内容：\n- [EmailCallback](/zh/plugin/notification-email.md)：训练完成/发生错误时，发送消息到邮箱\n- [LarkCallback](/zh/plugin/notification-lark.md)：训练完成/发生错误时，发送消息到飞书",
    "506": "一级标题：钉钉\n二级标题：准备工作\n内容：\n准备工作\n\n1. 在1个钉钉群（企业群）中，点击右上角的 **「设置」按钮**\n\n<img src=\"./notification-dingtalk/setting.png\" width=\"400\"/>\n\n2. 向下滚动，找到 **「机器人」**\n\n<img src=\"./notification-dingtalk/group-robot.png\" width=\"400\"/>\n\n3. 点击 **「添加机器人」**\n\n<img src=\"./notification-dingtalk/add-robot.png\" width=\"400\"/>\n\n4. 添加 **「自定义机器人」**\n\n<img src=\"./notification-dingtalk/custom-robot.png\" width=\"600\"/>\n\n<img src=\"./notification-dingtalk/add-robot-2.png\" width=\"600\"/>\n\n勾选「加签」，复制token到外部。\n\n<img src=\"./notification-dingtalk/add-robot-3.png\" width=\"600\"/>\n\n复制webhook，完成机器人创建：\n\n<img src=\"./notification-dingtalk/add-robot-4.png\" width=\"600\"/>\n\n至此，你完成了准备工作。",
    "507": "一级标题：钉钉\n二级标题：基本用法\n内容：\n基本用法\n\n使用钉钉通知插件的方法非常简单，只需要初始化1个`DingTalkCallback`对象：\n\n```python\nfrom swanlab.plugin.notification import DingTalkCallback\n\ndingtalk_callback = DingTalkCallback(\n    webhook_url=\"https://oapi.dingtalk.com/robot/xxxx\", \n    secret=\"xxxx\",\n)\n```\n\n然后将`dingtalk_callback`对象传入`swanlab.init`的`callbacks`参数中：\n\n```python\nswanlab.init(callbacks=[dingtalk_callback])\n```\n\n这样，当训练完成/发生错误时（触发`swanlab.finish()`），你将会收到钉钉信息通知。\n\n<img src=\"./notification-dingtalk/show.png\" width=\"600\"/>",
    "508": "一级标题：钉钉\n二级标题：自由提醒\n内容：\n自由提醒\n\n你还可以使用`DingTalkCallback`对象的`send_msg`方法，发送自定义的钉钉信息。\n\n这在提醒你某些指标达到某个阈值时非常有用！\n\n```python \nif accuracy > 0.95:\n    # 自定义场景发送消息\n    dingtalk_callback.send_msg(\n        content=f\"Current Accuracy: {accuracy}\",  # 通知内容\n    )\n```",
    "509": "一级标题：钉钉\n二级标题：外部注册插件\n内容：\n外部注册插件\n\n<!--@include: ./shared-snippet.md-->",
    "510": "一级标题：钉钉\n二级标题：限制\n内容：\n限制\n\n- 钉钉通知插件的训练完成/异常通知，使用的是`SwanKitCallback`的`on_stop`生命周期回调，所以如果你的进程被突然`kill`，或者训练机异常关机，那么会因为无法触发`on_stop`回调，从而导致未发送钉钉通知。\n\n- 完善方案请期待`SwanLab`的`平台开放API`上线。",
    "511": "一级标题：Discord\n二级标题：准备工作\n内容：\n::: info 参考文档\n- [Discord-Webhook群机器人配置说明](https://support.discord.com/hc/en-us/articles/228383668-Intro-to-Webhooks)\n:::\n\n\n1. 选择您想要接收SwanLab事件通知的 Discord 频道\n\n\n2. 点击对应频道右侧的 **「⚙️」** 对应的 **「编辑频道」** 按钮\n\n<img src=\"./notification-discord/edit-channel.png\" width=\"400\"/>\n\n3. 展开菜单后，选择 **「整合」 -> 「Webhhook」**\n\n<img src=\"./notification-discord/integration-webhook.png\" width=\"400\"/>\n\n\n4. 点击选项卡 **「新Webhook」** 自动创建新的 webhook 机器人\n\n<img src=\"./notification-discord/new-webhook.png\" width=\"400\"/>\n\n5. 点击 **「复制 Webhook URL」** 即可获取到对应的 webhook 地址",
    "512": "一级标题：Discord\n二级标题：基本用法\n内容：\n使用Discord通知插件的方法非常简单，只需要初始化1个`DiscordCallback`对象：\n\n```python\nfrom swanlab.plugin.notification import DiscordCallback\n\ndiscord_callback = DiscordCallback(\n    webhook_url='https://discord.com/api/webhooks/xxxxx/xxx', \n    language='zh'\n)\n```\n\n然后将`discord_callback`对象传入`swanlab.init`的`callbacks`参数中：\n\n```python\nswanlab.init(callbacks=[discord_callback])\n```\n\n这样，当训练完成/发生错误时（触发`swanlab.finish()`），你将会收到Discord消息通知。\n\n\n\n<img src=\"./notification-discord/discord-finish.png\" width=\"500\"/>",
    "513": "一级标题：Discord\n二级标题：自由提醒\n内容：\n你还可以使用`DiscordCallback`对象的`send_msg`方法，发送自定义的的Discord消息。\n\n这在提醒你某些指标达到某个阈值时非常有用！\n\n```python \nif accuracy > 0.95:\n    # 自定义场景发送消息\n    discord_callback.send_msg(\n        content=f\"Current Accuracy: {accuracy}\",  # 通知内容\n    )\n```",
    "514": "一级标题：Discord\n二级标题：限制\n内容：\n- Discord通知插件的训练完成/异常通知，使用的是`SwanKitCallback`的`on_stop`生命周期回调，所以如果你的进程被突然`kill`，或者训练机异常关机，那么会因为无法触发`on_stop`回调，从而导致未发送Discord通知。\n\n- 完善方案请期待`SwanLab`的`平台开放API`上线。",
    "515": "一级标题：邮件通知\n二级标题：准备工作\n内容：\n在使用插件前，首先你需要准备开通你的邮箱的**STMP服务**。以QQ邮箱为例：\n\n**步骤 1：进入邮箱设置**\n\n- 进入QQ邮箱网页，点击顶部的 ​​“设置”​ \n- 在设置菜单中，选择 ​​“账号”​ 选项。\n\n**​步骤 2：开启SMTP服务**\n\n- 找到 **“POP3/IMAP/SMTP/Exchange/CardDAV/CalDAV服务”**\n- 在“服务状态”旁边，点击 **“开启服务”**\n- 经过一些身份验证流程后，完成**STMP服务的开启**\n- （重要）保存给到你的**授权码**\n\n**​步骤 3：记录以下信息**\n- **SMTP服务器地址**: smtp.qq.com\n- **端口**: 465（SSL加密）或 587（TLS加密）\n- **发送邮箱**: 你的完整QQ邮箱地址（如 123456789@qq.com）\n- **密码**: 使用你刚刚获取的 ​授权码，而不是QQ邮箱的登录密码。\n\n其他的邮箱服务基本都支持STMP，可按照相似的流程开启服务。",
    "516": "一级标题：邮件通知\n二级标题：基本用法\n内容：\n使用邮件通知插件的方法非常简单，只需要初始化1个`EmailCallback`对象：\n\n```python\nfrom swanlab.plugin.notification import EmailCallback\n\n# 初始化邮件通知插件\nemail_callback = EmailCallback(\n    sender_email=\"<发送者邮箱，即开启SMTP服务的邮箱>\",\n    receiver_email=\"<接收者邮箱，即你想要收到邮件的邮件>\",\n    password=\"<你的授权码>\",\n    smtp_server=\"<你的邮箱服务器>\",\n    port=587,\n    language=\"zh\",\n)\n```\n\n然后将`email_callback`对象传入`swanlab.init`的`callbacks`参数中：\n\n```python\nswanlab.init(callbacks=[email_callback])\n```\n\n这样，当训练完成/发生错误时（触发`swanlab.finish()`），你将会收到邮件通知。\n![image](./notification-email/email.png)",
    "517": "一级标题：邮件通知\n二级标题：自由提醒\n内容：\n你还可以使用`EmailCallback`对象的`send_email`方法，发送自定义的邮件。\n\n这在提醒你某些指标达到某个阈值时非常有用！\n\n```python \nif accuracy > 0.95:\n    # 自定义场景发送邮件\n    email_callback.send_email(\n        subject=\"SwanLab | Accuracy > 0.95\",  # 邮件标题\n        content=f\"Current Accuracy: {accuracy}\",  # 邮件内容\n    )\n```",
    "518": "一级标题：邮件通知\n二级标题：外部注册插件\n内容：\n<!--@include: ./shared-snippet.md-->",
    "519": "一级标题：邮件通知\n二级标题：限制\n内容：\n- 邮件通知插件的训练完成/异常通知，使用的是`SwanKitCallback`的`on_stop`生命周期回调，所以如果你的进程被突然`kill`，或者训练机异常关机，那么会因为无法触发`on_stop`回调，从而导致未发送邮件通知。\n\n- 完善方案请期待`SwanLab`的`平台开放API`上线。",
    "520": "一级标题：飞书通知\n二级标题：准备工作\n内容：\n::: info 参考文档\n- [自定义机器人API使用指南](https://open.feishu.cn/document/client-docs/bot-v3/add-custom-bot?lang=zh-CN#f62e72d5)·\n- [在飞书群组中使用机器人](https://www.feishu.cn/hc/zh-CN/articles/360024984973-%E5%9C%A8%E7%BE%A4%E7%BB%84%E4%B8%AD%E4%BD%BF%E7%94%A8%E6%9C%BA%E5%99%A8%E4%BA%BA)\n:::\n\n\n1. 在1个飞书群中，点击右上角的 **「···」-「设置」**\n\n<img src=\"./notification-lark/setting.png\" width=\"400\"/>\n\n2. 点击 **「群机器人」**\n\n<img src=\"./notification-lark/group-robot.png\" width=\"400\"/>\n\n3. 点击 **「添加机器人」**\n\n<img src=\"./notification-lark/add-robot.png\" width=\"400\"/>\n\n4. 添加 **「自定义机器人」**\n\n<img src=\"./notification-lark/custom-robot.png\" width=\"600\"/>\n\n<img src=\"./notification-lark/custom-robot-detail.png\" width=\"600\"/>\n\n5. 复制 **「Webhook 地址」和 「签名」**\n\n<img src=\"./notification-lark/webhookurl.png\" width=\"600\"/>\n\n至此，你完成了准备工作。",
    "521": "一级标题：飞书通知\n二级标题：基本用法\n内容：\n使用飞书通知插件的方法非常简单，只需要初始化1个`LarkCallback`对象：\n\n```python\nfrom swanlab.plugin.notification import LarkCallback\n\nlark_callback = LarkCallback(\n    webhook_url=\"https://open.larkoffice.com/open-apis/bot/v2/hook/xxxx\", \n    secret=\"xxxx\",\n)\n```\n\n然后将`lark_callback`对象传入`swanlab.init`的`callbacks`参数中：\n\n```python\nswanlab.init(callbacks=[lark_callback])\n```\n\n这样，当训练完成/发生错误时（触发`swanlab.finish()`），你将会收到飞书信息通知。\n\n<img src=\"./notification-lark/show.png\" width=\"600\"/>",
    "522": "一级标题：飞书通知\n二级标题：自由提醒\n内容：\n你还可以使用`LarkCallback`对象的`send_msg`方法，发送自定义的飞书信息。\n\n这在提醒你某些指标达到某个阈值时非常有用！\n\n```python \nif accuracy > 0.95:\n    # 自定义场景发送消息\n    lark_callback.send_msg(\n        content=f\"Current Accuracy: {accuracy}\",  # 通知内容\n    )\n```",
    "523": "一级标题：飞书通知\n二级标题：外部注册插件\n内容：\n<!--@include: ./shared-snippet.md-->",
    "524": "一级标题：飞书通知\n二级标题：限制\n内容：\n- 飞书通知插件的训练完成/异常通知，使用的是`SwanKitCallback`的`on_stop`生命周期回调，所以如果你的进程被突然`kill`，或者训练机异常关机，那么会因为无法触发`on_stop`回调，从而导致未发送飞书通知。\n\n- 完善方案请期待`SwanLab`的`平台开放API`上线。",
    "525": "一级标题：Slack\n二级标题：准备工作\n内容：\n::: info 参考文档\n- [Slack-使用传入的webhooks发送消息](https://api.slack.com/messaging/webhooks)\n- [腾讯云-Slack群接收消息](https://cloud.tencent.com/document/product/1263/74219)\n:::\n\n\n1. 前往 [Slack-API](https://api.slack.com/apps) 页面，点击 **「Create an App」**\n\n<img src=\"./notification-slack/slack-create-app.png\" width=\"400\"/>\n\n\n2. 在弹窗中点击 **「From scratch」**\n\n<img src=\"./notification-slack/from-scratch.png\" width=\"400\"/>\n\n3. 填写 **「App Name」** ，并选择用于通知的 workspace，点击右下角的 **「Create App」**\n\n<img src=\"./notification-slack/name-app.png\" width=\"400\"/>\n\n4. 进入 App 配置菜单后，点击左侧的 **「Incoming Webhooks」**，并开启 **「Activate Incoming Webhooks」** 按钮；\n\n<img src=\"./notification-slack/slack-webhook-option.png\" width=\"400\"/>\n\n5. 在页面下方，点击 **「Add New Webhook to Workspace」**，将APP添加到工作区的频道中；\n\n\n<img src=\"./notification-slack/add-new-webhook-workspace.png\" width=\"400\"/>\n\n6. 在跳转的应用请求页面中，选择好APP要发送消息的频道，点击 **「允许」**\n\n<img src=\"./notification-slack/allow-channel.png\" width=\"400\"/>\n\n7.最后返回 APP 配置页面，复制APP的 Webhook URL\n\n<img src=\"./notification-slack/copy-url.png\" width=\"500\"/>",
    "526": "一级标题：Slack\n二级标题：基本用法\n内容：\n使用Slack通知插件的方法非常简单，只需要初始化1个`SlackCallback`对象：\n\n```python\nfrom swanlab.plugin.notification import SlackCallback\n\nslack_callback = SlackCallback(\n    webhook_url='https://hooks.slack.com/services/xxxx/xxxx/xxxx', \n    language='zh'\n)\n```\n\n然后将`slack_callback`对象传入`swanlab.init`的`callbacks`参数中：\n\n```python\nswanlab.init(callbacks=[slack_callback])\n```\n\n这样，当训练完成/发生错误时（触发`swanlab.finish()`），你将会收到Slack消息通知。\n\n\n<img src=\"./notification-slack/slack-finish.png\" width=\"500\"/>",
    "527": "一级标题：Slack\n二级标题：自由提醒\n内容：\n你还可以使用`SlackCallback`对象的`send_msg`方法，发送自定义的的Slack消息。\n\n这在提醒你某些指标达到某个阈值时非常有用！\n\n```python \nif accuracy > 0.95:\n    # 自定义场景发送消息\n    slack_callback.send_msg(\n        content=f\"Current Accuracy: {accuracy}\",  # 通知内容\n    )\n```",
    "528": "一级标题：Slack\n二级标题：限制\n内容：\n- Slack通知插件的训练完成/异常通知，使用的是`SwanKitCallback`的`on_stop`生命周期回调，所以如果你的进程被突然`kill`，或者训练机异常关机，那么会因为无法触发`on_stop`回调，从而导致未发送Slack通知。\n\n- 完善方案请期待`SwanLab`的`平台开放API`上线。",
    "529": "一级标题：企业微信\n二级标题：准备工作\n内容：\n::: info 参考文档\n- [企业微信-群机器人配置说明](https://developer.work.weixin.qq.com/document/path/91770)\n:::\n1. 在企业微信群中，点击右上角的 **「···」-「添加群机器人」**\n\n<img src=\"./notification-wxwork/wxwork-setting.png\" width=\"400\"/>\n\n2. 在弹出的对话框中点击 **「添加机器人」**\n\n<img src=\"./notification-wxwork/wxwork-addrobot.png\" width=\"400\"/>\n\n3. 继续点击  **「新创建一个机器人」**\n\n<img src=\"./notification-wxwork/wxwork-createnewrobot.png\" width=\"400\"/>\n\n4. 为机器人添加名称，点击 **「添加机器人」**\n\n<img src=\"./notification-wxwork/wxwork-name.png\" width=\"400\"/>\n\n5. 企业微信的机器人只需要复制 **「Webhook地址」** 即可\n\n<img src=\"./notification-wxwork/wxwork-webhook.png\" width=\"400\"/>",
    "530": "一级标题：企业微信\n二级标题：基本用法\n内容：\n使用企业微信通知插件的方法非常简单，只需要初始化1个`WXWorkCallback`对象：\n\n```python\nfrom swanlab.plugin.notification import WXWorkCallback\n\nwxwork_callback = WXWorkCallback(\n    webhook_url=\"https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key=xxxx\",\n)\n```\n\n然后将`wxwork_callback`对象传入`swanlab.init`的`callbacks`参数中：\n\n```python\nswanlab.init(callbacks=[wxwork_callback])\n```\n\n这样，当训练完成/发生错误时（触发`swanlab.finish()`），你将会收到企业微信消息通知。\n\n<img src=\"./notification-wxwork/wxwork-show.png\" width=\"500\"/>",
    "531": "一级标题：企业微信\n二级标题：自由提醒\n内容：\n你还可以使用`WXWorkCallback`对象的`send_msg`方法，发送自定义的的企业微信消息。\n\n这在提醒你某些指标达到某个阈值时非常有用！\n\n```python \nif accuracy > 0.95:\n    # 自定义场景发送消息\n    wxwork_callback.send_msg(\n        content=f\"Current Accuracy: {accuracy}\",  # 通知内容\n    )\n```",
    "532": "一级标题：企业微信\n二级标题：外部注册插件\n内容：\n<!--@include: ./shared-snippet.md-->",
    "533": "一级标题：企业微信\n二级标题：限制\n内容：\n- 企业微信通知插件的训练完成/异常通知，使用的是`SwanKitCallback`的`on_stop`生命周期回调，所以如果你的进程被突然`kill`，或者训练机异常关机，那么会因为无法触发`on_stop`回调，从而导致未发送企业微信通知。\n\n- 完善方案请期待`SwanLab`的`平台开放API`上线。\n\n根据提供的Markdown内容，按照主题分块整理如下：",
    "534": "一级标题：插件一览\n二级标题：通知类\n内容：\n- [邮件](notification-email)\n- [飞书](notification-lark)\n- [钉钉](notification-dingtalk)\n- [企业微信](notification-wxwork)\n- [Discord](notification-discord)\n- [Slack](notification-slack)",
    "535": "一级标题：插件一览\n二级标题：记录类\n内容：\n- [文件记录器](writer-filelogdir)\n- [CSV表格](writer-csv)",
    "536": "一级标题：SwanLab与其他框架的集成\n二级标题：使用`swanlab.register_callbacks`方法\n内容：\n如果你使用的是SwanLab与其他框架的集成，故而不太好找到`swanlab.init`，那么你可以使用`swanlab.register_callbacks`方法，在外部传入插件：\n\n```python\nimport swanlab\n\n# 等价于 swanlab.init(callbaks=[...])\nswanlab.register_callbacks([...])\n```",
    "537": "一级标题：CSV表格记录器\n二级标题：插件用法\n内容：\n**1. 初始化CSV记录器：**\n\n```python\nfrom swanlab.plugin.writer import CSVWriter\n\ncsv_writer = CSVWriter(dir=\"logs\")\n```\n\n`dir`参数指定了CSV文件的保存路径，默认保存到当前工作目录。\n\n**2. 传入插件：**\n\n```python\nswanlab.init(\n    ...\n    callbacks=[csv_writer]\n)\n```\n\n执行代码后，就会在`logs`目录下生成一个`swanlab_run.csv`文件，并开始记录数据。后续的每一次训练，都会在该csv文件中添加新的行。\n\n如果想要指定其他文件名，可以传入`filename`参数：\n\n```python\ncsv_writer = CSVWriter(dir=\"logs\", filename=\"my_csv_file.csv\")\n```",
    "538": "一级标题：CSV表格记录器\n二级标题：示例代码\n内容：\n```python\nimport swanlab\nfrom swanlab.plugin.writer import CSVWriter\nimport random\n\ncsv_writer = CSVWriter(dir=\"logs\")\n\n# 创建一个SwanLab项目\nswanlab.init(\n    # 设置项目名\n    project=\"my-awesome-project\",\n    \n    # 设置超参数\n    config={\n        \"learning_rate\": 0.02,\n        \"architecture\": \"CNN\",\n        \"dataset\": \"CIFAR-100\",\n        \"epochs\": 10,\n        \"batch_size\": 128\n    },\n    callbacks=[csv_writer]\n)\n\n# 模拟一次训练\nepochs = 10\noffset = random.random() / 5\nfor epoch in range(2, epochs):\n  acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n  loss = 2 ** -epoch + random.random() / epoch + offset\n\n  # 记录训练指标\n  swanlab.log({\"acc\": acc, \"loss2\": loss})\n\n# [可选] 完成训练，这在notebook环境中是必要的\nswanlab.finish()\n```",
    "539": "一级标题：CSV表格记录器\n二级标题：外部注册插件\n内容：\n<!--@include: ./shared-snippet.md-->",
    "540": "一级标题：CSV表格记录器\n二级标题：改进插件\n内容：\n:::warning 改进插件\nSwanLab插件均为开源代码，你可以在[Github源代码](https://github.com/SwanHubX/SwanLab/blob/main/swanlab/plugin/writer.py)中查看，欢迎提交你的建议和PR！\n:::",
    "541": "一级标题：文件记录器\n二级标题：插件用法\n内容：\n**1. 初始化LogdirFileWriter：**\n\n```python\nfrom swanlab.plugin.writer import LogdirFileWriter\n\nlogdirfile_writer = LogdirFileWriter(\n    sub_dir=\"code\",\n    files=[\n        \"config.yaml\",\n        \"README.md\",\n    ]\n)\n```\n\n- `sub_dir`参数如果不为None，则在run目录下创建1个sub_dir文件夹来保存文件\n- `files`参数指定了需要复制的文件列表（也支持仅传入1个str）。\n\n**2. 传入插件：**\n\n```python\nswanlab.init(\n    ...\n    callbacks=[logdirfile_writer]\n)\n```\n\n执行代码后，就会在`logdir`下对应的run开头目录下将`files`参数中的文件复制到该目录中（如果设置了`sub_dir`参数，则会复制到该子目录下）。",
    "542": "一级标题：文件记录器\n二级标题：示例代码\n内容：\n```python\nfrom swanlab.plugin.writer import LogdirFileWriter\nimport swanlab\n\nlogdirfile_writer = LogdirFileWriter(\n    sub_dir=\"code\",\n    file_path=[\"package.json\", \"README.md\"],\n)\n\nswanlab.init(project=\"test-plugin\", callbacks=[logdirfile_writer])\n\nswanlab.log({\"loss\": 0.2, \"acc\": 0.9})\nswanlab.finish()\n```\n\n![](./writer-filelogdir/paste.png)",
    "543": "一级标题：文件记录器\n二级标题：外部注册插件\n内容：\n<!--@include: ./shared-snippet.md-->"
}